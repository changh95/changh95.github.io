<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"changh95.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.0.2","exturl":true,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"disqus","active":false,"storage":false,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":"auto","trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="2일차 포스터">
<meta property="og:type" content="article">
<meta property="og:title" content="ICRA 2023 - 포스터 (Day 2)">
<meta property="og:url" content="https://changh95.github.io/20230530-icra2023-day2-poster/index.html">
<meta property="og:site_name" content="cv-learn">
<meta property="og:description" content="2일차 포스터">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/1.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/2.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/3.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/4.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/5.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/6.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/7.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/8.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/9.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/10.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/11.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/12.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/13.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/14.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/15.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/16.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/17.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/18.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/19.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/20.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/21.png">
<meta property="og:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/22.png">
<meta property="article:published_time" content="2023-05-30T05:48:45.000Z">
<meta property="article:modified_time" content="2023-06-15T08:51:06.776Z">
<meta property="article:author" content="cv-learn">
<meta property="article:tag" content="SLAM">
<meta property="article:tag" content="ICRA">
<meta property="article:tag" content="Robotics">
<meta property="article:tag" content="Spatial AI">
<meta property="article:tag" content="Posters">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://changh95.github.io/20230530-icra2023-day2-poster/1.png">


<link rel="canonical" href="https://changh95.github.io/20230530-icra2023-day2-poster/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>ICRA 2023 - 포스터 (Day 2) | cv-learn</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">cv-learn</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Vision, SLAM, Spatial AI</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Loosely-Coupled-Localization-Fusion-System-Based-on-Track-To-Track-Fusion-with-Bias-Alignment"><span class="nav-number">1.</span> <span class="nav-text">Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Mapping-with-only-the-Ground"><span class="nav-number">2.</span> <span class="nav-text">Mapping with only the Ground</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Structure-PLP-SLAM-Efficient-Sparse-Mapping-and-Localization-using-Point-Line-and-Plane-for-Monocular-RGB-D-and-Stereo-Cameras"><span class="nav-number">3.</span> <span class="nav-text">Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-based-SLAM-utilizing-unambigous-pose-parameters-considering-general-symmetry-types"><span class="nav-number">4.</span> <span class="nav-text">Object-based SLAM utilizing unambigous pose parameters considering general symmetry types</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Towards-View-invariant-and-Accurate-Loop-Detection-Based-on-Scene-Graph"><span class="nav-number">5.</span> <span class="nav-text">Towards View-invariant and Accurate Loop Detection Based on Scene Graph</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Zero-shot-Active-Visual-Search-ZAVIS-Intelligent-Object-Search-for-Robotic-Assistants"><span class="nav-number">6.</span> <span class="nav-text">Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#COVINS-G-A-Generic-Back-end-for-Collaborative-Visual-Inertial-SLAM"><span class="nav-number">7.</span> <span class="nav-text">COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-Probabilistic-Framework-for-Visual-Localization-in-Ambiguous-Scenes"><span class="nav-number">8.</span> <span class="nav-text">A Probabilistic Framework for Visual Localization in Ambiguous Scenes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DytanVO-Joint-Refinement-of-Visual-Odometry-and-Motion-Segmentation-in-Dynamic-Environments"><span class="nav-number">9.</span> <span class="nav-text">DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NoCal-Calibration-Free-Semi-Supervised-Learning-of-Odometry-and-Camera-Intrinsics"><span class="nav-number">10.</span> <span class="nav-text">NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MVTrans-Multi-view-Perception-of-Transparent-Objects"><span class="nav-number">11.</span> <span class="nav-text">MVTrans: Multi-view Perception of Transparent Objects</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Swarm-LIO-Decentralized-Swam-LiDAR-inertial-Odometry"><span class="nav-number">12.</span> <span class="nav-text">Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Robust-Incremental-Smoothing-and-Mapping-riSAM"><span class="nav-number">13.</span> <span class="nav-text">Robust Incremental Smoothing and Mapping (riSAM)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Loc-NeRF-Monte-Carlo-Localization-using-Neural-Radiance-Fields"><span class="nav-number">14.</span> <span class="nav-text">Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ditto-in-the-House-Building-Articulated-Models-of-Indoor-Scenes-through-INteractive-Perception"><span class="nav-number">15.</span> <span class="nav-text">Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Category-level-Shape-Estimation-for-Densely-Cluttered-Objects"><span class="nav-number">16.</span> <span class="nav-text">Category-level Shape Estimation for Densely Cluttered Objects</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Category-level-Global-Camera-Pose-Estimation-with-Multi-Hypothesis-Point-Cloud-Correspondence"><span class="nav-number">17.</span> <span class="nav-text">Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Simple-BEV-What-Really-Matters-for-Multi-Sensor-BEV-Perception"><span class="nav-number">18.</span> <span class="nav-text">Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BEVFusion-Multi-Task-Multi-Sensor-Fusion-with-Unified-Bird%E2%80%99s-Eye-View-Representation"><span class="nav-number">19.</span> <span class="nav-text">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cerberus-Low-Drift-Visual-Inertial-Leg-Odometry-for-Agile-Locomotion"><span class="nav-number">20.</span> <span class="nav-text">Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cross-Modal-Monocular-Localization-in-Prior-LiDAR-Maps-Utilizing-Semantic-Consistency"><span class="nav-number">21.</span> <span class="nav-text">Cross-Modal Monocular Localization in Prior LiDAR Maps Utilizing Semantic Consistency</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#L-C-Visual-inertial-Loose-Coupling-for-Resilient-and-Lightweight-Direct-Visual-Localization"><span class="nav-number">22.</span> <span class="nav-text">L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">cv-learn</p>
  <div class="site-description" itemprop="description">Vision, SLAM, Spatial AI</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">254</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">354</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;changh95"><i class="fab fa-github fa-fw"></i></span>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://changh95.github.io/20230530-icra2023-day2-poster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="cv-learn">
      <meta itemprop="description" content="Vision, SLAM, Spatial AI">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cv-learn">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ICRA 2023 - 포스터 (Day 2)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-05-30 14:48:45" itemprop="dateCreated datePublished" datetime="2023-05-30T14:48:45+09:00">2023-05-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-06-15 17:51:06" itemprop="dateModified" datetime="2023-06-15T17:51:06+09:00">2023-06-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/" itemprop="url" rel="index"><span itemprop="name">1. Spatial AI</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/1-1-SLAM/" itemprop="url" rel="index"><span itemprop="name">1.1 SLAM</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/1-1-SLAM/%ED%95%99%ED%9A%8C-%EB%B0%9C%ED%91%9C-%EB%A6%AC%EB%B7%B0/" itemprop="url" rel="index"><span itemprop="name">학회 발표 리뷰</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Loosely-Coupled-Localization-Fusion-System-Based-on-Track-To-Track-Fusion-with-Bias-Alignment"><a href="#Loosely-Coupled-Localization-Fusion-System-Based-on-Track-To-Track-Fusion-with-Bias-Alignment" class="headerlink" title="Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment"></a>Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment</h1><ul>
<li>Localization 시스템은 다양한 이유로 오차를 만들어냄.<ul>
<li>센서 캘리브레이션이 잘 안되면 calibration error</li>
<li>지도를 기반으로 위치를 추정하는데 지도가 부정확하면 map bias error</li>
<li>INS와 같은 시스템에서는 drift error</li>
</ul>
</li>
<li>시간이 지날수록 누적되는 에러를 reference 시스템과 비교해 bias를 추적함. 추적한 bias를 기반으로 target 시스템의 bias를 수정할 수 있음.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/1.png" class="" title="Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment">
<p> </p>
<hr>
<h1 id="Mapping-with-only-the-Ground"><a href="#Mapping-with-only-the-Ground" class="headerlink" title="Mapping with only the Ground"></a>Mapping with only the Ground</h1><ul>
<li>바닥 아스팔트 노면만 바라보며 수행하는 visual odometry.</li>
<li>이미지 추출 -&gt; 키포인트 검출 -&gt; 바닥평면으로 투영 -&gt; 이후 기본적인 VO 파이프라인 사용</li>
<li>루프클로저가 사용될 경우 정확도가 꽤 좋음</li>
<li>루프클로저를 사용하지 않을 경우 drift가 누적됨<ul>
<li>특히, 회전할 때 누적이 많이 됨</li>
</ul>
</li>
<li>하지만 테스트 해본 공간이 겨우 3m x 2m 라서 큰 공간에서 동작할지 확신할 수 없음.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/2.png" class="" title="Mapping with only the Ground">
<p> </p>
<hr>
<h1 id="Structure-PLP-SLAM-Efficient-Sparse-Mapping-and-Localization-using-Point-Line-and-Plane-for-Monocular-RGB-D-and-Stereo-Cameras"><a href="#Structure-PLP-SLAM-Efficient-Sparse-Mapping-and-Localization-using-Point-Line-and-Plane-for-Monocular-RGB-D-and-Stereo-Cameras" class="headerlink" title="Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras"></a>Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras</h1><ul>
<li>Line Segment Detection (LSD)로 선을 검출해서 2가지 용도로 사용한다.<ul>
<li>Point-Line map 생성 (Geometry)</li>
<li>Instance planar segmentation 이미지 생성 (Semantics)</li>
</ul>
</li>
<li>두개를 퓨전하면 Line-Planar map이 생성된다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/3.png" class="" title="Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras">
<p> </p>
<hr>
<h1 id="Object-based-SLAM-utilizing-unambigous-pose-parameters-considering-general-symmetry-types"><a href="#Object-based-SLAM-utilizing-unambigous-pose-parameters-considering-general-symmetry-types" class="headerlink" title="Object-based SLAM utilizing unambigous pose parameters considering general symmetry types"></a>Object-based SLAM utilizing unambigous pose parameters considering general symmetry types</h1><ul>
<li>서울대에서 나온 연구</li>
<li>Object-based SLAM을 할 때는 각각의 object의 pose를 아는 것이 중요하다.<ul>
<li>하지만, 종종 symmetry를 가진 물체들은 방향을 정의하기 어렵기 때문에 pose를 구하기 어려울 수 있다. 2. Contribution 파트를 보면…</li>
<li>[Asymmetry] 물체는 6dof 포즈를 바로 추정할 수 있다</li>
<li>[Discrete symmetry] 물체는 2개의 평면으로 ambiguous pose가 나타난다</li>
<li>[Continuous symmetry] 물체는 z축 기준으로 빙빙 돌려도 모두 똑같이 생겼기 때문에 무한개의 ambiguous pose가 생긴다.</li>
</ul>
</li>
<li>Multiple hypothesis tracking을 통해 특정 object가 asymmetry/discrete symmetry/continuous symmetry인지 구분을 할 수 있다.</li>
<li>이후, 각각의 타입에 맞춘 factor graph 최적화 방법을 제안한다.</li>
<li>내가 드렸던 질문으로는 object label마다 asymmetry/discrete/continuous 클래스를 나눠놓으면 무거운 multiple hypothesis 연산을 하지 않아도 되는데 왜 이 방법을 사용하셨는지 여쭤봤다.<ul>
<li>제시해주신 답으로는 ‘같은 object class임에도 디자인에 따라 symmetry가 생길수도 있기 때문에, symmetry를 기준으로 여러 object class를 학습하는건 효율적이지 않다. 특히나 새로운 디자인이 나오면 재학습 해야하는 경우도 생긴다’ 라고 하셨다.</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/4.png" class="" title="Object-based SLAM utilizing unambigous pose parameters considering general symmetry types">
<p> </p>
<hr>
<h1 id="Towards-View-invariant-and-Accurate-Loop-Detection-Based-on-Scene-Graph"><a href="#Towards-View-invariant-and-Accurate-Loop-Detection-Based-on-Scene-Graph" class="headerlink" title="Towards View-invariant and Accurate Loop Detection Based on Scene Graph"></a>Towards View-invariant and Accurate Loop Detection Based on Scene Graph</h1><ul>
<li>Semantic graph를 통해서 loop closure를 할 수 있는 descriptor를 생성하는 연구.</li>
<li>Illumination이나 appearance가 바뀌어도 잘 동작한다는 장점이 있다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/5.png" class="" title="Towards View-invariant and Accurate Loop Detection Based on Scene Graph">
<p> </p>
<hr>
<h1 id="Zero-shot-Active-Visual-Search-ZAVIS-Intelligent-Object-Search-for-Robotic-Assistants"><a href="#Zero-shot-Active-Visual-Search-ZAVIS-Intelligent-Object-Search-for-Robotic-Assistants" class="headerlink" title="Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants"></a>Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants</h1><ul>
<li>고려대 최성준 교수님 랩실 연구</li>
<li>Active visual search (AVS) 방법론에 대한 연구</li>
<li>방법론:<ul>
<li>Scan landmark: 라이다와 PTZ 카메라를 이용해서 semantic view 생성<ul>
<li>Open-set detector를 사용해서 사전에 학습되지 않은 class의 물체도 검출한다.</li>
<li>‘Unknown’ class를 검출했을 때, 유저 프롬프트 정보에 대해 CLIP 기반 text-image matching을 통해 실제 class를 찾아낸다.</li>
</ul>
</li>
<li>Waypoint generation: Semantic uncertainty (vision-language model의 text-image matching의 entropy)와 pre-trained language model의 common sense knowledge를 바탕으로 한 cost function을 통해 waypoint 생성<ul>
<li>COMET ATOMIC 2020이라는 common sense knowledge 데이터셋이 있다고 함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/6.png" class="" title="Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants">
<p> </p>
<hr>
<h1 id="COVINS-G-A-Generic-Back-end-for-Collaborative-Visual-Inertial-SLAM"><a href="#COVINS-G-A-Generic-Back-end-for-Collaborative-Visual-Inertial-SLAM" class="headerlink" title="COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM"></a>COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</h1><ul>
<li>기존의 COVINS 프레임워크를 조금 더 확장한 버전<ul>
<li>중앙 서버에서 multi-agent를 사용한 collaborative SLAM을 위한 백엔드 연산을 하는 프레임워크</li>
<li>COVINS는 ORB-SLAM3 만 사용할 수 있었고, Multi-agent bundle adjustment를 하느라 연산량이 너무 많았음.<ul>
<li>BA를 한다는건 모든 keypoint, pose, landmark를 통신으로 보내야하는건데, 통신 부하량도 너무 많았음.</li>
</ul>
</li>
</ul>
</li>
<li>COVINS-G는 keyframe의 keypoint 정보와 pose 정보만 송수신함.<ul>
<li>keypoint 정보를 기반으로 데이터베이스 관리를 하고 loop closure가 가능함.</li>
<li>Loop closure를 할 때, 2개의 agent의 trajectory간 absolute scale을 복원할 수 있어야함. 여기서 하나의 트릭을 사용<ul>
<li>각각의 agent들의 consecutive frame을 마치 stereo camera처럼 이용해서 동일 landmark를 보고있다고 하면 두개의 trajectory 간의 absolute scale을 알 수 있음. (포스터에서는 17 point algorithm이라고 하지만, 사실 그냥 2개의 stereo camera간의 absolute trajectory를 비교하는 것과 동일함)</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/7.png" class="" title="COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM">
<p> </p>
<hr>
<h1 id="A-Probabilistic-Framework-for-Visual-Localization-in-Ambiguous-Scenes"><a href="#A-Probabilistic-Framework-for-Visual-Localization-in-Ambiguous-Scenes" class="headerlink" title="A Probabilistic Framework for Visual Localization in Ambiguous Scenes"></a>A Probabilistic Framework for Visual Localization in Ambiguous Scenes</h1><ul>
<li>(솔직히 잘 모르겠는 연구…)</li>
<li>이미지 상 비슷하게 보이는 오브젝트/뷰가 많은 경우, visual localization을 하기 어렵다 (Problem 섹션 참고)</li>
<li>MLP를 이용해서 pose regression을 한다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/8.png" class="" title="A Probabilistic Framework for Visual Localization in Ambiguous Scenes">
<p> </p>
<hr>
<h1 id="DytanVO-Joint-Refinement-of-Visual-Odometry-and-Motion-Segmentation-in-Dynamic-Environments"><a href="#DytanVO-Joint-Refinement-of-Visual-Odometry-and-Motion-Segmentation-in-Dynamic-Environments" class="headerlink" title="DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments"></a>DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments</h1><img src="/20230530-icra2023-day2-poster/9.png" class="" title="DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments">
<p> </p>
<hr>
<h1 id="NoCal-Calibration-Free-Semi-Supervised-Learning-of-Odometry-and-Camera-Intrinsics"><a href="#NoCal-Calibration-Free-Semi-Supervised-Learning-of-Odometry-and-Camera-Intrinsics" class="headerlink" title="NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics"></a>NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics</h1><ul>
<li>비디오 시퀀스가 주어졌을 때, 1. NeRF 기반 scene rendering, 2. relative pose estimation (i.e. odometry), 3. camera calibration을 동시에 수행하는 시스템을 제안.</li>
<li>저자들에 의하면 사실 View syenthesis가 잘 되지 않는다고 한다. 하지만 view synthesis를 같이 함으로써 인해 odometry 추정 및 calibration이 꽤 잘 된다고 한다.</li>
<li>Camera model은 OpenCV perspective를 사용했다고 한다. 이 이야기를 듣고 나는 differentiable한 camera model을 사용하면 모델 자체가 backprop으로 인해 업데이트가 되며 자동 캘리브레이션이 될 것이라고 조언해줬다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/10.png" class="" title="NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics">
<p> </p>
<hr>
<h1 id="MVTrans-Multi-view-Perception-of-Transparent-Objects"><a href="#MVTrans-Multi-view-Perception-of-Transparent-Objects" class="headerlink" title="MVTrans: Multi-view Perception of Transparent Objects"></a>MVTrans: Multi-view Perception of Transparent Objects</h1><img src="/20230530-icra2023-day2-poster/11.png" class="" title="MVTrans: Multi-view Perception of Transparent Objects">
<p> </p>
<hr>
<h1 id="Swarm-LIO-Decentralized-Swam-LiDAR-inertial-Odometry"><a href="#Swarm-LIO-Decentralized-Swam-LiDAR-inertial-Odometry" class="headerlink" title="Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry"></a>Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry</h1><img src="/20230530-icra2023-day2-poster/12.png" class="" title="Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry">
<p> </p>
<hr>
<h1 id="Robust-Incremental-Smoothing-and-Mapping-riSAM"><a href="#Robust-Incremental-Smoothing-and-Mapping-riSAM" class="headerlink" title="Robust Incremental Smoothing and Mapping (riSAM)"></a>Robust Incremental Smoothing and Mapping (riSAM)</h1><img src="/20230530-icra2023-day2-poster/13.png" class="" title="Robust Incremental Smoothing and Mapping (riSAM)">
<p> </p>
<hr>
<h1 id="Loc-NeRF-Monte-Carlo-Localization-using-Neural-Radiance-Fields"><a href="#Loc-NeRF-Monte-Carlo-Localization-using-Neural-Radiance-Fields" class="headerlink" title="Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields"></a>Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields</h1><ul>
<li>Monte Carlo Localization (i.e. Particle filter 기반 위치 추정 기법)에 NeRF를 섞어본 실험적인 방법<ul>
<li>Monte Carlo Localization은 보통 이렇게 동작한다: 1. Motion model 기반으로 이동치 추정, 2. 이동 후 위치할 가능성이 있는 곳에 particle을 뿌림, 3. 각각의 particle 마다 observation (보통 LiDAR 스캔)을 지도와 매칭해서 가장 확률이 높은 particle을 정답으로 삼음.</li>
</ul>
</li>
<li>Loc-NeRF도 Monte Carlo Localization의 파이프라인을 따라가지만, observation이 NeRF의 novel view rendering을 이용함.<ul>
<li>사전에 Scene에 대해서 Neural radiance field를 학습해놓음.</li>
<li>위치 추정을 하기 위해 초기 motion model 값을 기반으로 particle을 뿌림.</li>
<li>각각의 particle마다 (i.e. pose candidate)마다 novel view rendering을 수행 함. 이 때, 모든 픽셀을 다 렌더링하면 너무 오래 걸리기 때문에, 64개의 픽셀만 렌더링을 수행함.</li>
<li>렌더링 된 결과와 실제 카메라에서 나온 결과가 가장 비슷한 particle이 다음 위치가 됨.</li>
</ul>
</li>
<li>기존의 Monte Carlo Localization 기법에 Particle annealing이라는 기법을 함께 사용함.<ul>
<li>첫 프레임에서 위치를 찾을 때에는 넓은 공간 속 위치를 찾기 때문에 정확한 위치를 찾을 수 없으나, particle filter가 진행되면서 점점 정답 위치로 수렴해가기 때문에, particle의 수가 많을 필요가 없다.</li>
<li>그렇기 때문에, 초기 iteration에서는 수많은 particle을 뿌려서 위치를 찾되, iteration이 높아질수록 particle의 수를 줄여 실시간 프로세싱이 가능하게 하는 기법 역시 제안한다.</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/14.png" class="" title="Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields">
<p> </p>
<hr>
<h1 id="Ditto-in-the-House-Building-Articulated-Models-of-Indoor-Scenes-through-INteractive-Perception"><a href="#Ditto-in-the-House-Building-Articulated-Models-of-Indoor-Scenes-through-INteractive-Perception" class="headerlink" title="Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception"></a>Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception</h1><ul>
<li>Affordance network를 이용해 ‘움직일 수 있는 static object’를 검출한다. 이를 통해 어떤 형태의 state change가 나타날 수 있는지 예상하는 네트워크를 만든다. (e.g. 냉장고 문이 열리고 닫힘)</li>
<li>이런 프로그램을 짤 때 어떤 라이브러리를 썼냐고 물어보니, PCL과 Open3D를 썼다고 한다. </li>
</ul>
<img src="/20230530-icra2023-day2-poster/15.png" class="" title="Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception">
<p> </p>
<hr>
<h1 id="Category-level-Shape-Estimation-for-Densely-Cluttered-Objects"><a href="#Category-level-Shape-Estimation-for-Densely-Cluttered-Objects" class="headerlink" title="Category-level Shape Estimation for Densely Cluttered Objects"></a>Category-level Shape Estimation for Densely Cluttered Objects</h1><ul>
<li>다수의 물체들이 서로 엉켜있어 아주 강한 occlusion이 존재할 때, 물체들의 Shape을 계산한다는 연구이다.<ul>
<li>이 때, 우리는 ‘어떤 class의 물체들이 있는지’는 알고 있지만, ‘해당 class의 물체들은 모두 길이, 넓이, 높이가 조금씩 다르다’ 라는 전제를 가지고 물체의 형태를 추정한다.</li>
</ul>
</li>
<li>우선 Instance segmentation을 통해 point cloud segmentation을 수행한다.<ul>
<li>Occlusion이 심하기 때문에, point cloud는 부분적으로만 복원이 될 것이다.</li>
<li>Class는 정확하게 추론했다는 전제를 가진다.</li>
</ul>
</li>
<li>Segmentation을 통해 추론한 class를 우리가 가지고 있는 물체의 shape template과 비교한다.</li>
<li>3D GCN을 통해 geometric transformation parameter를 추정한다.<ul>
<li>이 파라미터들은 길이, 넓이, 높이의 차이를 의미한다.</li>
<li>이 파라미터들을 구하면, 길이/넓이/높이에 맞춰 변형한 shape template을 구할 수 있다.<ul>
<li>즉, 실제로 point cloud에서 빈 곳을 채우는게 아니라, 적당히 맞는 길이/넓이/높이를 추정해 shape deformation을 하는 것이라고 볼 수 있다. </li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/16.png" class="" title="Category-level Shape Estimation for Densely Cluttered Objects">
<p> </p>
<hr>
<h1 id="Category-level-Global-Camera-Pose-Estimation-with-Multi-Hypothesis-Point-Cloud-Correspondence"><a href="#Category-level-Global-Camera-Pose-Estimation-with-Multi-Hypothesis-Point-Cloud-Correspondence" class="headerlink" title="Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence"></a>Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence</h1><ul>
<li>딥러닝을 사용하지 않고 object를 바라보는 camera pose estimation을 하는 방법을 소개한다.<ul>
<li>이 때, 우리가 바라보고 있는 object는 class는 알고 있지만, ground truth 3D model과 다른 형태를 가지고 있는 상황이다.</li>
<li>예를 들어, 우리가 가진 모델은 BMW 520d 자동차의 3D 모델이지만, 실제로 바라보고 있는 자동차는 BMW 320d 일 수 있다는 것이다. 대충 전체 쉐입은 비슷하지만, 디테일 적인 부분에서 다를 수 있다.</li>
</ul>
</li>
<li>3D 모델에서 3D keypoint를 추출해서 매칭을 한 후, affinity matrix라는 것을 통해서 제대로 매칭을 했는지 확인을 한다. 이 후, Least squares를 이용해서 초기 포즈를 추정한 후, Weighted pairwise distance loss를 기반으로 포즈 최적화를 진행한다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/17.png" class="" title="Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence">
<p> </p>
<hr>
<h1 id="Simple-BEV-What-Really-Matters-for-Multi-Sensor-BEV-Perception"><a href="#Simple-BEV-What-Really-Matters-for-Multi-Sensor-BEV-Perception" class="headerlink" title="Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?"></a>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</h1><ul>
<li><ol>
<li>최신 BEV 생성 방식을 비교하면서 ‘BEV를 생성하는데에 어떤 점들이 중요한가?’를 정리하고, 2. 본인들의 BEV 알고리즘을 소개한다.</li>
</ol>
</li>
<li>BEV를 생성하는데에 중요한 점들<ul>
<li>‘Predicting semantic map representations from images using pyramid occupancy networks’, ‘Lift, splat, shoot: Encoding images from arbitrary camera rigs by Implicitly unprojecting to 3D’, ‘Translating images into maps’, ‘Learning bird’s eye-view representation from multi-camera images via spatiotemporal transformers’와 저자의 BEV 알고리즘을 비교한다.<ul>
<li>위 논문들 사이의 성능 차이는, 사실 논문에 있는 성능 벤치마크에서 이야기하는 것 보다 훨씬 작다 (다들 고만고만하다…)</li>
<li>Unweighted splatiing, depth-based splatting, deformable attention과 같은 멋있어보이지만 무거운 알고리즘들보다, 사실 우리가 쓴 bilinear sampling과 같은 가벼운 알고리즘도 성능이 잘 나온다.</li>
</ul>
</li>
<li>BEV 모델을 학습할 때, batch size를 키우면 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, 고해상도 이미지를 생성할 경우 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, 더 깊은 backbone을 사용하면 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, augmentation으로 cropping/resizing/camera shuffling을 사용하면 정확도가 높아진다.<ul>
<li>camera dropout는 도움이 되지 않는다.</li>
</ul>
</li>
<li>BEV 모델을 학습할 때, LiDAR와 RADAR 정보를 섞으면 정확도가 많이 올라간다.<ul>
<li>LiDAR + RADAR 정보를 섞기 위해서는 그냥 단순히 feature에 concatenate하면 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/18.png" class="" title="Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?">
<p> </p>
<hr>
<h1 id="BEVFusion-Multi-Task-Multi-Sensor-Fusion-with-Unified-Bird’s-Eye-View-Representation"><a href="#BEVFusion-Multi-Task-Multi-Sensor-Fusion-with-Unified-Bird’s-Eye-View-Representation" class="headerlink" title="BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation"></a>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation</h1><img src="/20230530-icra2023-day2-poster/19.png" class="" title="BEVFusion: Multi-Task Multi-SEnsor Fusion with Unified Bird" alt="s-Eye View Representation">
<p> </p>
<hr>
<h1 id="Cerberus-Low-Drift-Visual-Inertial-Leg-Odometry-for-Agile-Locomotion"><a href="#Cerberus-Low-Drift-Visual-Inertial-Leg-Odometry-for-Agile-Locomotion" class="headerlink" title="Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion"></a>Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion</h1><img src="/20230530-icra2023-day2-poster/20.png" class="" title="Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion">
<p> </p>
<hr>
<h1 id="Cross-Modal-Monocular-Localization-in-Prior-LiDAR-Maps-Utilizing-Semantic-Consistency"><a href="#Cross-Modal-Monocular-Localization-in-Prior-LiDAR-Maps-Utilizing-Semantic-Consistency" class="headerlink" title="Cross-Modal Monocular Localization in Prior LiDAR Maps Utilizing Semantic Consistency"></a>Cross-Modal Monocular Localization in Prior LiDAR Maps Utilizing Semantic Consistency</h1><ul>
<li>Semantic LiDAR 지도가 있을 때, 카메라만 가지고 위치를 추정하는 방법이다.</li>
<li>우선 카메라 이미지로부터 semantic segmentation과 ORB-SLAM3를 돌려서 semantic visual map을 생성한다.<ul>
<li>여기에 semantic LiDAR map을 가져와서 ICP를 통해서 포즈를 구한다.</li>
</ul>
</li>
<li>내 질문: 실제 3D 공간에서의 LiDAR의 포인트 클라우드 위치와 Visual 포인트 클라우드의 위치가 다르지 않나? 그러면 정합이 잘 안되서 정확도가 잘 안나올텐데.<ul>
<li>답변: 그러고보니 그렇네… 근데 실험 결과 보면 잘됨.</li>
<li>(ㅋㅋㅋ… 왜 되는지는 모르겠지만 일단 된다 라는 답변…)</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/21.png" class="" title="Cross-Modal Monouclar Localization in Prior LiDAR Maps Utilizing Semantic Consistency">
<p> </p>
<hr>
<h1 id="L-C-Visual-inertial-Loose-Coupling-for-Resilient-and-Lightweight-Direct-Visual-Localization"><a href="#L-C-Visual-inertial-Loose-Coupling-for-Resilient-and-Lightweight-Direct-Visual-Localization" class="headerlink" title="L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization"></a>L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization</h1><img src="/20230530-icra2023-day2-poster/22.png" class="" title="L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization">

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20230529-icra2023-day1-posters/" rel="bookmark">ICRA 2023 - 포스터 (Day 1)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20230531-icra2023-day3-poster/" rel="bookmark">ICRA 2023 - 포스터 (Day 3)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20230531-icra2023-day4-poster/" rel="bookmark">ICRA 2023 - 포스터 (Day 4)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20230602-icra2023-day5-poster/" rel="bookmark">ICRA 2023 - 포스터 (Day 5)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20230529-icra2023-day-1/" rel="bookmark">ICRA 2023 - Distributed graph algorithms 워크샵 (Day 1)</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
              <a href="/tags/ICRA/" rel="tag"># ICRA</a>
              <a href="/tags/Robotics/" rel="tag"># Robotics</a>
              <a href="/tags/Spatial-AI/" rel="tag"># Spatial AI</a>
              <a href="/tags/Posters/" rel="tag"># Posters</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/20230529-icra2023-day-1-1/" rel="prev" title="ICRA 2023 - Pretraining in robotics 워크샵 (Day 1)">
                  <i class="fa fa-chevron-left"></i> ICRA 2023 - Pretraining in robotics 워크샵 (Day 1)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/20230531-icra2023-day3-poster/" rel="next" title="ICRA 2023 - 포스터 (Day 3)">
                  ICRA 2023 - 포스터 (Day 3) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments">
  <script src="https://utteranc.es/client.js" repo="changh95/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script>
  </div>
  
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cv-learn</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

<script src="/js/local-search.js"></script>



<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
