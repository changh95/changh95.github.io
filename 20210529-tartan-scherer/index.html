<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"changh95.github.io","root":"/","images":"/images","scheme":"Pisces","version":"8.0.2","exturl":true,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"disqus","active":false,"storage":false,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":"auto","trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="Tartan Series 세미나 중 Prof. Sebastian Scherer 교수님 세미나 정리.">
<meta property="og:type" content="article">
<meta property="og:title" content="Tartan Series 2021 - Challenges in SLAM - What&#39;s ahead (Prof. Sebastian Scherer)">
<meta property="og:url" content="https://changh95.github.io/20210529-tartan-scherer/index.html">
<meta property="og:site_name" content="cv-learn">
<meta property="og:description" content="Tartan Series 세미나 중 Prof. Sebastian Scherer 교수님 세미나 정리.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/1.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/2.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/3.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/4.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/5.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/6.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/7.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/8.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/9.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/10.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/11.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/12.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/13.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/14.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/15.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/16.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/17.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/18.gif">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/19.png">
<meta property="og:image" content="https://changh95.github.io/20210529-tartan-scherer/20.png">
<meta property="article:published_time" content="2021-05-29T06:14:31.000Z">
<meta property="article:modified_time" content="2023-06-09T06:12:01.839Z">
<meta property="article:author" content="cv-learn">
<meta property="article:tag" content="SLAM">
<meta property="article:tag" content="Visual-SLAM">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Deep SLAM">
<meta property="article:tag" content="Tartan">
<meta property="article:tag" content="AirLab">
<meta property="article:tag" content="CMU">
<meta property="article:tag" content="Sebastian Scherer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://changh95.github.io/20210529-tartan-scherer/1.png">


<link rel="canonical" href="https://changh95.github.io/20210529-tartan-scherer/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>
<title>Tartan Series 2021 - Challenges in SLAM - What's ahead (Prof. Sebastian Scherer) | cv-learn</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">cv-learn</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Vision, SLAM, Spatial AI</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM%EC%9D%B4-%EC%B2%98%EC%9D%8C%EC%9D%B4%EB%9D%BC%EB%A9%B4%E2%80%A6"><span class="nav-number">1.</span> <span class="nav-text">SLAM이 처음이라면…</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM%EC%97%90%EC%84%9C-%EC%9E%90%EC%A3%BC-%EC%93%B0%EB%8A%94-%EB%8B%A8%EC%96%B4%EB%93%A4"><span class="nav-number">2.</span> <span class="nav-text">SLAM에서 자주 쓰는 단어들</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM%EC%9D%B4%EB%9E%80"><span class="nav-number">3.</span> <span class="nav-text">SLAM이란?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM-%EA%B8%B0%EC%88%A0%EC%9D%98-%EB%B0%9C%EC%A0%84-%EB%B0%A9%ED%96%A5"><span class="nav-number">4.</span> <span class="nav-text">SLAM 기술의 발전 방향</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM-%EB%B0%A9%EB%B2%95%EB%A1%A0-%EA%B0%9C%EC%9A%94"><span class="nav-number">5.</span> <span class="nav-text">SLAM 방법론 개요</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LiDAR-SLAM"><span class="nav-number">5.1.</span> <span class="nav-text">LiDAR SLAM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visual-SLAM"><span class="nav-number">5.2.</span> <span class="nav-text">Visual SLAM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RGB-D-Visual-Inertial-SLAM"><span class="nav-number">5.3.</span> <span class="nav-text">RGB-D &#x2F; Visual-Inertial SLAM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%80%9CDeep%E2%80%9D-Visual-SLAM"><span class="nav-number">5.4.</span> <span class="nav-text">“Deep” Visual SLAM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Sensor-Fusion"><span class="nav-number">5.5.</span> <span class="nav-text">Multi-Sensor Fusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SLAM-%EC%84%B1%EB%8A%A5%EC%9D%84-%EC%B8%A1%EC%A0%95%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95"><span class="nav-number">6.</span> <span class="nav-text">SLAM 성능을 측정하는 방법</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B"><span class="nav-number">6.1.</span> <span class="nav-text">데이터셋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EC%B8%A1%EC%A0%95-metric"><span class="nav-number">6.2.</span> <span class="nav-text">측정 metric</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TartanVO-Learning-based-Visual-Odometry"><span class="nav-number">7.</span> <span class="nav-text">TartanVO: Learning-based Visual Odometry</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Super-Odometry-IMU-centric-LiDAR-Visual-Inertial-Estimator-for-Challenging-Environments"><span class="nav-number">8.</span> <span class="nav-text">Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visual-localization"><span class="nav-number">9.</span> <span class="nav-text">Visual localization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2D-3D-Line-%EB%A7%A4%EC%B9%AD-%EA%B8%B0%EB%B0%98-Localization"><span class="nav-number">9.1.</span> <span class="nav-text">2D-3D Line 매칭 기반 Localization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EC%8B%9C%EC%A0%90%EB%B3%80%ED%99%94-%EC%A1%B0%EB%AA%85%EB%B3%80%ED%99%94-%EB%82%A0%EC%94%A8%EB%B3%80%ED%99%94%EC%97%90-%EA%B0%95%EC%9D%B8%ED%95%9C-descriptor-learning"><span class="nav-number">9.2.</span> <span class="nav-text">시점변화&#x2F;조명변화&#x2F;날씨변화에 강인한 descriptor learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lifelong-Graph-Learning%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-feature-matching-place-recognition"><span class="nav-number">9.3.</span> <span class="nav-text">Lifelong Graph Learning을 이용한 feature matching &#x2F; place recognition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Large-scale-sub-millimeter-mapping-with-lightweight-and-low-cost-weight-sensors"><span class="nav-number">10.</span> <span class="nav-text">Large scale sub-millimeter mapping (with lightweight and low-cost weight sensors)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EC%B9%B4%EB%A9%94%EB%9D%BC-%EB%9D%BC%EC%9D%B4%EB%8B%A4-IMU-%ED%93%A8%EC%A0%84"><span class="nav-number">10.1.</span> <span class="nav-text">카메라+라이다+IMU 퓨전</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EC%A0%80%ED%95%B4%EC%83%81%EB%8F%84-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EA%B8%B0%EB%B0%98-%EA%B3%A0%ED%95%B4%EC%83%81%EB%8F%84-dense-stereo-reconstruction"><span class="nav-number">10.2.</span> <span class="nav-text">저해상도 이미지 기반 고해상도 dense stereo reconstruction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Open-questions"><span class="nav-number">11.</span> <span class="nav-text">Open questions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%EC%B0%B8%EA%B0%80%EC%9E%90-%EC%A7%88%EB%AC%B8"><span class="nav-number">12.</span> <span class="nav-text">참가자 질문</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">cv-learn</p>
  <div class="site-description" itemprop="description">Vision, SLAM, Spatial AI</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">254</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">354</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;changh95"><i class="fab fa-github fa-fw"></i></span>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://changh95.github.io/20210529-tartan-scherer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="cv-learn">
      <meta itemprop="description" content="Vision, SLAM, Spatial AI">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cv-learn">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tartan Series 2021 - Challenges in SLAM - What's ahead (Prof. Sebastian Scherer)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-05-29 15:14:31" itemprop="dateCreated datePublished" datetime="2021-05-29T15:14:31+09:00">2021-05-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2023-06-09 15:12:01" itemprop="dateModified" datetime="2023-06-09T15:12:01+09:00">2023-06-09</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/" itemprop="url" rel="index"><span itemprop="name">1. Spatial AI</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/1-1-SLAM/" itemprop="url" rel="index"><span itemprop="name">1.1 SLAM</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/1-Spatial-AI/1-1-SLAM/%ED%95%99%ED%9A%8C-%EB%B0%9C%ED%91%9C-%EB%A6%AC%EB%B7%B0/" itemprop="url" rel="index"><span itemprop="name">학회 발표 리뷰</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>강의 링크 - <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9hY1lCU3JEcEVkUQ==">https://youtu.be/acYBSrDpEdQ<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="SLAM이-처음이라면…"><a href="#SLAM이-처음이라면…" class="headerlink" title="SLAM이 처음이라면…"></a>SLAM이 처음이라면…</h2><ul>
<li>선형대수 공부자료 - <span class="exturl" data-url="aHR0cDovL3Zpc2lvbi5zdGFuZm9yZC5lZHUvdGVhY2hpbmcvY3MxMzFfZmFsbDE2MTcvbGVjdHVyZXMvbGVjdHVyZTJfbGluYWxnX3Jldmlld19jczEzMV8yMDE2LnBkZg==">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>확률과 통계 공부자료 - <span class="exturl" data-url="aHR0cDovL2hlYWx5LmNyZWF0ZS5zdGVkd2FyZHMuZWR1L0NoZW1pc3RyeS9DSEVNNDM0MS9CYXllc1ByaW1lcjIucGRm">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>AirLab Summer School - <span class="exturl" data-url="aHR0cDovL3RoZWFpcmxhYi5vcmcvc3VtbWVyMjAyMC8=">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>CVPR 2020 SLAM 워크샵 - <span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvdmlzbG9jc2xhbWN2cHIyMDIwL2ludml0ZWQtc3BlYWtlcnM=">링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p> </p>
<hr>
<h2 id="SLAM에서-자주-쓰는-단어들"><a href="#SLAM에서-자주-쓰는-단어들" class="headerlink" title="SLAM에서 자주 쓰는 단어들"></a>SLAM에서 자주 쓰는 단어들</h2><p><img src="./1.png" alt="terms"></p>
<ul>
<li>Pose<ul>
<li>위치 (Position) + 방향 (Orientation)</li>
</ul>
</li>
<li>Odometry<ul>
<li>두 pose간의 상대적 차이</li>
</ul>
</li>
<li>Localization<ul>
<li>내가 지도 상 어디에 있는지 풀어내는 문제</li>
</ul>
</li>
<li>Mapping<ul>
<li>지도를 그려내는 문제</li>
</ul>
</li>
<li>Drift<ul>
<li>연속적으로 위치를 추정할 때 쌓여가는 pose에 대한 오차</li>
</ul>
</li>
<li>Loop closure<ul>
<li>동일한 장소로 돌아왔을 때 생기는 loop 내부에서 drift를 해소하는 방법</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM이란"><a href="#SLAM이란" class="headerlink" title="SLAM이란?"></a>SLAM이란?</h2><ul>
<li>기술적 의미: Localization ⇄ Mapping 루프를 통해 위치/맵 추정을 하는 것.<ol>
<li>먼저 나의 위치를 확인 (localization)</li>
<li>맵을 증축함 (mapping)</li>
<li>증축된 맵에서 나의 위치를 확인 (localization)</li>
<li>맵을 증축함 (mapping)</li>
<li>…</li>
</ol>
</li>
<li>일반적으로 보는 의미: 다양한 센서들로 다양한 추정을 함<ul>
<li>카메라, 레이더, 라이다, GPS 등과 같이 다양한 센서 값을 어떻게 조합해서 위치/맵 추정을 할 것인가?</li>
<li>움직이는 객체가 있을 경우에는? Semantic 정보도 추출할 수 있는지? </li>
<li>종종 ‘위치 추정’ 기술을 쉽게 부르기 위해서 SLAM이라고 부르기도 함 (사실 알고보면 그냥 odometry나 mapping인 경우도 많음)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-기술의-발전-방향"><a href="#SLAM-기술의-발전-방향" class="headerlink" title="SLAM 기술의 발전 방향"></a>SLAM 기술의 발전 방향</h2><ul>
<li>SLAM 성능<ul>
<li>Robustness의 발전<ul>
<li>Visual / geometric degeneration / challenging scenes (e.g. 안개, 비/눈, 화재, 탄광)</li>
</ul>
</li>
<li>Efficiency의 발전<ul>
<li>Lightweight image localization</li>
<li>Dense reconstruction</li>
</ul>
</li>
<li>Accuracy의 발전<ul>
<li>High resolution</li>
</ul>
</li>
</ul>
</li>
<li>신기능<ul>
<li>Collaborative SLAM<ul>
<li>여러 대의 agent가 돌아다니면서 각각 맵을 만드는 것.</li>
</ul>
</li>
<li>Semantic SLAM<ul>
<li>SLAM을 하면서 semantic 정보도 동시에 추출하는 것.</li>
</ul>
</li>
<li>High resolution map<ul>
<li>거대 공간 (i.e. large scale)에서 mm단위의 정확한 맵 생성</li>
</ul>
</li>
<li>Unified map representation<ul>
<li>카메라로 읽던, 라이다로 읽던, 레이더로 읽던 호환되는 맵</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-방법론-개요"><a href="#SLAM-방법론-개요" class="headerlink" title="SLAM 방법론 개요"></a>SLAM 방법론 개요</h2><p><img src="./2.png" alt="Overview"></p>
<blockquote>
<p>위 이미지에서 설명된 Visual / LiDAR 외로 GPS, 레이더, sonar 등 다른 방식들도 있음.</p>
</blockquote>
<h3 id="LiDAR-SLAM"><a href="#LiDAR-SLAM" class="headerlink" title="LiDAR SLAM"></a>LiDAR SLAM</h3><p><img src="./3.png" alt="LiDAR SLAM"></p>
<ul>
<li>굉장히 정확한 편</li>
<li>공간의 생김새에 따라 정확도/안정성이 변화함</li>
</ul>
<h3 id="Visual-SLAM"><a href="#Visual-SLAM" class="headerlink" title="Visual SLAM"></a>Visual SLAM</h3><p><img src="./4.png" alt="Visual SLAM"></p>
<ul>
<li>저렴한 센서</li>
<li>공간에 존재하는 texture에 따라 정확도/안정성이 변화함</li>
<li>3가지 Visual SLAM 방법론이 존재함<ul>
<li>Sparse SLAM: Sparse local feature를 (e.g. SIFT, ORB) 기반으로 SLAM 추정</li>
<li>Semi-dense SLAM : Sparse와 Dense SLAM의 중간</li>
<li>Dense SLAM: 이미지 전체의 정보를 사용해서 SLAM 추정</li>
</ul>
</li>
</ul>
<h3 id="RGB-D-Visual-Inertial-SLAM"><a href="#RGB-D-Visual-Inertial-SLAM" class="headerlink" title="RGB-D / Visual-Inertial SLAM"></a>RGB-D / Visual-Inertial SLAM</h3><p><img src="./5.png" alt="RGB-D/Visual-Inertial SLAM"></p>
<ul>
<li>센서 1개만 쓰면 잘 안될때가 많다 (센서들마다 잘 되는곳/안되는 곳이 있다)<ul>
<li>여러개의 센서를 써서 잘 안되는 부분을 커버한다.</li>
<li>e.g. Visual + IMU 방식은 monocular 방식이 (i.e. 1개 카메라) 풀지 못하는 metric scale 복원 (i.e. 실제 m단위 스케일) 작업이 가능하다.</li>
</ul>
</li>
</ul>
<h3 id="“Deep”-Visual-SLAM"><a href="#“Deep”-Visual-SLAM" class="headerlink" title="“Deep” Visual SLAM"></a>“Deep” Visual SLAM</h3><p><img src="./6.png" alt="Deep Visual SLAM"></p>
<ul>
<li>뉴럴넷을 사용해서 state estimation을 진행</li>
<li>기존의 방식이 가지던 한계점을 뉴럴넷이 극복할 수 있을 것이라는 희망</li>
<li>Semantic 정보를 추출해내려는 시도</li>
<li>현재 이러한 방식으로 풀려고 하는 문제들<ul>
<li>텍스처가 전혀 없는 곳에서 SLAM을 할 수 있을까?</li>
<li>High dynamic range 환경에서 SLAM을 할 수 있을까?</li>
<li>모션 블러가 있는 곳에서 SLAM을 할 수 있을까?</li>
<li>움직이는 객체가 많은 곳에서 SLAM을 할 수 있을까?</li>
</ul>
</li>
</ul>
<h3 id="Multi-Sensor-Fusion"><a href="#Multi-Sensor-Fusion" class="headerlink" title="Multi-Sensor Fusion"></a>Multi-Sensor Fusion</h3><p><img src="./7.png" alt="Multi sensor fusion"></p>
<ul>
<li>여러개의 센서를 사용<ul>
<li>많은 정보를 얻음</li>
<li>센서들마다 가진 단점을 서로 상쇄</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-성능을-측정하는-방법"><a href="#SLAM-성능을-측정하는-방법" class="headerlink" title="SLAM 성능을 측정하는 방법"></a>SLAM 성능을 측정하는 방법</h2><h3 id="데이터셋"><a href="#데이터셋" class="headerlink" title="데이터셋"></a>데이터셋</h3><p><img src="./8.png" alt="Datasets"></p>
<ul>
<li>KITTI, EuRoC, TUM 데이터셋은 SLAM 연구자들 사이에서 많이 쓰이는 데이터셋임.</li>
<li>하지만 위 데이터셋은 굉장히 단조로움<ul>
<li>환경, 라벨, 모션 패턴이 모두 단조로움.</li>
<li>다양한 데이터가 없기 때문에, 위 데이터셋으로 뉴럴넷을 학습할 경우 특정 컨디션에 오버핏 할 가능성이 높음.</li>
</ul>
</li>
<li>그러니 <span class="exturl" data-url="aHR0cHM6Ly90aGVhaXJsYWIub3JnL3RhcnRhbmFpci1kYXRhc2V0Lw==">Tartan Dataset<i class="fa fa-external-link-alt"></i></span>을 써라! (결국 본인 랩실 홍보 ㅋㅋ)<ul>
<li>Urban, rural, domestic, public, scifi 씬 등 20개 환경 구비</li>
<li>RGB, Depth, Segmentation, Flow, Pose 등 데이터 구비</li>
<li>Random translation / rotation의 500+ 모션 구비</li>
<li>빡센 환경 구비 (e.g. 연기, 안개, 어두운 밤 + 밝은 불빛, 비, 눈)</li>
</ul>
</li>
</ul>
<h3 id="측정-metric"><a href="#측정-metric" class="headerlink" title="측정 metric"></a>측정 metric</h3><ul>
<li><strong>Absolute Trajectory Error (ATE)</strong>와 <strong>Relative Pose Error (RPE)</strong> 기반으로 성능을 측정하는 경우가 많음.<ul>
<li>ATE와 RPE는 중간에 끊기는 데이터 (i.e. tracking lost)에 대응하지 못함.</li>
<li>Challenging scene에서는 끊기는 경우가 허다함.</li>
</ul>
</li>
<li>AirLab에서는 robustness에 대한 새로운 척도를 제안<ul>
<li>Valid Rate = Length of valid trajectory / Total length</li>
<li>Valid = Successfully initialised + not lose tracking<ul>
<li>‘Valid’ 척도는 사실 굉장히 주관적임…</li>
</ul>
</li>
<li>더 좋은 metric을 만들려고 연구중이라고 함.</li>
<li>어찌되었건 이 metric으로 봤었을 때, ORB-SLAM은 KITTI 벤치마크 기준 잘된다고 알려져있으나 Tartan Dataset에서는 완전 말아먹는 모습을 보임.</li>
</ul>
</li>
<li>TartanAir-V2를 만들고 있다고 함.<ul>
<li>더 많은 환경을 만들 예정.</li>
<li>IMU, LiDAR, Spherical, Fisheye, Event, Radar 등을 추가할 예정.</li>
<li>Dynamic object를 넣을 예정</li>
<li>GRound robot motion 패턴도 추가할 예정.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="TartanVO-Learning-based-Visual-Odometry"><a href="#TartanVO-Learning-based-Visual-Odometry" class="headerlink" title="TartanVO: Learning-based Visual Odometry"></a>TartanVO: Learning-based Visual Odometry</h2><p><img src="./9.png" alt="TartanVO"><br><img src="./10.png" alt="TartanVO"></p>
<ul>
<li>TartanAir 처럼 다양한 환경/모션 정보가 있는 데이터셋이 있으니, SLAM 뉴럴넷을 학습해볼 수 있겠다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9OUTFVRWgzdGhiVQ==">TartanVO<i class="fa fa-external-link-alt"></i></span></li>
<li>기존의 Feature detection / matching 단계를 하나의 뉴럴넷으로 대체</li>
<li>기존의 Motion estimation 과정을 deep optical flow + pose 뉴럴넷으로 대체</li>
</ul>
</li>
<li>극심한 모션 블러에도 실패하지 않음 (기존의 SLAM 방식은 tracking lost가 남)</li>
</ul>
<p> </p>
<hr>
<h2 id="Super-Odometry-IMU-centric-LiDAR-Visual-Inertial-Estimator-for-Challenging-Environments"><a href="#Super-Odometry-IMU-centric-LiDAR-Visual-Inertial-Estimator-for-Challenging-Environments" class="headerlink" title="Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments"></a>Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments</h2><p><img src="./11.png" alt="Super Odometry"></p>
<ul>
<li>Robustness를 높이는 방법 중 하나는 다양한 종류의 센서를 사용하는 것임.</li>
<li>Super Odometry는 IMU를 중심으로 카메라와 라이다 같은 센서들을 퓨전해서 사용함.<ul>
<li>왜 IMU 중심으로 사용하는가?<ul>
<li>카메라는 texture가 부족할 때 실패함</li>
<li>라이다는 geometry가 부족할 때 실패함</li>
<li>이에 반해 IMU는 전혀 외적 요인을 타지 않음</li>
</ul>
</li>
<li>특정 센서가 실패하는 상황이 올 때, 해당 센서로 부터 오는 정보는 사용하지 않고 잘 작동하고 있는 센서에서만 정보를 받아와서 다양한 상황에서도 안정적으로 위치추정을 할 수 있음.</li>
</ul>
</li>
<li>예시로, 2가지 상황을 보여줌<ul>
<li>평평한 벽이 좌우로 나있는 긴 복도에서는 라이다가 실패함. 하지만 카메라 정보로부터 odometry를 계속 받기때문에 잘 됨.</li>
<li>어두운 방에 들어가면 아무것도 보이지 않아 카메라가 실패함. 하지만 라이다 정보를 계속 받아 odometry를 잘 유지함.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Visual-localization"><a href="#Visual-localization" class="headerlink" title="Visual localization"></a>Visual localization</h2><ul>
<li>Visual localization = “2D 이미지만 가지고 3D 공간에서 내가 어디있는지 알 수 있을까?”<ul>
<li>3D 공간에 대한 2D appearance 정보는 전혀 없고, 3D 포인트 클라우드만 있을 때는 어떻게 할까?</li>
<li>비슷한 공간들에 대한 place recognition은 어떻게 할까?</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="2D-3D-Line-매칭-기반-Localization"><a href="#2D-3D-Line-매칭-기반-Localization" class="headerlink" title="2D-3D Line 매칭 기반 Localization"></a>2D-3D Line 매칭 기반 Localization</h3><p><img src="./12.png" alt="2D-3D Line Correspondence based localization"><br><img src="./13.png" alt="Line detector"></p>
<ul>
<li>3D LiDAR map에서 카메라로 어떻게 위치를 찾을 수 있을까?<ul>
<li>Cross modality!</li>
</ul>
</li>
<li>Edge 정보를 이용해서 카메라 이미지 상의 2D line과 포인트 클라우드 지도상의 3D line을 매칭해보자!<ul>
<li>이를 위해 새롭게 딥러닝 기반 line detector를 만들었다.<ul>
<li>어떠한 카메라에서도 잘 작동한다 (e.g. pinhole, fisheye, spherical cameras)</li>
<li>Intrinsic에 대해 전혀 영향을 받지 않는다.</li>
</ul>
</li>
</ul>
</li>
<li>기존의 SLAM/odometry 방식보다 훨씬 적은 drift를 가지면서 할 수 있다<ul>
<li>(당연할걸수도…? localization은 drift를 가질 수 없음)</li>
</ul>
</li>
<li>이 방식의 단점은, line feature는 실내처럼 사람이 만든 공간에서는 많이 나타나지만 자연공간에서는 많이 나타나지 않다는 점임.</li>
<li>참고논문:<ul>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS9wdWJsaWNhdGlvbnMvbGluZS1iYXNlZC0yZC0zZC1yZWdpc3RyYXRpb24tYW5kLWNhbWVyYS1sb2NhbGl6YXRpb24taW4tc3RydWN0dXJlZC1lbnZpcm9ubWVudHMv">Line-based 2D-3D registration and camera localization in structured envrionemnts<i class="fa fa-external-link-alt"></i></span></li>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDA3NDA=">Monocular camera localization in prior LiDAR Maps with 2D-3D correspondences<i class="fa fa-external-link-alt"></i></span></li>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMTEuMDMxNzQ=">ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="시점변화-조명변화-날씨변화에-강인한-descriptor-learning"><a href="#시점변화-조명변화-날씨변화에-강인한-descriptor-learning" class="headerlink" title="시점변화/조명변화/날씨변화에 강인한 descriptor learning"></a>시점변화/조명변화/날씨변화에 강인한 descriptor learning</h3><p><img src="./14.png" alt="Cross-domain (Image-to-LiDAR) descriptor learning module "></p>
<ul>
<li>자연공간에서는 어떤 방식으로 매칭을 해야할까?<ul>
<li>어떤 방식이 좋은지 모르겠다면… 그 방식을 딥러닝으로 학습하도록 하자.</li>
</ul>
</li>
<li>Visual-LiDAR 도메인 사이에서 사용될 수 있는 descriptor를 학습하였다.<ul>
<li>전체 네트워크는 matching module과 similarity module로 나눠진다.</li>
</ul>
</li>
<li>참고 논문:<ul>
<li>Yin 2019 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDIuMTAwNTg=">A multi-domain feature learning method for visual place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2020 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzNDE3Mjc=">SeqSphereVLAD: Sequence Matching Enhanced Orientation-invariant Place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2021 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzNjEzMTY=">FusionVLAD: A Multi-View Deep Fusion Networks for Viewpoint-Free 3D Place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2021 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMTI4ODM=">i3dLoc: Image-to-range Cross-domain Localization Robust t oinconsistent Environmental Conditions<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="Lifelong-Graph-Learning을-이용한-feature-matching-place-recognition"><a href="#Lifelong-Graph-Learning을-이용한-feature-matching-place-recognition" class="headerlink" title="Lifelong Graph Learning을 이용한 feature matching / place recognition"></a>Lifelong Graph Learning을 이용한 feature matching / place recognition</h3><p><img src="./15.png" alt="lifelong graph learning"></p>
<ul>
<li>SLAM에서 loop closure를 할 때 place recognition을 통해서 이전에 와본 장소로 왔다는것을 인식하고, local feature matching을 통해 localization을 한다. 이후 graph optimization을 한다.</li>
<li>이 때 사용되는 local feature extraction과 matching 과정을 lifelong graph learning을 이용해서 개선시킨다.<ul>
<li>Memory module을 통해 실시간으로 매칭을 개선한다</li>
<li>Memory replay를 통해 기억이 사라지는 것 (i.e. catastrophic forgetting)을 방지한다</li>
<li>참고논문: Wang 2021 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDkuMDA2NDc=">Lifelong Graph Learning<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Large-scale-sub-millimeter-mapping-with-lightweight-and-low-cost-weight-sensors"><a href="#Large-scale-sub-millimeter-mapping-with-lightweight-and-low-cost-weight-sensors" class="headerlink" title="Large scale sub-millimeter mapping (with lightweight and low-cost weight sensors)"></a>Large scale sub-millimeter mapping (with lightweight and low-cost weight sensors)</h2><p><img src="./16.png" alt="HARD"></p>
<ul>
<li>위 사진들과 같이 고해상도 SLAM이 작동하기 어려운 공간들이 있다.</li>
<li>위와 같은 환경에서 강인하게 SLAM을 작동하기 위해서 2가지 전략을 사용한다.<ol>
<li>최대한 Field of view (i.e. 시야 범위)를 넓힌다<ul>
<li>넓은 범위를 봄으로써 특정 공간에서 degenerate case가 나타나도 다른 시야 범위의 공간정보를 이용해서 실패를 방지한다.</li>
</ul>
</li>
<li>다양한 센서들을 퓨전해서 좋은 sparse model을 얻은 후, dense 모델로 재구성한다.<ul>
<li>카메라+라이다+IMU로 sparse reconstruction을 먼저 한다.</li>
<li>이후 stereo depth와 laser range projection을 퓨전해서 dense reconstruction을 수행한다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<p> </p>
<h3 id="카메라-라이다-IMU-퓨전"><a href="#카메라-라이다-IMU-퓨전" class="headerlink" title="카메라+라이다+IMU 퓨전"></a>카메라+라이다+IMU 퓨전</h3><p><img src="./17.png" alt="recon"><br><img src="./18.gif" alt="recon"></p>
<ul>
<li>많이 사용되는 COLMAP, OpenMVG와 같은 이미지 기반 reconstruction 기법에서는 제대로 scale을 측정하지 못하는 경우가 많다.</li>
<li>위의 연구는 joint camera-lidar optimisation을 통해 scale 정보까지 확실하게 추정할 수 있다.</li>
<li>참고자료:<ul>
<li>Zhen 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTEuMDMzNjk=">LiDAR-enhanced Structure-from-Motion<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2019 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDcuMDA5MzA=">A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3-D Reconstructions<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2019 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS93cC1jb250ZW50L3VwbG9hZHMvMjAxOS8wNi9yb290LnBkZg==">Estimating the localizability in tunnel-like environments using LiDAR and UWB<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2018 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MTAuMTI1MTU=">A Unified 3D Mapping Framework Using a 3D or 2D LiDAR<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2017 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS93cC1jb250ZW50L3VwbG9hZHMvMjAxNy8wNC9lc2tmLnBkZg==">Robust localization and localizability estimation with a rotating laser scanner<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="저해상도-이미지-기반-고해상도-dense-stereo-reconstruction"><a href="#저해상도-이미지-기반-고해상도-dense-stereo-reconstruction" class="headerlink" title="저해상도 이미지 기반 고해상도 dense stereo reconstruction"></a>저해상도 이미지 기반 고해상도 dense stereo reconstruction</h3><p><img src="./19.png" alt="ORStereo"><br><img src="./20.png" alt="ORStereo"></p>
<ul>
<li>Stereo reconstruction을 잘 하는 방법에는 ‘4K의 고해상도 카메라’ + ‘넓은 baseline’을 가지고 촬영하면 된다.<ul>
<li>이렇게 하면 sub-millimeter 정확도까지 얻어낼 수 있다.</li>
</ul>
</li>
<li>기존의 방식으로는 수천만개의 픽셀들에 대해 disparity 계산을 할 수 없다 (딥러닝을 쓰던 안쓰던 RAM을 너무 많이 차지할것이다)</li>
<li>제안하는 연구는, 기존의 4K 이미지를 다운샘플링해서 딥러닝 기반 방식으로 disparity와 occlusion을 구한 다음에, disparity를 4K로 업샘플링해서 기존의 4K 이미지와 부분적으로 비교하여 disparity와 occlusion을 얻어낸다.<ul>
<li>이런 방식으로 하면 기존의 저해상도 이미지에서 얻은 disparity 정보보다 훨씬 더 많은 디테일을 얻을 수 있다.</li>
<li>이 정교한 disparity 정보를 dense reconstruction에 사용하면, 위와 같이 굉장히 작은 균열 (sub-millimeter)도 검출할 수 있다.</li>
</ul>
</li>
<li>참고자료:<ul>
<li>Hu 2020 - <a href="">Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction</a></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Open-questions"><a href="#Open-questions" class="headerlink" title="Open questions"></a>Open questions</h2><ul>
<li>현재 SLAM을 기반으로 perception, planning, control이 이뤄지고 있는 로봇들이 많다.<ul>
<li>이러한 시스템에는 Metric SLAM이 필수적인데, 막상 연구개발 해보니까 이게 쉽지 않다.</li>
<li>Metric SLAM을 대체할 수 있는 것이 있을까?</li>
<li>만약에 SLAM이 우리가 생각했던대로 성공할 수 없는 기술이라면???</li>
</ul>
</li>
<li>‘강인함’을 재는 척도는 어떤게 좋을까?</li>
<li>맵의 품질은 어떻게 재야할까?</li>
<li>딥러닝을 SLAM에 진짜 쓸 수 있는걸까?<ul>
<li>Generalization은 어떻게 해야할까?</li>
<li>Online learning 방식이 좋을까? 아니면 시뮬레이션에서 더 학습하고 오는게 좋을까?<ul>
<li>시뮬레이션을 쓴다면 domain adaptation은?</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="참가자-질문"><a href="#참가자-질문" class="headerlink" title="참가자 질문"></a>참가자 질문</h2><ul>
<li>Ground truth는 어떻게 얻는가?<ul>
<li>실외: Survey scanner로 맵을 땀. GPS로 위치 추정</li>
<li>실내: Survey scanner / 라이다로 맵을 땀. 모션캡처로 위치 추정</li>
</ul>
</li>
<li>Edge 기기에서 잘 도는 SLAM 추천해주세요!<ul>
<li>무슨 센서를 쓰는지, 어떤 정보를 얻고싶은지에 따라 다르다. 하나의 SLAM으로 다 해먹을 수 없음.</li>
</ul>
</li>
<li>Visual place recognition과 SLAM의 접점은 어떻게 되는가?<ul>
<li>증강현실 등에서 robust localization은 중요하다 (i.e. global cooridinate를 알아야하는 경우)</li>
<li>3D map은 있는데, 2D 이미지 센서로 연결점을 만들어야할 때 중요하다.</li>
<li>Loop closure할 때도 중요하다.<ul>
<li>라스트마일 배달로봇, 맹인 가이드 로봇 등에 중요하다.</li>
</ul>
</li>
</ul>
</li>
<li>인턴십 뽑나요! (ㅋㅋ)<ul>
<li>여름 인턴십 프로그램이 있다 (학부생)</li>
</ul>
</li>
<li>슬램 성능은 어떻게 재나요?<ul>
<li>ATE, RPE</li>
<li>맵에 대한 정보는 조금 더 연구가 필요함.</li>
</ul>
</li>
<li>안개끼고 눈내리는 곳에서 센서는 뭘 써야할까요?<ul>
<li>Thermal camera + IMU + radar 좋은듯!</li>
</ul>
</li>
<li>Brownout effect에는 어떻게 대응하나요?<ul>
<li>(Brownout effect = 먼지가 엄청 많은 곳)</li>
<li>필터로 어느정도 제거해줄 수 있음.</li>
<li>하지만 너무 심한 경우에는 그냥 카메라/라이다 센서 데이터를 사용하지 않고 레이더만 사용한다던가</li>
<li>또는 특정 빛의 스펙트럼을 받는 카메라를 사용할 수 있음.</li>
</ul>
</li>
<li>SLAM에서 traditional / learning-based 중에 어떤걸 써야할까? (Prof. Kaess 질문)<ul>
<li>글쎄… 프론트엔드에서는 하나만 쓰는거보다는 엔지니어링 관점에서 섞어쓰면 어떨까 생각한다.<ul>
<li>피쳐가 있을 때는 traditional, 없을때는 learning-based를 쓰면 어떨까?</li>
</ul>
</li>
<li>백엔드에서는… 그건 당신이 더 전문가잖아! (ㅋㅋㅋ)<ul>
<li>기존의 bundle adjustment, pose graph optimisation, scan matching 등은 잘됨.<ul>
<li>지금 현재 방법에서 속도를 더 빠르게 하는 것 외로는 다른 어떤 방식이 있는지 잘 모르겠음.</li>
<li>현재 방식이 잘 안되는 경우는 대부분 ‘gaussian model assumption’에서 나오는 것이라고 봄.</li>
</ul>
</li>
<li>딥러닝 방식으로 시도를 하기도 했지만… 아직 인정해줄만큼 성과가 안나옴. 하지만 길게 보았을때 어떻게 될지는 모르겠음</li>
</ul>
</li>
</ul>
</li>
<li>센서퓨전을 할 때 어떤 방법을 써야하는가? ~~~한 케이스가 있는데, EKF를 써야할지 UKF를 써야할지 모르겠다.<ul>
<li>왠만한 경우에는 EKF도 잘 된다.</li>
<li>UKF를 쓰고 싶으면 ‘Unscented’ 파트가 진짜 필요한지 생각해봐야한다.<ul>
<li>Non-Gaussian인가?</li>
</ul>
</li>
</ul>
</li>
<li>Loosely-coupled vs Tightly-coupled의 차이가 무엇인가?<ul>
<li>Loosely-coupled: 여러개의 센서가 각각 odometry를 계산하고, 그걸 합친다.</li>
<li>Tightly-coupled: 여러개의 센서 값을 기반으로 하나의 odometry를 계산한다.</li>
</ul>
</li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20210604-tartan-kaess/" rel="bookmark">Tartan Series 2021 - Factor Graphs and Robust Perception (Prof. Michael Kaess)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20201227-cvpr2020-slam-malisiewicz/" rel="bookmark">CVPR 2020 - Deep Visual SLAM Frontends - SuperPoint, SuperGlue and SuperMaps (Tomasz Malisiewicz 발표)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20201228-cvpr2020-slam-yang/" rel="bookmark">CVPR 2020 - Visual SLAM with Object and Plane (Shichao Yang 발표)</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20201229-dec-2020-slam-news/" rel="bookmark">2020년 10~12월 SLAM 논문 소식</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/20210407-3dgv-andrew-davison/" rel="bookmark">3DGV 2021 - Representations and Computational Patterns in Spatial AI (Prof. Andrew Davison)</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/SLAM/" rel="tag"># SLAM</a>
              <a href="/tags/Visual-SLAM/" rel="tag"># Visual-SLAM</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Deep-SLAM/" rel="tag"># Deep SLAM</a>
              <a href="/tags/Tartan/" rel="tag"># Tartan</a>
              <a href="/tags/AirLab/" rel="tag"># AirLab</a>
              <a href="/tags/CMU/" rel="tag"># CMU</a>
              <a href="/tags/Sebastian-Scherer/" rel="tag"># Sebastian Scherer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/20210804-2021-may-slam-news/" rel="prev" title="2021년 5월 SLAM 뉴스">
                  <i class="fa fa-chevron-left"></i> 2021년 5월 SLAM 뉴스
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/20210530-icra-2021/" rel="next" title="ICRA 2021 튜토리얼 / 워크샵 리스트">
                  ICRA 2021 튜토리얼 / 워크샵 리스트 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments">
  <script src="https://utteranc.es/client.js" repo="changh95/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script>
  </div>
  
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cv-learn</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  

<script src="/js/local-search.js"></script>



<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
