<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Notion blog에서 Github blog로 옮기는 이유</title>
    <url>/20201124-Notion-blog-to-Github-blog/</url>
    <content><![CDATA[<p>최근 cv-learn 블로그를 쾌적한 환경으로 옮길 계획을 세웠습니다. 단순한 공부 노트에서 시작한 cv-learn인데, 이제는 어떻게 하면 좀 더 명쾌하게 컴퓨터 비전과 SLAM에 대한 정보를 전달할 수 있을지 고민을 하게 되더라구요. 우선 CV, SLAM, 그리고 cv-learn 블로그를 사랑해주신 많은 분들께 감사를 드립니다.</p>
<p>오늘은 과거부터 현재까지 cv-learn 블로그가 어떤 변화를 거쳐왔고, 또 블로그 이사를 위해 고민하고 직접 사용해봤던 다양한 플랫폼들을 비교해보도록 하겠습니다.</p>
</br>

<h1 id="0-태초에는…"><a href="#0-태초에는…" class="headerlink" title="0. 태초에는…"></a>0. 태초에는…</h1><hr>
<p>가장 초기의 cv-learn은 <span class="exturl" data-url="aHR0cDovL25vdGlvbi5zby8=">Notion.so<i class="fa fa-external-link-alt"></i></span> 의 기본 페이지 템플릿을 사용하였습니다. Notion을 사용해서 글을 적는 것은 굉장히 편합니다. WYSIWYG (What you see is what you get) - 바로바로 눈에 보이는 대로 적을 수 있기 때문에, 의식의 흐름이 끊기지 않고 적을 수 있다는 것이 Notion의 가장 큰 장점이라고 생각합니다. Notion의 위젯도 굉장히 이쁩니다.</p>
<p>하지만 Notion에는 큰 단점이 있습니다. 하나도 아니고 사실 몇개가 있습니다.</p>
<ul>
<li>첫번째로는 <strong><em>읽기 어려운 페이지 url</em></strong> 입니다. url만 봐서는 글의 내용이 전혀 추측이 되지 않습니다.</li>
</ul>
<img src="/20201124-Notion-blog-to-Github-blog/Untitled.png" class="" title="Notion url being ugly">

<ul>
<li>두번째로는, <strong><em>Notion 글들의 url 주소는 글의 제목이 바뀌면 새로운 url 주소를 가지게 되는 점</em></strong> 입니다. 글의 제목을 조금 수정하면 이전에 Facebook 등의 소셜 스크랩으로 공유했던 주소로는 해당 글에 접근할 수 없게 됩니다. 이 때문에 저는 글의 초안을 작성할 때 무조건 제목의 내용을 픽스해야했는데, 이 점이 굉장히 답답했습니다.</li>
<li>세번째로는, <strong><em>카테고리 탐색 및 글 검색 기능</em></strong>이 없습니다.</li>
<li>마지막으로, <strong><em>페이지 로딩이 굉장히 느립니다</em></strong>.</li>
</ul>
<p>이러한 단점들 때문에 다른 플랫폼으로 옮기는 것을 고려하게 되었습니다.</p>
</br>

<h1 id="1-노션-페이지의-진화-→-cv-learn-com"><a href="#1-노션-페이지의-진화-→-cv-learn-com" class="headerlink" title="1. 노션 페이지의 진화 → cv-learn.com"></a>1. 노션 페이지의 진화 → <span class="exturl" data-url="aHR0cDovL2N2LWxlYXJuLmNvbS8=">cv-learn.com<i class="fa fa-external-link-alt"></i></span></h1><hr>
<p>첫번째 대안으로는 <strong><em>Github pages</em></strong> 를 고려했습니다. Notion처럼 Inline maths도 되고, 카테고리 탐색 기능도 있고, url 주소도 마음대로 만들 수 있습니다. 하지만 저장소의 용량 제한이 있습니다. 이는 학회에서 촬영한 영상 업로드가 어렵다는 것을 의미합니다. </p>
<p>둘째는 <strong><em>Tistory 블로그</em></strong>였습니다. 컴퓨터 비전 블로그의 조상, 다크프로그래머님 블로그도 Tistory 블로그를 사용하죠, 저 역시 눈이 갈 수 밖에 없었습니다. 하지만 Notion 에서 다양한 블록들을 사용하다가 이쁜 블록이 없는 Markdown 형태로 적는 것은 생각보다 많이 불편했습니다.</p>
<p>셋째는 <strong><em>Notion 블로그</em></strong>를 호스팅하는 것이였습니다. 이 방법을 사용하면 Notion의 이쁜 블록 기능을 계속 사용하면서 빠르게 글을 적을 수 있었습니다. 호스팅을 통해 url 등의 문제 역시 해소할 수 있었습니다. 초기에는 Inline maths 기능이 없어서 불편했지만, 이 기능도 곧 Notion측에서 구현해줬습니다. </p>
<p>Notion 블로그 호스팅이 가장 좋은 옵션으로 보였고, cv-learn 블로그라는 이름으로 다시 태어나게 되었습니다. 아래 모습이 여러분이 기억하시는 cv-learn 블로그의 모습입니다.</p>
<img src="/20201124-Notion-blog-to-Github-blog/Untitled_4.png" class="" title="Original cv-learn">

<p>잠깐 다른 얘기를 하자면… 사실 cv-learn 블로그의 이름은 원래 cv-learn이 아니였습니다. 원래는 TorchVision 이라는 이름으로 시작했었죠. 하지만 이 이름은 호스팅을 위한 도메인을 찾을 때 크게 문제가 되었습니다. PyTorch에서 이미 TorchVision이라는 이름을 쓰고 있었습니다. 여러 고민을 하다가, 컴퓨터 비전을 공부하는 블로그이니, cv-learn이라고 짓기로 했습니다. </p>
<p><span class="exturl" data-url="aHR0cDovL2N2LWxlYXJuLmNvbS8=">http://cv-learn.com<i class="fa fa-external-link-alt"></i></span> 도메인을 사고 난 후, <strong><em>Cloudflare 호스팅 서비스를 통해 javascript 워커 할당을해서 노션 렌더링</em></strong>을 하였습니다. 이 도메인 아래에서 cv-learn 블로그를 운영하게 되었습니다. 많은 분들께서 감사하게도 cv-learn 블로그를 찾아주셨고, 어느새 최신 글이 한두달 가까이 나오지 않았음에도 한달 약 1500명 정도씩 꾸준하게 방문해주시는 블로그가 되었습니다.</p>
<img src="/20201124-Notion-blog-to-Github-blog/Untitled_1.png" class="" title="Cloudflare analytics - monthly">

</br> 

<h1 id="2-새로운-플랫폼으로의-이전"><a href="#2-새로운-플랫폼으로의-이전" class="headerlink" title="2. 새로운 플랫폼으로의 이전"></a>2. 새로운 플랫폼으로의 이전</h1><hr>
<p>하지만 Notion + Cloudflare 호스팅도 완벽하진 않았습니다.</p>
<p>우선 cv-learn 도메인을 사용하기 위해 비용을 지불해야 했습니다. 3년? 5년에 약 5~7만원 정도였던 것으로 기억합니다. Cloudflare는 무료 호스팅 플랜을 사용하고 있었는데, 이 방식으로는 하루에 약 10만건의 request 를 허용할 수 있었습니다. 초기에 저는 이 ‘request’를 접속 수로 이해했고, ‘내 블로그에 하루에 10만명의 접속자가 생길리가 없지’ 하던 참에 아래와 같은 이메일을 받았습니다.</p>
<img src="/20201124-Notion-blog-to-Github-blog/Untitled_2.png" class="" title="90% of daily request limit for Cloudflare Workers reached">


<br>

<p>???</p>
<p>왜???</p>
<p>사실 <strong><em>Notion 페이지는 실시간 업데이트를 유지하기 위해 누군가 접속중이라면 정기적으로 최소 몇백건씩 request를 날리는 구조</em></strong>였습니다. 실제로 10만명이 제 페이지에 접속해야 문제가 생기는 것이 아니였던 것입니다. 오히려, 지금처럼 하루에 ~100명정도 오시는 지금의 상황이 위태로운 상황이였습니다. 실제로, 새 글을 올렸을 때 Daily request limit을 넘긴 적도 있어서 블로그가 로딩이 되지 않는 경우도 있었습니다. </p>
<p>이 문제를 해결하는 간단한 솔루션은 Cloudflare의 유료 플랜을 통해 Daily request limit을 높이면 됩니다. 다만… 제가 이 블로그로 수익을 창출하는 것도 아닌데, 도메인 구매 + 유지에 돈을 쓰고 호스팅까지도 돈을 쓰기에는 너무 과한 것 같았습니다. 그리고 무엇보다 지인 분들의 멋진 기술 블로그를 보면 저처럼 돈을 따로 들이지 않으시더라구요. 나도 저렇게 할 수 있지 않을까,라는 생각이 들었습니다.</p>
<p>그렇게 새로운 플랫폼으로 두번째 블로그 이사 플랜을 세우게 되었고, 기왕에 이사가는거 커져버린 cv-learn을 좀 더 좋은 플랫폼에서 서빙하고 싶다는 생각에 이번에는 몇가지의 제약조건을 더 걸었습니다.</p>
<ol>
<li>무료 호스팅 + 빠른 로딩</li>
<li>cv-learn의 이름을 유지할 수 있는 플랫폼</li>
<li>LaTeX 입력 가능</li>
<li>Google 및 검색 엔진에서 노출 가능</li>
<li>댓글 기능 가능</li>
</ol>
<p>이 조건들을 충족할 수 있는 플랫폼들은 <strong><em>Tistory</em></strong>, <strong><em>Velog</em></strong>, <strong><em>Github-pages</em></strong>가 있었습니다. </p>
</br>

<h1 id="3-Tistory-vs-Velog-vs-Github-pages"><a href="#3-Tistory-vs-Velog-vs-Github-pages" class="headerlink" title="3. Tistory vs Velog vs Github-pages"></a>3. Tistory vs Velog vs Github-pages</h1><hr>
<p>Tistory를 가입하고, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JvbHRsZXNzZW5naW5lZXIvTm90aW9uMlRpc3Rvcnk=">Notion2Tistory<i class="fa fa-external-link-alt"></i></span> 라는 툴을 사용해봤습니다. 이 툴은 Notion에서 작성한 글을 특유의 블록 형태까지 그대로 Tistory로 복사할 수 있게 해주는 툴입니다. cv-learn에 작성한 글들을 그대로 Tistory에 복제하려고 해봤습니다만… 막상 옮기다보니 예전 내용을 조금 수정하고 싶은 생각이 들더라구요. 하지만 이 툴로는 한번 내용을 복제하고나면 Tistory 에디터에서 글 내용을 다시 수정하기 까다롭다는 단점이 있었습니다.</p>
<p>Velog의 경우, 지인이 사용을 권유하기도 했습니다. Velog는 우선 cv-learn 보다 로딩속도가 빨랐고, 그리고 Velog 플랫폼 내에서 내 글이 자동으로 홍보가 된다는 점도 좋았습니다. 개발자가 주된 플랫폼이라는 것도 좋았습니다. 하지만 Google에서 검색 노출이 거의 되지 않았습니다. 따로 SEO를 걸지 않은 cv-learn 블로그가 오히려 검색이 더 잘 되는 느낌이 들었습니다. 이 점 때문에 Velog를 선택하기가 꺼려졌습니다.</p>
<p>Github-pages가 대부분의 고민을 해결해줄 수 있었습니다. 무료 호스팅에, cv-learn 이름도 지킬 수 있고, 테마도 제 마음대로, LaTeX 입력도 쉽고, SEO 설정도 할 수 있습니다. 댓글은 Disqus나 다른 시스템을 연결하면 되었구요. 무엇보다, 제 주변에 멋지신 두분이 운영하시는 블로그들이 Github Pages 였기에 많은 아이디어들을 얻을 수 있었습니다 (항상 감사합니다 <span class="exturl" data-url="aHR0cHM6Ly9ob3lhMDEyLmdpdGh1Yi5pby8=">hoya012 블로그<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9qaW55b25namVvbmcuZ2l0aHViLmlvLw==">jinyongjeong 블로그<i class="fa fa-external-link-alt"></i></span>!).</p>
<p>하지만 Github-pages는 Tistory나 Velog 등에 비해 손이 많이 갑니다. 그냥 글을 쓰고 싶을 뿐인데, static site generator를 깔고 설정해야하고… 여간 귀찮은게 아닙니다. Static site generator에는 Jekyll, Hugo, Hexo, Gatsby 등등 다양한 시스템이 있었는데, 그 중 저는 Hexo를 우선 선택하였습니다. 별 다른 이유는 없습니다. Jekyll은 ruby를 쓴다는게 마음에 들지 않았고 (저는 ruby가 무슨 언어인지도 잘 모릅니다…), 그 다음에 골라본게 Hexo인데 생각보다 쉬워서 정착했습니다. Hugo가 더 빠르고 좋다고 하는데, 천천히 테스트해보고 바꿔야한다면 늦지 않게 바꿔볼 생각입니다. 어찌되었건, Hexo를 사용하면서 이쁜 블로그를 빠르게 셋업할 수 있었습니다.</p>
<p>현재까지의 블로그 구성은 매우 만족하고 있습니다. Github 아이디로 댓글을 달 수 있는 <span class="exturl" data-url="aHR0cHM6Ly91dHRlcmFuYy5lcy8=">플러그인<i class="fa fa-external-link-alt"></i></span>도 있고, 카테고리/태그 등으로 게시글을 구분할 수 있고, 속도도 굉장히 빠릅니다. LaTeX도 물론 입력 가능하고, 자동으로 SEO도 설정이 됬습니다.</p>
<br>

<h1 id="4-앞으로…"><a href="#4-앞으로…" class="headerlink" title="4. 앞으로…"></a>4. 앞으로…</h1><hr>
<p>2020년 11월 24일 현재 이 블로그는 cv-learn.com 에 연결되어있지 않습니다. 빠르게 글을 이전하고, 새로운 글도 적어가면서 어느정도 준비가 되었을 때 도메인을 이전하려고 합니다. 이번 글이 테크 블로그 개설을 고려하시는 분들께 도움이 됬길 바라고, 또 앞으로도 cv-learn 블로그에 많은 관심 부탁드립니다! </p>
<br>
<br>
게시글 공유:<span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9pbnRlbnQvdHdlZXQ/dGV4dD0lMjJOb3Rpb24lMjBibG9nJUVDJTk3JTkwJUVDJTg0JTlDJTIwR2l0aHViJTIwYmxvZyVFQiVBMSU5QyUyMCVFQyU5OCVBRSVFQSVCOCVCMCVFQiU4QSU5NCUyMCVFQyU5RCVCNCVFQyU5QyVBMCUyMiUyMGh0dHBzOi8vY2hhbmdoOTUuZ2l0aHViLmlvLzIwMjAxMTI0LU5vdGlvbi1ibG9nLXRvLUdpdGh1Yi1ibG9nLyUyMHZpYSUyMEBoZXhvanM=">Twitter<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL3NoYXJlci9zaGFyZXIucGhwP3U9aHR0cHM6Ly9jaGFuZ2g5NS5naXRodWIuaW8vMjAyMDExMjQtTm90aW9uLWJsb2ctdG8tR2l0aHViLWJsb2cvJnQ9Tm90aW9uJTIwYmxvZyVFQyU5NyU5MCVFQyU4NCU5QyUyMEdpdGh1YiUyMGJsb2clRUIlQTElOUMlMjAlRUMlOTglQUUlRUElQjglQjAlRUIlOEElOTQlMjAlRUMlOUQlQjQlRUMlOUMlQTA=">Facebook<i class="fa fa-external-link-alt"></i></span>, and <span class="exturl" data-url="aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL3NoYXJpbmcvc2hhcmUtb2Zmc2l0ZS8/dXJsPWh0dHBzOi8vY2hhbmdoOTUuZ2l0aHViLmlvLzIwMjAxMTI0LU5vdGlvbi1ibG9nLXRvLUdpdGh1Yi1ibG9nLyZ0aXRsZT1Ob3Rpb24lMjBibG9nJUVDJTk3JTkwJUVDJTg0JTlDJTIwR2l0aHViJTIwYmxvZyVFQiVBMSU5QyUyMCVFQyU5OCVBRSVFQSVCOCVCMCVFQiU4QSU5NCUyMCVFQyU5RCVCNCVFQyU5QyVBMA==">LinkedIn<i class="fa fa-external-link-alt"></i></span>]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.3 Blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>tech</tag>
        <tag>Notion</tag>
        <tag>gh-pages</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 선형 자료 구조</title>
    <url>/20201125-cpp-linear-structures/</url>
    <content><![CDATA[<h2 id="Contiguous-vs-Non-contiguous-data-structures"><a href="#Contiguous-vs-Non-contiguous-data-structures" class="headerlink" title="Contiguous vs Non-contiguous data structures"></a>Contiguous vs Non-contiguous data structures</h2><p><img src="https://examradar.com/wp-content/uploads/2016/09/Contiguous-and-Non-contiguous-structures.gif" alt="contiguous vs linked data structure"></p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9leGFtcmFkYXIuY29tL2ludHJvZHVjdGlvbi1kYXRhLXN0cnVjdHVyZXMv">이미지 출처<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h3 id="Continguous-data-structure"><a href="#Continguous-data-structure" class="headerlink" title="Continguous data structure"></a>Continguous data structure</h3><ul>
<li><code>std::array</code>, <code>std::vector</code> …</li>
<li>모든 데이터 원소가 연속된 형태로 저장됨.<ul>
<li>첫번째 데이터 원소의 메모리 주소를 Base address라고 함.</li>
</ul>
</li>
<li>데이터 원소에 곧바로 접근 가능 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>).<ul>
<li><code>BA + i * sizeof(type)</code></li>
</ul>
</li>
<li><strong>Cache locality</strong>: 데이터 원소에 접근할 때 주변 원소들도 함께 cache로 가져옴.<ul>
<li>메모리에서 접근하는 것 보다 <strong>cache에서 접근하는 것이 몇십~몇백배 더 빠름</strong>.</li>
</ul>
</li>
<li>Static 형태와 Dynamic 형태로 할당 가능.<ul>
<li>Static: Stack에 할당<ul>
<li><code>int arr[size]</code></li>
</ul>
</li>
<li>Dynamic: Heap에 할당<ul>
<li><code>int* arr = new int[size]</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Non-contiguous-data-structure"><a href="#Non-contiguous-data-structure" class="headerlink" title="Non-contiguous data structure"></a>Non-contiguous data structure</h3><ul>
<li><code>std::list</code>, <code>std::forward_list</code>…</li>
<li>여러 다른 위치에 데이터가 저장됨.</li>
<li><strong>Linked list</strong> (위 이미지의 오른쪽에 그려진 자료구조)<ul>
<li>여러 node로 구성됨.</li>
<li>각각의 node는 데이터 + 다음 node를 가리키는 pointer로 구성됨.</li>
<li>데이터 원소에 접근하려면 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이 필요함.<ul>
<li>head -&gt; node -&gt; node -&gt; node…를 통해 i번 이동해야함.</li>
</ul>
</li>
<li><strong>데이터 원소의 삽입/삭제</strong>가 빠름 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>)</li>
<li>Cache locality를 기대할 수 없음.<ul>
<li>데이터 원소가 연속되어있지 않기 때문.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Contiguous</th>
<th align="center">Non-contiguous</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Random element access</td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
</tr>
<tr>
<td align="center">push_back()</td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
</tr>
<tr>
<td align="center">insert()</td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
<td align="center"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></td>
</tr>
<tr>
<td align="center">cache locality</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Data structures</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>Heap</title>
    <url>/20201127-heap/</url>
    <content><![CDATA[<h2 id="Heap의-조건"><a href="#Heap의-조건" class="headerlink" title="Heap의 조건"></a>Heap의 조건</h2><ul>
<li>(Max heap인 경우)</li>
<li>Maximum 값에 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 접근</li>
<li>Maximum 값 삭제에 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.106ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4467 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(3689, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4078, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li><code>push()</code>에 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.106ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4467 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(3689, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4078, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
<p><img src="https://web.cecs.pdx.edu/~sheard/course/Cs163/Graphics/CompleteBinary.jpg"></p>
<ul>
<li>위의 조건을 맞추기 위해서는 complete binary tree를 사용해야함.<ul>
<li>마지막 level의 node를 제외하고는 모두 2개의 child node가 있어야함.</li>
</ul>
</li>
<li>Complete binary tree를 사용하면 데이터 값 접근에 용이.</li>
</ul>
<h2 id="push"><a href="#push" class="headerlink" title="push()"></a>push()</h2><ul>
<li>새 데이터 node는 맨 끝에 추가하면 됨.</li>
<li>모든 node는 child node보다 더 큰 값을 가져야함.<ul>
<li>새로 데이터 node가 추가될 때, 부모 node와 값을 비교해서 필요하다면 swap 작업을 거침.</li>
</ul>
</li>
<li>Complete balanced tree의 높이는 최대 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.62ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2926 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(1260, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1649, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(2537, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이므로 시간 복잡도는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.106ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4467 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(3689, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4078, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이 됨.</li>
</ul>
<h2 id="pop"><a href="#pop" class="headerlink" title="pop()"></a>pop()</h2><img src="https://www.techiedelight.com/wp-content/uploads/2016/11/Pop-min-heap.png" alt="drawing" width="600">

<ul>
<li>Root node 값만 삭제할 수 있음.</li>
<li>Root node를 삭제하면 그 위치가 비어버림.<ul>
<li>그렇기 때문에, root node와 마지막 node의 값을 교환한 뒤, 마지막 node (기존의 root node)를 삭제함.</li>
<li>그 후 새로 root node가 된 값을 child node와 비교해서 heap의 조건을 맞춤 (i.e. 모든 node는 child node보다 더 큰 값을 가져야함)</li>
</ul>
</li>
</ul>
<h2 id="std-make-heap"><a href="#std-make-heap" class="headerlink" title="std::make_heap"></a>std::make_heap</h2><ul>
<li><code>std::vector</code> 또는 다른 배열들을 사용할 때 <code>std::make_heap</code>를 사용해서 곧바로 heap을 구현할 수 있음.</li>
<li>이는 <code>std::priority_queue</code>가 아닌 이미 <code>std::vector</code> 구조체를 사용할 때 유용함.</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Data structures</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 비선형 자료 구조</title>
    <url>/20201127-nonlinear-data-structure/</url>
    <content><![CDATA[<h2 id="Nonlinear-data-structure"><a href="#Nonlinear-data-structure" class="headerlink" title="Nonlinear data structure"></a>Nonlinear data structure</h2><ul>
<li><code>std::vector</code>, <code>std::array</code>, <code>std::forward_list</code>, <code>std::list</code>, <code>std::deque</code> 등과 같은 linear data structure에서는 iterator를 통해 빠르게 데이터를 훑을 수 있었다.</li>
<li>비선형 자료구조는 좀 더 복잡한 형태의 데이터를 다룬다.</li>
</ul>
<h2 id="Nonlinearity"><a href="#Nonlinearity" class="headerlink" title="Nonlinearity"></a>Nonlinearity</h2><ul>
<li>Hierarchical problem<ul>
<li>각각의 데이터가 계층에 대한 종속 관계를 가진다면 어떻게 할 것인가?</li>
<li>e.g. 조직도</li>
</ul>
</li>
</ul>
<p><img src="https://t1.daumcdn.net/cfile/tistory/99A91A3E5D404C2410"></p>
<ul>
<li>Cyclic dependency<ul>
<li>데이터 끼리의 관계도를 그릴 때 순환되어 돌아오는 구조라면?</li>
</ul>
</li>
</ul>
<p><img src="https://www.researchgate.net/profile/Alberto-Paoluzzi/publication/2402984/figure/fig4/AS:667857369186306@1536241013277/Directed-a-cyclic-graph-used-to-represent-a-set-of-coordinated-activities-Labels-of-arcs.png"></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Data structures</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>Tree (Tree란? Binary Search Tree란? Balanced tree란? - Red-black tree)</title>
    <url>/20201127-tree/</url>
    <content><![CDATA[<h1 id="Tree-소개"><a href="#Tree-소개" class="headerlink" title="Tree 소개"></a>Tree 소개</h1><p> </p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><p><img src="https://www.softwaretestinghelp.com/wp-content/qa/uploads/2019/08/1-tree-and-its-various-parts.png"></p>
<ul>
<li><strong>Root node</strong>: 가장 상단의 node이며 parent node가 없음.</li>
<li><strong>Leaf node</strong>: 가장 하단에 위치한 node들이며 child node가 없음.</li>
<li><strong>Subtree</strong>: 중간 단계에서 특정 node 이하의 모든 node를 포함한 트리를 subtree라고 부름.</li>
<li><strong>Parent node</strong>: Root node를 제외한 node 들 중 child node를 가지고 있는 node들. </li>
<li><strong>Ancestor Node</strong>: Root node와 어떤 특정 node 사이에 끼어있는 node들. </li>
<li><strong>Key</strong>: node가 가지고 있는 값.</li>
<li><strong>Level</strong>: node의 층. Root node는 항상 level 1, 그 다음 층부터 ++.</li>
</ul>
<h2 id="트리-순회"><a href="#트리-순회" class="headerlink" title="트리 순회"></a>트리 순회</h2><ul>
<li>Pre-order traversal<ul>
<li>현재 node를 먼저 방문. 그 다음 왼쪽 하위 node를, 그 후 오른쪽 하위 node를 재귀적으로 방문.</li>
</ul>
</li>
<li>In-order traversal<ul>
<li>왼쪽 node를 먼저 방문, 그 다음 현재 node, 그 후 오른쪽 node 방문.</li>
</ul>
</li>
<li>Post-order traversal<ul>
<li>두 자식 node를 먼저 방문, 그 후 현재 node 방문.</li>
</ul>
</li>
<li>Level-order traversal<ul>
<li>트리의 맨 위 레벨부터 아래 레벨까지, 왼쪽에서 오른쪽 노드 순서로 방문.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="BST-Binary-Search-Tree"><a href="#BST-Binary-Search-Tree" class="headerlink" title="BST (Binary Search Tree)"></a>BST (Binary Search Tree)</h1><h2 id="BST-특징"><a href="#BST-특징" class="headerlink" title="BST 특징"></a>BST 특징</h2><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Binary_search_tree.svg/1280px-Binary_search_tree.svg.png" alt="drawing" width="500">

<p><img src=""></p>
<ul>
<li>부모 노드의 값 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.312ex" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.751ex" role="img" focusable="false" viewBox="0 -636 778 774"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g></g></g></svg></mjx-container> 왼쪽 자식 노드의 값<ul>
<li>부모 노드보다 작거나 같은 모든 데이터 원소는 항상 왼쪽에 위치</li>
</ul>
</li>
<li>부모 노드의 값 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.312ex" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.751ex" role="img" focusable="false" viewBox="0 -636 778 774"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g></g></g></svg></mjx-container> 오른쪽 자식 노드의 값<ul>
<li>부모 노드보다 크거나 같은 원소는 항상 오른쪽에 위치</li>
</ul>
</li>
<li>원소 검색을 할 때, 각 level마다 검색 범위가 절반으로 줄어듬.<ul>
<li>즉, 트리의 모든 원소를 방문하지 않아도 됨.</li>
</ul>
</li>
<li>트리의 높이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.533ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3329.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(783, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(477, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1663.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2052.6, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(2940.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>데이터 노드 검색 또는 삽입 동작 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.106ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4467 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(3689, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4078, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
<h2 id="BST-원소-삽입"><a href="#BST-원소-삽입" class="headerlink" title="BST 원소 삽입"></a>BST 원소 삽입</h2><ul>
<li>그냥 downstream으로 추가하면 됨</li>
<li><img src="/20201127-tree/add.png" class="" title="add">

</li>
</ul>
<h2 id="BST-원소-삭제"><a href="#BST-원소-삭제" class="headerlink" title="BST 원소 삭제"></a>BST 원소 삭제</h2><ul>
<li>node 삭제 후에도 BST 속성을 만족하도록 기존의 하위단 node를 끌어올려와야함.</li>
<li>삭제할 node에게<ul>
<li>child node가 없으면 그냥 지움</li>
<li>child node가 1개면 자리를 계승시킴.</li>
<li>child node가 2개면 현재 삭제하는 node보다 더 큰 숫자의 node가 자리를 계승함.</li>
</ul>
</li>
</ul>
<img src="/20201127-tree/remove_1.png" class="" title="remove">
<img src="/20201127-tree/remove_2.png" class="" title="remove2">
<img src="/20201127-tree/remove_3.png" class="" title="remove3">

<p> </p>
<hr>
<h1 id="Balanced-tree-AVL-red-black-tree"><a href="#Balanced-tree-AVL-red-black-tree" class="headerlink" title="Balanced tree (AVL, red-black tree)"></a>Balanced tree (AVL, red-black tree)</h1><h2 id="BST-의-단점"><a href="#BST-의-단점" class="headerlink" title="BST 의 단점"></a>BST 의 단점</h2><p><img src="https://qph.fs.quoracdn.net/main-qimg-eccb472654af69385afe691e9958a713.webp"></p>
<ul>
<li>데이터가 sort된 상태에서 가장 작은 숫자를 root node로 만들고, 그 후 작은 순서대로 insert를 하게 된다면, 한쪽으로 치우쳐진 tree가 생길 것이다.<ul>
<li>비슷한 예제로, 역방향 sort + 가장 큰 숫자를 root node로 만듬 + 큰 순서대로 insert를 하면 역시 또 한쪽으로 치우쳐진 tree가 생긴다.</li>
</ul>
</li>
<li><code>find()</code>함수의 시간 복잡도는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.824ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4342 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(1728, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2194, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2539, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(3016, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(3592, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(3953, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이다.</li>
<li>그렇기에 tree의 높이는 최적화 되야한다.<ul>
<li>이 작업을 ‘balancing’이라고 한다.</li>
<li>이 작업을 자동으로 하면 ‘self-balancing’이 된다.</li>
</ul>
</li>
<li>self-balancing tree의 종류에는 AVL tree와 red-black tree가 있다.<ul>
<li><code>std::map</code>이 red-black tree 형태로 구현되어있다.</li>
<li>왜 red-black tree로 되어있을까?<ul>
<li>AVL tree의 rebalancing rotation은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.455ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4179 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3401, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3790, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>Red-black tree의 rebalancing rotation은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNTI4ODMyMC93aHktaXMtc3RkbWFwLWltcGxlbWVudGVkLWFzLWEtcmVkLWJsYWNrLXRyZWU=">“Why is std::map implemented as a red-black tree?”<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Data structures</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (0) - Const reference를 쓰세요!</title>
    <url>/20201203-faconti-Value-semantics-vs-references/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Value Semantics vs references”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9wcmVmZXJfcmVmZXJlbmNlcy8=">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h2 id="Pass-by-Value"><a href="#Pass-by-Value" class="headerlink" title="Pass by Value"></a>Pass by Value</h2><hr>
<p>이번 글의 내용은 사실 어느정도 C++ 개발을 해보신 분들은 이미 잘 아는 내용일 것이라고 믿습니다.</p>
<p>그래도 저는 종종 이런 코드를 짜는 사람들을 봅니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">OpenFile</span><span class="params">(str::<span class="built_in">string</span> filename)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DrawPath</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Points&gt; path)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Pose <span class="title">DetectFace</span><span class="params">(Image image)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Matrix3D <span class="title">Rotate</span><span class="params">(Matrix3D mat, AxisAngle axis_angle)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>위의 함수들은 제가 방금 만들어낸 것이긴 하지만, 실제로 제품으로 배포되는 코드에서도 종종 보이는 코드들입니다. </p>
<p>위 코드들은 모두 함수 인자를 <strong><em>passing by value</em></strong> 형태로 넘깁니다.</p>
<p>즉, 이 함수를 호출할 때 마다 함수 인자들은 우선 복사가 되고, 그 후에 함수로 넘겨집니다.</p>
<img src="/20201203-faconti-Value-semantics-vs-references/why_copy.jpg" class="" title="왜 복사하는건데 대체">

<p>복사 작업은 종종 굉장히 무거운 작업이 될 수 있습니다. 작은 객체를 복사할 때는 그리 무겁지 않지만, 엄청 큰 객체를 복사하면 무거울 수 있죠. 동적 heap memory allocation이 들어간다면 더 무거울 수도 있겠습니다. </p>
<p>위의 예제들 중 아마 무시할 수 있을 정도의 굉장히 작은 양의 overhead가 나타나는 객체도 있습니다. <code>Matrix3D</code> 나 <code>AngleAxis</code> 같은 객체는 heap allocation이 없이도 복사가 됩니다. </p>
<p>하지만 overhead가 작아도, 굳이 CPU 사이클을 이런 의미없는 복사에 낭비할 필요가 있을까요? 복사를 안하면 되지 않나요?</p>
<p>아래 예제를 한번 봅시다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">OpenFile</span><span class="params">(<span class="keyword">const</span> str::<span class="built_in">string</span>&amp; filename)</span></span>; <span class="comment">// string_view is even better</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DrawPath</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Points&gt;&amp; path)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Pose <span class="title">DetectFace</span><span class="params">(<span class="keyword">const</span> Image&amp; image)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Matrix3D <span class="title">Rotate</span><span class="params">(<span class="keyword">const</span> Matrix3D&amp; mat, <span class="keyword">const</span> AxisAngle&amp; axis_angle)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>이번 예제에서 우리는 <strong><em>reference semantic</em></strong>이라는 기법을 사용했습니다.</p>
<p>이 방법 대신에 <strong><em>C-style</em></strong>의 포인터를 사용해서 같은 결과를 얻을 수도 있겠습니다. 다만 포인터와 비교했을 때 reference semantic 기법이 다른 점이라면, reference semantic은 컴파일러에 다음과 같은 의미를 전달합니다.</p>
<ul>
<li>함수 인자들이 <strong>Constant</strong> 하다는 점. 함수를 호출한 쪽에서도, 함수 내부에서도 이 인자 값을 수정하지 않겠다는 점을 명시합니다. </li>
<li>함수 인자들이 <strong>Reference</strong>라는 점. 이미 값이 존재하는 인자들을 ‘refer’ 하는 것입니다. C-style 포인터는 <code>nullptr</code> 가 값이 될 수 있지만, reference인 경우는 안되겠네요.</li>
<li>포인터를 사용하는게 아니니, 우리는 이 객체의 <strong>ownership</strong>을 넘겨주는게 아닙니다. </li>
</ul>
<p>어쨋든 이 방법을 사용하면 아래에서 보는 것 처럼 계산량이 엄청나게 줄어듭니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">GetSpaces_Value</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> spaces = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">char</span> c: str)&#123;</span><br><span class="line">        <span class="keyword">if</span>( c == <span class="string">&#x27; &#x27;</span>) spaces++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> spaces;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">GetSpaces_Ref</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> spaces = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">char</span> c: str)&#123;</span><br><span class="line">        <span class="keyword">if</span>( c == <span class="string">&#x27; &#x27;</span>) spaces++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> spaces;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">LONG_STR</span><span class="params">(<span class="string">&quot;a long string that can&#x27;t use Small String Optimization&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PassStringByValue</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        <span class="keyword">size_t</span> n = GetSpaces_Value(LONG_STR);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PassStringByRef</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        <span class="keyword">size_t</span> n = GetSpaces_Ref(LONG_STR);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//----------------------------------</span></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">Sum_Value</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span>&gt; vect)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> val: vect) &#123; sum += val; &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">Sum_Ref</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span>&gt;&amp; vect)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> val: vect) &#123; sum += val; &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span>&gt; vect_in = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PassVectorByValue</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        <span class="keyword">size_t</span> n = Sum_Value(vect_in);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PassVectorByRef</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        <span class="keyword">size_t</span> n = Sum_Ref(vect_in);</span><br><span class="line">        benchmark::DoNotOptimize(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src="/20201203-faconti-Value-semantics-vs-references/const_reference.png" class="" title="const reference가 최고입니다">


<p>명백하게, <strong><em>passing by reference의 성능이 더 좋습니다</em></strong>.</p>
<br>
<br>

<h2 id="Const-reference를-쓰면-안되는-예시"><a href="#Const-reference를-쓰면-안되는-예시" class="headerlink" title="Const reference를 쓰면 안되는 예시"></a>Const reference를 쓰면 안되는 예시</h2><blockquote>
<p>“오옹… 앞으로는 <code>const&amp;</code>로 내 코드를 도배해야겠다~”</p>
</blockquote>
<p>이런 생각을 하기 전에, 다음 예제도 한번 봅시다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Vector3D</span>&#123;</span></span><br><span class="line">    <span class="keyword">double</span> x;</span><br><span class="line">    <span class="keyword">double</span> y;</span><br><span class="line">    <span class="keyword">double</span> z;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">Vector3D <span class="title">MultiplyByTwo_Value</span><span class="params">(Vector3D p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> &#123; p.x*<span class="number">2</span>, p.y*<span class="number">2</span>, p.z*<span class="number">2</span> &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Vector3D <span class="title">MultiplyByTwo_Ref</span><span class="params">(<span class="keyword">const</span> Vector3D&amp; p)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> &#123; p.x*<span class="number">2</span>, p.y*<span class="number">2</span>, p.z*<span class="number">2</span> &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MultiplyVector_Value</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    Vector3D in = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        Vector3D out = MultiplyByTwo_Value(in);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MultiplyVector_Ref</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">    Vector3D in = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">        Vector3D out = MultiplyByTwo_Ref(in);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="/20201203-faconti-Value-semantics-vs-references/multiply_vector.png" class="" title="const&amp; 써도 차이없음">

<p>의외로, 이번에는 <code>const&amp;</code>를 써도 아무 차이가 없습니다.</p>
<p>우리가 heap allocation이 필요없을만큼 작은 (그냥 몇 바이트 정도로 작은 데이터) 객체를 복사하는거면, passing by reference를 써도 거의 차이가 없습니다. 하지만 그렇다고 더 느려지는것도 아니죠. 그러니까 <code>const&amp;</code>를 써야할지 말지 모르겠을때는, 일단 써보면 좋을 것 같습니다. </p>
<p>다만 문제가 되는 경우도 있습니다.</p>
<p>Primitive types를 const reference로 복사할 때는 컴파일러가 추가적인 instruction을 더 만든다고 하지만 (여기서 테스트 해봤습니다 <span class="exturl" data-url="aHR0cHM6Ly9nb2Rib2x0Lm9yZy96Ly1ydXNhYg==">https://godbolt.org/z/-rusab<i class="fa fa-external-link-alt"></i></span>), 하지만 컴파일 옵션에 <code>-O3</code> 를 추가하면 이 문제 역시 해결됩니다. 하지만 매번 이 옵션을 넣어주긴 귀찮으니, <strong><em>저는 보통 ‘8 바이트 정도 되는 인자, 또는 그것보다 작은 인자 (e.g. int, double, chars, long)`은 절대로 pass by reference를 쓰지 않습니다</em></strong>. Primitive type에 const, &amp; 등을 사용하면, 성능 향상은 하나도 되지 않으면서, 동시에 코드가 더 헷갈려보이고 못생겨집니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">YouAreTryingTooHardDude</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span>&amp; a, <span class="keyword">const</span> <span class="keyword">double</span>&amp; b)</span></span>; <span class="comment">// 이건 하지 맙시다</span></span><br></pre></td></tr></table></figure>

<br>
<br>
게시글 공유:<span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9pbnRlbnQvdHdlZXQ/dGV4dD0lMjIoJUVCJUIyJTg4JUVDJTk3JUFEKSUyMCVFQyU4QiVBNCVFQyU4QiU5QyVFQSVCMCU4NCUyMCVFQyVCQiVCNCVFRCU5MyVBOCVFRCU4NCVCMCUyMCVFQiVCOSU4NCVFQyVBMCU4NCVFQyU5RCU4NCUyMCVFQyU5QyU4NCVFRCU5NSU5QyUyMEMrKyUyMCVFQyVCNSU5QyVFQyVBMCU4MSVFRCU5OSU5NCUyMCgwKSUyMC0lMjBDb25zdCUyMHJlZmVyZW5jZSVFQiVBNSVCQyUyMCVFQyU5MyVCMCVFQyU4NCVCOCVFQyU5QSU5NCElMjIlMjBodHRwczovL2NoYW5naDk1LmdpdGh1Yi5pby8yMDIwMTIwMy1mYWNvbnRpLVZhbHVlLXNlbWFudGljcy12cy1yZWZlcmVuY2VzLyUyMHZpYSUyMEBoZXhvanM=">Twitter<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL3NoYXJlci9zaGFyZXIucGhwP3U9aHR0cHM6Ly9jaGFuZ2g5NS5naXRodWIuaW8vMjAyMDEyMDMtZmFjb250aS1WYWx1ZS1zZW1hbnRpY3MtdnMtcmVmZXJlbmNlcy8mdD0oJUVCJUIyJTg4JUVDJTk3JUFEKSUyMCVFQyU4QiVBNCVFQyU4QiU5QyVFQSVCMCU4NCUyMCVFQyVCQiVCNCVFRCU5MyVBOCVFRCU4NCVCMCUyMCVFQiVCOSU4NCVFQyVBMCU4NCVFQyU5RCU4NCUyMCVFQyU5QyU4NCVFRCU5NSU5QyUyMEMrKyUyMCVFQyVCNSU5QyVFQyVBMCU4MSVFRCU5OSU5NCUyMCgwKSUyMC0lMjBDb25zdCUyMHJlZmVyZW5jZSVFQiVBNSVCQyUyMCVFQyU5MyVCMCVFQyU4NCVCOCVFQyU5QSU5NCE=">Facebook<i class="fa fa-external-link-alt"></i></span>, and <span class="exturl" data-url="aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL3NoYXJpbmcvc2hhcmUtb2Zmc2l0ZS8/dXJsPWh0dHBzOi8vY2hhbmdoOTUuZ2l0aHViLmlvLzIwMjAxMjAzLWZhY29udGktVmFsdWUtc2VtYW50aWNzLXZzLXJlZmVyZW5jZXMvJnRpdGxlPSglRUIlQjIlODglRUMlOTclQUQpJTIwJUVDJThCJUE0JUVDJThCJTlDJUVBJUIwJTg0JTIwJUVDJUJCJUI0JUVEJTkzJUE4JUVEJTg0JUIwJTIwJUVCJUI5JTg0JUVDJUEwJTg0JUVDJTlEJTg0JTIwJUVDJTlDJTg0JUVEJTk1JTlDJTIwQysrJTIwJUVDJUI1JTlDJUVDJUEwJTgxJUVEJTk5JTk0JTIwKDApJTIwLSUyMENvbnN0JTIwcmVmZXJlbmNlJUVCJUE1JUJDJTIwJUVDJTkzJUIwJUVDJTg0JUI4JUVDJTlBJTk0IQ==">LinkedIn<i class="fa fa-external-link-alt"></i></span>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (2) - Linked List 쓰지 마세요!</title>
    <url>/20201205-faconti-do-not-use-linked-list/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “I have learnt linked-lists at university, should I use them?” Nooope.”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9ub19saXN0cy8=">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<p>역자 코멘트: 원작자의 요청으로 최대한 원글의 뉘앙스를 그대로 옮겨왔습니다. 저는 <code>std::list</code>가 글에서 표현하는 것 처럼 쓸모없다고 생각하지는 않습니다. 그랬다면 이미 C++ 언어에서 사라졌겠지요 ㅎㅎ… 적시적기에 사용한다면 <code>std::list</code> 역시 타 자료구조보다 좋은 성능을 내겠습니다. 하지만, 실시간 컴퓨터 비전의 용도에서 그런 상황이 자주 나타나지 않는 점에도 동의합니다. 이 점 유의하시고 읽어주시길 부탁드립니다 :)</p>
<br>

<h1 id="std-list를-쓰고-계신다면-당장-그만두세요"><a href="#std-list를-쓰고-계신다면-당장-그만두세요" class="headerlink" title="std::list를 쓰고 계신다면, 당장 그만두세요."></a><code>std::list</code>를 쓰고 계신다면, 당장 그만두세요.</h1><hr>
<img src="/20201205-faconti-do-not-use-linked-list/linked_list.png" class="" title="Linked List를 왜 씁니까?">

<p>사실 linked list의 효율성에 대한 벤치마크는 이미 이전에도 다른 사람들이 많이 했었습니다. 제가 똑같은 벤치마크를 다시 돌릴 필요는 없다고 봅니다.</p>
<ul>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9iYXB0aXN0ZS13aWNodC5jb20vcG9zdHMvMjAxMi8xMS9jcHAtYmVuY2htYXJrLXZlY3Rvci12cy1saXN0Lmh0bWw=">std::vector vs std::list benchmark<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>(역자 코멘트: 위의 링크는 잘못 첨부된 그래프가 몇개 포함되어있습니다. 가장 중요한 random remove 관련 그래프가 잘못 올라와있더라구요. 그러니 정확한 벤치마크 결과를 위해서는 위 링크보다는 여기 링크: <span class="exturl" data-url="aHR0cHM6Ly9iYXB0aXN0ZS13aWNodC5jb20vcG9zdHMvMjAxMi8xMi9jcHAtYmVuY2htYXJrLXZlY3Rvci1saXN0LWRlcXVlLmh0bWw=">C++ benchmark – std::vector VS std::list VS std::deque<i class="fa fa-external-link-alt"></i></span>를 참조하시길 바랍니다.)</li>
</ul>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pc29jcHAub3JnL2Jsb2cvMjAxNC8wNi9zdHJvdXN0cnVwLWxpc3Rz">Are lists evil? Bjarne Stroustrup<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>(역자 코멘트: 이 링크 내용을 읽어보시면 C++의 창시자인 Bjarne Stroustrup는 list가 완전 잘못된건 아니라고 하고 있습니다. 정확하게 이해하고 쓰면 list도 다른 자료구조보다 좋은 성능을 낼 수 있습니다.)</li>
</ul>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1ZUXM2SUMtdmdtbw==">Video from Bjarne Stroustrup keynote<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ul>
<p>“그래도 내 경우에는 무조건 linked list를 써야할 것 같은데?” 라고 생각하실 수도 있는데, <strong>장담하건데 linked list 안 쓰셔도 됩니다</strong>.</p>
<p>STL 자료구조에는 이미 <code>std::list</code> 보다 훨씬 더 훌륭한 자료구조가 있습니다. <a href="https://es.cppreference.com/w/cpp/container/deque"><code>std::deque&lt;&gt;</code></a> 라는 친구가 99%의 경우 당신의 문제를 해결해줄 겁니다.</p>
<p>아니면, 우리의 영원한 친구 <code>std::vector</code>가 해결해줄 수도 있죠. 종종 <code>std::vector</code>는 list보다 더 효율적이게 작동합니다.</p>
<p>나는 너무 힙하고 멋져야하고 남들보다 뛰어나야하니 <code>std::vector</code>나 <code>std::deque</code> 같은건 절대로 쓸 수 없다, 라고 하신다면 <span class="exturl" data-url="aHR0cHM6Ly9wbGZsaWIub3JnL2NvbG9ueS5odG0=">plf::colony<i class="fa fa-external-link-alt"></i></span>를 보시는걸 추천드려요.</p>
<p><strong><em>근데 진짜로, 그냥 <code>std::vector</code>나 <code>std::deque</code> 쓰세요.</em></strong></p>
 <br>

<h2 id="실제로-쓰인-예시-Intel-RealSense-카메라-드라이버의-성능-끌어올리기"><a href="#실제로-쓰인-예시-Intel-RealSense-카메라-드라이버의-성능-끌어올리기" class="headerlink" title="실제로 쓰인 예시: Intel RealSense 카메라 드라이버의 성능 끌어올리기"></a>실제로 쓰인 예시: Intel RealSense 카메라 드라이버의 성능 끌어올리기</h2><hr>
<p>제가 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0ludGVsUmVhbFNlbnNl">Intel RealSense<i class="fa fa-external-link-alt"></i></span> 코드에 컨트리뷰트 했던 Pull Request에 대해 얘기해보려고 합니다.</p>
<img src="/20201205-faconti-do-not-use-linked-list/realsense.png" class="" title="영롱한 RealSense">

<p>대체 무슨 생각으로 그랬는지는 모르겠지만, RealSense의 코드에서는 <code>std::list&lt;&gt;</code>를 쓰고 있었습니다. Intel에서 왜 아직도 이런걸 쓰고 있었는지 정말 이해가 안되네요.</p>
<p><strong><em>농담입니다, 인텔 개발자들 사랑해요 알러뷰!</em></strong></p>
<p>여기 제가 건의했던 PR의 링크입니다. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0ludGVsUmVhbFNlbnNlL3JlYWxzZW5zZS1yb3MvcHVsbC8xMDk3">Considerable CPU saving in BaseRealSenseNode::publishPointCloud()<i class="fa fa-external-link-alt"></i></span></p>
<p>요약하자면, 이 PR은 간단한 내용 2개를 수정했습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 여기 매 카메라 프레임마다 생성되는 list가 있습니다.</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">list</span>&lt;<span class="keyword">unsigned</span>&gt; valid_indices;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 이 list를 vector로 바꿨습니다. 이 vector는 class member로써, 사용 전에 clear됩니다.</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span>&gt; _valid_indices;</span><br></pre></td></tr></table></figure>

<p>추가로, 매 프레임마다 <code>sensor_msgs::PointCloud2 msg_pointcloud</code>라는 큰 오브젝트가 생성됬습니다. 이 큰 오브젝트를 class member로 변환되어 재사용함으로써 더 빨라졌지요.</p>
<p>이렇게 수정해줌으로써, 약 20~30% 정도의 속도를 향상시켰습니다.</p>
<p>코드 몇줄 바꿔서 이 정도 속도 향상을 얻는건 엄청난거죠.</p>
<hr>
<h2 id="역자-추가-내용"><a href="#역자-추가-내용" class="headerlink" title="역자 추가 내용"></a>역자 추가 내용</h2><p><span class="exturl" data-url="aHR0cHM6Ly9iYXB0aXN0ZS13aWNodC5jb20vcG9zdHMvMjAxMi8xMi9jcHAtYmVuY2htYXJrLXZlY3Rvci1saXN0LWRlcXVlLmh0bWw=">C++ benchmark – std::vector VS std::list VS std::deque<i class="fa fa-external-link-alt"></i></span> 링크를 보시면 다음과 같은 내용이 나옵니다.</p>
<ul>
<li>대부분의 경우, <code>std::vector</code>와 <code>std::deque</code>가 <code>std::list</code>에 비해 더 빠르다.</li>
<li><code>std::list</code>가 <code>std::vector</code>와 <code>std::deque</code>에 비해 더 빠른 경우는 아래와 같다.<ul>
<li>Random insert (중간에 새 element를 insert) 작업에서, insert 되는 객체의 크기가 큰 경우</li>
<li>Random insert (중간에 새 element를 insert) 작업에서, insert 되는 객체가 non-trivial인 경우</li>
<li>Random remove (중간에 element를 erase) 작업</li>
<li>push_front (최앞단에 새 element를 insert) 작업 </li>
<li>sort 작업에서, element 객체가 큰 경우</li>
</ul>
</li>
</ul>
<p>즉, 위와 같은 작업에서는 <code>std::list</code>를 써야하는 것이 맞으며, 이는 원글이 주장하는 내용과 다릅니다.</p>
<p>원글이 <code>std::list</code>를 쓸 필요가 없다고 주장한 부분은, 아무래도 <strong><em>위와 같은 경우가 실시간 컴퓨터 비전에서 자주 나타나지 않는 상황</em></strong>이기에 그런 것 같습니다.</p>
<p>우선적으로, 실시간 컴퓨터 비전에서는 센서로부터 오는 데이터를 순차적으로 저장하는 경우가 많기 때문에, ‘중간에 새 element를 insert’ 하는 경우가 많이 없습니다. </p>
<p>또, 새 데이터가 들어오면 가장 뒷단에 저장하는 경우가 많기 때문에, <code>push_front()</code> 같은 작업도 자주 나타나지 않죠.</p>
<p>‘중간에서 element를 erase’하는 작업도 보통 잘 나타나지 않습니다. Raw 센서 데이터에서 ‘최대 정확도를 낼 수 있는 최소한의 데이터’만 남기는 것이 메모리와 계산량의 효율성에 큰 영향을 줍니다. 그렇기에 Raw 데이터에서 전처리를 통해 남는 데이터가 많지 않습니다. 보통 기존의 컨테이너에서 element를 지우기보다는 새로운 컨테이너에 전처리를 통과한 데이터를 저장하고, raw 데이터는 지우는 방식을 사용합니다.</p>
<p>Sort는 자주 사용되는 기법이지만, 경우에 따라서 비교해야하는 element 객체가 크거나 작을 수 있습니다. 여기에 있어서 많은 기법들이 element 객체를 직접 비교하는게 아닌, 어떠한 distance metric을 사용하여 비교합니다. L2 Norm, Hamming distance 등이 있습니다. 예시로, Visual feature matching을 위해 descriptor distance를 구하는데, 각각의 descriptor를 직접 비교하는 것이 아닌 floating distance / hamming distance 등으로 metric distance값을 계산하여, 매치의 정확도 값을 따로 저장하고, 그 정확도를 기점으로 sort하는 경우가 많습니다. </p>
<p>하지만 컴퓨터 비전 쪽 계산에는 정말로 다양한 방식으로 계산을 하고 데이터를 저장해야합니다. 제가 생각하지 못한 부분에서 위와 같은 경우가 나타날 수 있고, 그런 경우에는 <code>std::list</code>가 진가를 보여줄 수 있는 부분이 있을 것이라고 생각합니다. 이 글을 읽으시는 분들께는 <code>std::list</code>를 사용하실 때 <code>std::list</code>, <code>std::vector</code>, <code>std::deque</code> 중 어떤 컨테이너를 사용하는게 더 효율적일지 한번 더 고려하실 수 있는 기회가 되길 바랍니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (1) - std::vector&lt;&gt;::reserve()를 쓰세요!</title>
    <url>/20201205-faconti-use-vector-reserve/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Use std::vector&lt;&gt;::reserve by default”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9yZXNlcnZlLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="std-vector-최고…"><a href="#std-vector-최고…" class="headerlink" title="std::vector 최고…"></a>std::vector 최고…</h1><hr>
<p><code>std::vector&lt;&gt;</code>는 다른 자료구조에 비해 엄청난 장점이 있습니다. 메모리 상에서 vector element가 바로 옆에 따닥따닥 붙어있다는 점입니다. </p>
<p><code>std::vector&lt;&gt;</code>의 이런 특성이 왜 성능향상을 시키냐, 를 설명하려면 이 글이 꽤 길어질겁니다. 프로세서가 어떻게 메모리를 처리하는지 설명을 해야할테니까요. </p>
<p>좀 더 자세히 알고싶으신 분들은, “C++ cache aware programming”이라고 구글링 해보시거나, 아니면 아래 자료를 보시는걸 추천드립니다.</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYXJpc3RlaWEuY29tL1RhbGtOb3Rlcy9jb2RlZGl2ZS1DUFVDYWNoZXNIYW5kb3V0cy5wZGY=">CPU Caches and why you Care<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1OejlTaUYwUVZLWQ==">Writing cache friendly C++ (video)<i class="fa fa-external-link-alt"></i></span> </li>
</ul>
<p><code>std::vector&lt;&gt;</code>의 element에 대해 iterator를 돌리는건 엄청나게 빠릅니다. 또 vector의 끝단 element를 지우거나 추가할 때도 잘 되죠.</p>
<br>

<h1 id="reserve를-써야할-때"><a href="#reserve를-써야할-때" class="headerlink" title="reserve를 써야할 때"></a><code>reserve</code>를 써야할 때</h1><hr>
<p><code>reserve()</code>를 왜 써야할까요? </p>
<p>이 질문에 답을 하려면 <code>std::vector&lt;&gt;</code>가 어떻게 작동하는지 이해해야합니다.</p>
<p>비어있거나, 꽉 차있는 <code>std::vector&lt;&gt;</code>에 새로운 element를 넣을 때, 우리는…</p>
<ul>
<li>새로운 메모리 블록을 할당해야하거나</li>
<li>이전 메모리 블록에 저장되어있는 모든 element 정보를 새로운 블록으로 이동해야합니다.</li>
</ul>
<p>어떤 방법을 택하든 엄청 비싼 작업입니다. 그리고 비싼 작업은 지양해야합니다. 뭐, 가끔은 그냥 눈물을 머금고 해야하지만요…</p>
<p>이 새로운 메모리 블록은 보통 <strong><em>이전 블록의 2배 크기</em></strong>를 가집니다. 그렇기에, 우리의 vector가 <code>size()</code>도 <code>capacity()</code>도 100개의 element를 가지고 있을 때 우리가 <code>push_back()</code>을 해버리면, 새로운 메모리 블록은 200개의 element를 가질 수 있게 할당됩니다.</p>
<p>200개의 메모리 블록을 가지는게 잘못됬다는게 아닙니다. 다만 이 allocation 작업이 반복해서 나타난다면 엄청나게 느려지겠죠. 이런 allocation 과정을 피하기 위해 우리는 메모리 블록을 <strong><em>‘미리 할당’ (i.e. reserve)</em></strong> 할 수 있습니다. </p>
<p>아래 코드를 한번 봅시다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">NoReserve</span><span class="params">(benchmark::State&amp; state)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    <span class="comment">// create a vector and add 100 elements</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">size_t</span>&gt; v;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;<span class="number">100</span>; i++)&#123;  v.push_back(i);  &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WithReserve</span><span class="params">(benchmark::State&amp; state)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    <span class="comment">// create a vector and add 100 elements, but reserve first</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">size_t</span>&gt; v;</span><br><span class="line">    v.reserve(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;<span class="number">100</span>; i++)&#123;  v.push_back(i);  &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ObsessiveRecycling</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// create the vector only once</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">size_t</span>&gt; v;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    <span class="comment">// clear it. Capacity is still 100+ from previous run</span></span><br><span class="line">    v.clear();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;<span class="number">100</span>; i++)&#123;  v.push_back(i);  &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/20201205-faconti-use-vector-reserve/vector_reserve.png" class="" title="vector reserve">

<p>차이를 한번 보세요! Element가 100개 밖에 안되는데 이렇게 차이가 납니다.</p>
<p>Element의 수가 몇개가 되냐에 따라서 reserve의 성능 향상치가 달라질 수 있습니다. 하지만 하나 확실한 점은, <strong><em>reserve를 쓰면 무조건 빨라진다는 겁니다</em></strong>.</p>
<p>그리고 또, <code>ObsessiveReclycing</code> 함수를 자세히 봅시다. 작은 vector에서는 성능차이가 있는데, 큰 vector에서는 별 차이가 없습니다.</p>
<p>아, 물론 <code>ObsessiveRecycling</code>이 더 빠릅니다. 그냥 어떤 사이즈를 쓰냐에 따라서 ‘눈에 띄게 달라지는지’는 조금 차이가 날 수 있다는거죠.</p>
<br>

<h2 id="Profiler에서-std-vector-lt-gt-때문에-생기는-문제인지-바로-알아보는-법"><a href="#Profiler에서-std-vector-lt-gt-때문에-생기는-문제인지-바로-알아보는-법" class="headerlink" title="Profiler에서 std::vector&lt;&gt; 때문에 생기는 문제인지 바로 알아보는 법"></a>Profiler에서 <code>std::vector&lt;&gt;</code> 때문에 생기는 문제인지 바로 알아보는 법</h2><p>아래 사진은 어플리케이션 runtime에서 측정된 메모리 할당량입니다 (Heaptrack이라는 프로파일러를 사용했습니다.)</p>
<img src="/20201205-faconti-use-vector-reserve/growing_vector.png" class="" title="사이즈가 점점 커지는 vector">

<p>그냥 딱 봐도, 뭔가 메모리가 계속 두배씩 커지네요. 대체 뭐 때문일까요? 아, vector 때문이겠네요. 다른 자료구조는 linear 하게 커지니까요.</p>
<p>이렇게 Profiler로 보면, 종종 <code>std::vector&lt;&gt;</code> index에 관련된 버그를 잡을 수 있습니다. 100개의 element를 담는 vector를 만들었는데, 메모리가 2배 늘어나는게 보인다면 분명히 100개를 초과했을테니까요. 이렇게 버그를 잡을 수 있습니다. </p>
<br>
<br>
게시글 공유:<span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9pbnRlbnQvdHdlZXQ/dGV4dD0lMjIoJUVCJUIyJTg4JUVDJTk3JUFEKSUyMCVFQyU4QiVBNCVFQyU4QiU5QyVFQSVCMCU4NCUyMCVFQyVCQiVCNCVFRCU5MyVBOCVFRCU4NCVCMCUyMCVFQiVCOSU4NCVFQyVBMCU4NCVFQyU5RCU4NCUyMCVFQyU5QyU4NCVFRCU5NSU5QyUyMEMrKyUyMCVFQyVCNSU5QyVFQyVBMCU4MSVFRCU5OSU5NCUyMCgxKSUyMC0lMjBzdGQ6OnZlY3RvciUzQyUzRTo6cmVzZXJ2ZSgpJUVCJUE1JUJDJTIwJUVDJTkzJUIwJUVDJTg0JUI4JUVDJTlBJTk0ISUyMiUyMGh0dHBzOi8vY2hhbmdoOTUuZ2l0aHViLmlvLzIwMjAxMjA1LWZhY29udGktdXNlLXZlY3Rvci1yZXNlcnZlLyUyMHZpYSUyMEBoZXhvanM=">Twitter<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL3NoYXJlci9zaGFyZXIucGhwP3U9aHR0cHM6Ly9jaGFuZ2g5NS5naXRodWIuaW8vMjAyMDEyMDUtZmFjb250aS11c2UtdmVjdG9yLXJlc2VydmUvJnQ9KCVFQiVCMiU4OCVFQyU5NyVBRCklMjAlRUMlOEIlQTQlRUMlOEIlOUMlRUElQjAlODQlMjAlRUMlQkIlQjQlRUQlOTMlQTglRUQlODQlQjAlMjAlRUIlQjklODQlRUMlQTAlODQlRUMlOUQlODQlMjAlRUMlOUMlODQlRUQlOTUlOUMlMjBDKyslMjAlRUMlQjUlOUMlRUMlQTAlODElRUQlOTklOTQlMjAoMSklMjAtJTIwc3RkOjp2ZWN0b3IlM0MlM0U6OnJlc2VydmUoKSVFQiVBNSVCQyUyMCVFQyU5MyVCMCVFQyU4NCVCOCVFQyU5QSU5NCE=">Facebook<i class="fa fa-external-link-alt"></i></span>, and <span class="exturl" data-url="aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL3NoYXJpbmcvc2hhcmUtb2Zmc2l0ZS8/dXJsPWh0dHBzOi8vY2hhbmdoOTUuZ2l0aHViLmlvLzIwMjAxMjA1LWZhY29udGktdXNlLXZlY3Rvci1yZXNlcnZlLyZ0aXRsZT0oJUVCJUIyJTg4JUVDJTk3JUFEKSUyMCVFQyU4QiVBNCVFQyU4QiU5QyVFQSVCMCU4NCUyMCVFQyVCQiVCNCVFRCU5MyVBOCVFRCU4NCVCMCUyMCVFQiVCOSU4NCVFQyVBMCU4NCVFQyU5RCU4NCUyMCVFQyU5QyU4NCVFRCU5NSU5QyUyMEMrKyUyMCVFQyVCNSU5QyVFQyVBMCU4MSVFRCU5OSU5NCUyMCgxKSUyMC0lMjBzdGQ6OnZlY3RvciUzQyUzRTo6cmVzZXJ2ZSgpJUVCJUE1JUJDJTIwJUVDJTkzJUIwJUVDJTg0JUI4JUVDJTlBJTk0IQ==">LinkedIn<i class="fa fa-external-link-alt"></i></span>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (3) - std::map을 써야할까요?</title>
    <url>/20201206-do-you-need-map/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Do you actually need to use std::map?”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9kb250X25lZWRfbWFwLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="어…-진짜로-std-map을-써야하나요"><a href="#어…-진짜로-std-map을-써야하나요" class="headerlink" title="어… 진짜로 std::map을 써야하나요?"></a>어… 진짜로 std::map을 써야하나요?</h1><hr>
<p><code>std::map</code>은 C++에서 가장 많이 사용하는 associative container 형태의 자료구조입니다. 시간이 지나면서 조금 인기가 떨어지긴 했어두요.</p>
<p>Associative container는 우리가 <strong>Key / Value 페어 값</strong>을 가지고 있고, 우리가 Key 값이 있을 때 해당하는 Value 값을 찾기 위해 사용합니다.</p>
<p>하지만, <code>std::map</code>은 생성될 때 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUmVkJUUyJTgwJTkzYmxhY2tfdHJlZQ==">red-black tree<i class="fa fa-external-link-alt"></i></span>가 생성되는 방식을 따릅니다. 그렇다보니, <code>std::list</code>와 비슷한 이유로 선호되지 않기도 하죠. (i.e. 메모리 레이아웃이 cache 메모리와 잘 맞지 않을 뿐더러, insertion 등을 할 때 memory allocation의 방식 등등…)</p>
<p><code>std::map</code>을 쓰시기 전에, 우선 아래와 같은 질문을 한번 해봅시다.:</p>
<ul>
<li>모든 데이터 페어가 key값에 대해 <strong><em>정렬</em></strong> 될 필요가 있는가?</li>
<li>이 컨테이너 내부에 있는 모든 값들에 대해 자주 iterate를 해야하는가?</li>
</ul>
<p>첫번째 질문의 답이 ‘아니오’ 였다면, <code>std::unordered_map</code> 으로 바꾸시는걸 추천드립니다.</p>
<p>제 벤치마크에서는 <code>std::unordered_map</code>을 사용하면 항상 성능이 올랐습니다. 이론적으로 상황에 따라 <code>std::map</code>이 더 좋은 성능을 낼 수 있지만, 저는 아직 그런 케이스를 본 적이 없네요.<br>(역자 코멘트: <code>std::map</code>은 밸런스 트리 구조, <code>std::unordered_map</code>은 해쉬 테이블을 사용합니다. 그렇기 때문에, 1. 저장하는 데이터의 양이 많고, 2.겹치는 데이터가 많을 수록 pigeonhole principle에 따라 <code>std::unordered_map</code>에서 hash collision이 나타나고 더 느려지게 됩니다)</p>
<p>두번째 질문의 답이 ‘네’라면… 문제가 굉장히 흥미로워집니다.</p>
<p>잠시 제 최적화 썰을 들어보시지요.</p>
<br>

<h2 id="Velodyne-드라이버-최적화-썰"><a href="#Velodyne-드라이버-최적화-썰" class="headerlink" title="Velodyne 드라이버 최적화 썰"></a>Velodyne 드라이버 최적화 썰</h2><hr>
<p>여기 제가 꽤 자부심을 가지고 있는 Pull Request가 있습니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jvcy1kcml2ZXJzL3ZlbG9keW5lL3B1bGwvMTk0">Avoid unnecessary computation in RawData::unpack<i class="fa fa-external-link-alt"></i></span></p>
<p>작은 변화가 아주 큰 성능 개선을 만들었지요.</p>
<p>우선 이 드라이버가 어떤 기능을 하는지 설명드리겠습니다.</p>
<img src="/20201206-do-you-need-map/velodyne.png" class="" title="Velodyne LiDAR 드라이버">

<p><strong>벨로다인 (Velodyne)</strong> LiDAR 센서는 매 초 마다 수많은 포인트 값을 계산합니다 (i.e. 장애물과의 거리 값). </p>
<p>수 많은 자율주행 자동차에서 이 센서를 사용합니다.</p>
<p>벨로다인 드라이버는 polar coordinates에서 얻어진 관측 값을 3D cartesian coordinates (i.e. PointCloud) 형태로 바꿔줍니다.</p>
<p>저는 이 벨로다인 드라이버를 <strong>Hotspot</strong>이라는 profiler를 통해 분석해봤습니다.</p>
<p>그리고… <code>std::map::operator[]</code>라는게 CPU를 많이 잡아먹는다는 것을 보았죠.</p>
<p>코드를 뜯어봤고, 아래와 같은 점을 찾았습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">map</span>&lt;<span class="keyword">int</span>, LaserCorrection&gt; laser_corrections;</span><br></pre></td></tr></table></figure>
<p><code>LaserCorrection</code>이라는 객체는 레이저 센서 값을 보정하기 위한 calibration 정보를 담고 있습니다.</p>
<p>Map에 들어있는 <code>int</code> key는 [0, N-1]의 range를 가지고 있습니다. 벨로다인 센서의 채널 수를 뜻하는데, 16, 32, 64, 128 등이 있죠.</p>
<p>그리고 <code>laser_corrections</code>는 딱 한번만 만들어지고 (insertion도 딱 한번만 합니다), 그 후에는 계속 루프에서 재사용됩니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 원본 코드를 조금 요약해서...</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; BLOCKS_PER_PACKET; i++) &#123;</span><br><span class="line">   <span class="comment">//some code</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; NUM_SCANS; j++) </span><br><span class="line">   &#123;   </span><br><span class="line">       <span class="keyword">int</span> laser_number = <span class="comment">// omitted for simplicity</span></span><br><span class="line">       <span class="keyword">const</span> LaserCorrection &amp;corrections = laser_corrections[laser_number];</span><br><span class="line">       <span class="comment">// some code</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/20201206-do-you-need-map/that_is_logn.jpg" class="" title="저거 log_n인데">

<p>흠, 이런 순수한 코드에…</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">laser_corrections[laser_number];</span><br></pre></td></tr></table></figure>

<p>red-black tree 속 탐색을 하는 더러운 코드(?) 가 있었다니!</p>
<p>여기서 주의하셔야할 점은, index값은 랜덤값이 아닙니다. 이 값은 언제나 0~N-1이며, N은 작은 수에요.</p>
<p>그래서 다음와 같이 수정해서 제안을 했고, 아래와 같은 피드백을 받았습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;LaserCorrection&gt; laser_corrections;</span><br></pre></td></tr></table></figure>

<img src="/20201206-do-you-need-map/quote.png" class="" title="답장">

<p>요약하자면, 여기에는 associative containe가 애초부터 필요하지 않았습니다. 그냥 <code>std::vector</code>의 포지션 값을 사용해도 됬죠.</p>
<p>벨로다인 개발자들이 잘못했다는게 아닙니다. 이러한 문제는 회고를 할 때만 보이는게 정상이고, 또 실제로 프로파일러를 돌려보면서 시간을 재봐야 쓸데없는 오버헤드를 찾을 수 있기 때문이죠.</p>
<p>개발자의 입장에서는 대부분의 코드가 엄청나게 복잡하고 어려운 수학 계산을 하기 때문에, bottleneck이 수학쪽 계산에서 나타난다고 생각하기 쉽습니다. 하지만 종종 진짜 bottleneck은 <code>std::map</code>과 같은 기본적인 곳에서 나타나기도 하며, 이런 bottleneck을 알아차리기는 정말로 쉽지 않습니다.</p>
<br>


<h2 id="한발짝-더-나아가서…-key-value-를-담는-vector는-어떨까요"><a href="#한발짝-더-나아가서…-key-value-를-담는-vector는-어떨까요" class="headerlink" title="한발짝 더 나아가서… [key,value]를 담는 vector는 어떨까요?"></a>한발짝 더 나아가서… [key,value]를 담는 vector는 어떨까요?</h2><hr>
<p>이번 예시는 사실 조금 특이한 케이스였습니다. 우선 Key 값이 참 편리하게도 int로 되어있었고, 또 0과 N 사이의 작은 값이였죠.</p>
<p>뭐 그래도, 저는 가끔 ‘진짜’ associative container 대신 이런 코드를 짜기도 합니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::<span class="built_in">pair</span>&lt;KeyType, ValueType&gt; &gt; my_map;</span><br></pre></td></tr></table></figure>

<p>이러한 자료구조의 장점은 <strong><em>모든 element에 iteration을 자주 돌려야할 때 엄청난 효율성</em></strong>을 보여줍니다.</p>
<p>대부분의 다른 자료구조는 이 방법의 속도를 이길 수가 없어요!</p>
<br>

<blockquote>
<p>“어,, 근데 저는 element들이 ordered인 형태가 되야하는데… 그래서 제가 <code>std::map</code>을 쓴 거거든요…”</p>
</blockquote>
<p>Ordering이 필요하시면… ordering 하시면 됩니다!</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::sort( my_map.begin(), my_map.end() ) ;</span><br></pre></td></tr></table></figure>

<br>

<blockquote>
<p>“어,, 근데 저는 map에서 element를 찾아야하는걸요”</p>
</blockquote>
<p>그 경우에는, <span class="exturl" data-url="aHR0cDovL3d3dy5jcGx1c3BsdXMuY29tL3JlZmVyZW5jZS9hbGdvcml0aG0vbG93ZXJfYm91bmQv">std::lower_bound<i class="fa fa-external-link-alt"></i></span> 함수를 사용해서 <strong><em>ordered vector</em></strong>로부터 key값을 이용해 value를 얻어낼 수 있습니다. <code>std::lower_bound</code>와 <code>std::upper_bound</code>의 complexity는 <strong><em>O(log n)</em></strong>입니다. <code>std::map</code>이랑 같지만, iteration은 훨씬 더 빠르게 돌 수 있죠.</p>
<br>

<h1 id="정리하자면…"><a href="#정리하자면…" class="headerlink" title="정리하자면…"></a>정리하자면…</h1><hr>
<ul>
<li>데이터에 어떻게 접근을 하고싶은지 충분히 고민을 해야합니다.</li>
<li>Insertion/deletion을 자주할지, 자주하지 않을지 제대로 파악해야합니다.</li>
<li>Associative container를 사용함으로써 생기는 cost를 무시하시면 안됩니다.</li>
<li><code>std::unordered_map</code>을 먼저 써보세요… 아니면 <code>std::vector</code>를 써보세요!</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (4) - 사이즈가 작은 vector를 쓸 때...</title>
    <url>/20201208-small-vector-optimisation/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Small vector optimization”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9zbWFsbF92ZWN0b3JzLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="Small-벡터-최적화"><a href="#Small-벡터-최적화" class="headerlink" title="Small 벡터 최적화"></a>Small 벡터 최적화</h1><hr>
<p>여기까지 읽으셨다면… 여러분들께서는 이제 우리가 associative container가 정말로 필요하지 않는 이상 왠만하면 <code>std::vector</code>를 사용해봐야 하는 것을 이해하실 겁니다.</p>
<p><code>std::vector</code>를 사용하면서 사이즈가 늘어날 때 마다 비싼 heap allocation를 방지하기 위해 우리는 <code>reserve</code>를 사용했었습니다. 하지만, 이 방법을 쓰면 <strong><em>무조건 처음에 단 한번</em></strong>은 heap allocation을 해야하죠. 이 점이 굉장히 아쉽습니다. 좋은 방법이 없을까요?</p>
<p>사실 여기에도 방법이 있습니다. 추후에 적을 <a href="https://changh95.github.io/20201210-small-string-optimisation/">Small String Optimisation (SSO)</a> 글에도 이 방법을 사용할겁니다.</p>
<br>

<h1 id="“Static”-vector와-“Small”-vector"><a href="#“Static”-vector와-“Small”-vector" class="headerlink" title="“Static” vector와 “Small” vector"></a>“Static” vector와 “Small” vector</h1><hr>
<p>우리가 사용할 vector의 크기가 작고, 최악의 상황에도 작은 크기로 있을 것을 알 때가 있습니다. 이 경우에는 모든 element들을 heap가 아니라 stack에 넣어둘 수 있습니다. Heap allocation은 비싸거든요.</p>
<p>사이즈가 작은 vector를 쓴다니, 이런 경우가 얼마나 자주 있겠나~ 라고 생각하실 수도 있지만, 생각보다 굉장히 자주 나타납니다. 당장 2주 전만해도, 제가 사용하는 라이브러리에서 이러한 패턴을 발견했습니다. 이 vector의 크기는 0에서 최대 8까지 element를 가집니다.</p>
<p>30분 정도 리팩토링을 했고, 20%정도의 성능 향상을 얻었습니다!</p>
<p>리팩토링의 결과를 요약하자면… </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; my_data; <span class="comment">// 이 vector를 사용하려면 heap allocation이 필요합니다.</span></span><br></pre></td></tr></table></figure>
<p>위 코드를 이런 방식으로 바꿨죠.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> my_data[MAX_SIZE]; <span class="comment">// Heap allocation이 없는 형태 </span></span><br><span class="line"><span class="keyword">int</span> size_my_data;</span><br></pre></td></tr></table></figure>
<p>이 <code>StaticVector</code>의 간단한 구현을 한번 함께 봅시다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;initializer_list&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">size_t</span> N&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StaticVector</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"></span><br><span class="line">  <span class="keyword">using</span> iterator       = <span class="keyword">typename</span> <span class="built_in">std</span>::<span class="built_in">array</span>&lt;T,N&gt;::iterator;</span><br><span class="line">  <span class="keyword">using</span> const_iterator = <span class="keyword">typename</span> <span class="built_in">std</span>::<span class="built_in">array</span>&lt;T,N&gt;::const_iterator;</span><br><span class="line"></span><br><span class="line">  StaticVector(<span class="keyword">uint8_t</span> n=<span class="number">0</span>): _size(n) &#123;</span><br><span class="line">    <span class="keyword">if</span>( _size &gt; N )&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">&quot;SmallVector overflow&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  StaticVector(<span class="keyword">const</span> StaticVector&amp; other) = <span class="keyword">default</span>;</span><br><span class="line">  StaticVector(StaticVector&amp;&amp; other) = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  StaticVector(<span class="built_in">std</span>::<span class="built_in">initializer_list</span>&lt;T&gt; init)</span><br><span class="line">  &#123;</span><br><span class="line">    _size = init.size();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;_size; i++) &#123; _storage[i] = init[i]; &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">push_back</span><span class="params">(T val)</span></span>&#123;</span><br><span class="line">    _storage[_size++] = val;</span><br><span class="line">    <span class="keyword">if</span>( _size &gt; N )&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">&quot;SmallVector overflow&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">pop_back</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( _size == <span class="number">0</span> )&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">&quot;SmallVector underflow&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    back().~T(); <span class="comment">// call destructor</span></span><br><span class="line">    _size--;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">size_t</span> <span class="title">size</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _size; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>&#123; <span class="keyword">while</span>(_size&gt;<span class="number">0</span>) &#123; pop_back(); &#125; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">T&amp; <span class="title">front</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> _storage.front(); &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> T&amp; <span class="title">front</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _storage.front(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">T&amp; <span class="title">back</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> _storage[_size<span class="number">-1</span>]; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> T&amp; <span class="title">back</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _storage[_size<span class="number">-1</span>]; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">iterator <span class="title">begin</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> _storage.begin(); &#125;</span><br><span class="line">  <span class="function">const_iterator <span class="title">begin</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _storage.begin(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">iterator <span class="title">end</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> _storage.end(); &#125;</span><br><span class="line">  <span class="function">const_iterator <span class="title">end</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _storage.end(); &#125;</span><br><span class="line"></span><br><span class="line">  T&amp; <span class="keyword">operator</span>[](<span class="keyword">uint8_t</span> index) &#123; <span class="keyword">return</span> _storage[index]; &#125;</span><br><span class="line">  <span class="keyword">const</span> T&amp; <span class="keyword">operator</span>[](<span class="keyword">uint8_t</span> index) <span class="keyword">const</span> &#123; <span class="keyword">return</span> _storage[index]; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">T&amp; <span class="title">data</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> _storage.data(); &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> T&amp; <span class="title">data</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> _storage.data(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">array</span>&lt;T,N&gt; _storage;</span><br><span class="line">  <span class="keyword">uint8_t</span> _size = <span class="number">0</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>StaticVector는 겉으로 보기에는 <code>std::vector</code>와 굉장히 유사하지만… 결과는 매우 서프라이징하네요.</p>
<img src="/20201208-small-vector-optimisation/inconceivably.jpg" class="" title="아 예 써프라이징 성능~~">

<p>종종 우리는 vector 컨테이너가 최대 <strong><em>N</em></strong>개의 element를 가질 확률이 높다는건 알지만, 항상 그럴것인지 확신하지 못할 때가 있죠.</p>
<p>그런 경우에 우리는 <strong><em>Small vector (작은 벡터)</em></strong>를 사용할 수 있습니다. 작은 vector는 첫 N개의 element는 stack에 미리 저장한 메모리 값을 사용하고, 그리고 그 벡터가 더 확장해야할 때**<em>만**</em> 새로운 heap allocation을 통해 사이즈를 확장할 수 있습니다.</p>
<br>

<h2 id="StaticVector와-SmallVector를-실제로-쓰려면…"><a href="#StaticVector와-SmallVector를-실제로-쓰려면…" class="headerlink" title="StaticVector와 SmallVector를 실제로 쓰려면…"></a>StaticVector와 SmallVector를 실제로 쓰려면…</h2><hr>
<p>찾아봤더니 이런 트릭은 실제로 많은 라이브러리에서 이미 구현이 되어있더라구요. 예시로는 아래의 라이브러리들이 있습니다.</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYm9vc3Qub3JnL2RvYy9saWJzLzFfNzNfMC9kb2MvaHRtbC9jb250YWluZXIuaHRtbA==">Boost::container<i class="fa fa-external-link-alt"></i></span>. (이런 트릭이 존재한다는건, Boost에 이미 있다는 뜻입니다 ㅋㅋ)</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Fic2VpbC9hYnNlaWwtY3BwL3RyZWUvbWFzdGVyL2Fic2wvY29udGFpbmVy">Abseil<i class="fa fa-external-link-alt"></i></span>. 여기서는 <code>fixed array</code> 또는 <code>inlinec_vector</code>라고 합니다. </li>
<li>제대로 공부하고 싶으신 분들은, 여기 링크를 타고 가시는 것을 추천드립니다. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xsdm0vbGx2bS1wcm9qZWN0L2Jsb2IvbWFzdGVyL2xsdm0vaW5jbHVkZS9sbHZtL0FEVC9TbWFsbFZlY3Rvci5o">SmallVector used internally by LLVM<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (5) - String은 vector처럼 써야합니다</title>
    <url>/20201209-strings-are-almost-vectors/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Strings are (almost) vectors”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9zdHJpbmdzX2FyZV92ZWN0b3JzLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="String에-대해-진지하게-고민할-필요가-있나요"><a href="#String에-대해-진지하게-고민할-필요가-있나요" class="headerlink" title="String에 대해 진지하게 고민할 필요가 있나요?"></a>String에 대해 진지하게 고민할 필요가 있나요?</h1><hr>
<p><code>std::string</code>은 엄청나게 좋습니다. <strong><em>C</em></strong>언어에서 사용하는 raw pointer와 길이 값과 코드를 짜면서 생기는 온간 세상 근심걱정에 비해서요.</p>
<p>농담입니다. C 개발자들, 사랑해요!</p>
<blockquote>
<p>… 하지만 그래도 어느정도 불쌍하다고 생각이 드는건 어쩔 수 없습니다.</p>
</blockquote>
<p>생각해보면, string은 사실 <code>std::vector&lt;char&gt;</code>랑 목적 면에서 크게 다른 점이 없습니다. 그냥 <code>std::string</code>에는 몇가지 쓰기 좋은 툴이 더 있다는 것만 다른 점이겠죠.</p>
<p>하지만 우리가 생각하지 못했던 <strong><em>또 다른 점</em></strong>이 있습니다. “Small string Optimisation (SSO)”가 가능하다는 점이죠.</p>
<p>SSO에 대해서 <a href="https://changh95.github.io/20201210-small-string-optimisation/">여기</a> 글에 적어두었습니다.</p>
<p>제가 말씀드리고 싶은 점은, <code>std::string</code>도 다른 memory allocation이 필요할 수도 있는 자료구조를 다루는 것 처럼 비슷한 방식으로 다룰 수 있어야 효율적으로 사용할 수 있다는 겁니다.</p>
<br>

<h2 id="ToString"><a href="#ToString" class="headerlink" title="ToString"></a>ToString</h2><hr>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Color</span>&#123;</span></span><br><span class="line">    BLUE,</span><br><span class="line">    RED,</span><br><span class="line">    YELLOW</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">ToStringBad</span><span class="params">(Color c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(c) &#123;</span><br><span class="line">    <span class="keyword">case</span> BLUE:   <span class="keyword">return</span> <span class="string">&quot;BLUE&quot;</span>;</span><br><span class="line">    <span class="keyword">case</span> RED:    <span class="keyword">return</span> <span class="string">&quot;RED&quot;</span>;</span><br><span class="line">    <span class="keyword">case</span> YELLOW: <span class="keyword">return</span> <span class="string">&quot;YELLOW&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; <span class="title">ToStringBetter</span><span class="params">(Color c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> color_name[<span class="number">3</span>] =&#123;<span class="string">&quot;BLUE&quot;</span>, <span class="string">&quot;RED&quot;</span>, <span class="string">&quot;YELLOW&quot;</span>&#125;;</span><br><span class="line">    <span class="keyword">switch</span>(c) &#123;</span><br><span class="line">    <span class="keyword">case</span> BLUE:   <span class="keyword">return</span> color_name[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">case</span> RED:    <span class="keyword">return</span> color_name[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">case</span> YELLOW: <span class="keyword">return</span> color_name[<span class="number">2</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>여기 예제에서 보이는 것 처럼, 가능하다면 최대한 새로운 string을 만들고 또 만드는 행위는 피해야합니다. </p>
<img src="/20201209-strings-are-almost-vectors/tostring.png" class="" title="To String...">

<br>

<h2 id="Temp-string을-재사용합시다"><a href="#Temp-string을-재사용합시다" class="headerlink" title="Temp string을 재사용합시다"></a>Temp string을 재사용합시다</h2><hr>
<p>아래 예시를 보시면 이전에 할당한 메모리를 재사용 할 수**<em>도**</em> 있는 예시를 보실 수 있습니다. </p>
<p>두번째 함수가 첫번째 함수보다 항상 빠를 것이라고 단정할 수는 없습니다. 하지만 왠만하면 빠르지 않을까 싶어요.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 첫번째 방법은 매번 새로운 string을 만드는겁니다.</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">ModifyString</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; input)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> output = input;</span><br><span class="line">    output.append(<span class="string">&quot;... indeed&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 이미 할당한 string을 다시 사용합니다. 이 string이 우리가 필요한 메모리 공간을 이미 충분히 가지고 있길 빌어야죠. </span></span><br><span class="line"><span class="comment">// (아닐 수도 있구요... 그러면 느릴수도 있어요)</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ModifyStringBetter</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; input, <span class="built_in">std</span>::<span class="built_in">string</span>&amp; output)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    output = input;</span><br><span class="line">    output.append(<span class="string">&quot;... indeed&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>벤치마크를 돌렸더니, 예상한대로 나왔습니다.</p>
<img src="/20201209-strings-are-almost-vectors/modifystring.png" class="" title="Modify string">
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (6) - String 만들어서 쓰기 vs String 내용 복사해서 쓰기</title>
    <url>/20201210-small-string-optimisation/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Small string optimizations”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9zbWFsbF9zdHJpbmdzLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="Small-String-최적화-small-string-optimisation-SSO"><a href="#Small-String-최적화-small-string-optimisation-SSO" class="headerlink" title="Small String 최적화 (small string optimisation - SSO)"></a>Small String 최적화 (small string optimisation - SSO)</h1><hr>
<p>제가 “<code>std::string</code>은 결국 <code>std::vector&lt;char&gt;</code>와 다를게 많이 없다고 했던 것을 기억하시나요.</p>
<p>훌륭하신 분들께서, 이미 할당된 메모리 안에서 string을 사용하는 방법을 고안하셨습니다. </p>
<p><code>std::string</code>의 크기는 64-bit 플랫폼에서 <strong><em>24 바이트</em></strong>입니다 (데이터 포인터, size, capacity 등을 저장합니다). 근데 여기서 우리가 메모리를 할당하기 전 까지 <strong><em>‘static’</em></strong>하게 23바이트의 메모리를 저장하는 방법이 있어요.</p>
<p>이 방법을 사용하면 성능이 엄청나게 좋아집니다!</p>
<img src="/20201210-small-string-optimisation/relax_sso.jpg" class="" title="Relax, SSO!">

<p>이 방법을 <strong>Small String Optimisation (SSO)</strong> 라고 합니다. 이 방법의 구현 방법이 궁금하신 분들을 위해 링크를 준비해놨습니다.</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VsbGlvdGdvb2RyaWNoL1NTTy0yMw==">SSO-23<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1rUFI4aDQtcVpkaw==">CppCon 2016: “The strange details of std::string at Facebook”<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>주의하셔야할 점은, 사용하는 컴파일러의 버전에 따라서 사용할 수 있는 크기가 23바이트보다 작을 수도 있습니다. 23바이트는 이론적으로 가능한 최대 크기에요.</p>
<br>

<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><hr>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* SHORT_STR = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShortStringCreation</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// String을 계속해서 만드는 예제입니다.</span></span><br><span class="line">  <span class="comment">// SHORT_STR 정도면 23바이트 안에 들어갑니다.</span></span><br><span class="line">  <span class="comment">// 즉 SSO가 돌아요!</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">created_string</span><span class="params">(SHORT_STR)</span></span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShortStringCopy</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 여기 예제에서는 string을 한번 만들고, 내용을 계속 복사합니다.</span></span><br><span class="line">  <span class="comment">// ... 카피가 계속 만드는거보다 빠를 줄 알았는데, 왜 ShortStringCreation보다 느릴까요?</span></span><br><span class="line">  <span class="comment">// 알고보니, 컴파일러가 생각보다 많이 똑똑한거였습니다. SSO 최고,, </span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> x; <span class="comment">// create once</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    x = SHORT_STR; <span class="comment">// copy</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* LONG_STR = <span class="string">&quot;this will not fit into small string optimization&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LongStringCreation</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// LONG_STR 정도의 긴 string은 분명히 memory allocation을 트리거 할겁니다.</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">created_string</span><span class="params">(LONG_STR)</span></span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LongStringCopy</span><span class="params">(benchmark::State&amp; state)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// String 내용을 복사할 때는 원래 이렇게 빨라야합니다.</span></span><br><span class="line">  <span class="comment">// 긴 string 을 쓸 때는 무조건 한번 만들고 recycle 해야해요.</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> x;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> _ : state) &#123;</span><br><span class="line">    x = LONG_STR;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>우리가 보통 알고 있는 “매번 새롭게 String을 만들면 느릴꺼야!” 라는 생각이 여기서 깨지게 됩니다. String이 짧다면, 복사를 하는 것 보다 더 빠를 수 있어요.<br>물론 긴 string을 만드는 것은 예상했던대로 memory allocation이 들어가서 느립니다. 이 경우는 한번만 만들고 내용을 계속 복사해주는 게 더 빨라요.</p>
<img src="/20201210-small-string-optimisation/sso_in_action.png" class="" title="SSO 실행">


]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (8) - 두번 계산하지 마세요! (당연하게도)</title>
    <url>/20201211-dont-compute-twice/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “don’t compute it twice”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8yZF90cmFuc2Zvcm1zLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="두번-계산하지-마세요"><a href="#두번-계산하지-마세요" class="headerlink" title="두번 계산하지 마세요!"></a>두번 계산하지 마세요!</h1><hr>
<p>의미가 없는 계산을 하지 않는 것은 최적화 측면에서 엄청나게 당연한겁니다.</p>
<p>문제는 우리가 보통 코드를 짤 때 ‘의미가 없는 계산’인지 모르고 짤 때가 많다는 거죠. 이런 문제는 코드 리뷰 또는 회고를 할 때만 보입니다.</p>
<p>사실 이런 문제는 (심지어 같은 목적의 같은 맥락에서 사용되는 코드를) 여러 오픈소스 프로젝트 코드에서 종종 보이곤 합니다.</p>
<p>몇 백개의 Github star를 뽐내는 프로젝트들에서도 이런 부분이 최적화가 안 된 경우도 많습니다.</p>
<p>아래 좋은 예제를 가져왔습니다. Velodyne 라이다를 이용한 LOAM (라이다 오도메트리) 프로젝트의 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhYm9zaGlubC9sb2FtX3ZlbG9keW5lL3B1bGwvMjA=">‘Speed up improvement for laserOdometry and scanRegister (20%)’<i class="fa fa-external-link-alt"></i></span> PR 입니다.</p>
<br>

<h2 id="제일-자주-보이는-실수-LUT를-사용합시다"><a href="#제일-자주-보이는-실수-LUT를-사용합시다" class="headerlink" title="제일 자주 보이는 실수: LUT를 사용합시다."></a>제일 자주 보이는 실수: LUT를 사용합시다.</h2><hr>
<p>아래 코드를 한번 봅시다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> x1 = x*<span class="built_in">cos</span>(ang) - y*<span class="built_in">sin</span>(ang) + tx;</span><br><span class="line"><span class="keyword">double</span> y1 = x*<span class="built_in">sin</span>(ang) + y*<span class="built_in">cos</span>(ang) + ty;</span><br></pre></td></tr></table></figure>

<p>그래픽스나 로보틱스를 공부하신 분들에게는 이 코드가 어떤건지 바로 알아보실겁니다. </p>
<p>2D point의 Affine transformation이죠.</p>
<p>이 코드에는 최적화가 될 수 있는 부분이 뻔하게 보입니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> Cos = <span class="built_in">cos</span>(angle);</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> Sin = <span class="built_in">sin</span>(angle);</span><br><span class="line"><span class="keyword">double</span> x1 = x*Cos - y*Sin + tx;</span><br><span class="line"><span class="keyword">double</span> y1 = x*Sin + y*Cos + ty;</span><br></pre></td></tr></table></figure>

<p>Trigonometic function은 생각보다 계산량이 꽤 높습니다. 이 함수로 동일한 값을 두번이나 계산하는 것은 효율적이지 않습니다. 실제로, 아래의 개선된 코드가 위의 코드보다 두배 빠릅니다. </p>
<p>이런 affine transformation 코드에서는 다양한 <code>angle</code>값이 사용될 수 있습니다. 특정 floating point의 값이 사용될 수도 있고, 아니면 고정 integer 값이 사용될 수도 있죠. </p>
<p>우리가 사용할 <code>angle</code>값이 많지 않을 때는, 해당 값들을 미리 계산해서 따로 저장해두는 <strong><em>look-up table (LUT)</em></strong>를 만들어두면 좋습니다.</p>
<p>아까 링크에 적어둔 PR이 풀려고 하는 문제가 딱 이 케이스였습니다. Laser scan data가 polar coordinate에서 Cartesian coordinate로 변환되는 코드 부분이였습니다.</p>
<img src="/20201211-dont-compute-twice/laser_scan_matcher.png" class="" title="Laser scan matching">

<p>이 코드를 아래에 간단하게 정리해뒀습니다. 매 point마다 trigonometric function이 적용되며, 이 계산은 1초에 몇 천번 수행되죠.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 비효율적인 코드...!</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; scan_distance; <span class="comment">// 인풋</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Pos2D&gt; cartesian_points; <span class="comment">// 아웃풋</span></span><br><span class="line"></span><br><span class="line">cartesian_points.reserve( scan_distance.size() );</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;scan_distance.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> dist = scan_distance[i];</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> angle = angle_minimum + (angle_increment*i);</span><br><span class="line">    <span class="keyword">double</span> x = dist*<span class="built_in">cos</span>(angle);</span><br><span class="line">    <span class="keyword">double</span> y = dist*<span class="built_in">sin</span>(angle);</span><br><span class="line">    cartesian_points.push_back( Pos2D(x,y) );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br >

<p>여기 우리 잠깐 멈추고 생각해봅시다: </p>
<ul>
<li><p><strong>최소 angle 값</strong>과 **매 angle이 바뀌는 값 (increment value)**는 일정합니다.</p>
</li>
<li><p><strong>scan_distance</strong>의 size도 항상 일정합니다. </p>
<p>이런 경우가 우리가 LUT를 써서 성능을 향상시킬 수 있는 좋은 예시입니다.</p>
</li>
</ul>
<br>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="comment">//------ 여기 부분을 한번만 계산하고 LUT를 만듭니다 -------</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; LUT_cos;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; LUT_sin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;scan_distance.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> angle = angle_minimum + (angle_increment*i);</span><br><span class="line">    LUT_cos.push_back( <span class="built_in">cos</span>(angle) );</span><br><span class="line">    LUT_sin.push_back( <span class="built_in">sin</span>(angle) );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 그리고 여기 conversion에서 LUT값을 불러와 효울적으로 계산을 수행합니다. ------</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; scan_distance;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Pos2D&gt; cartesian_points;</span><br><span class="line"></span><br><span class="line">cartesian_points.reserve( scan_distance.size() );</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;scan_distance.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> dist = scan_distance[i];</span><br><span class="line">    <span class="keyword">double</span> x = dist*LUT_cos[i];</span><br><span class="line">    <span class="keyword">double</span> y = dist*LUT_sin[i];</span><br><span class="line">    cartesian_points.push_back( Pos2D(x,y) );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h1 id="정리하자면…"><a href="#정리하자면…" class="headerlink" title="정리하자면…"></a>정리하자면…</h1><hr>
<p>위에서 우리는 간단한 예제를 보았습니다. </p>
<p>여기서 우리가 알아야할것은 비싼 계산이 들어갈 때마다 (e.g. SQL 쿼리, state를 거치지 않는 계산) 우리는 미리 저장된 값을 사용하거나 LUT를 사용하는 것을 고려해봐야합니다.</p>
<p>하지만 항상 그렇듯이, 최적화를 적용하기 전에 현재 코드가 얼마나 비효율적인지 <strong><em>실제로 측정</em></strong>해보고, 그 후에 최적화가 의미가 있을 지 정하는 것이 제일 중요합니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
        <tag>Robotics</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (7) - String 이어 붙일 때 &#39;+&#39; 쓰지 마세요!</title>
    <url>/20201211-string-concat/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “String concatenation the false sense of security of <code>operator+</code>“을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9zdHJpbmdzX2NvbmNhdGVuYXRpb24v">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="String-이어붙이기"><a href="#String-이어붙이기" class="headerlink" title="String 이어붙이기"></a>String 이어붙이기</h1><hr>
<p>이 글을 시작하기 전에 룰 #1을 설명하고 시작하겠습니다.</p>
<p>룰 #1 : <strong><em>“Profiler로 봤을 때 오버헤드가 있는 경우에만 최적화를 하세요”</em></strong></p>
<p>이전 글에서도 얘기했듯이, string은 <code>std::vector&lt;char&gt;</code>과 다를바가 없습니다. 그렇기 때문에 데이터를 저장하기 위해 heap allocation를 해야할 수도 있죠.</p>
<p>C++에서 string을 이어붙이는 것은 (i.e. concatenate) 굉장히 쉽습니다. 하지만 조심해야할 점이 있습니다. </p>
<br>

<h2 id="많이-사용하는-concatenation-방법"><a href="#많이-사용하는-concatenation-방법" class="headerlink" title="많이 사용하는 concatenation 방법"></a>많이 사용하는 concatenation 방법</h2><hr>
<p>우리가 자주 쓰는 방법은 아래와 같습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>:<span class="built_in">string</span> big_string = first + <span class="string">&quot; &quot;</span> + second + <span class="string">&quot; &quot;</span> + third;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Where...</span></span><br><span class="line"><span class="comment">// std::string first(&quot;This is my first string.&quot;);</span></span><br><span class="line"><span class="comment">// std::string second(&quot;This is the second string I want to append.&quot;);</span></span><br><span class="line"><span class="comment">// std::string third(&quot;This is the third and last string to append.&quot;); </span></span><br></pre></td></tr></table></figure>

<p>우리가 많이 썼던 방식이지만… 최적화 글을 몇개 읽으신 당신에게는 아마 이제 뭔가 쎄한 느낌이 들겁니다. </p>
<blockquote>
<p>‘이렇게 적으면 Heap allocation에 문제가 있을텐데…??”</p>
</blockquote>
<img src="/20201211-string-concat/spider_senses.png" class="" title="string sense가 팅글링한다~~">

<br>

<p>쎄한 느낌에 최적화 센스가 팅글링합니다.</p>
<p>Heap allocation을 고려해서 다시 적어보겠습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>:<span class="built_in">string</span> big_string = (((first + <span class="string">&quot; &quot;</span>) + second) + <span class="string">&quot; &quot;</span>) + third;</span><br></pre></td></tr></table></figure>

<p>이 정도 길이의 string을 이어붙이려면, heap allocation이 많이 수행되어야하고, 매번 이전 메모리에서 신규 메모리로 복사가 되어야합니다.</p>
<p><code>std::string</code>이 <code>std::vector::reserve()</code>와 같은 기능이 있었으면 좋겠는데 말이죠… :(</p>
<p>…는 실제로 <span class="exturl" data-url="aHR0cHM6Ly9lbi5jcHByZWZlcmVuY2UuY29tL3cvY3BwL3N0cmluZy9iYXNpY19zdHJpbmcvcmVzZXJ2ZQ==">이 기능<i class="fa fa-external-link-alt"></i></span>이 있었네요? 띠용용</p>
<br>

<h2 id="무작정-가져다-붙이기"><a href="#무작정-가져다-붙이기" class="headerlink" title="무작정 가져다 붙이기"></a>무작정 가져다 붙이기</h2><hr>
<p><code>reserve</code>를 사용해서 heap allocation의 횟수를 한번으로 줄여봅시다.</p>
<p><code>big_string</code> 안에 들어가는 글자의 수를 미리 세어두고 <code>reserve</code>를 사용하면 이렇게 될겁니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> big_one;</span><br><span class="line">big_one.reserve(first_str.size() + </span><br><span class="line">                second_str.size() + </span><br><span class="line">                third_str.size() + </span><br><span class="line">                <span class="built_in">strlen</span>(<span class="string">&quot; &quot;</span>)*<span class="number">2</span> );</span><br><span class="line"></span><br><span class="line">big_one += first;</span><br><span class="line">big_one += <span class="string">&quot; &quot;</span>;</span><br><span class="line">big_one += second;</span><br><span class="line">big_one += <span class="string">&quot; &quot;</span>;</span><br><span class="line">big_one += third;</span><br></pre></td></tr></table></figure>

<p>이 코드를 보시는 여러분들이 무슨 생각을 하시는지 다 압니다. </p>
<img src="/20201211-string-concat/feel_bad.jpg" class="" title="어우야 대체 왜 제발 좀">

<p>으… 이 더러운 코드는 뭐야…</p>
<p>라고 생각하게 되지만, 실제로 Profiler를 돌려보면 이 코드는 기존의 방법보다 <strong><em>2.5배</em></strong> 나 더 빠릅니다. </p>
<p>속도는 좋은데, 진짜 가독성 어떻게하죠 이거?</p>
<br>

<h2 id="더-나은-방법-Variadic-concatenation"><a href="#더-나은-방법-Variadic-concatenation" class="headerlink" title="더 나은 방법? - Variadic concatenation"></a>더 나은 방법? - Variadic concatenation</h2><hr>
<p>좀 더 빠르고, 재사용하기 좋고, 읽기 쉬운 string concat 방법이 없을까요?</p>
<p>Modern C++을 사용해야하는 이유가 여기에 있습니다. <strong><em>Variadic templates</em></strong>를 사용해봅시다.</p>
<p>Variadic template가 뭔지 모르신다면, 여기 템플릿에 대해 설명을 잘 해주는 굉장히 좋은 <span class="exturl" data-url="aHR0cHM6Ly9hcm5lLW1lcnR6LmRlLzIwMTYvMTEvbW9yZS12YXJpYWRpYy10ZW1wbGF0ZXMv">링크<i class="fa fa-external-link-alt"></i></span>가 있습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//--- String의 전체 크기를 구해주는 함수 ---</span></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">StrSize</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* str)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">strlen</span>(str);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">StrSize</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; str)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> str.size();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Head</span>, <span class="title">class</span>... <span class="title">Tail</span>&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">size_t</span> <span class="title">StrSize</span><span class="params">(<span class="keyword">const</span> Head&amp; head, Tail <span class="keyword">const</span>&amp;... tail)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> StrSize(head) + StrSize(tail...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//--- String append 를 해주는 함수 ---</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Head</span>&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StrAppend</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span>&amp; out, <span class="keyword">const</span> Head&amp; head)</span> </span>&#123;</span><br><span class="line">  out = head;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Head</span>, <span class="title">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StrAppend</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span>&amp; out, <span class="keyword">const</span> Head&amp; head, Args <span class="keyword">const</span>&amp;... args)</span> </span>&#123;</span><br><span class="line">  out += head;</span><br><span class="line">  StrAppend(out, args...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//--- String concat을 해주는 함수 ---</span></span><br><span class="line"><span class="keyword">template</span> &lt;class... Args&gt; </span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">StrCat</span><span class="params">(Args <span class="keyword">const</span>&amp;... args)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">size_t</span> tot_size = StrSize(args...);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> out;</span><br><span class="line">  out.reserve(tot_size);</span><br><span class="line"></span><br><span class="line">  StrAppend(out, args...);</span><br><span class="line">  <span class="keyword">return</span> out;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>복잡한 코드가 좀 많았죠? </p>
<p>그래도 우리는 이제 아래와 같이 굉장히 읽기 좋고 편한 방법으로 string concat을 할 수 있습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>:<span class="built_in">string</span> big_string = StrCat(first, <span class="string">&quot; &quot;</span>, second, <span class="string">&quot; &quot;</span>, third );</span><br></pre></td></tr></table></figure>

<p>그럼 이제 얼마나 빨라졌는지 봅시다.</p>
<img src="/20201211-string-concat/string_concatenation.png" class="" title="string concat - benchmark">

<p>이 Variadic template 방식이 위의 ‘더러운’ 코드보다 느린 이유는…</p>
<p>사실 잘 모르겠습니다!</p>
<p>대신, variadic template 방식은 기존의 방식보다 2배나 빠르고 또 읽기 쉽다는건 확신합니다.</p>
<br>

<h2 id="제-코드를-복붙하시기-전에…"><a href="#제-코드를-복붙하시기-전에…" class="headerlink" title="제 코드를 복붙하시기 전에…"></a>제 코드를 복붙하시기 전에…</h2><hr>
<p>제가 구현한 <code>StrCat</code>은 사실 그렇게 사용성이 좋지 않습니다. 제가 이 코드를 만든 이유는, 단순히 기존의 string concat보다 이 방식이 더 빠르다는걸 보여드리고 싶었어요.</p>
<p>실제로 구현 하실 때는, 위의 코드도 다 잊어버리시고 그냥 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZtdGxpYi9mbXQ=">{fmt} 라이브러리<i class="fa fa-external-link-alt"></i></span>를 사용하세요.</p>
<p>fmt 라이브러리는 쓰기도 엄청 쉽고, 도큐먼트도 잘 만들어져있고, 그리고 string formatting 할 때 <strong><em>엄청나게 빠릅니다</em></strong>.</p>
<p>그리고 <span class="exturl" data-url="aHR0cHM6Ly9lbi5jcHByZWZlcmVuY2UuY29tL3cvY3BwL3V0aWxpdHkvZm9ybWF0">C++20 std::format<i class="fa fa-external-link-alt"></i></span>의 구현체이기도 하지요.</p>
<p>이즉슨, {fmt}를 쓰시거나 C++20을 쓰시면 읽기 쉽고 빠른 string 코드를 쓰실 수 있다는 겁니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (9) - 이미지 속 픽셀 계산 빨리 하는 방법 (또는 2D matrix 계산 빨리 하는 방법)</title>
    <url>/20201212-2d-matrix-iteration/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Iterating over 2D matrix”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8yZF9tYXRyaXhfaXRlcmF0aW9uLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="2D-matrix-iteration-하기"><a href="#2D-matrix-iteration-하기" class="headerlink" title="2D matrix iteration 하기"></a>2D matrix iteration 하기</h1><hr>
<p>2D matrix는 로보틱스에서 엄청나게 자주 사용됩니다.</p>
<p>이미지, gridmap/costmap 등등을 2D 매트릭스로 표현하지요. </p>
<p>최근에 저는 우리 팀에서 costmap 계산에서 사용되는 알고리즘이 꽤 느리다고 느꼈고, <strong>Hotspot</strong>을 이용해서 프로파일링을 해봤습니다.</p>
<p>두개의 costmap을 합쳐서 새로운 costmap을 만드는 과정에서 바틀넥이 있었습니다.</p>
<p>대충 이런 코드였죠.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 대충 적은 코드입니다.</span></span><br><span class="line"><span class="keyword">for</span>( <span class="keyword">size_t</span> y = y_min; y &lt; y_max; y++ ) </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">size_t</span> x = x_min; x &lt; x_max; x++ ) </span><br><span class="line">    &#123;</span><br><span class="line">        matrix_out( x,y ) = <span class="built_in">std</span>::max( mat_a( x,y ), mat_b( x,y ) ); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>이해하기는 쉬운 코드였습니다.</p>
<p>잘 적었으니까요. 그쵸? </p>
<p>하지만 이 계산은 실제로 돌 때 너무 많은 CPU 사이클을 잡아먹고 있었습니다.</p>
<p>어쩔수 없이 저는 최적화 여행을 또 떠나게 되었지요 (휴)</p>
<br>

<h2 id="C-에서-좋은-2D-매트릭스-만드는-방법"><a href="#C-에서-좋은-2D-매트릭스-만드는-방법" class="headerlink" title="C++에서 좋은 2D 매트릭스 만드는 방법"></a>C++에서 좋은 2D 매트릭스 만드는 방법</h2><hr>
<p>이런 코드를 적어본 적 있으시나요?</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 컴공 학생 때 교수님은 이런 코드를 적으셨는데요...</span></span><br><span class="line"><span class="keyword">float</span>** matrix = (<span class="keyword">float</span>**) <span class="built_in">malloc</span>( rows_count * <span class="keyword">sizeof</span>(<span class="keyword">float</span>*) );</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> r=<span class="number">0</span>; r&lt;rows_count; r++) </span><br><span class="line">&#123;</span><br><span class="line">    matrix[r] = (<span class="keyword">float</span>*) <span class="built_in">malloc</span>( columns_count * <span class="keyword">sizeof</span>(<span class="keyword">float</span>) );</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// element access는 이렇게 합니다</span></span><br><span class="line">matrix[row][col] = <span class="number">42</span>;</span><br></pre></td></tr></table></figure>

<br>

<p>이러면 망한겁니다.</p>
<p>이 코드를 본 이상, 어쩔 수 없이 이 코드를 USB에 담아서 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTW91bnRfRG9vbQ==">모르도르로<i class="fa fa-external-link-alt"></i></span> 가져가 용암 속에 던질 수 밖에 없어요.</p>
<img src="/20201212-2d-matrix-iteration/mordor.jpg" class="" title="이 코드는 불태워버려야합니다">


<br>

<p>우선, 저희는 <span class="exturl" data-url="aHR0cDovL2VpZ2VuLnR1eGZhbWlseS5vcmcv">Eigen 라이브러리<i class="fa fa-external-link-alt"></i></span>를 사용해보는 것을 고려해봐야합니다. Eigen 라이브러리는 성능도 좋고, 이해하기 쉬운 API를 가지고 있어요.</p>
<p>실사용은 Eigen으로 한다지만, 공부를 하기 위한 목적으로는 물론 직접 짜봐야겠지요 (하지만… 진짜 그냥 Eigen 쓰시는게 좋아요. 진심으로.). 아래 코드에 예시가 있습니다.</p>
<br>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="class"><span class="keyword">class</span> <span class="title">Matrix2D</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Matrix2D(<span class="keyword">size_t</span> rows, <span class="keyword">size_t</span> columns):  _num_rows(rows)</span><br><span class="line">    &#123;</span><br><span class="line">        _data.resize( rows * columns );</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">size_t</span> <span class="title">rows</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123; </span><br><span class="line">    	<span class="keyword">return</span> _num_rows; </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">T&amp; <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">size_t</span> row, <span class="keyword">size_t</span> col)</span>  </span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">size_t</span> index = col*_num_rows + row; </span><br><span class="line">        <span class="keyword">return</span> _data[index];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    T&amp; <span class="keyword">operator</span>[](<span class="keyword">size_t</span> index)  </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> _data[index];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 다른 method는 딱히 설명하지 않겠습니다.</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;T&gt; _data;</span><br><span class="line">    <span class="keyword">size_t</span> _num_rows;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// element access는 이렇게 하시면 됩니다.</span></span><br><span class="line">matrix(row, col) = <span class="number">42</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<br>

<p>이 방법이 가장 캐시메모리 사용에 유용한 매트릭스 생성 방법입니다. Single memory allocation에, 데이터는 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvcm93LXdpc2UtdnMtY29sdW1uLXdpc2UtdHJhdmVyc2FsLW1hdHJpeC8=">column-wise<i class="fa fa-external-link-alt"></i></span>방식으로 저장됩니다.</p>
<p>우리가 쉽게 이해하는 row/column 값을 vector의 index값으로 변환하기 위해서는, 단 한번의 곱셈연산과 덧셈연산이 필요합니다.</p>
<br>

<h2 id="원래-문제로-돌아가서…"><a href="#원래-문제로-돌아가서…" class="headerlink" title="원래 문제로 돌아가서…"></a>원래 문제로 돌아가서…</h2><hr>
<p>처음에 보여드렸던 제 코드로 돌아가봅시다.</p>
<p>우리는 여러번 iteration을 해야하는데, 매번 <code>(x_max-x_min)*(y_max-y_min)</code> 계산을 할 겁니다.</p>
<p>종종, 꽤 많은 pixel 또는 cell에 대한 계산이 들어가겠죠.</p>
<p>매 iteration마다 우리는 다음과 같은 방법으로 index 값을 세번 계산해야합니다.  </p>
<pre><code> size_t index = col*_num_rows + row;</code></pre>
<p>곱셈 계산이 엄청나게 많군요! 좋지 않습니다.</p>
<br>

<p>아래와 같은 코드로 바꿔보았습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// index를 직접 계산해봅시다.</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">size_t</span> y = y_min; y &lt; y_max; y++) </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">size_t</span> offset_out =  y * matrix_out.rows();</span><br><span class="line">    <span class="keyword">size_t</span> offset_a   =  y * mat_a.rows();</span><br><span class="line">    <span class="keyword">size_t</span> offset_b   =  y * mat_b.rows();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> x = x_min; x &lt; x_max; x++) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">size_t</span> index_out =  offset_out + x;</span><br><span class="line">        <span class="keyword">size_t</span> index_a   =  offset_a + x;</span><br><span class="line">        <span class="keyword">size_t</span> index_b   =  offset_b + x;</span><br><span class="line">        matrix_out( index_out ) = <span class="built_in">std</span>::max( mat_a( index_a ), mat_b( index_b ) ); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<br>

<p>무슨 생각을 하고 계신지 압니다. <strong><em>제 눈도 아파요</em></strong>.</p>
<p>코드가 엄청 더럽죠.</p>
<p>하지만 이로써 얻는 성능 개선은 무시할 수 없는 정도입니다.</p>
<p>사실 당연한 결과일 수도 있겠습니다. 곱셈연산의 수가 3배나 줄어들었으니까요.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
        <tag>Robotics</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (10) - STL에 없는 새로운 컨테이너</title>
    <url>/20201214-exotic-container/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Optimizing using an exotic associative container”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9ib29zdF9mbGF0bWFwLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><hr>
<p>지난주에 제가 사용하는 모듈 중 CPU를 많이 잡아먹는 친구를 뜯어보았습니다.</p>
<p>정확히 어떤 모듈인지 설명드릴 수는 없지만, 대충 로우레벨 하드웨어 인터페이스로 저희 회사 로봇 제품의 모터를 제어하는 코드입니다.</p>
<p>저는 이 코드가 CPU 코어 하나를 다 차지할정도로 이 소프트웨어가 무거우면 안되는 것을 잘 알고 있습니다. 하지만, 안타깝게도 지금의 코드는 그런 상태이죠.</p>
<p>언제나와 같이, 저는 제 무기인 <strong>Hotspot</strong>을 꺼내들었고, CPU가 코드의 어떤 부분에서 시간을 제일 많이 보내는지 알아봤습니다.</p>
<img src="/20201214-exotic-container/motor_profile1.png" class="" title="모터 프로파일링 1">

<p>좀 지저분하지요? 어쩔 수 없습니다.</p>
<p><span class="exturl" data-url="aHR0cDovL3d3dy5icmVuZGFuZ3JlZ2cuY29tL2ZsYW1lZ3JhcGhzLmh0bWw=">Flamegraph<i class="fa fa-external-link-alt"></i></span>를 처음 보시는 분들에게 간단하게 설명을 드리자면, caller function이 제일 밑에 있고, 점차 위로 callee function이 쌓여가는거라고 보시면 됩니다.</p>
<p>여기서 제가 발견한 것은, 제 CPU의 30%가 <code>std::unordered_map&lt;&gt;::operator[]</code>에 쓰이고 있다는 것입니다.</p>
<img src="/20201214-exotic-container/motor_profile2.png" class="" title="모터 프로파일링 2">

<p>오른쪽에는 엄청나게 큰 블록이 있고, 왼쪽에는 작은 블록이 여러개 많이 있습니다.</p>
<p>오른쪽 블록은 caching으로 쉽게 풀 수 있고, 왼쪽은 좀 풀기 어렵죠.</p>
<p>문제가 되는 코드는 아래와 같습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 간단한 코드</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">unordered_map</span>&lt;Address, Entry*&gt; m_dictionary;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Address</span>&#123;</span></span><br><span class="line">    <span class="keyword">int16_t</span> index;</span><br><span class="line">    <span class="keyword">unt8_t</span> subindex;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<br>

<h1 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h1><hr>
<p>코드를 좀 더 뜯어봤습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 간단하게 적은 코드</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">hasEntry</span><span class="params">(<span class="keyword">const</span> Address&amp; address)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> m_dictionary.count(address) != <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Value <span class="title">getEntry</span><span class="params">(<span class="keyword">const</span> Address&amp; address)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( !hasEntry(address) &#123;</span><br><span class="line">        <span class="keyword">throw</span> ...</span><br><span class="line">    &#125;</span><br><span class="line">    Entry* = m_dictionary[address];</span><br><span class="line">    <span class="comment">// ... 나머지 코드</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>


<img src="/20201214-exotic-container/two_lookups.jpg" class="" title="두번 룩업하네요!">

<br>

<p><strong><em>두번이나 값을 찾네요</em></strong>! </p>
<p>누가 짠거야 이 코드!</p>
<p>아래처럼 코드를 바꿔줬습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 한번만 값을 찾도록 바꿈</span></span><br><span class="line"><span class="function">Value <span class="title">getEntry</span><span class="params">(<span class="keyword">const</span> Address&amp; address)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> it = m_dictionary.find(address);</span><br><span class="line">    <span class="keyword">if</span>( it ==  m_dictionary.end() ) &#123;</span><br><span class="line">        <span class="keyword">throw</span> ...</span><br><span class="line">    &#125;</span><br><span class="line">    Entry* = it-&gt;second;</span><br><span class="line">    <span class="comment">// ... 나머지 코드</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>이렇게 코드를 바꿔주는 것 만으로도 <code>std::unordered_map</code>의 오버헤드를 반이나 줄일 수 있었습니다.</p>
<p>이제 오른쪽, 왼쪽 블록의 문제를 해결해봅시다.</p>
<br>

<h2 id="오른쪽-블록-캐싱을-합시다-i-e-따로-저장해둡시다"><a href="#오른쪽-블록-캐싱을-합시다-i-e-따로-저장해둡시다" class="headerlink" title="오른쪽 블록: 캐싱을 합시다! (i.e. 따로 저장해둡시다!)"></a>오른쪽 블록: 캐싱을 합시다! (i.e. 따로 저장해둡시다!)</h2><hr>
<p>우선 짚고 넘어가야하는 점은, 이 <code>std::unordered_map</code> 형태의 dictionary는 한번 만들어지면 run-time에서 절대 바뀌지 않는다는 점 입니다.</p>
<p>이 점을 이용해서 우리는 flamegraph의 오른쪽에서 엄청 큰 블록에 최적화를 할 수 있습니다. 바로 캐싱을 이용하면 됩니다. </p>
<p>Map에서 값을 찾을 때 우리는 <code>Address</code>를 통해 <code>Entry*</code>를 찾습니다. 하지만 이 pair가 절대로 변하지 않는다면, 그냥 <code>Entry*</code>만 저장해도 되지 않을까요?</p>
<p>그 결과, <code>std::unordered_map</code>를 사용한 오른쪽 블록의 큰 오버헤드가 사라졌고 , 약 15%의 CPU만 사용하는 효율적인 코드가 되었습니다.</p>
<p>이렇게 따로 값을 저장하는 LUT를 만들고 사용하는 것은 굉장히 효과적입니다. 좀 더 자세히 알고싶으신 분들은 <a href="https://changh95.github.io/20201211-dont-compute-twice/">지난 글</a>을 참조하시길 바랍니다.</p>
<br>

<h2 id="왼쪽-블록-boost-container-flat-map로-광명찾기"><a href="#왼쪽-블록-boost-container-flat-map로-광명찾기" class="headerlink" title="왼쪽 블록: boost::container_flat_map로 광명찾기"></a>왼쪽 블록: <code>boost::container_flat_map</code>로 광명찾기</h2><hr>
<p>이제 왼쪽의 코드를 봅시다. </p>
<p>왼쪽의 경우, 모두 <code>std::unordered_map</code>을 사용하기는 하지만, 사용하는 코드가 다 제각각입니다. 이 모든 코드를 하나하나 다 LUT로 바꿔주기는 굉장히 귀찮습니다.</p>
<p>하지만 그렇다고 가만히 둘 수도 없죠. ‘Death by 1000 papercuts’가 딱 이 케이스입니다.</p>
<p>그리고 제 머릿속을 스쳐가는 무언가가 있었습니다.</p>
<p><code>std;:has&lt;Address&gt;</code>는 왜 이렇게 오래걸리는걸까요? 이게 딱 그 hash table이 잘 작동하지 못하는 (Big O() 노테이션이 작동하지 않는) 몇개의 <strong><em>레어 케이스</em></strong> 인걸까요?</p>
<p><code>std::unordered_map</code> 룩업은 O(1) 이여야 할 터입니다. </p>
<p>분명 잘 되야할텐데, 기가차고 코가찰 노릇입니다.</p>
<br>

<h3 id="Boost-라이브러리-flat-map"><a href="#Boost-라이브러리-flat-map" class="headerlink" title="Boost 라이브러리 - flat_map"></a>Boost 라이브러리 - flat_map</h3><hr>
<p>이 문제를 풀려면 꽤나 머리를 싸매야 할 것 같습니다.</p>
<p>한번 다른 시각에서 문제를 바라보겠습니다. O(logn)을 쓰면서, hash function의 단점이 없는 마법같은 컨테이너는 없는걸까요?</p>
<p>자 신사숙녀 여러분, <span class="exturl" data-url="aHR0cHM6Ly93d3cuYm9vc3Qub3JnL2RvYy9saWJzLzFfNzRfMC9kb2MvaHRtbC9jb250YWluZXIvbm9uX3N0YW5kYXJkX2NvbnRhaW5lcnMuaHRtbCNjb250YWluZXIubm9uX3N0YW5kYXJkX2NvbnRhaW5lcnMuZmxhdF94eHg=">boost::container_flat_map<i class="fa fa-external-link-alt"></i></span>를 소개합니다.</p>
<p>위의 링크에서 소개하는 내용을 반복하지는 않겠습니다. 도큐먼트가 상당히 설명을 잘 해줍니다. </p>
<p>아주 짧게 설명하면, map을 기존의 ordered vector처럼 사용하겠다는 겁니다. <a href="https://changh95.github.io/20201206-do-you-need-map/">이전 글</a>에서 제가 설명했던 방식과 굉장히 유사합니다.</p>
<p>결과는 놀랍습니다.</p>
<p>오버헤드가 그냥 줄은게 아니라, <strong><em>아예 사라져버렸습니다</em></strong>.</p>
<p><code>flat_map&lt;&gt;::operator[]</code>의 비용은 거의 없더군요.</p>
<p>단순하게 <code>flat_map</code>으로 바꿔주는 것 만으로도 위의 모든 내용이 한줄의 코드로 바뀌면서 오버헤드가 모두 사라졌습니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
        <tag>Data Structure</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (11) - PCL 라이브러리 최적화</title>
    <url>/20201215-case-study-pcl/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Case study: filter a Point Cloud faster”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9wY2xfZmlsdGVyLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<p><span class="exturl" data-url="aHR0cHM6Ly9wb2ludGNsb3Vkcy5vcmcv">Point Cloud Library (PCL)<i class="fa fa-external-link-alt"></i></span>는 로보틱스, 자율주행, 3D 인지 쪽에서 굉장히 유명한 라이브러리죠. 이 라이브러리는 위 분야들에 <strong><em>엄청난 기여</em></strong>를 했습니다. </p>
<img src="/20201215-case-study-pcl/pcl.jpg" class="" title="PCL library">

<p>PCL은 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BvaW50Q2xvdWRMaWJyYXJ5L3BjbA==">12,000개가 넘는 커밋과 5천개가 넘는 Github star<i class="fa fa-external-link-alt"></i></span>를 가진 거대 오픈소스 프로젝트입니다.</p>
<p>약 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BvaW50Q2xvdWRMaWJyYXJ5L3BjbC9ncmFwaHMvY29udHJpYnV0b3Jz">400명이 넘는 컨트리뷰터<i class="fa fa-external-link-alt"></i></span>가 있는데, 이런 경우에는 이 분야 고수들이 기능을 만드느라 최적화를 할 기회가 없다고 생각하실 수도 있겠습니다.</p>
<p>그리고 여기 제가 있습니다.</p>
<p>나: “ㅋㅋㅋ”</p>
<br>

<h2 id="ConditionalRemoval-필터"><a href="#ConditionalRemoval-필터" class="headerlink" title="ConditionalRemoval 필터"></a>ConditionalRemoval 필터</h2><hr>
<p>PCL을 조금 써보신 분들이라면 <span class="exturl" data-url="aHR0cHM6Ly9wY2wtdHV0b3JpYWxzLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9yZW1vdmVfb3V0bGllcnMuaHRtbA==">pcl::ConditionalRemoval<i class="fa fa-external-link-alt"></i></span> 기능을 한번쯤은 써보셨을 것 같습니다. </p>
<p><span class="exturl" data-url="aHR0cHM6Ly9wY2wtdHV0b3JpYWxzLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9yZW1vdmVfb3V0bGllcnMuaHRtbA==">공식 튜토리얼<i class="fa fa-external-link-alt"></i></span>에서는 해당 기능을 다음과 같은 방식으로 사용하라고 알려줍니다:</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 읽기 쉽게 조금 코드를 바꿨습니다</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> pcl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> range_cond  = <span class="built_in">std</span>::make_shared&lt;ConditionAnd&lt;PointXYZ&gt; ();</span><br><span class="line">range_cond-&gt;addComparison ( </span><br><span class="line">    <span class="built_in">std</span>::make_shared&lt;FieldComparison&lt;PointXYZ&gt;(<span class="string">&quot;z&quot;</span>, ComparisonOps::GT, <span class="number">0.0</span>));</span><br><span class="line">range_cond-&gt;addComparison (</span><br><span class="line">    <span class="built_in">std</span>::make_shared&lt;FieldComparison&lt;PointXYZ&gt;(<span class="string">&quot;z&quot;</span>, ComparisonOps::LT, <span class="number">1.0</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 필터 생성</span></span><br><span class="line">ConditionalRemoval&lt;PointXYZ&gt; condition_removal;</span><br><span class="line">condition_removal.setCondition (range_cond);</span><br><span class="line">condition_removal.setInputCloud (input_cloud);</span><br><span class="line"><span class="comment">// 필터 적용</span></span><br><span class="line">condition_removal.filter (*cloud_filtered);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>이 코드는 간단하게 Point cloud에 필터를 적용하는데, 필터를 통과할 수 있는 point의 특성에 대한 조건을 걸어주는 겁니다. 여기서 그 조건은 다음과 같습니다. 0.0보다 크고 (Greater than == GT) 1.0보다 작은 경우 (Less than == LT). </p>
</blockquote>
<br>

<p>PCL 사용에 익숙하지 않으신 분들을 위해 조금 더 쉽게 정리하겠습니다.</p>
<ul>
<li>필터 조건을 하나 만듭니다. 첫번째 조건은 ‘Point의 xyz좌표 중 Z 값이 0.0보다 커야한다’ 입니다.</li>
<li>또 다른 필터 조건을 하나 만듭니다. 두번째 조건은 ‘Point의 xyz좌표 중 Z 값이 1.0보다 작아야한다’ 입니다. </li>
<li>이 조건들은 <code>ConditionAnd</code>에 추가됩니다.</li>
<li><code>ConditionalRemoval</code>을 사용해 이 두개의 조건들을 Point cloud에 적용할겁니다.</li>
<li>필터가 적용되면, 이 조건을 통과하는 point들만 point cloud에 남게 됩니다. </li>
</ul>
<br>
보통의 Point cloud는 몇천개~몇만개의 포인트를 가지고 있습니다.

<p>한번 생각해봅시다: </p>
<img src="/20201215-case-study-pcl/think_about_it.jpg" class="" title="생각 좀 해보고">

<p>Point cloud는 사실상 <strong><em>vector</em></strong>에 point 들을 담아놓은 데이터가 아니겠습니까?</p>
<p>각각의 point들은 아래처럼 생겼겠죠.</p>
<br>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 엄청 간단하게 축약한겁니다.</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PointXYZ</span>&#123;</span></span><br><span class="line">  <span class="keyword">float</span> x;</span><br><span class="line">  <span class="keyword">float</span> y;</span><br><span class="line">  <span class="keyword">float</span> z; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<br>

<p>필터를 통과하고 난 후의 point cloud를 새로 만든다고 생각해봅시다. 필터 조건은 아래와 같습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">0.0</span> &lt; point.z &lt; <span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<br>

<p>제가 어떻게 코드를 바꿀껀지 물어보신다면, 저는 우선 이렇게 할 것 같습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> cloud_filtered = <span class="built_in">std</span>::make_shared&lt;PointCloud&lt;PointXYZ&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; point: input_cloud-&gt;points) </span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span>( point.z &gt; <span class="number">0.0</span> &amp;&amp; point.z &lt; <span class="number">1.0</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    cloud_filtered-&gt;push_back( point );</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>굉장히 간단한 <strong>“naive filter”</strong> 입니다.</p>
<p>실제 벤치마크를 보여드리기 전에, 우선 말씀드려야 할 점은… <code>pcl</code> 필터는 사실 이거보다 더 많은 체크를 하긴 합니다. 포인트 클라우드는 종종 이상한 레어 케이스가 있는데, 그걸 방지하는 체크가 많이 내장되어있습니다.</p>
<p>하지만 우선 저희가 아래와 같은 필터 조건을 건 것은 기억해주세요.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">pcl::FieldComparison&lt;pcl::PointXYZ&gt; (<span class="string">&quot;z&quot;</span>, pcl::ComparisonOps::GT, <span class="number">0.0</span>)));</span><br><span class="line">pcl::FieldComparison&lt;pcl::PointXYZ&gt; (<span class="string">&quot;z&quot;</span>, pcl::ComparisonOps::LT, <span class="number">1.0</span>)));</span><br></pre></td></tr></table></figure>

<br>

<p>생각해보면, 어딘가 ‘parser’가 하나 있어야할겁니다.</p>
<p>이 parser를 짠다면 아무래도 <code>switch</code> 문을 써서 만들었을 것 같기도 하지만… 몇천~몇만개의 포인트마다 switch를 쓸 사람은 없을 것 같습니다.</p>
<p>…라고 생각했지만. 제엔장!! <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BvaW50Q2xvdWRMaWJyYXJ5L3BjbC9ibG9iL3BjbC0xLjExLjAvZmlsdGVycy9pbmNsdWRlL3BjbC9maWx0ZXJzL2ltcGwvY29uZGl0aW9uYWxfcmVtb3ZhbC5ocHAjTDk4LUwxMjc=">진짜 switch 썼네요!<i class="fa fa-external-link-alt"></i></span></p>
<p>말도 안됩니다 . 몇천~몇만개의 포인트를, 각각 하나마다 switch문을 그것도 2개나 돌리고 있군요.</p>
<p>요약하자면 - 이 함수들은 이해하기 쉽게 만들기 위해서 <strong><em>속도를 버리고 있습니다</em></strong>.</p>
<p>이렇게 짜인건 어쩔 수 없죠.</p>
<p>대신 우리가 쓸 때 바꿔주면 됩니다 ;)</p>
<br>

<h2 id="빠르고-읽기-쉬운-코드를-짜봅시다"><a href="#빠르고-읽기-쉬운-코드를-짜봅시다" class="headerlink" title="빠르고 읽기 쉬운 코드를 짜봅시다!"></a>빠르고 읽기 쉬운 코드를 짜봅시다!</h2><hr>
<p>수많은 pcl 함수들이 <code>switch</code>문을 쓰면서 성능이 박살났습니다. </p>
<p>우리가 직접 pcl 함수들을 짜봅시다. <code>pcl::ConditionBase</code>는 어떨까요?</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> PointT&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenericCondition</span> :</span> <span class="keyword">public</span> pcl::ConditionBase&lt;PointT&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;GenericCondition&lt;PointT&gt;&gt; Ptr;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> GenericCondition&lt;PointT&gt;&gt; ConstPtr;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::function&lt;<span class="keyword">bool</span>(<span class="keyword">const</span> PointT&amp;)&gt; FunctorT;</span><br><span class="line"></span><br><span class="line">  GenericCondition(FunctorT evaluator): </span><br><span class="line">    pcl::ConditionBase&lt;PointT&gt;(),_evaluator( evaluator ) </span><br><span class="line">  &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">evaluate</span> <span class="params">(<span class="keyword">const</span> PointT &amp;point)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// just delegate ALL the work to the injected std::function</span></span><br><span class="line">    <span class="keyword">return</span> _evaluator(point);</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  FunctorT _evaluator;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<br>
딱 이 코드만 있어도 됩니다.

<p>저는 단순히 <code>pcl::ConditionBase</code>속 <code>std::function&lt;bool&gt;(const PointT&amp;)</code>를 랩핑해준겁니다. 다른건 없어요.</p>
<p>이렇게 바뀐 코드를 적용해봅시다.</p>
<br>

<p><strong>예전 코드</strong>는 아래와 같습니다. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> range_cond  = <span class="built_in">std</span>::make_shared&lt;ConditionAnd&lt;PointXYZ&gt; ();</span><br><span class="line">range_cond-&gt;addComparison ( </span><br><span class="line">    <span class="built_in">std</span>::make_shared&lt;FieldComparison&lt;PointXYZ&gt;(<span class="string">&quot;z&quot;</span>, ComparisonOps::GT, <span class="number">0.0</span>));</span><br><span class="line">range_cond-&gt;addComparison (</span><br><span class="line">    <span class="built_in">std</span>::make_shared&lt;FieldComparison&lt;PointXYZ&gt;(<span class="string">&quot;z&quot;</span>, ComparisonOps::LT, <span class="number">1.0</span>)));</span><br></pre></td></tr></table></figure>

<br>

<p>그리고 여기 <strong>새 코드</strong> 입니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> range_cond = <span class="built_in">std</span>::make_shared&lt;GenericCondition&lt;PointXYZ&gt;&gt;(</span><br><span class="line">  [](<span class="keyword">const</span> PointXYZ&amp; point)&#123; </span><br><span class="line">      <span class="keyword">return</span> point.z &gt; <span class="number">0.0</span> &amp;&amp; point.z &lt; <span class="number">1.0</span>; </span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure>

<p>나머지 코드는 바뀐게 없습니다!</p>
<img src="/20201215-case-study-pcl/beautiful.jpg" class="" title="아름답습니다...">

<p>아름답습니다…</p>
<br>

<h2 id="벤치마크-결과"><a href="#벤치마크-결과" class="headerlink" title="벤치마크 결과"></a>벤치마크 결과</h2><hr>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY29udGlkYXZpZGUvQ1BQX09wdGltaXphdGlvbnNfRGlhcnkvdHJlZS9tYXN0ZXIvY3BwL3BjbF9jb25kaXRpb25hbF9yZW1vdmFsLmNwcA==">여기<i class="fa fa-external-link-alt"></i></span> 제가 짜둔 코드가 있습니다. 이 코드를 그대로 가져가셔서 테스트 해보셔도 됩니다.</p>
<p>여기 아래 샘플 포인트 클라우드에 4개의 필터를 돌린 벤치마크가 있습니다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------------------------</span><br><span class="line">Benchmark                   Time             CPU   Iterations</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">PCL_Filter            1403083 ns      1403084 ns          498</span><br><span class="line">Naive_Filter           107418 ns       107417 ns         6586</span><br><span class="line">PCL_Filter_Generic     668223 ns       668191 ns         1069</span><br></pre></td></tr></table></figure>

<p>물론 결과 값은 필터 조건이 몇개인지에 따라, 포인트 클라우드의 크기에 따라 달라질 수 있습니다.</p>
<br>

<h1 id="배워갈-점"><a href="#배워갈-점" class="headerlink" title="배워갈 점"></a>배워갈 점</h1><ul>
<li>“naive” 필터도 여러 케이스에서 사용해도 됩니다. 엄청 빠릅니다.</li>
<li><code>pcl::ConditionalRemoval</code>는 계속 써도 됩니다. 대신 기존의 <code>pcl::Conditions</code>는 너무 느리니까 가능하면 쓰지 마세요. 대신 <code>GenericCondition</code>을 쓰면 더 읽기 쉽고 빠르게 작동할 수 있습니다. </li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
        <tag>Robotics</tag>
        <tag>PCL</tag>
      </tags>
  </entry>
  <entry>
    <title>(번역) 실시간 컴퓨터 비전을 위한 C++ 최적화 (12) - 필요없는 조건문은 없애줍시다!</title>
    <url>/20201216-fast-palindrome/</url>
    <content><![CDATA[<p>Davide Faconti의 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC8=">CPP Optimization Diary 블로그<i class="fa fa-external-link-alt"></i></span> 글 중 “Case study: filter a Point Cloud faster”을 적당히 번역했습니다. 원글 링크는 <span class="exturl" data-url="aHR0cHM6Ly9jcHAtb3B0aW1pemF0aW9ucy5uZXRsaWZ5LmFwcC9wYWxpbmRyb21lLw==">여기<i class="fa fa-external-link-alt"></i></span>를 봐주세요.</p>
<br>

<p>이번 글은 다른 글들에 비해서 컴퓨터 비전과 크게 관련은 없을 수 있습니다.</p>
<p>그래도 적는 이유는… 조건 분기문을 없앰으로써 얼마나 코드가 빨라질 수 있는지 보여줄 수 있는 좋은 예시라고 생각되서입니다.</p>
<p><code>if</code>문은 보통 엄청나게 빠르기 때문에 런타임 cost를 보통 생각하지 않아도 됩니다. 하지만 종종 특수 케이스에서는 실제로 눈에 보일만큼 성능 향상이 나타날 때도 있습니다.</p>
<br>

<h2 id="코딩-인터뷰"><a href="#코딩-인터뷰" class="headerlink" title="코딩 인터뷰"></a>코딩 인터뷰</h2><hr>
<p>제가 이 글을 적고 있었을 때는 (역자 코멘트: 제가 아니라, 원글의 저자입니다!), 저는 엄청 활발하게 이직할 회사를 찾고 있었습니다. 그러다보니 자연스럽게 코딩 인터뷰도 보게 되었지요.</p>
<p>코딩 인터뷰는 왠만하면 할만합니다.</p>
<p>언젠가 하루는 이런 질문을 받았습니다.</p>
<blockquote>
<p>String이 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUGFsaW5kcm9tZQ==">palindrome<i class="fa fa-external-link-alt"></i></span>인지 확인할 수 있는 함수를 짜주시겠어요? (역자 코멘트: palindrome은 거꾸로 뒤집어도 똑같은 단어를 의미합니다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS92SmZZSmh0ZGNLRQ==">로꾸거<i class="fa fa-external-link-alt"></i></span> 를 참조하세요.)</p>
</blockquote>
<p>자연스럽게 이런 생각이 들더군요</p>
<img src="/20201216-fast-palindrome/fizzbuzz.jpg" class="" title="fizzbuzz">

<p>나랑 말장난하자는거군!</p>
<p>이런 질문을 하는게 잘못된건 아닙니다 ㅎㅎ. 아이스 브레이킹 용도로 딱 좋은 질문인 것 같아요. 하지만 저는 좀 더 real-world code를 적는 걸 예상했는데 말이죠.</p>
<p>어찌되었건, 이렇게 답을 했습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">IsPalindrome</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> N = str.size();</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> N_half = N / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;N_half; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>( str[i] != str[N<span class="number">-1</span>-i])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>이 정도 문제는 껌이지요 :)</p>
<p>너무 쉽습니다!</p>
<p>여기에 최적화를 끼얹어서 재미를 좀 봅시다!</p>
<br>

<h2 id="더-빠른-IsPalindrome"><a href="#더-빠른-IsPalindrome" class="headerlink" title="더 빠른 IsPalindrome()"></a>더 빠른 IsPalindrome()</h2><hr>
<p>오리지널 버전도 나쁘진 않았습니다.</p>
<ul>
<li>복사가 없었구요</li>
<li>찾는 즉시 loop를 멈췄구요</li>
<li>모든 코너 케이스에 대해 대응할 수 있었습니다.</li>
</ul>
<p>하지만 여기서 더 빠르게 만들 수 있는 방법이 떠올랐습니다. <code>if</code>가 반복되는 수를 줄일 수 있죠.</p>
<p>한 글자씩 비교하는게 아니라, ‘word’끼리 비교를 하는겁니다.</p>
<p><code>uint32_t</code>에 word를 저장해봅시다. <code>uint32_t</code>는 4바이트를 한번에 다룰 수 있습니다.</p>
<p>구현하면 아래 코드처럼 되겠습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;byteswap.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">bool</span> <span class="title">IsPalindromeWord</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> N = str.size();</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> N_half = (N/<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> S = <span class="keyword">sizeof</span>(<span class="keyword">uint32_t</span>);</span><br><span class="line">    <span class="comment">// number of words of size S in N_half</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">size_t</span> N_words = (N_half / S);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// example: if N = 18, half string is 9 bytes and</span></span><br><span class="line">    <span class="comment">// we need to compair 2 pairs of words and 1 pair of chars</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">size_t</span> index = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;N_words; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">uint32_t</span> word_left, word_right;</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;word_left, &amp;str[index], S);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;word_right, &amp;str[N - S - index], S);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( word_left != bswap_32(word_right))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        index += S;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// remaining bytes.</span></span><br><span class="line">    <span class="keyword">while</span>(index &lt; N_half)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>( str[index] != str[N<span class="number">-1</span>-index])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        index++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>여기서 우리는 string을 4바이트의 ‘블럭’으로 쪼갤겁니다. 좌,우 순서대로 쪼갤거고, 중간에 남는 값들은 그대로 str로 둘겁니다.</p>
<p>블럭끼리 비교는 <strong><em>단 한번</em></strong>만 하면 됩니다.</p>
<p>뒤에서오는 블럭은 순서가 거꾸로 뒤집어져야 합니다. 우리는 이 방법을 위해 STL에 있는 <span class="exturl" data-url="aHR0cHM6Ly9tYW43Lm9yZy9saW51eC9tYW4tcGFnZXMvbWFuMy9ic3dhcF8zMi4zLmh0bWw=">bswap_32<i class="fa fa-external-link-alt"></i></span>라는 방식을 사용하겠습니다.  </p>
<br>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">uint32_t</span> <span class="title">Swap</span><span class="params">(<span class="keyword">const</span> <span class="keyword">uint32_t</span>&amp; val)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">    <span class="keyword">char</span> c[<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">uint32_t</span> n;</span><br><span class="line">  &#125; data;</span><br><span class="line">  data.n = val;</span><br><span class="line">  <span class="built_in">std</span>::swap(data.c[<span class="number">0</span>], data.c[<span class="number">3</span>]);</span><br><span class="line">  <span class="built_in">std</span>::swap(data.c[<span class="number">1</span>], data.c[<span class="number">2</span>]);</span><br><span class="line">  <span class="keyword">return</span> data.n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h1 id="벤치마크"><a href="#벤치마크" class="headerlink" title="벤치마크"></a>벤치마크</h1><hr>
<img src="/20201216-fast-palindrome/palindrome_benchmark.png" class="" title="벤치마크">

<p>긴 string을 사용했을 때에 (8바이트보다 컸을 때) 성능은 약 <strong><em>50%</em></strong> 정도, 그리고 짧은 string에서는 약 <strong><em>150</em></strong>% 정도 성능 향상이 있었습니다.</p>
<p>실제로 엄청나게 긴 string의 경우에는, 128이나 256 비트를 사용할수도 있겠습니다. 이 경우에는 <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmJsb2cvMjAyMC8wNy8wOC9pbXByb3ZpbmctcGVyZm9ybWFuY2Utd2l0aC1zaW1kLWludHJpbnNpY3MtaW4tdGhyZWUtdXNlLWNhc2VzLw==">SIMD<i class="fa fa-external-link-alt"></i></span>를 사용할 수 있겠네요. 하지만 SIMD는 이번 글의 목적이 아닙니다.</p>
<h1 id="정리하자면…"><a href="#정리하자면…" class="headerlink" title="정리하자면…"></a>정리하자면…</h1><p>이번 글에서 나온 코드는 ‘심플하고 읽기 좋은 코드를 적고, 최적화에 너무 집중하지 마세요’라는 말의 완전한 반대를 실천합니다.</p>
<p>이런 경우가 사실 좋다고 할 수는 없습니다. 가독성 좋고 유지보수가 가능한 코드를 짜는 것은 중요하지요.</p>
<p>여기 나온 예제는 대체적으로 ‘for fun’용도로 짠 것이고, 생각하기 어려운 <strong><em>특정 케이스</em></strong>에서도 최적화의 가능성이 있다는 것을 보여줍니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>실시간 컴퓨터 비전을 위한 C++ 최적화 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
        <tag>Coding Interviews</tag>
      </tags>
  </entry>
  <entry>
    <title>컴퓨터 비전에서 std::vector 초기화 vs reserve 성능비교</title>
    <url>/20201217-std-vector-reserve/</url>
    <content><![CDATA[<h2 id="std-vector의-문제"><a href="#std-vector의-문제" class="headerlink" title="std::vector의 문제"></a><code>std::vector</code>의 문제</h2><hr>
<p><code>std::vector</code>가 꽉 찬 상황에서 새로운 element를 추가한다고 해보자.</p>
<p>이미 메모리가 다 찼기 때문에, 새롭게 메모리를 할당해야한다. 이 때, vector는 Heap 속 새로운 위치에 메모리를 할당하고, 모든 데이터를 새 메모리 위치로 복사한다.</p>
<p>보통 새롭게 할당되는 메모리는 기존 메모리의 2배 크기가 된다.</p>
<p>이 메모리가 또다시 다 차게 되면, 위의 과정을 반복하게 된다.</p>
<p>&nbsp;</p>
<p>이 과정은</p>
<ol>
<li>Heap 메모리 할당 + 기존 메모리를 복사 작업에 시간이 들어가고</li>
<li>복사되는 메모리의 크기가 크면 복사 시간이 늘어나기 때문에</li>
</ol>
<p><code>std::vector</code>를 자주 사용하는 실시간 컴퓨터 비전 입장에서는 꽤나 골치아픈 문제이다.</p>
<p>&nbsp;</p>
<h2 id="해결법"><a href="#해결법" class="headerlink" title="해결법"></a>해결법</h2><hr>
<p>우리가 미리 해당 vector에 들어갈 최대 element 수를 알고 있다면, 그만큼 미리 메모리를 할당해둠으로써 추가적인 heap allocation을 피할 수 있다.</p>
<p>이 때, 우리는 두가지 방법: <code>reserve()</code>를 쓰는 방식과, 초기화 방식으로 메모리를 할당할 수 있다.</p>
<p>각 방식의 구현은 아래 코드를 참조하자.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Reserve 방식</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">    vec.reserve(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; elem : vec)</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; elem &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; vec.size() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; vec.capacity() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialization 방식</span></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">vec2</span><span class="params">(<span class="number">1000</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; elem : vec2)</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; elem &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; vec2.size() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; vec2.capacity() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h2 id="reserve-방식-vs-초기화-방식-차이점"><a href="#reserve-방식-vs-초기화-방식-차이점" class="headerlink" title="reserve() 방식 vs 초기화 방식 차이점"></a><code>reserve()</code> 방식 vs 초기화 방식 차이점</h2><hr>
<h3 id="1-저장되는-데이터"><a href="#1-저장되는-데이터" class="headerlink" title="1. 저장되는 데이터"></a>1. 저장되는 데이터</h3><ul>
<li><code>reserve()</code> 방식은 size = 0, capacity = 1000 을 출력한 것에 비해,</li>
<li>초기화 방식은 size = 1000, capacity = 1000 을 출력하였다.</li>
</ul>
<p>즉, <code>reserve()</code> 방식은 메모리만 할당하는 것, 그리고 초기화 방식은 실제로 어떤 값을 만들어서 넣는다는 것이다. </p>
<p>실제로 확인해봤을 때, <code>reserve()</code> 방식은 어떠한 값도 <code>elem</code>에서 출력되지 않았고, 초기화 방식은 <code>elem</code>으로부터 0이 1000개가 출력되었다.</p>
<p>0이 중요한 정보로 사용된다면 (e.g. distance 값, true/false), <code>reserve()</code> 방식을 사용하는 것이 좀 더 안전할 것 같다.   </p>
<p>또, 아래와 같은 방식으로 체크를 한다면 <code>reserve()</code> 방식이 유리하다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">FeatureDetector::detectAndCompute</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;VisualFeature&gt;&amp; features)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// ... some code</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;VisualFeature&gt; features;</span><br><span class="line">    features.reserve(<span class="number">2000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!FeatureDetector.detectAndCompute(features))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// code...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h2 id="생성되는-속도"><a href="#생성되는-속도" class="headerlink" title="생성되는 속도"></a>생성되는 속도</h2><p>그렇다면, 어떤 방식을 사용하는 것이 더 빠를까? </p>
<p>아래의 코드로 테스트 해봤다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::microseconds t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Reserve 방식</span></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">    vec.reserve(<span class="number">1000</span>);</span><br><span class="line">    </span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to create vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialization 방식</span></span><br><span class="line"></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">vec2</span><span class="params">(<span class="number">1000</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to create vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<p>1000 개의 element로 테스트를 해봤을 때, </p>
<ul>
<li><code>reserve()</code> 방식은 43 마이크로초, </li>
<li>초기화 방식은 2 마이크로초 정도가 걸렸다.</li>
</ul>
<p>이렇게 보면 초기화 방식의 승리라고 볼 수 있다.</p>
<p>하지만 100,000개의 element로 테스트를 해봤을 때, </p>
<ul>
<li><code>reserve()</code> 방식은 65 마이크로초, </li>
<li>초기화 방식은 390 마이크로초 정도가 걸렸다.</li>
</ul>
<p>&nbsp;</p>
<h2 id="작은-Element를-채워넣는-속도"><a href="#작은-Element를-채워넣는-속도" class="headerlink" title="작은 Element를 채워넣는 속도"></a>작은 Element를 채워넣는 속도</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::microseconds t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Reserve 방식</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">    vec.reserve(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> zero = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; vec.size(); i++)</span><br><span class="line">        vec.push_back(zero);</span><br><span class="line"></span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to fill a vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialization 방식</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">vec2</span><span class="params">(<span class="number">100000</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; vec2.size(); i++)</span><br><span class="line">        vec2[i] = zero;</span><br><span class="line">        </span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to fill a vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<p>그렇다면 Element를 채워넣는 속도는 얼마나 차이가 날까?</p>
<p>1000개로 실험해봤을 때, </p>
<ul>
<li><code>reserve()</code>는 0 us, </li>
<li>초기화 방법은 1 us가 나왔다.</li>
</ul>
<p>별 차이가 없다고 볼 수 있다.</p>
<p>100,000개로 실험해봤을 때, </p>
<ul>
<li><code>reserve()</code>는 0 us, </li>
<li>초기화 방법은 246 us가 걸렸다. </li>
</ul>
<p>&nbsp;</p>
<h2 id="큰-Element를-채워넣는-속도"><a href="#큰-Element를-채워넣는-속도" class="headerlink" title="큰 Element를 채워넣는 속도"></a>큰 Element를 채워넣는 속도</h2><hr>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 큰 데이터</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">someBigData</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt; data = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;(<span class="number">1000</span>);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt; data2 = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;(<span class="number">1000</span>);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt; data3 = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;(<span class="number">1000</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::system_clock::time_point endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">std</span>::chrono::microseconds t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Reserve 방식</span></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;someBigData&gt; vec;</span><br><span class="line">    vec.reserve(<span class="number">1000</span>);</span><br><span class="line">    </span><br><span class="line">    someBigData data;</span><br><span class="line">    <span class="comment">// startTime = std::chrono::system_clock::now();</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; vec.size(); i++)</span><br><span class="line">        vec.push_back(data);</span><br><span class="line"></span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to create vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialization 방식</span></span><br><span class="line">    startTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;someBigData&gt; <span class="title">vec2</span><span class="params">(<span class="number">1000</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// startTime = std::chrono::system_clock::now();</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">uint32_t</span> i = <span class="number">0</span>; i &lt; vec2.size(); i++)</span><br><span class="line">        vec2[i] = data;</span><br><span class="line">        </span><br><span class="line">    endTime = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    </span><br><span class="line">    t = <span class="built_in">std</span>::chrono::duration_cast&lt;<span class="built_in">std</span>::chrono::microseconds&gt;(endTime - startTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Time to create vector in us : &quot;</span> &lt;&lt; t.count() &lt;&lt; <span class="string">&quot; microseconds&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>큰 Element를 채워넣는 속도 비교로 실험을 마친다.</p>
<p><code>someBigData</code>라는 구조체를 만들고, 이를 기반을 vector를 만들고 element를 채워넣어보았다.</p>
<ul>
<li><p><code>reserve()</code> 방식은 생성과정까지 포함하면 50 us, </p>
<ul>
<li>포함하지 않고 push_back만 보았을 때는 0 us가 걸렸다.</li>
</ul>
</li>
<li><p>초기화 방식은 생성과정까지 포함하면 14521 us, </p>
<ul>
<li>포함하지 않고 index 접근만 하면 4032 us가 걸렸다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h2 id="Discussion-및-결론"><a href="#Discussion-및-결론" class="headerlink" title="Discussion 및 결론"></a>Discussion 및 결론</h2><hr>
<p>본 실험 결과를 다음과 같이 평가하였다.</p>
<ul>
<li><code>0</code>이 vector element로써 중요한 의미로 사용된다면, <code>reserve()</code>를 사용하는 방식이 안전하다.</li>
<li>Empty vector를 체크하는데에는 <code>reserve()</code>가 유리하다.</li>
<li>적은 양의 (~1000개) 작은 element를 가진 vector를 다룰 때에는 초기화 방법이 유리하다. 생성 속도는 <code>reserve()</code> 방식에 비해 훨씬 빠르고, element를 채워넣는 속도는 별 차이가 없다.</li>
<li>그 외 많은 양의 element를 가진 vector (~100,000개), 또는 큰 데이터를 담는 vector의 경우에는 <code>reserve()</code> 방식이 더 빠르다.</li>
</ul>
<p>작은 element를 담아야하는 상황이 어떤 것이 있을까? Feature matching을 하고 hamming distance 등을 저장할 때 사용할 수 있을 것 같다.</p>
<p>큰 데이터 element를 담아야하는 상황은 어떤 것이 있을까? Feature 정보를 전부 담아둘 때 사용할 수 있을 것 같다. <code>std::pair&lt;featureLocation, featureDescriptor&gt;</code>와 같이 말이다.</p>
<p>이 두 방법의 속도가 이렇게 차이나는 이유가 무엇일까? </p>
<p>특히나 큰 element를 저장할 때 이렇게 속도가 차이나는 이유가 궁금해진다.</p>
<p>Cache locality 라고 예전에 어디서 들어본 것 같은데, 이 키워드를 중점으로 조금 더 찾아봐야겠다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CV</tag>
        <tag>Optimisation</tag>
      </tags>
  </entry>
  <entry>
    <title>워릭 대학교 팟캐스트 인터뷰 - 프로그래밍 공부 + LaTeX로 CV작성 + Github + 취업준비 개인 포트폴리오/블로그 관련 이모저모</title>
    <url>/20201220-warwick-podcast/</url>
    <content><![CDATA[<p>2020년 12월 19일, 워릭대학교 한인학생회의 <span class="exturl" data-url="aHR0cHM6Ly93d3cuaW5zdGFncmFtLmNvbS9wL0NJcUxranNEcDNXLz91dG1fc291cmNlPWlnX3dlYl9jb3B5X2xpbms=">팟캐스트<i class="fa fa-external-link-alt"></i></span>에 초청받았다.</p>
<img src="/20201220-warwick-podcast/zoom.jpg" class="" title="zoom podcast">

<p>워릭대학교의 한인학생들은 문과 학생들이 대다수이며 프로그래밍에 약한 편이다.<br>이공계 학생들 역시 프로그래밍을 주특기로 공부하는 경우는 굉장히 드물다.<br>필수 과정으로 MATLAB, STATA, R 등으로 간단한 프로그래밍을 배우지만, 대다수의 학생들은 이 과정을 즐기지 못하고 추후에 회사에서 다시 배우곤 한다.</p>
<p>하지만 최근 컨설팅과 투자은행에 지원할 때 프로그래밍을 할 줄 알면 경쟁력이 된다는 이야기가 돌면서, 문과 학생들도 프로그래밍에 관심을 가지게 되는 것 같다.</p>
<p>아래 이번 팟캐스트에서 전달하고 싶었던 내용을 간단하게 요약했다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="프로그래밍"><a href="#프로그래밍" class="headerlink" title="프로그래밍"></a>프로그래밍</h2><h3 id="프로그래밍을-하면-좋은-점"><a href="#프로그래밍을-하면-좋은-점" class="headerlink" title="프로그래밍을 하면 좋은 점"></a>프로그래밍을 하면 좋은 점</h3><ul>
<li>반복되는 작업, 또는 숫자를 다루는 작업을 남들보다 빠르게 수행할 수 있음.</li>
<li>Excel이 생기기 전과 후를 생각해보기.<ul>
<li>10명이 붙어서 표를 정리하고 그래프를 그려야했던 작업을, 혼자서 엑셀로 수행 가능.</li>
<li>비슷한 논리로, 10+명이 붙어서 엑셀로 작업하던 작업을, 혼자서 프로그래밍으로 수행 가능</li>
</ul>
</li>
<li>개인에게는 업무효율 상승, 기업에게는 인건비 절약 - 취업시장에서 경쟁력을 갖출 수 있음</li>
</ul>
<p>&nbsp;</p>
<h3 id="프로그래밍-언어-종류"><a href="#프로그래밍-언어-종류" class="headerlink" title="프로그래밍 언어 종류"></a>프로그래밍 언어 종류</h3><ul>
<li>엄청 많음. C, C++, C#, Python, Java, Javascript, Swift, Haskell, Go…</li>
<li>본인의 섹터에서 어떤 언어를 사용하는지 알아야함.<ul>
<li>보통 Analyst쪽에서는 R, STATA가 주 언어임. 하지만 Python이 새로 뜨는 중이며 이걸 배우길 추천.</li>
<li>Trading 쪽에서는 속도 때문에 C, C++을 선호하기도 함. 하지만 아마 이쪽은 소프트웨어 직군일 확률이 높음.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="프로그래밍을-시작하는-방법"><a href="#프로그래밍을-시작하는-방법" class="headerlink" title="프로그래밍을 시작하는 방법"></a>프로그래밍을 시작하는 방법</h3><ul>
<li>처음 프로그래밍을 시작하는 사람은 온라인 코스 / 책 아무거나 들어도 됨.<ul>
<li>추천하는 책은 <span class="exturl" data-url="aHR0cHM6Ly93aWtpZG9jcy5uZXQvYm9vay8x">점프투파이썬<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>언어는 Python으로 시작하는걸 추천.<ul>
<li>쉬운 언어라 빨리 배울 수 있고 (독학 2-3개월)</li>
<li>범용성이 좋음 (기본 매크로, 스프레드시트, 그래프 및 데이터 시각화, 데이터 분석, 머신러닝/딥러닝…)</li>
</ul>
</li>
<li>주변에 프로그래밍을 잘 하는 멘토를 하나 두면 배우는 속도가 급격하게 빨라짐</li>
<li>코스로 언어를 마스터하겠다는 생각을 가지면 안됨.<ul>
<li>코스는 최대한 일찍 끝내고, 실제로 적용할 수 있는 첫 프로젝트를 빨리 끝내는게 제일 실력이 빠르게 늘음.  </li>
<li>본인 과제에 적용을 해보면 좋음.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="프로그래밍을-하는데에-필요한-능력-성격"><a href="#프로그래밍을-하는데에-필요한-능력-성격" class="headerlink" title="프로그래밍을 하는데에 필요한 능력 / 성격"></a>프로그래밍을 하는데에 필요한 능력 / 성격</h3><ul>
<li>‘프로그래밍을 하려면 컴퓨터 공학과를 나와야함’은 틀린 말임.<ul>
<li>충분히 학교다니면서 독학 가능함.</li>
<li>물론 컴공과 친구들처럼 효율적인 코드는 못 짤수도 있음. 그 친구들은 학과 내용이 컴퓨터에 대해 공부하는거라…<ul>
<li>그래도 투자은행이나 컨설팅에서 코딩을 서브스킬로 가지는 직무에서 전혀 문제가 되지 않음. 10년차가 넘어도 문제 안될듯.</li>
</ul>
</li>
</ul>
</li>
<li>‘프로그래밍을 하려면 수학을 잘해야함’이란 말은 틀린 말임.<ul>
<li>컴공과가 수학을 배우는 이유는 조금이라도 효율적이게 계산을 하기 위해서임.</li>
<li>우리는 그 컴공과가 만들어놓은걸 가져다 쓸거기 때문에, 수학 잘 몰라도 됨.<ul>
<li>오히려 본인 전공의 수학이 더 중요함.</li>
</ul>
</li>
</ul>
</li>
<li>‘프로그래밍은 누구나 할 수 있다’ 도 (안타깝게도) 틀린 말임.<ul>
<li>모니터 화면만 보면서 어떻게 코드를 짜야하는지 생각해야함. 몇시간, 몇일, 몇주가 될 수도 있음.<ul>
<li>‘나는 사람들과 이야기하고, 아이디어를 나누고, 딜을 이끌어내는게 더 좋다’ 라던지, ‘난 앞으로 몇십년 커리어를 가만히 앉아서 코딩을 하면서 보내기 싫다’ 하는 사람들은 코딩이 아닌 본인의 장점을 살리는게 더 좋음.</li>
</ul>
</li>
<li>십몇년 경력의 베테랑 프로그래머도 코딩을 하면 막히는 순간이 분명 찾아옴. 이 때 포기하지 않고 구글링을 몇시간씩 해서라도 풀 수 있는 끈기가 있어야함.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="프로그래밍을-하면-얼마나-업무-효율이-좋아지는지"><a href="#프로그래밍을-하면-얼마나-업무-효율이-좋아지는지" class="headerlink" title="프로그래밍을 하면 얼마나 업무 효율이 좋아지는지?"></a>프로그래밍을 하면 얼마나 업무 효율이 좋아지는지?</h3><ul>
<li>어제 밤에 지인의 파이썬 코드 작성을 잠시 도와준 예제를 들면…<ul>
<li>코스피/코스닥 상장 회사 데이터가 약 3000개가 있고, 정부가 새롭게 규제를 걸은 회사들의 이름이 약 500개가 있었음. </li>
<li>여기서 정부가 규제를 걸은 회사들을 코스피/코스닥 데이터로부터 찾아서 추출해야함</li>
<li>직접 하나씩 찾으려면 약 3~4시간 정도 소요 예상 + 실수를 할수도 있음</li>
<li>파이썬 코드를 작성했을 때 약 20~30분 소요 + 코드가 돌아가는데 1.5초 걸림.<ul>
<li>새로운 데이터가 백만개가 들어와도 똑같은 코드 재사용 가능.</li>
</ul>
</li>
</ul>
</li>
<li>위와 같이 반복적인 일은 사람보다 컴퓨터가 더 빠르게 할 수 있으며, 해야하는 업무의 스케일이 커질수록 더 좋은 효율성을 가짐.<ul>
<li>지인인 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8xNmhzQTNDeU03TQ==">코딩하는 공익<i class="fa fa-external-link-alt"></i></span>의 유명할 썰로, 반년동안 해야하는 반복업무를 하루만에 끝낸 사례가 있음. </li>
</ul>
</li>
<li>요즘 컴퓨터의 발전으로 모든 회사가 엄청난 양의 데이터를 가지고 있음.<ul>
<li>특히나 투자은행, 컨설팅 회사는 엄청난 양의 데이터를 가지고 있을 것임.</li>
<li>이제는 데이터의 양이 엑셀로 처리하기에는 너무 많을 수 있으므로, 프로그래밍을 잘 할줄 안다면 엄청난 업무효율성을 가지게 될 것임.</li>
</ul>
</li>
<li>물론 코드를 처음 짜보는 사람은 일주일이 넘게 걸릴수도 있음. 이 경우 손으로 하는거보다 느림.<ul>
<li>프로그래밍을 배우는 단계에서는 이게 문제가 되지 않음. 본인의 실력 향상을 목표로 하는게 좋음.</li>
<li>하지만 실제 업무에서는 시간 판단을 잘 해서 가장 시간효율적인 방법을 찾아야함.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="머신러닝-딥러닝이란-AI란"><a href="#머신러닝-딥러닝이란-AI란" class="headerlink" title="머신러닝 / 딥러닝이란? AI란?"></a>머신러닝 / 딥러닝이란? AI란?</h3><ul>
<li>쉽게 이야기하면, 데이터로부터 어떠한 패턴을 찾아내는 기술임.</li>
<li>그리고 이 둘이 요즘 흔히 이야기하는 AI 기술임.</li>
<li>데이터 분석이 머신러닝을 이용함.</li>
<li>딥러닝은 머신러닝 기법 중 하나인데, 굉장히 좋은 성능을 보여주기 때문에 유명해짐.</li>
<li>예전엔 연구자들이 가설을 통해 공식과 룰을 만들었다. 이 공식과 룰을 찾는 과정은 오래 걸렸고, 종종 부정확하기도 했음.<ul>
<li>최근 컴퓨터 기기가 급격하게 발전하면서, 많은 양의 데이터가 모이게 됨.</li>
<li>이 데이터로부터 패턴을 컴퓨터가 자동으로 찾아줘서, 그 패턴을 공식 / 룰로 사용하게 하는 것이 머신러닝 / 딥러닝임.</li>
</ul>
</li>
<li>머신러닝 / 딥러닝의 기초가 되는 것은 확률과 통계, 선형대수이다. 이 수학 이론들을 바탕으로 컴퓨터가 계산을 해줄 수 있도록 프로그래밍을 하면 되는 것임.<ul>
<li>그렇기 때문에 머신러닝/딥러닝을 커리어로 가지고 싶다면, 수학과 프로그래밍에서 남들보다 앞서야함. 수학과 학생들보다, 프로그래밍 학생들보다.<ul>
<li>현재 AI 업계에서는 기본적으로 석/박사를 요구하고, 가끔 굉장히 경력이 좋은 학사를 뽑음 ( 이 경우 거의 이공계/수학과/컴공과).</li>
</ul>
</li>
<li>하지만 머신러닝/딥러닝을 서브스킬로 쓰고 싶다면, 요즘 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL21hY2hpbmUtbGVhcm5pbmc=">좋은 온라인 코스<i class="fa fa-external-link-alt"></i></span>가 많이 있기 때문에 그걸로 시작해도 됨. 서브스킬로만 쓸꺼면 남들보다 앞서지 않아도 됨 ㅎㅎ</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="AI-프로그래밍이-계속-발전하는데-내-미래-일자리는-안전한가-지금이라도-프로그래머로-전향해야하는가"><a href="#AI-프로그래밍이-계속-발전하는데-내-미래-일자리는-안전한가-지금이라도-프로그래머로-전향해야하는가" class="headerlink" title="AI, 프로그래밍이 계속 발전하는데, 내 미래 일자리는 안전한가? 지금이라도 프로그래머로 전향해야하는가?"></a>AI, 프로그래밍이 계속 발전하는데, 내 미래 일자리는 안전한가? 지금이라도 프로그래머로 전향해야하는가?</h3><ul>
<li>AI, 프로그래밍 채용 시장이 급격하게 커져서, 상대적으로 내 분야 채용 시장이 작게 보이는 것임.</li>
<li>단순반복 업무를 요구하는 일자리는 금방 사라지고, 전공지식/인사이트/경험을 요구하는 분야는 쉽게 사라지지 못할 것임.<ul>
<li>예를 들어, 회계쪽에서 단순 숫자 맞추기 등등은 프로그래머가 만든 자동화 계산 프로그램 등으로 금방 교체될 것임.</li>
<li>제품 기획, 마케팅, 인사, 엔지니어링 같은 작업은 금방 교체되기 어려울 것임.</li>
</ul>
</li>
<li>데이터를 다루거나, 숫자를 다루는 직업에서는 프로그래밍을 요구할 가능성이 큼.</li>
<li>하지만 그렇다고 전공지식을 포기하고 프로그래밍을 공부하는 것은 좋지 않음.<ul>
<li>프로그래밍은 학위 없이도 인정받기 쉽지만, 경제학/경영학/자연과학/엔지니어링 등은 학위없이 인정받기 어려움.<ul>
<li>즉, 내가 프로그래밍을 독학해서 경쟁력을 갖추긴 쉽지만, 프로그래머가 내 전공지식을 독학해서 경쟁력을 갖추기는 어려움.</li>
</ul>
</li>
<li>회사가 실력있는 프로그래머가 필요하면 당신에게 요구하는 것이 아닌, 새로운 컴공과 프로그래머를 뽑을 것임.<ul>
<li>그러니 나는 내가 잘 할 수 있는거에 집중해야함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<hr>
<h2 id="취업-준비"><a href="#취업-준비" class="headerlink" title="취업 준비"></a>취업 준비</h2><h3 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h3><ul>
<li>프로그래밍 실력은 코드나 결과로 보여주는거임.<ul>
<li><a href="www.github.com">Github</a>라는 코드 저장소가 있음.<ul>
<li>많은 개발자들이 포트폴리오로 사용하기도함. 예시로 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1">내 프로필<i class="fa fa-external-link-alt"></i></span>을 첨부함.</li>
<li>Github 코드 내역에 자신있다면 이력서에 Github 주소를 첨부하는 것도 좋음.</li>
</ul>
</li>
<li>이력서에 ‘Skills: Python (Intermediate)로 적는건 아무 의미없음. 주식도 직접 투자해본사람이 잘 하지, 책만 읽은 사람은 아무 쓸모 없는것 처럼.<ul>
<li>이력서에 Project 등에서 ‘파이썬을 이용해 XXX를 만들었고, 이것을 사용해서 $3000 수익을 얻었습니다’ 으로 채워넣는게 효과적임.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="CV-Resume"><a href="#CV-Resume" class="headerlink" title="CV / Resume"></a>CV / Resume</h3><ul>
<li>MS워드로 Resume를 적을 경우, 보기 좋게 만드느라 여백이 애매하게 조정될 때가 있음.<ul>
<li>사람이 보기엔 좋아도, 최근 AI기반 CV screening에서 오류가 나타나기도 함.</li>
<li>그러므로, AI기반 CV screening이 잘 읽을 수 있는 resume를 만들어주는 것이 좋음.</li>
<li>LaTeX (레이텍, 라텍) 기반으로 작성하면 보기도 깔끔하고 AI가 읽기에도 깔끔함.</li>
<li>PC에서 읽을 때 링크도 사용 가능함.</li>
<li>LaTeX로 작성하려면, 살짝 코드짜는 것 비슷하게 작성해야함.<ul>
<li><a href="./resume.tex">샘플 LaTeX 코드</a></li>
<li><a href="./WKS_resume_template.pdf">샘플 pdf</a></li>
</ul>
</li>
<li>TeX 파일을 작성하려면 <a href="overleaf.com">Overleaf</a>를 사용하면 편함.<ul>
<li>추후 필요하면 WKS에 튜토리얼 요청을…</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="Portfolio-Blog"><a href="#Portfolio-Blog" class="headerlink" title="Portfolio / Blog"></a>Portfolio / Blog</h3><ul>
<li>이력서에 포트폴리오 페이지 및 블로그를 첨부하면 좋음.<ul>
<li>포트폴리오 페이지에는 이력서에 담지 못한 프로젝트 세부 내용을 적어둠.</li>
<li>블로그에서는 전공지식에 대한 이해를 보여주는 글을 작성하면 됨.</li>
<li>그리고 포트폴리오/블로그 링크를 이력서에 남겨두면 됨.<ul>
<li>지원한 회사에서 지원자에게 관심이 있다면, 그 사람이 남겨둔 포트폴리오 페이지 정도는 훑어봄.</li>
<li>면접관이 내가 진행한 프로젝트에 대해 좀 더 자세하게 알고 시작한다면, 면접에서 더 깊은 이야기를 나눌 수 있음. 이 경우 뻔한 이야기를 나눈 다른 면접자들에 비해 합격할 확률이 상승함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>블로그를 학교 소사이어티나 외부 커뮤니티에서 잘 PR을 하면 인지도를 얻을 수 있음.<ul>
<li>인지도는 곧 좋은 프로젝트 기회나 네트워킹 기회로 이어짐<ul>
<li>선순환!</li>
</ul>
</li>
<li>물론 작성하는데에 시간과 노력이 많이 들어감.</li>
<li>블로그 플랫폼으로는 Medium, WordPress, BloggerSpot 등이 있음. 국내 블로그로는 네이버 블로그나 티스토리 블로그도 나쁘지 않음. 프로그래밍/개발 능력을 보여주고 싶다면 Github pages (<span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL3RoZW1lcy8=">Hexo<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly90aGVtZXMuZ29odWdvLmlvLw==">Hugo<i class="fa fa-external-link-alt"></i></span>…), Notion 등이 있음.<ul>
<li>현재 보고 있는 블로그가 Github pages로 만든 것이고</li>
<li>이전에 사용하던 블로그는 <a href="cv-learn.com">Notion</a>으로 만든 것임.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>블로그/포트폴리오를 Github pages로 만들고 싶다면 이 <span class="exturl" data-url="aHR0cHM6Ly9jdi1sZWFybi5jb20vR2l0aHViLUFjYWRlbWljLVBhZ2VzLTEtMTg2YTdiMTg4YzM1NGFkNDhkMWExYTFlYTU2ZjQxZWI=">튜토리얼<i class="fa fa-external-link-alt"></i></span>을 참고하면 좋음.</li>
</ul>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Seminar</tag>
        <tag>Resume</tag>
        <tag>University</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2020 - Visual SLAM with Object and Plane (Shichao Yang 발표)</title>
    <url>/20201228-cvpr2020-slam-yang/</url>
    <content><![CDATA[<h1 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h1><h2 id="Shichao-Yang의-연구-주제"><a href="#Shichao-Yang의-연구-주제" class="headerlink" title="Shichao Yang의 연구 주제"></a>Shichao Yang의 연구 주제</h2><p>Shape prior를 사용하지 않는 Object와 plane을 이용한 Monocular SLAM.<br>Large scale로 indoor와 outdoor에서 사용 가능.<br>Scene understanding이 SLAM에 도움이 되는 것을 증명함.</p>
<p>Shichao Yang은 <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1RblZsZXhYaTlfYyZhYl9jaGFubmVsPVNoaWNoYW9ZYW5n">CubeSLAM<i class="fa fa-external-link-alt"></i></span>의 저자이다.</p>
<hr>
<h1 id="Point를-사용하는-SLAM의-단점"><a href="#Point를-사용하는-SLAM의-단점" class="headerlink" title="Point를 사용하는 SLAM의 단점"></a>Point를 사용하는 SLAM의 단점</h1><p>Point feature를 사용하는 SLAM이 잘 안되는 케이스가 있다.</p>
<ul>
<li>텍스처가 적은 벽이나 바닥</li>
<li>움직이는 object가 있을 때</li>
<li>회전량이 많을 때</li>
<li>다양한 조명 변화</li>
</ul>
<p>그리고, sparse point map 보다 더 정보가 많이 필요한 task 들도 있다</p>
<ul>
<li>증강현실에서 물리적 상호작용</li>
<li>자율주행 중 주변 자동차 위치 파악 (i.e. 정확한 거리 파악)</li>
</ul>
<p>사람은 그러면 어떻게 localization과 mapping을 잘하는걸까?<br>일단 Point를 보는건 아닌 것 같다.<br>Objects와 Planes를 보는 것 같다.<br>Objects와 plane을 본다는 것은 scene understanding이 있다는 것을 의미한다.</p>
<hr>
<h1 id="연구-분야-소개"><a href="#연구-분야-소개" class="headerlink" title="연구 분야 소개"></a>연구 분야 소개</h1><h2 id="최적화-방법론"><a href="#최적화-방법론" class="headerlink" title="최적화 방법론"></a>최적화 방법론</h2><ol>
<li>Decoupled 방법 - Point 기반 SLAM을 수행하고, point cloud로부터 3D object와 plane 검출</li>
</ol>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzYyNDc5OTI=">Bao 2012 CVPR : Semantic structure from motion with points, regions and objects<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9jdnByXzIwMTcvcGFwZXJzL0RvbmdfVmlzdWFsLUluZXJ0aWFsLVNlbWFudGljX1NjZW5lX1JlcHJlc2VudGF0aW9uX0NWUFJfMjAxN19wYXBlci5wZGY=">Dong 2017 CVPR : Visual-inerial-semantic scene representation for 3D object detection<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMTI5ODA=">Huang 2020 CVPR : ClusterVO - Clustering moving instances and estimating visual odometry<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>하나씩 계산하면 되는 쉬운 방법.<br>하지만 SLAM이 잘 안되거나 Point cloud의 퀄리티가 떨어지면 object detection도 잘 안된다.</p>
<ol start="2">
<li>Tightly coupled 방법 - Camera motion, Object / Plane geometry를 동시에 최적화</li>
</ol>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY2MTkwMjI=">Salas-Moreno 2012 CVPR : SLAM++ - SLAM at the level of objects<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDQuMDc2MzI=">Lee 2017 ICCV : Joint layout estimation and global multi-view registration for indoor reconstruction<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9JQ0NWXzIwMTcvcGFwZXJzL0dheV9Qcm9iYWJpbGlzdGljX1N0cnVjdHVyZV9Gcm9tX0lDQ1ZfMjAxN19wYXBlci5wZGY=">Gay 2017 ICCV : Probabilistic Structure from Motion with Objects (PSfMO)<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDQuMDQwMTEucGRm">Nicholson 2018 RAL : QuadricSLAM - Dual quadrics from object detections as landmarks in object-oriented SLAM<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDcuMDIwNjI=">Li 2018 ECCV : Stereo vision-based semantic 3D object and ego-motion tracking<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>동시에 최적화를 하면서 정확도를 이끌어내려는 시도이다.</p>
<br>

<h2 id="Object를-표현하는-방법"><a href="#Object를-표현하는-방법" class="headerlink" title="Object를 표현하는 방법"></a>Object를 표현하는 방법</h2><br>

<img src="/20201228-cvpr2020-slam-yang/object_representation.png" class="" title="Object Representation">

<h2 id="최적화-cost-function"><a href="#최적화-cost-function" class="headerlink" title="최적화 cost function"></a>최적화 cost function</h2><ul>
<li><p>Object-camera 3D relative pose</p>
<ul>
<li>Object pose estimation을 한 후에, object들의 relative pose를 graph 형태로 표현하는 방법. (e.g. <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY2MTkwMjI=">SLAM++<i class="fa fa-external-link-alt"></i></span>)</li>
<li>구현하기는 쉽지만 3D pose estimation이 굉장히 정확해야함.<ul>
<li>그렇기 때문에 depth sensor를 많이 사용하는 편</li>
</ul>
</li>
</ul>
</li>
<li><p>Object-camera 2D observation</p>
<ul>
<li>간단하게는 bounding box error나 segmentation error, 복잡한 난이도로는 photometric error나 silhouette error를 사용할 수 있음.</li>
</ul>
</li>
<li><p>Object-point 3D position</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUGVyc3BlY3RpdmUtbi1Qb2ludA==">PnP<i class="fa fa-external-link-alt"></i></span> 방식과 비슷하게, 이미지에서의 2D keypoint와 object의 3D keypoint를 매칭하고 reprojection error를 재는 방법. </li>
</ul>
</li>
</ul>
<hr>
<h1 id="3D-Object-detection"><a href="#3D-Object-detection" class="headerlink" title="3D Object detection"></a>3D Object detection</h1><ul>
<li><p>End-to-end CNN</p>
<ul>
<li>Relative pose를 regression으로 풀 수 있는 네트워크들은 대부분 shape prior를 사용함</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDQuMDAxNzUucGRm">Li 2018 : DeepIM: Deep Iterative Matching for 6D Pose Estimation<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li><p>Deep learning with geometry</p>
<ul>
<li>Shape prior 없이 할 수 있지만, 간단한 object만 가능함 (e.g. 자동차)</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MTIuMDA0OTY=">Mousavian 2017 : 3D Bounding Box Estimation Using Deep Learning and Geometry<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MudG9yb250by5lZHUvfnVydGFzdW4vcHVibGljYXRpb25zL2NoZW5fZXRhbF9jdnByMTYucGRm">Chen 2016 : Monocular 3D Object Detection for Autonomous Driving<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<br>

<img src="/20201228-cvpr2020-slam-yang/3d_detection.png" class="" title="General approach">

<p>하지만 SLAM의 용도로는 우리는 한번도 본 적 없거나 복잡한 object에도 generalise 할 수 있어야한다.</p>
<p>여기 제안하는 새로운 방법은 2D bounding box와 SLAM으로 얻어낸 camera pose를 사용한다.<br>여러시점의 Camera의 위치로부터 2D bounding box가 projective geometry 기반으로 커버할 수 있는 공간을 계산한다.<br>그리고 이 공간을 잘 표현하는 cuboid 모델로 표현해주면 간단하게 표현할 수 있다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/cuboid.png" class="" title="Cuboid model generation">

<p>위 사진은 vanishing point를 사용하여 cuboid 모델을 만드는 방법을 소개한다.<br>Cuboid는 2D 이미지로 보았을 때 3개의 vanishing point가 있어야한다.<br>굉장히 정확하고 또 간단하게 풀 수 있는 방법이다.<br>계산된 Cuboid 모델은 항상 2D bounding box 안에서 발견된다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/proposals.png" class="" title="Cuboid model proposals">

<p>위의 방식으로 cuboid 모델을 만들면, 보통 여러개의 가능성이 나온다.<br>휴리스틱하게 룰을 만들어서 제일 정확한 모델에게 가장 높은 score를 주는 방식을 이전까지는 많이 써왔다.<br>하지만 이 scoring system을 딥러닝으로 만들 수 있지 않을까 생각해서 도전해봤다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/result.png" class="" title="Results">

<p>실제로 돌려본 결과, object shape prior가 없는데도 cuboid 모양을 잘 잡는 것을 볼 수 있다.<br>다양한 object들에 cuboid model을 생성할 수 있었다.<br>Indoor 데이터셋에서는 굉장히 잘 되었다.<br>Outdoor 데이터셋은 <span class="exturl" data-url="aHR0cDovL3d3dy5jdmxpYnMubmV0L2RhdGFzZXRzL2tpdHRpL2V2YWxfb2JqZWN0LnBocD9vYmpfYmVuY2htYXJrPTNk">KITTI 데이터셋<i class="fa fa-external-link-alt"></i></span>을 사용했는데, 우리의 방식보다 결과가 좋게 나왔다.<br>그도 그럴만한게, KITTI 데이터셋에는 shape prior + 다양한 차들의 데이터가 충분히 있다.<br>Shape prior가 없는 우리 방식의 성능을 뛰어넘을 수 있다는 것을 인지해야한다.</p>
<hr>
<h1 id="Plane-detection"><a href="#Plane-detection" class="headerlink" title="Plane detection"></a>Plane detection</h1><ul>
<li>Model-based 방식<ul>
<li>Cuboid 형태의 방만 가능</li>
<li><span class="exturl" data-url="aHR0cDovL2Rob2llbS53ZWIuZW5nci5pbGxpbm9pcy5lZHUvcHVibGljYXRpb25zL2ljY3YyMDA5X2hlZGF1X2luZG9vci5wZGY=">Hedau 2009 : Recovering the Spatial Layout of Cluttered Rooms<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY3NTExNTM=">Schwing 2013 : Box in the Box: Joint 3D Layout and Object Reasoning from Single Images<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p>이런 방식은 복도와 같이 반대편 벽이 보이지 않는 경우에는 사용할 수 없다.</p>
<ul>
<li>Learning-based 방식<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9jdnByXzIwMTgvcGFwZXJzL1pvdV9MYXlvdXROZXRfUmVjb25zdHJ1Y3RpbmdfdGhlX0NWUFJfMjAxOF9wYXBlci5wZGY=">Zou 2018 : LayoutNet: Reconstructing the 3D Room Layout rom a Single RGB Image<i class="fa fa-external-link-alt"></i></span> </li>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9jdnByXzIwMTgvcGFwZXJzL0xpdV9QbGFuZU5ldF9QaWVjZS1XaXNlX1BsYW5hcl9DVlBSXzIwMThfcGFwZXIucGRm">Liu 2018 : PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p>딥러닝 기반 layout detection은 매 프레임마다 layout prediction을 하기 때문에 정확한 값을 얻을 수 없다. (i.e. layout이 떨린다)</p>
<p>여기에 SLAM을 섞으면 어떨까?<br>SLAM의 경우, 하나의 feature point를 다양한 각도에서 봐도 안정적이게 추출할 수 있다.<br>이 점을 이용하면, layout도 여러 각도에서 봐도 안정적이게 추출할 수 있을 것이다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/plane_proposal.png" class="" title="Plane proposal">

<p>우선, 기존의 line detector를 써서 line을 찾는다.<br>Occlusion된 line이 있을 수 있으니, segmentation을 사용해서 object를 찾고 segmentation mask 내부에 line을 더 이어서 그려준다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/plane_proposal_2.png" class="" title="Plane proposal 2">

<p>위의 방법으로 여러개의 plane candidate가 나올 것이다.<br>벽과 바닥이 90도 직각이라는 가정을 두고, wall plane과 floor plane을 뽑아준다.<br>이 때, 3D object detection도 할 것이라면, 동시에 같이 뽑을 수도 있다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/object_plane_joint.png" class="" title="Object / Plane joint optimisation">

<p>3D object pose와 wall/floor plane 정보를 함께 이용해서 joint optimisation을 할 수 있다.<br>예를 들어, object의 기울기는 floor plane과 평행해야할 수도 있다.<br>또, object의 cuboid는 wall/floor plane과 intersect 할 수 없다. (i.e. intersect하면 벽을 뚫은거니까…)<br>이러한 추가적인 geometric constraint를 추가해줌으로써 더욱 정확하게 결과를 뽑을 수 있다.</p>
<hr>
<h1 id="Monocular-SLAM-with-objects-and-planes"><a href="#Monocular-SLAM-with-objects-and-planes" class="headerlink" title="Monocular SLAM with objects and planes"></a>Monocular SLAM with objects and planes</h1><br>

<img src="/20201228-cvpr2020-slam-yang/representations.png" class="" title="Objects and planes as factors">

<p>기존의 SLAM에서는 factor graph optimisation을 할 때 camera pose와 map point location만 고려한다.<br>여기에 object와 plane 정보도 함께 넣어 최적화 하는것이다.<br>즉, tightly-coupled optimisation을 수행하는 것이다.</p>
<p>Objects와 Planes는 어떤 방식으로 factor로 표현할 수 있을까?<br>Plane의 경우, 3 dof를 가지는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.664ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3829.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(847.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(1903.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(2181.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mtext" transform="translate(2781.6, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(3031.6, 0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(3551.6, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container> 가 되겠다.<br>Cuboid object의 경우, 6 dof pose와 (i.e. 3d rotation + 3d translation) 3 dof size를 (i.e. width, length, height) 가지겠다.<br>종종 특정 용도에서 자동차의 경우 size를 고정하기도 한다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/measurement_error.png" class="" title="Measurement errors">

<p>최적화를 수행할 때는 measurement error 값을 이용한다.<br>여기서 우리는 두가지 방식으로 measurement error를 사용할 수 있다.</p>
<p>첫번째는 3D measurement이다.<br>다양한 카메라 각도에서 object의 pose와 size를 추정하면서 나타나는 차이 값이다.</p>
<p>두번째는 2D measurement이다.<br>Cuboid를 2D 이미지에 projection시켜 생기는 사각형을 2D bounding box와 비교하는 reprojection error를 사용하는 것이다.<br>이 방법은 3D measurement를 사용하는 방법보다 더 robust하다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/measurement_error2.png" class="" title="Measurement errors 2">

<p>Plane 정보는 항상 바닥에서 추출한 line으로부터 만들어진다.<br>그렇기 때문에, 우리가 추정한 plane 정보는 unproject된 바닥 line과 비교하여 에러 값을 추정할 수 있다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/data_association.png" class="" title="Data association">

<p>위의 방법들을 사용하면, 우리는 자연스럽게 point feature와 object 정보에 대해서 data association을 할 수 있다.<br>여기에 epipolar checking 등을 하면 더 정확해질 수 있다.<br>이 방법으로 cluttered / repetitive / occluded object에 대해서도 강인하게 트랙킹하는 것을 보았다.<br>이 방법은 기존의 object tracking이나 template 기반 트랙킹 방법보다 정확하다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/eval.png" class="" title="Evaluation">

<p>Object의 크기를 fixed-size로 고정해놓고 이 방법을 구현한 것이 CubeSLAM이다.<br>CubeSLAM은 loop closure과 같은 기능도 없지만, scale drift가 전혀 없는 것을 볼 수 있다.<br>ORB-SLAM과 비교하였을 때 훨씬 정확한 모습을 볼 수 있다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/eval2.png" class="" title="Evaluation 2">

<p>이런 경우에는 feature point의 수가 굉장히 적기 때문에 기존의 ORB-SLAM과 같은 방식은 모두 실패한다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/eval3.png" class="" title="Evaluation 3">

<p>왼쪽의 reconstruction 결과를 보면 object의 정보만 사용했을 때는 pose가 많이 흔들린 것을 볼 수 있다.<br>이는 wall 정보를 사용하지 않았기 때문이라고 판단이 된다.<br>오른쪽에 보이는 결과는 후속 연구에서 wall 정보를 사용한 것인데, 훨씬 더 좋은 결과가 나타났다.<br>오피스의 한 층을 돌고 와도 굉장히 정확한 object pose + odometry 추정이 된다.<br>데모영상은 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9qekJNc0tDbTB1aw==">여기<i class="fa fa-external-link-alt"></i></span>를 보면 된다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/eval4.png" class="" title="Evaluation 4">

<p>이렇게 Object detection + SLAM을 이용한 방식은 기존의 방식과 비교했을 때 더 좋은 성능을 보여준다.<br>Single view 3D object detection 보다 더 정확하고,<br>또 기존의 monocular SLAM 시스템보다 정확하다.</p>
<hr>
<h1 id="Dynamic-object-SLAM"><a href="#Dynamic-object-SLAM" class="headerlink" title="Dynamic object SLAM"></a>Dynamic object SLAM</h1><p>기존의 SLAM 방법론들은 모두 움직이는 객체는 outlier로 처리한다.<br>하지만 실제 환경은 그렇지 않다.<br>움직이는 객체들이 많으면 어떻게 해야할까?</p>
<p>우선 문제를 간단하게 하기 위해, 모든 움직이는 객체는 rigid-body라고 가정한다.<br>그리고 움직임에 대한 motion model이 있다고 가정한다.<br>그리고 아래와 같은 graph를 만들어서 풀면 된다.</p>
<br>

<img src="/20201228-cvpr2020-slam-yang/dynamic.png" class="" title="Dynamic">


]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Deep SLAM</tag>
        <tag>3D</tag>
        <tag>AR</tag>
        <tag>CubeSLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern CMake</title>
    <url>/20210105-modern-cmake/</url>
    <content><![CDATA[<p>Joshua Lospinoso의 ‘<span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9iRGRrSnUtblZUbw==">Modern Cmake: An introduction<i class="fa fa-external-link-alt"></i></span>‘ 영상을 보고 공부한 노트입니다.<br>추천하는 책은 ‘<span class="exturl" data-url="aHR0cHM6Ly9jcmFzY2l0LmNvbS9wcm9mZXNzaW9uYWwtY21ha2Uv">Professional CMake: A Practical Guide<i class="fa fa-external-link-alt"></i></span>‘</p>
<hr>
<h1 id="가장-쉬운-인트로"><a href="#가장-쉬운-인트로" class="headerlink" title="가장 쉬운 인트로"></a>가장 쉬운 인트로</h1><h2 id="Executable-프로그램-만들기"><a href="#Executable-프로그램-만들기" class="headerlink" title="Executable 프로그램 만들기"></a>Executable 프로그램 만들기</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Hello, world!\n"</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>위와 같은 형태의 .cpp 파일이 있다고 해보자.<br>우리는 이 파일을 기반으로 binary executable이나 library를 만들고싶다.<br>이를 위해 우리는 CMake를 사용한다.</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.15</span>)</span><br><span class="line"><span class="keyword">project</span>(HelloWorld)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(App main.cpp)</span><br></pre></td></tr></table></figure>

<ul>
<li>CMake 버전은 3.xx 이후로 사용한다면 어떤걸 사용하던지 크게 상관없다.</li>
<li>하나의 <code>CMakeLists.txt</code>는 단 1개의 project만 설정할 수 있다.<ul>
<li>그 안에서 여러개의 library와 executable (i.e. binary 프로그램)을 생성할 수 있다.</li>
<li>위의 예시는 <code>main.cpp</code>라는 소스파일을 사용해서 <code>App</code>이라는 binary 프로그램을 만드는 것이다.</li>
</ul>
</li>
<li>여기서 <code>App</code>에 해당하는 것을 target이라고 한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ls -1</span><br><span class="line">   CMakeLists.txt</span><br><span class="line">   main.cpp</span><br><span class="line">$ mkdir build &amp;&amp; <span class="built_in">cd</span> build/</span><br><span class="line">$ cmake .. -GNinja</span><br><span class="line">$ ninja</span><br><span class="line">$ ./App</span><br><span class="line">Hello, world!</span><br></pre></td></tr></table></figure>

<p>터미널에서 다음과 같은 커맨드를 사용하면 프로젝트를 생성하고 실행할 수 있다.<br><code>-GNinja</code> 라는 generator flag는 추후에 기술한다.<br>ninja 대신 make를 사용해도 된다 (Lospinoso는 ninja를 더 좋아한다고 한다).</p>
<br>

<h3 id="cmake-커맨드-amp-ninja-커맨드-설명"><a href="#cmake-커맨드-amp-ninja-커맨드-설명" class="headerlink" title="cmake 커맨드 & ninja 커맨드 설명"></a>cmake 커맨드 &amp; ninja 커맨드 설명</h3><img src="/20210105-modern-cmake/cmake_run.png" class="" title="CMake run">

<p><code>cmake .. -GNinja</code> 커맨드를 실행했을 때, 위와 같은 파일들이 생성된다.<br>이 중 제일 중요한 것은 <code>CMakeCache.txt</code>인데, 이 파일 안에 빌드에 필요한 캐쉬 값들이 저장된다.</p>
<img src="/20210105-modern-cmake/ninja_run.png" class="" title="ninja run">

<p>이 후 <code>ninja</code> 커맨드를 실행하면 빌드를 위해 ninja 패키지가 사용하는 intermediate object와 log, dependency 등이 생성된다.<br>그리고 제일 중요한 빌드의 결과물인 <code>build/App</code> executable이 생성된다.</p>
<br>

<h3 id="add-executable에-사용-가능한-옵션"><a href="#add-executable에-사용-가능한-옵션" class="headerlink" title="add_executable에 사용 가능한 옵션"></a>add_executable에 사용 가능한 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_executable</span> (&lt;name&gt; [WIN32] [MACOSX_BUNDLE]</span><br><span class="line">                [EXCLUDE_FROM_ALL]</span><br><span class="line">                [source1] [source2 ...])</span><br></pre></td></tr></table></figure>

<ul>
<li><code>&lt;name&gt;</code>은 executable의 이름<ul>
<li>Globally unique 해야함</li>
</ul>
</li>
<li><code>WIN32</code>와 <code>MACOSX_BUNDLE</code>은 OS에 따른 빌드 방식 선언</li>
<li><code>EXCLUDE_FROM_ALL</code>은 <code>make all</code>과 같은 상위레벨 빌드 명령에서 이 target 빌드를 제외하는 플래그</li>
<li> 헤더 파일은 보통 자동으로 찾아지고, cpp 소스파일만 넣어준다.</li>
</ul>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_EXECUTABLE_SUFFIX .bin)</span><br></pre></td></tr></table></figure>

<ul>
<li>Windows에서 target은 <code>App.exe</code>로 나온다</li>
<li>Linux에서 target은 suffix가 없이 <code>App</code>으로 나온다.</li>
<li>이러한 셋팅은 위 <code>set</code> 명령어로 오버라이드 할 수 있다.<ul>
<li>위의 예제로는 <code>App.bin</code>이 나오게 된다.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Library-만들기"><a href="#Library-만들기" class="headerlink" title="Library 만들기"></a>Library 만들기</h2><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(Cat cat.cpp)</span><br></pre></td></tr></table></figure>

<ul>
<li>Executable을 만드는 방식과 비슷하다.</li>
<li><code>Cat</code>이 target이고, <code>cat.cpp</code>는 소스파일이다.</li>
</ul>
<br>

<h3 id="add-library에-사용-가능한-옵션"><a href="#add-library에-사용-가능한-옵션" class="headerlink" title="add_library에 사용 가능한 옵션"></a>add_library에 사용 가능한 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(&lt;name&gt; [STATIC | SHARED | MODULE | OBJECT]</span><br><span class="line">            [EXCLUDE_FROM_ALL]</span><br><span class="line">            [source1] [source2 ...])</span><br></pre></td></tr></table></figure>

<ul>
<li><code>STATIC</code>이나 <code>SHARED</code> library를 고를 수 있다.</li>
<li>필요에 따라 <code>MODULE</code> (i.e. plug-in) 라이브러리를 만들 수도 있다.<ul>
<li>C++20의 ‘module’과는 다르다.</li>
<li>보통 Mac에서 사용하는 것 같다.</li>
</ul>
</li>
<li>Library type 정보를 넣지 않으면, <code>BUILD_SHARED_LIBS</code>라는 boolean variable에 저장된 정보로 static 또는 shared를 결정한다.</li>
</ul>
<hr>
<h2 id="Executable에-Library-링크하기"><a href="#Executable에-Library-링크하기" class="headerlink" title="Executable에 Library 링크하기."></a>Executable에 Library 링크하기.</h2><p><code>App</code>이 <code>Cat</code> 라이브러리를 사용한다면…</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(App PUBLIC Cat)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>PUBLIC</code>은 <code>Cat</code> 라이브러리가 사용하는 모든 symbol이 App에서 노출된다는 것이다.<ul>
<li><code>PRIVATE</code>, <code>INTERFACE</code> 등도 있다. 추후에 기술한다.</li>
</ul>
</li>
<li><code>target_link_libraries</code> 명령어는 target이 생성된 후에 사용되어야한다.<ul>
<li><code>App</code>과 <code>Cat</code>이 만들어진 다음에 쓸 수 있다는 뜻이다.</li>
</ul>
</li>
</ul>
<br>

<h3 id="target-link-libraries에-사용-가능한-옵션"><a href="#target-link-libraries에-사용-가능한-옵션" class="headerlink" title="target_link_libraries에 사용 가능한 옵션"></a>target_link_libraries에 사용 가능한 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(&lt;<span class="keyword">target</span>&gt;</span><br><span class="line">                      &lt;PRIVATE|PUBLIC|INTERFACE&gt; &lt;item&gt; ...</span><br><span class="line">                      [&lt;PRIVATE|PUBLIC|INTERFACE&gt; &lt;item&gt; ...] ...)                      </span><br></pre></td></tr></table></figure>

<ul>
<li>동시에 다수의 라이브러리를 링크할 수 있다.<ul>
<li>라이브러리마다 각각의 방식으로 링크할 수 있다.</li>
</ul>
</li>
</ul>
<img src="/20210105-modern-cmake/target_link_libraries.png" class="" title="target_link_libraries">

<p>큰 프로젝트에서는 위와 같은 구조도 만들 수 있다.</p>
<hr>
<h2 id="파일-디렉토리-추가"><a href="#파일-디렉토리-추가" class="headerlink" title="파일/디렉토리 추가"></a>파일/디렉토리 추가</h2><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_subdirectory</span>(dir1)</span><br><span class="line"><span class="keyword">include</span>(dir2/HelperScript.cmake)</span><br></pre></td></tr></table></figure>

<ul>
<li>Directory를 추가하고 싶을 때는 보통 <code>add_subdirectory</code> 명령어를 사용한다.</li>
<li>CMake script를 직접 추가하고 싶을 때는 <code>include</code> 명령어를 사용한다.</li>
</ul>
<br>

<h3 id="add-subdirectory에-사용-가능한-옵션"><a href="#add-subdirectory에-사용-가능한-옵션" class="headerlink" title="add_subdirectory에 사용 가능한 옵션"></a>add_subdirectory에 사용 가능한 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">add_subdirectory</span>(source_dir [binary_dir] [EXCLUDE_FROM_ALL])</span><br></pre></td></tr></table></figure>

<ul>
<li><code>binary_dir</code>은 보통 잘 안쓰인다. 정말 필요한 경우가 아닌 경우에는 추천 안한다.</li>
</ul>
<br>

<h3 id="include에-사용-가능한-옵션"><a href="#include에-사용-가능한-옵션" class="headerlink" title="include에 사용 가능한 옵션"></a>include에 사용 가능한 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span>(&lt;<span class="keyword">file</span>|module&gt; [OPTIONAL] [RESULT_VARIABLE &lt;var&gt;] [NO_POLICY_SCOPE]) </span><br></pre></td></tr></table></figure>

<ul>
<li><code>OPTIONAL</code>이 있다면, 파일이나 모듈을 찾지 못했을 때 에러를 반환하지 않는다.</li>
<li><code>RESULT_VARIABLE</code>은 파일을 찾으면 full filename을 주고, 못찾으면 NOTFOUND를 준다.</li>
<li><code>NO_POLICY_SCOPE</code>는 외부 파일에서 CMake policy를 사용하고 싶을 때 사용한다.</li>
</ul>
<hr>
<h2 id="CMake-디버깅하기"><a href="#CMake-디버깅하기" class="headerlink" title="CMake 디버깅하기"></a>CMake 디버깅하기</h2><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">message</span>([&lt;mode&gt;] <span class="string">"message text"</span> ...)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>message</code>에 <code>&lt;mode&gt;</code> 파라미터를 설정함으로써 다양한 디버깅을 할 수 있다.</li>
<li>사용 가능한 <code>&lt;mode&gt;</code> 파라미터에는<code>TRACE</code>, <code>DEBUG</code>, <code>VERBOSE</code>, <code>STATUS</code>, <code>NOTICE</code>, <code>DEPRECATION</code>, <code>AUTHOR_WARNING</code>, <code>WARNING</code>, <code>SEND_ERROR</code>, <code>FATAL_ERROR</code>가 있다.<ul>
<li>이 중에 가장 많이 사용하는 것은 <code>STATUS</code>와 <code>FATAL_ERROR</code>이다.</li>
</ul>
</li>
</ul>
<h3 id="간단한-디버깅-예시"><a href="#간단한-디버깅-예시" class="headerlink" title="간단한 디버깅 예시"></a>간단한 디버깅 예시</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">message</span>(STATUS a)</span><br><span class="line"><span class="keyword">message</span>(ERROR b)</span><br><span class="line"><span class="keyword">message</span>(FATAL_ERROR c)</span><br><span class="line"><span class="keyword">message</span>(STATUS a)</span><br></pre></td></tr></table></figure>

<p>위와 같은 CMakeLists.txt를 작성했다면,</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cmake -GNinja ..</span><br><span class="line">...</span><br><span class="line">-- a</span><br><span class="line">ERRORb</span><br><span class="line">CMake Error at CMakeLists.txt:5 (message):</span><br><span class="line"> c</span><br><span class="line"> -- Configuring incomplete, errors occured!</span><br></pre></td></tr></table></figure>

<p>이런 결과가 나온다.<br><code>ERRORb</code>가 나온 이유는, <code>ERROR</code>는 <code>message</code>가 사용할 수 있는 <code>&lt;mode&gt;</code>가 아니기 때문에 string으로 취급되기 때문이다.</p>
<h3 id="message-mode를-variable로-선언하기"><a href="#message-mode를-variable로-선언하기" class="headerlink" title="message mode를 variable로 선언하기"></a>message mode를 variable로 선언하기</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span>(MODE TRACE CACHE <span class="keyword">STRING</span> <span class="string">"Message mode/level"</span>)</span><br><span class="line"><span class="keyword">message</span>(<span class="variable">${MODE}</span> <span class="string">"message(${MODE})"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>Cache variable을 통해 mode 값을 조정할 수 있다.<ul>
<li>Cache variable에 대해서는 추후에 설명한다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cmake .. -GNinja -DMODE:STRING=STATUS</span><br><span class="line">-- message(STATUS)</span><br><span class="line">$ cmake .. -GNinja -DMODE:STRING=NOTICE</span><br><span class="line">message(NOTICE)</span><br></pre></td></tr></table></figure>

<h3 id="CMake의-모든-message-보기"><a href="#CMake의-모든-message-보기" class="headerlink" title="CMake의 모든 message 보기"></a>CMake의 모든 message 보기</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake --log-level=TRACE ...</span><br></pre></td></tr></table></figure>

<p>위와 같은 방식으로 cmake를 돌렸을 때, 리눅스 환경에서는</p>
<ul>
<li><code>STATUS</code>는 stdout으로</li>
<li><code>NOTICE</code>는 stderr로 출력된다.<ul>
<li>cmake-gui를 쓴 경우에는 빨간색으로 출력된다.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h1><h2 id="Variables-사용하기"><a href="#Variables-사용하기" class="headerlink" title="Variables 사용하기"></a>Variables 사용하기</h2><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span>(NormalVariable <span class="string">"A normal variable"</span>)</span><br><span class="line"><span class="keyword">set</span>(CacheVariable <span class="string">"A cache variable"</span> CACHE <span class="keyword">STRING</span> <span class="string">"Some description"</span>)</span><br></pre></td></tr></table></figure>

<p>CMake에서 쓸 수 있는 variable에는 두가지가 있다.</p>
<ul>
<li>보통의 variable<ul>
<li>CMakeLists 내부에서만 사용 가능</li>
</ul>
</li>
<li>Cache variable<ul>
<li>Project 외부에서 선언 가능함 (e.g. terminal에서…) </li>
</ul>
</li>
</ul>
<h3 id="사용-예시"><a href="#사용-예시" class="headerlink" title="사용 예시"></a>사용 예시</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- NormalVariable = A normal variable</span><br><span class="line">-- CacheVariable = A cache variable</span><br></pre></td></tr></table></figure>

<p>실제로 돌렸을 때는 위와 같이 나온다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep -IrE <span class="string">"Variable"</span> -C1 CMakeCache.txt</span><br><span class="line">CMakeCache.txt-//Some description</span><br><span class="line">CMakeCache.txt:CacheVariable:STRING=A cache variable</span><br><span class="line">CMakeCache.txt-</span><br></pre></td></tr></table></figure>

<p>Cache variable만 <code>CmakeCache.txt</code>에 저장된다.</p>
<h3 id="같은-이름의-variable들"><a href="#같은-이름의-variable들" class="headerlink" title="같은 이름의 variable들"></a>같은 이름의 variable들</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span>(Variable <span class="string">"A normal variable"</span>)</span><br><span class="line"><span class="keyword">set</span>(Variable <span class="string">"A cache variable"</span> CACHE <span class="keyword">STRING</span> <span class="string">"Conflicts"</span>)</span><br><span class="line"><span class="keyword">message</span>(STATUS <span class="string">"CacheVariable = ${CacheVariable})</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">if (Variable STREQUAL "</span>A cache variable<span class="string">")</span></span><br><span class="line"><span class="string">    message(STATUS "</span>Variable (<span class="variable">${Variable}</span>) is Cache<span class="string">")</span></span><br><span class="line"><span class="string">else()</span></span><br><span class="line"><span class="string">    message(STATUS "</span>Variable (<span class="variable">${Variable}</span>) is Normal<span class="string">")</span></span><br><span class="line"><span class="string">endif()</span></span><br></pre></td></tr></table></figure>

<p>위와 같은 형태로 CMakeList를 만들었다고 해보자.<br>동일한 이름의 normal variable과 cache variable을 만들었다.</p>
<p>이 경우 작동은 하지만, 아래와 같은 참사가 나타난다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cmake ..</span><br><span class="line">-- CacheVariable = A cache variable</span><br><span class="line">-- Variable (A cache variable) is Cache</span><br><span class="line"></span><br><span class="line">$ cmake ..</span><br><span class="line">-- CacheVariable = A cache variable</span><br><span class="line">-- Variable (A normal variable) is Normal</span><br></pre></td></tr></table></figure>

<p>처음 돌렸을 때는 Cache variable이고,<br>두번째 돌렸을 때는 Normal variable로 나온다.</p>
<p>위와 같은 방식을 쓰지 말자!<br>스크립트를 돌릴 때 마다 매번 같은 결과가 나오는 좋은 스크립트를 짜도록 하자.</p>
<h3 id="set에-사용할-수-있는-옵션"><a href="#set에-사용할-수-있는-옵션" class="headerlink" title="set에 사용할 수 있는 옵션"></a>set에 사용할 수 있는 옵션</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set a normal variable</span></span><br><span class="line"><span class="keyword">set</span>(&lt;variable&gt; &lt;value&gt;... [PARENT_SCOPE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a cache variale (cache entry)</span></span><br><span class="line"><span class="keyword">set</span>(&lt;variable&gt; &lt;value&gt;... CACHE &lt;type&gt; &lt;docstring&gt; [FORCE])</span><br><span class="line"><span class="comment"># (Where &lt;type&gt; is one of BOOL, FILEPATH, PATH, STRING, or INTERNAL)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set an environment variable directly</span></span><br><span class="line"><span class="keyword">set</span>(ENV{&lt;variable&gt;} [&lt;value&gt;])</span><br></pre></td></tr></table></figure>

<h3 id="Cache-variable의-특징"><a href="#Cache-variable의-특징" class="headerlink" title="Cache variable의 특징"></a>Cache variable의 특징</h3><p>Cache variable은 한번 만들고나면 <code>CMakeLists.txt</code>를 통해서 수정이 불가능하다.<br>왜냐하면 Cache variable 정보가 이미 <code>CMakeCache.txt</code>에 저장이 되어버렸기 때문에, 해당 파일을 수정하거나 지우지 않는 이상 cache variable 정보에 접근할 수 없다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cmake .. -DCacheVariable:STRING=<span class="string">"Permanent override"</span></span><br><span class="line">-- CacheVariable = Permanent override</span><br><span class="line"></span><br><span class="line">$ cmake ..</span><br><span class="line">-- CacheVariable = Permanent override</span><br><span class="line"></span><br><span class="line">$ rm CMakeCache.txt</span><br><span class="line"></span><br><span class="line">$ cmake ..</span><br><span class="line">-- CacheVariable = A cache variable</span><br></pre></td></tr></table></figure>

<h3 id="CMakeCache-txt의-특징"><a href="#CMakeCache-txt의-특징" class="headerlink" title="CMakeCache.txt의 특징"></a>CMakeCache.txt의 특징</h3><p><code>CMakeCache.txt</code> 파일은 최상위 빌드 디렉토리에 있으며, 빌드에 대한 모든 정보를 담고 있다.</p>
<p>이 파일을 새로 만들거나 수정한다면, <code>configure</code>나 <code>generate</code>를 할 때 모든 <code>CMakeLists.txt</code> 파일이 다시 처리되어야한다.</p>
<hr>
<h2 id="다양한-variable-사용하기"><a href="#다양한-variable-사용하기" class="headerlink" title="다양한 variable 사용하기"></a>다양한 variable 사용하기</h2><ul>
<li>Variable의 종류에는 string, list, boolean이 있다.<ul>
<li>List는 string들이 <code>;</code>로 분리되어있는 형태이다.</li>
</ul>
</li>
</ul>
<h3 id="String-사용하기"><a href="#String-사용하기" class="headerlink" title="String 사용하기"></a>String 사용하기</h3><img src="/20210105-modern-cmake/string1.png" class="" title="string1">

<img src="/20210105-modern-cmake/string2.png" class="" title="string2">

<img src="/20210105-modern-cmake/string3.png" class="" title="string3">

<h3 id="List-사용하기"><a href="#List-사용하기" class="headerlink" title="List 사용하기"></a>List 사용하기</h3><img src="/20210105-modern-cmake/list1.png" class="" title="list1">

<img src="/20210105-modern-cmake/list2.png" class="" title="list2">

<hr>
<h1 id="Control-flow"><a href="#Control-flow" class="headerlink" title="Control flow"></a>Control flow</h1><p>CMake에 control flow 함수는 많지 않다.</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (A)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure>

<p><code>x(...)</code>라는 control flow 함수가 있다면,<br>그 함수는 끝날 때 <code>endx()</code>로 끝나게 된다.</p>
<h2 id="Control-flow-함수"><a href="#Control-flow-함수" class="headerlink" title="Control flow 함수"></a>Control flow 함수</h2><ul>
<li><code>if()</code> ; <code>else()</code> ; <code>elseif()</code> ; <code>endif()</code></li>
<li><code>while()</code> ; <code>endwhile()</code></li>
<li><code>foreach()</code> ; <code>endforeach()</code></li>
<li><code>function()</code> ; <code>endfunction()</code></li>
<li><code>macro()</code>; <code>endmacro()</code></li>
<li><code>break()</code> ; <code>continue()</code> - 루프 컨트롤</li>
</ul>
<h2 id="if문"><a href="#if문" class="headerlink" title="if문"></a>if문</h2><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(&lt;condition&gt;)</span><br><span class="line">    &lt;commands&gt;</span><br><span class="line"><span class="keyword">elseif</span>(&lt;condition&gt;)</span><br><span class="line">    &lt;commands&gt;</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">    &lt;commands&gt;</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure>

<ul>
<li>리턴 타입은 <ul>
<li>True: 1, ON, YES, TRUE, Y, 또는 non-zero number</li>
<li>False: 0, OFF, NO, FALSE, N, IGNORE, NOTFOUND, 빈 스트링</li>
</ul>
</li>
</ul>
<hr>
<p>이 강의는 전체적인 그림이 잘 잡히지 않는다.<br>좀 더 쉬운 CMake 강의를 보고 돌아오는걸로…</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CMake</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 프로그램 속도를 빠르게 만들어주는 gcc 컴파일러 플래그</title>
    <url>/20210107-gcc-ffast-math/</url>
    <content><![CDATA[<p>C++로 프로그램을 만드는 이유 중 하나는 <strong>빠른 실행 속도</strong>이다.<br>gcc 컴파일러를 사용해서 C++ 프로그램을 컴파일 할 때, 평소보다 <strong>훨씬 더 빠르게 프로그램이 작동하도록 컴파일하는 방법</strong>이 있다.<br><span class="exturl" data-url="aHR0cDovL2djYy5nbnUub3JnL29ubGluZWRvY3MvZ2NjL09wdGltaXplLU9wdGlvbnMuaHRtbA==">온라인 도큐먼트에 다양한 컴파일 옵션<i class="fa fa-external-link-alt"></i></span>들이 잘 설명되어있다. </p>
<p> </p>
<img src="/20210107-gcc-ffast-math/tldr.gif" class="" title="tldr">

<p><strong>근데 이걸 언제 다 읽어!</strong><br>간단하게 중요한 몇개의 컴파일 플래그만 알아보도록 한다.</p>
<hr>
<h2 id="ffast-math-플래그"><a href="#ffast-math-플래그" class="headerlink" title="ffast-math 플래그"></a>ffast-math 플래그</h2><p><code>ffast-math</code>는 기존의 IEEE에서 정의한 C++ 컴파일 방식을 따르지 않고, <strong>더 효율적인 방식으로 프로그램을 컴파일</strong>한다.</p>
<p>예를 들면,</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">x = x*x*x*x*x*x*x*x;</span><br></pre></td></tr></table></figure>
<p>와 같은 코드가 있다고 해보자.<br>이 방식은 7 번의 곱셈 계산이 필요할 것이다.</p>
<p> </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">x *= x;</span><br><span class="line">x *= x;</span><br><span class="line">x *= x;</span><br></pre></td></tr></table></figure>

<p><code>ffast-math</code> 플래그를 사용하면 위 연산은 이렇게 바뀐다.<br>동일한 연산을 3번의 곱셈 연산으로 바꾸었다.<br>이론적으로, 이 연산은 약 <strong>7/3배 빠르다</strong>.</p>
<p> </p>
<h2 id="사용할-때-유의할-점-1"><a href="#사용할-때-유의할-점-1" class="headerlink" title="사용할 때 유의할 점 1"></a>사용할 때 유의할 점 1</h2><p><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS8xMC4xMTQ1LzEwMzE2Mi4xMDMxNjM=">Goldberg 논문<i class="fa fa-external-link-alt"></i></span>에 나온 것 처럼 floating-point의 계산은 associative하지 않다.<br>그러므로 <strong><code>ffast-math</code> 연산 방식에서는 실제 값에 오류를 포함</strong>할 수 밖에 없다.<br>이러한 점 때문에 <code>ffast-math</code> 방식은 IEEE에서 정의한 방식을 따르지 못한다.</p>
<p>위와 같은 특징 때문에, 정확한 값을 계산해야하는 것이라면 <code>ffast-math</code>를 사용하면 안된다.<br><strong>하지만 대충 어림잡아서 맞는 값을 원하는 것이라면?</strong><br>예를 들어서 계산 값이 10.1923845 mm가 나오는데, 우리가 원하는게 대충 10.2 mm 정도인지 아는 것으로 충분한 것이라면 그 뒤 소수점으로는 오류가 있어도 상관없다.<br>이런 경우에는 <code>ffast-math</code>가 효과적일 것이다.</p>
<p><strong>실시간 컴퓨터 비전에서는 이 방식이 효과적</strong>일 수 있다.<br><strong>어차피 센서 값에서 오류를 포함</strong>하고 있을테니, 정도껏만 정확하기만 해도 충분하기 때문이다.</p>
<p> </p>
<h2 id="사용할-때-유의할-점-2"><a href="#사용할-때-유의할-점-2" class="headerlink" title="사용할 때 유의할 점 2"></a>사용할 때 유의할 점 2</h2><p>사실 위의 것 외로도 다른 주의할 점들이 있다.<br><code>ffast-math</code> 플래그는 아래와 같은 컴파일 옵션들을 사용한다.</p>
<ul>
<li>fno-math-errno</li>
<li>funsafe-math-optimizations </li>
<li>ffinite-math-only</li>
<li>fno-rounding-math</li>
<li>fno-signaling-nans</li>
<li>fcx-limited-range</li>
<li>fexcess-precision=fast</li>
</ul>
<p>이 각각의 옵션들의 한계를 잘 확인하고, 내 코드에서 사용할 수 있는지 확인해봐야한다.</p>
<p>예를 들어, <code>ffinite-math-only</code> 플래그와 <code>fno-signaling-nans</code> 플래그는 <code>NaN</code>을 사용하는 연산은 사용할 수 없게 만든다.</p>
<hr>
<h2 id="fassociative-math-플래그"><a href="#fassociative-math-플래그" class="headerlink" title="fassociative-math 플래그"></a>fassociative-math 플래그</h2><p><code>fassociative-math</code> 플래그는 <strong><code>ffast-math</code>의 순한맛 버전</strong>…?의 느낌이다.</p>
<p><code>ffast-math</code>에서 보여준 arithmetic re-ordering 예시를 그대로 수행한다.<br>이 때문에 NaN과 관련된 문제도 나타나게 된다.<br>하지만 이것 외로는 다른 제약은 없다.</p>
<p>내 코드의 성능을 올리기 위해, <code>fassociatve-math</code> 플래그와 <code>ffast-math</code> 플래그를 하나씩 시도해보는 것도 좋다고 생각한다.</p>
<hr>
<h2 id="Ofast-플래그"><a href="#Ofast-플래그" class="headerlink" title="Ofast 플래그"></a>Ofast 플래그</h2><p><strong>가장 강력한 최적화 플래그</strong>이다.<br>IEEE고 정확도고 뭐고 다 비켜!!!!!!<br>속도가 짱짱이시다!!! 같은 느낌…</p>
<img src="/20210107-gcc-ffast-math/ggf.png" class="" title="gotta go fest">

<p><code>ffast-math</code>와 <code>fallow-store-data-races</code> 플래그를 포함하기 때문에, IEEE 규칙을 무시한 것은 물론이고 <code>ffast-math</code>에서 주의해야할 점들도 모두 가지고 있다.<br>그 외로, 아래에 보이는 것 처럼 control flow 관련 최적화 플래그도 모두 포함한다.</p>
<ul>
<li>fgcse-after-reload </li>
<li>fipa-cp-clone</li>
<li>floop-interchange </li>
<li>floop-unroll-and-jam </li>
<li>fpeel-loops </li>
<li>fpredictive-commoning </li>
<li>fsplit-loops </li>
<li>fsplit-paths </li>
<li>ftree-loop-distribution </li>
<li>ftree-loop-vectorize </li>
<li>ftree-partial-pre </li>
<li>ftree-slp-vectorize </li>
<li>funswitch-loops </li>
<li>fvect-cost-model </li>
<li>fvect-cost-model=dynamic </li>
<li>fversion-loops-for-strides</li>
</ul>
<hr>
<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><p>내가 짠 <strong>코드의 성능을 단 하나의 플래그로 뻠핑하고싶다</strong>면, 위의 플래그들을 사용해보면 좋다.<br>순서대로 <strong><code>fassociative-math</code> -&gt; <code>ffast-math</code> -&gt; <code>Ofast</code> 를 시도해보자</strong>.<br>근데 안될수도 있으니 테스트를 잘 해보자.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>pipenv 공부</title>
    <url>/20210107-pipenv/</url>
    <content><![CDATA[<p>파이썬을 이용해서 실험 자동화 스크립트를 짜다가 이런 고민이 들었다.</p>
<ul>
<li><strong>Clean slate 파이썬 환경</strong>에서 코드를 돌려볼 수 있을까?</li>
<li>전역 패키지 설정이 아닌, <strong>프로젝트마다 패키지 설정</strong>을 만들 수 있을까?</li>
</ul>
<p>주변인들로 추천받은건 Docker, Conda, pipenv가 있었다.</p>
<p>Docker는 현재 우리 팀의 서버 상황때문에 사용하기 조금 껄끄러웠다.</p>
<p>Conda는 두가지 셋팅이 있는데, Anaconda 셋팅으로 사용하기에는 용량이 너무 크고 (3gb), Miniconda 셋팅을 고려할 수 있었다.<br>하지만 팀원들이 파이썬에 익숙하지 않아서 ‘제 코드를 실행시키려면 conda라는 패키지를 깔아주세요~’라고 했을 때 별로 좋아하지 않을 가능성이 높았다.<br>무엇보다 conda의 주 목적은 scientific computing이지, 가상환경 설정이 아니였기 때문에… 다른 가상환경 패키지를 먼저 찾아보게 되었다.</p>
<br>

<p>그리고 약 30분 전, <code>pipenv</code>라는 패키지를 알게 되었다.</p>
<p>pipenv 패키지는 </p>
<ol>
<li><strong>pip처럼 파이썬 패키지를 다운</strong>받고</li>
<li><strong>가상환경을 관리</strong>할 수 있게 해준다.</li>
</ol>
<p>딱 내가 원하던거다!</p>
<p>pipenv 이전에는 <code>virtualenv</code> 또는 <code>venv</code> 패키지를 통해서 가상환경을 만들고, 그 내부에서 <code>pip</code>을 사용해서 패키지를 다운로드 받았다고 한다.<br>하지만 파이썬 <strong>3.x 버전으로 넘어온 지금 시점에서는 pipenv가 훨씬 안전하고 편하다</strong>고 하니, pipenv를 사용해보기로 한다.</p>
<hr>
<h1 id="pipenv-설치-가상환경-설정"><a href="#pipenv-설치-가상환경-설정" class="headerlink" title="pipenv 설치 + 가상환경 설정"></a>pipenv 설치 + 가상환경 설정</h1><p>우선 clean state 리눅스에서는 python3는 있지만 pip3는 없다.<br>터미널에서 <code>sudo apt install python3-pip</code> 명령어를 통해 pip을 설치해준다.<br>그 후 다시 터미널에서 <code>pip3 install pipenv</code> 명령어를 통해 pipenv를 설치한다.</p>
<p>pipenv가 설치되면, 터미널에서 <code>pipenv shell</code> 명령어를 통해 가상환경을 만들 수 있다.<br><code>pipenv shell</code> 명령어를 실행하면, 실행한 위치에서 <code>pipfile</code>이라는 파일이 만들어진다.<br>pipfile에는 이 가상환경 안에 존재하는 패키지들과 디펜던시 패키지들, 그리고 파이썬 버전이 표기된다.</p>
<p>그리고 터미널을 보면 <code>(python_test) ...</code> 라는 형태가 보이는데, 이는 내가 현재 활성화된 ‘python_test’라는 가상환경 안에 있다는 것을 보여준다.<br>‘python_test’는 내가 <code>pipenv shell</code>을 실행한 경로의 폴더 이름이다!<br><strong>신기!</strong></p>
<p>보통의 터미널에서 <code>python --version</code>을 실행하면 python2가 없기 때문에 이런 명령어가 나온다.<br>하지만 가상환경 안에서 동일한 명령어를 실행하면 자동으로 python3를 찾는다.<br>왜냐하면 이 가상환경 안에서 <code>python</code>은 우리의 전역설정인 python3 3.8.5이기 때문이다.</p>
<img src="/20210107-pipenv/activated2.png" class="" title="activated2">


<hr>
<h1 id="패키지-설치"><a href="#패키지-설치" class="headerlink" title="패키지 설치"></a>패키지 설치</h1><p>가상환경이 켜진 상태에서는 <code>pipenv install ***</code> 명령어로 패키지를 설치할 수 있다.<br>예시로 나는 <code>pipenv install numpy</code>를 해봤다.<br>뱀 이모티콘이 나오면서 numpy가 설치되었다.</p>
<p>pipfile이 업데이트가 되고, pipfile_lock이라는게 생긴다.</p>
<p>pipfile에서 package 부분을 보면 <code>numpy = "*"</code>라는게 생겼다.<br><code>*</code>는 최신버전이라는 뜻이다.</p>
<p>pipfile_lock에는 뭔가 복잡한게 생겼다.<br>대강 내부의 정보는 패키지의 버전 정보, 디펜던시 정보, 디펜던시의 디펜던시 정보가 있다.<br>pipfile_lock이 있으면 추후에 패키지가 업데이트되면서 내가 사용했던 버전들을 찾기 어려워질 때를 대비할 수 있다.</p>
<img src="/20210107-pipenv/pipfile.png" class="" title="pipfile2">

<img src="/20210107-pipenv/pipfile_lock.png" class="" title="pipfile_lock">

<p>터미널에서 <code>pipenv lock -r</code> 명령어를 사용해서 pipfile_lock에 저장된 내용들을 확인할 수도 있다.</p>
<p>패키지를 지우려면 <code>pipenv uninstall ***</code>를 하면 된다.</p>
<p>배포할 때 필요하지 않은 패키지는 <code>pipenv install *** --dev</code> 명령어를 사용하면 된다.<br>이 경우 pipfile에서 ‘dev-packages’에 들어가게 된다.</p>
<p>다른 프로젝트에서 <code>requirements.txt</code> 파일을 가져왔다면, <code>pipenv install -r ./requirements.txt</code> 명령어를 통해 필수 패키지를 한번에 설치할 수 있다.</p>
<p>패키지들의 디펜던시 관계를 보고싶다면 <code>pipenv graph</code>를 사용하면 된다.</p>
<hr>
<h1 id="배포-준비"><a href="#배포-준비" class="headerlink" title="배포 준비"></a>배포 준비</h1><p>배포를 할 때는 pipfile에 적힌것 처럼 최신 버전 (i.e. <code>*</code>)를 사용하는게 아닌, 특정 버전을 명시해야한다.<br>그렇기 때문에 배포할 때 우리는 pipfile이 아닌 pipfile_lock을 사용한다.<br><code>pipenv lock</code> 명령어를 사용해서 pipfile_lock을 업데이트 시킬 수 있다.</p>
<p>pipfile_lock으로부터 패키지 설치를 진행하기 위해서는 <code>pipenv install --ignore-pipfile</code> 명령어를 사용하면 된다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.3 Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pipenv</tag>
      </tags>
  </entry>
  <entry>
    <title>대학원 들어가기 전 알아야 할 수학 [0] - 책 소개</title>
    <url>/20210113-maths-for-graduate-school/</url>
    <content><![CDATA[<img src="/20210113-maths-for-graduate-school/book.jpg" class="" title="The book">

<p>케임브리지 대학 연구실에 방문연구원 생활이 끝나면서 기념품을 하나 사고싶었다.<br>케임브리지 시내의 한 서점에서 눈에 띄는 책을 구매했다.<br>내 책의 뒷면에 £30 이라는 가격표와 CAMBRIDGE UNIVERSITY PRESS라고 적힌걸 보면…<br>아마 그 서점은 케임브리지 출판사 전용 서점이였나보다 ㅋㅋ</p>
<p>이 기념품은 내 책장에 몇년동안 그대로 방치되었다가, 지난 주 쯤 페이스북에서 지인이 이 책에 대해 질문을 하시길래 다시 흥미가 생겨 읽게되었다.</p>
<p>아직 첫장밖에 읽지 않았지만, 이 책에는 두가지 특성이 있는 것 같다.</p>
<ul>
<li><strong>수학을 학문으로써 공부하는 학생 입장</strong>을 잘 고려한다.<ul>
<li>그만큼 응용분야의 입장은 크게 고려하지 않는다 (e.g. 머신러닝, 컴퓨터 비전…)</li>
</ul>
</li>
<li>개별적인 테크닉이나 방법론보다는, 최대한 그 <strong>학문의 본질을 파악</strong>할 수 있게 한다.</li>
</ul>
]]></content>
      <categories>
        <category>4. Maths</category>
        <category>대학원 들어가기 전 알아야할 수학 시리즈</category>
      </categories>
      <tags>
        <tag>Maths</tag>
        <tag>Graduate school</tag>
      </tags>
  </entry>
  <entry>
    <title>대학원 들어가기 전 알아야 할 수학 [1] - 선형대수</title>
    <url>/20210114-graduate-school-math-linear-algebra/</url>
    <content><![CDATA[<p>(<strong>모바일은 가로로 돌려서 보시는게 편합니다</strong>)</p>
<p>첫장은 선형대수였고, 이 책은 ‘선형대수는 무엇을 하는 학문인가?’ 라는 질문을 던진다.</p>
<br>

<hr>
<h1 id="선형대수의-본질"><a href="#선형대수의-본질" class="headerlink" title="선형대수의 본질"></a>선형대수의 본질</h1><img src="/20210114-graduate-school-math-linear-algebra/systems.png" class="" title="Systems of linear equations">

<br>

<p><strong>선형대수의 본질</strong>은 <strong>System of linear equations 문제를 푸는 것</strong>이다.<br>그리고 이 문제를 풀기위해 다양한 matrix를 다루는 방법들을 공부한다.</p>
<p>조금 과장하자면, 모든 수학문제는 선형대수 문제로 변환할 수 있어야만 풀 수 있다. (라고 한다)<br>선형대수를 통해 공통된 특성을 공유하는 object들의 관계를 하나의 vector space로 다룰 수 있게 해준다.<br>또 이 vector space들 사이의 관계를 linear transformation으로 표현할 수 있다.</p>
<p>선형대수는 <strong>n개의 linear equation들과 n개의 unknown이 있을 때</strong>, <strong>solution이 있는지 없는지</strong> 알아낼 수 있는 다양한 방법들을 제공한다.</p>
<br>

<hr>
<h1 id="가장-기초적인-Vector-space-mathbb-R-n"><a href="#가장-기초적인-Vector-space-mathbb-R-n" class="headerlink" title="가장 기초적인 Vector space: $\mathbb{R}^n$"></a>가장 기초적인 Vector space: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></h1><ul>
<li>가장 본질적인 vector space는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>로써 모든 real number의 집합이다.<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="21.882ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 9671.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(889, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1864.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(2309.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3647.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4092.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5138.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5805.6, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="msub" transform="translate(6361.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7505.1, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8449.9, 0)"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mo" transform="translate(9171.9, 0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></li>
</ul>
<br>

<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>은 왜 vector space일까?<ul>
<li><strong>Vector addition</strong>이 가능하다.<ul>
<li>즉, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>에 속하는 두개의 값을 더했을 때 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>에 속하는 다른 값이 나온다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="49.272ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 21778.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1364.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1809.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3147.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3592.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4638.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5250, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(6250.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(6639.3, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(490, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(7532.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(7977.5, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(9316.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(9760.8, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(10725.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(11391.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(12447.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(12836.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(14034.4, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(15034.6, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(490, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(15928.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(16372.9, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(17711.5, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(18156.2, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(19424.7, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(20424.9, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(21389.2, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
</li>
</ul>
</li>
<li><strong>Scalar multiplication</strong>이 가능하다.<ul>
<li>즉, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>에 속하는 값과 scalar (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.027ex" xmlns="http://www.w3.org/2000/svg" width="1.319ex" height="1.597ex" role="img" focusable="false" viewBox="0 -694 583 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></mjx-container>) 값을 곱했을 때 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>에 속하는 다른 값이 나온다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="29.724ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 13138.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="mo" transform="translate(583, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(972, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1947.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(2392.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3730.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4175.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5221.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5888.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(6944.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7333.4, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="msub" transform="translate(7916.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(8891.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(9336.6, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(10675.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(11119.9, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="msub" transform="translate(11702.9, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(12749.2, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>는 n차원의 <span class="exturl" data-url="aHR0cHM6Ly9rby53aWtpcGVkaWEub3JnL3dpa2kvJUVDJUEwJTk1JUVDJTg4JTk4">real number (i.e. 정수)<i class="fa fa-external-link-alt"></i></span>를 가진 벡터를 의미한다.<ul>
<li>차원의 수를 늘리거나 줄일 수 있을까?<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.706ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1196.3 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="3.151ex" height="1.635ex" role="img" focusable="false" viewBox="0 -722.6 1392.8 722.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(722, 410.1) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>으로의 변환 관계, 즉 <strong>n차원에서 m차원으로의 맵핑은 매트릭스로 표현</strong>할 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -4.355ex" xmlns="http://www.w3.org/2000/svg" width="62.833ex" height="9.842ex" role="img" focusable="false" viewBox="0 -2425 27772.2 4350"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(750, 0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(1634.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(2690.6, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(133.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500, 0)"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(2553.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(4839.5, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(7145.1, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(637.7, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(3057.4, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(5286.5, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(7684.6, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(2610.4, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(4839.5, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(7011.5, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(9510.6, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g><g data-mml-node="mrow" transform="translate(13076.2, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(35.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(384.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(1921.3, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g><g data-mml-node="mo" transform="translate(16150.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(17206, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(267.3, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500, 0)"></path></g></g></g><g data-mml-node="msub" transform="translate(1286.1, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2483.9, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(3484.1, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(4878.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(5878.5, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="msub" transform="translate(7235.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(4269.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(1553.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2751.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(3751.4, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(5145.6, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(6145.8, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="msub" transform="translate(7769.9, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(9691.2, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g></g></g></svg></mjx-container></p>
<br>

<ul>
<li>아래와 같은 <strong>system of linear equations</strong>이 있다고 해보자.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="9.413ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 4160.4 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(429, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(832.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1277.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(2615.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3060.6, 0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(429, -150) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>과 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="11.625ex" height="1.437ex" role="img" focusable="false" viewBox="0 -441 5138.2 635"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500, 0)"></path></g></g></g><g data-mml-node="mo" transform="translate(1286.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1730.8, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3069.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3514.1, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>이 주어졌고, 우리는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="8.609ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 3805.2 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(975.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1420.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="msub" transform="translate(2758.9, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>을 구하고싶다고 해보자.</li>
</ul>
</li>
</ul>
<img src="/20210114-graduate-school-math-linear-algebra/systems.png" class="" title="Systems of linear equations">

<ul>
<li>많은 선형대수 문제는 위와 같은 system of linear equations 문제로 요약될 수 있다.<ul>
<li>적은 수의 equation만 있다면 <a href="https://ko.wikipedia.org/wiki/%EA%B0%80%EC%9A%B0%EC%8A%A4_%EC%86%8C%EA%B1%B0%EB%B2%95"><strong>Gaussian elimination (i.e. 가우스 소거법)</strong></a> 등의 방식을 통해 손으로 계산할 수 있다.</li>
<li>하지만 <strong>많은 수의 equation + unknowns를 풀어야한다면 계산량이 무지막지하게 많아진다</strong>.<ul>
<li>이론적으로 어렵지 않으나, 수많은 숫자들을 트랙킹하는게 골치아프다…</li>
</ul>
</li>
</ul>
</li>
<li>우선 위와 같은 형태의 문제들을 아래와 같은 방식으로 요약할 수 있다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>를 벡터화, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container>를 매트릭스화 한다.</li>
<li>그러면 문제를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.014ex" xmlns="http://www.w3.org/2000/svg" width="8.065ex" height="1.593ex" role="img" focusable="false" viewBox="0 -698 3564.6 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D400" d="M296 0Q278 3 164 3Q58 3 49 0H40V62H92Q144 62 144 64Q388 682 397 689Q403 698 434 698Q463 698 471 689Q475 686 538 530T663 218L724 64Q724 62 776 62H828V0H817Q796 3 658 3Q509 3 485 0H472V62H517Q561 62 561 63L517 175H262L240 120Q218 65 217 64Q217 62 261 62H306V0H296ZM390 237L492 238L440 365Q390 491 388 491Q287 239 287 237H390Z"></path></g><g data-mml-node="mi" transform="translate(869, 0)"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g><g data-mml-node="mo" transform="translate(1753.8, 0)"><path data-c="3D" d="M87 333Q64 343 64 362Q64 383 84 391Q89 393 448 393H807Q808 392 811 390T817 386T823 381T827 374T829 363Q829 345 807 333H87ZM87 109Q64 118 64 139Q64 159 86 168Q89 169 448 169H807L812 166Q816 163 818 162T823 157T827 149T829 139Q829 118 807 109H87Z"></path></g><g data-mml-node="mi" transform="translate(2925.6, 0)"><path data-c="1D41B" d="M32 686L123 690Q214 694 215 694H221V409Q289 450 378 450Q479 450 539 387T600 221Q600 122 535 58T358 -6H355Q272 -6 203 53L160 1L129 0H98V301Q98 362 98 435T99 525Q99 591 97 604T83 620Q69 624 42 624H29V686H32ZM227 105L232 99Q237 93 242 87T258 73T280 59T306 49T339 45Q380 45 411 66T451 131Q457 160 457 230Q457 264 456 284T448 329T430 367T396 389T343 398Q282 398 235 355L227 348V105Z"></path></g></g></g></g></svg></mjx-container>와 같이 보기 편한 형태로 바꿀 수 있다.</li>
</ul>
</li>
</ul>
<br>

<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.979ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 3084.6 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1599.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2655.6, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -4.355ex" xmlns="http://www.w3.org/2000/svg" width="53.746ex" height="9.842ex" role="img" focusable="false" viewBox="0 -2425 23755.7 4350"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mstyle"><g data-mml-node="mspace"></g></g><g data-mml-node="mi" transform="translate(1000, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2027.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(3083.6, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(133.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500, 0)"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(2553.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(4839.5, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(7145.1, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(637.7, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(3057.4, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(5286.5, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g><g data-mml-node="mtd" transform="translate(7684.6, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g><g data-mml-node="mtd" transform="translate(2610.4, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(4839.5, 0)"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mtd" transform="translate(7011.5, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(529, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(9510.6, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g><g data-mml-node="mo" transform="translate(13469.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(13913.8, 0)"><g data-mml-node="mi"><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path></g></g><g data-mml-node="mo" transform="translate(14719.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(15775.4, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(35.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(384.1, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(1921.3, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g><g data-mml-node="mo" transform="translate(18571.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(19016.3, 0)"><g data-mml-node="mi"><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z"></path></g></g><g data-mml-node="mo" transform="translate(19850.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mrow" transform="translate(20905.9, 0)"><g data-mml-node="mo"><path data-c="239B" d="M837 1154Q843 1148 843 1145Q843 1141 818 1106T753 1002T667 841T574 604T494 299Q417 -84 417 -609Q417 -641 416 -647T411 -654Q409 -655 366 -655Q299 -655 297 -654Q292 -652 292 -643T291 -583Q293 -400 304 -242T347 110T432 470T574 813T785 1136Q787 1139 790 1142T794 1147T796 1150T799 1152T802 1153T807 1154T813 1154H819H837Z" transform="translate(0, 1271)"></path><path data-c="239D" d="M843 -635Q843 -638 837 -644H820Q801 -644 800 -643Q792 -635 785 -626Q684 -503 605 -363T473 -75T385 216T330 518T302 809T291 1093Q291 1144 291 1153T296 1164Q298 1165 366 1165Q409 1165 411 1164Q415 1163 416 1157T417 1119Q417 529 517 109T833 -617Q843 -631 843 -635Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239C" d="M413 -9Q412 -9 407 -9T388 -10T354 -10Q300 -10 297 -9Q294 -8 293 -5Q291 5 291 127V300Q291 602 292 605L296 609Q298 610 366 610Q382 610 392 610T407 610T412 609Q416 609 416 592T417 473V127Q417 -9 413 -9Z" transform="scale(1, 2.255)"></path></svg></g><g data-mml-node="mtable" transform="translate(875, 0)"><g data-mml-node="mtr" transform="translate(0, 1675)"><g data-mml-node="mtd" transform="translate(133.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -275)"><g data-mml-node="mtd" transform="translate(410.9, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="22EE" d="M78 30Q78 54 95 72T138 90Q162 90 180 74T199 31Q199 6 182 -12T139 -30T96 -13T78 30ZM78 440Q78 464 95 482T138 500Q162 500 180 484T199 441Q199 416 182 398T139 380T96 397T78 440ZM78 840Q78 864 95 882T138 900Q162 900 180 884T199 841Q199 816 182 798T139 780T96 797T78 840Z"></path></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -1675)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="TeXAtom" transform="translate(429, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g><g data-mml-node="mo" transform="translate(1974.8, 0)"><path data-c="239E" d="M31 1143Q31 1154 49 1154H59Q72 1154 75 1152T89 1136Q190 1013 269 873T401 585T489 294T544 -8T572 -299T583 -583Q583 -634 583 -643T577 -654Q575 -655 508 -655Q465 -655 463 -654Q459 -653 458 -647T457 -609Q457 -58 371 340T100 1037Q87 1059 61 1098T31 1143Z" transform="translate(0, 1271)"></path><path data-c="23A0" d="M56 -644H50Q31 -644 31 -635Q31 -632 37 -622Q69 -579 100 -527Q286 -228 371 170T457 1119Q457 1161 462 1164Q464 1165 520 1165Q575 1165 577 1164Q582 1162 582 1153T583 1093Q581 910 570 752T527 400T442 40T300 -303T89 -626Q78 -640 75 -642T61 -644H56Z" transform="translate(0, -1281)"></path><svg width="875" height="932" y="-216" x="0" viewBox="0 210.5 875 932"><path data-c="239F" d="M579 -9Q578 -9 573 -9T554 -10T520 -10Q466 -10 463 -9Q460 -8 459 -5Q457 5 457 127V300Q457 602 458 605L462 609Q464 610 532 610Q548 610 558 610T573 610T578 609Q582 609 582 592T583 473V127Q583 -9 579 -9Z" transform="scale(1, 2.255)"></path></svg></g></g></g></g></svg></mjx-container></p>
<br>

<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.361ex" height="1.312ex" role="img" focusable="false" viewBox="0 -540 2811.6 580"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mi" transform="translate(2211.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>일 때 (i.e. equation의 수가 unknown의 수 보다 많을 때) unknown은 구할 수 없다 (i.e. <strong>No solution</strong>).<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvT3ZlcmRldGVybWluZWRfc3lzdGVt">Over-determined<i class="fa fa-external-link-alt"></i></span> 문제라고 하며, 이 경우는 least squares optimisation을 통해 최소 에러값을 가지는 Unknown value를 구할 수 있다.</li>
<li>컴퓨터 비전에서 자주 나타나는 문제이다.</li>
</ul>
</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.361ex" height="1.312ex" role="img" focusable="false" viewBox="0 -540 2811.6 580"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(2211.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>일 때 (i.e. unknown의 수가 equation의 수 보다 많을 때)는 <strong>무한히 많은 solution</strong>이 존재한다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvVW5kZXJkZXRlcm1pbmVkX3N5c3RlbQ==">Under-determined<i class="fa fa-external-link-alt"></i></span> 문제라고 한다.</li>
</ul>
</li>
<li><strong>선형대수에서 가장 많이 집중하는 케이스</strong>는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.361ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 2811.6 665"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1155.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2211.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>인 상황이다.<ul>
<li>이 경우 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.697ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 750 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container>는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.481ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2422.4 502"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1822.4, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 매트릭스의 형태를 띄우겠고, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="6.979ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 3084.6 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1599.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2655.6, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 문제를 풀어서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.375ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5027.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1364.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1809.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3147.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3592.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4638.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>의 값을 알아내려고 한다.</li>
<li>x를 풀기 위해서는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="9.136ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 4038.2 915.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(849.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(1905.6, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(750, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(3609.2, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>가 성립되어야한다.<ul>
<li>이를 위해서는 A의 inverse, 즉 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="3.854ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1703.7 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(750, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>가 존재해야한다.</li>
<li><strong>선형대수의 상당한 부분은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="3.854ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1703.7 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(750, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>가 존재하는지에 대해 알아내는 방법</strong>을 포함한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="벡터-공간-Vector-spaces-와-선형변환-Linear-Transformation"><a href="#벡터-공간-Vector-spaces-와-선형변환-Linear-Transformation" class="headerlink" title="벡터 공간 (Vector spaces)와 선형변환 (Linear Transformation)"></a>벡터 공간 (Vector spaces)와 선형변환 (Linear Transformation)</h1><h2 id="Vector-space의-정의"><a href="#Vector-space의-정의" class="headerlink" title="Vector space의 정의"></a>Vector space의 정의</h2><ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>라는 set은 아래와 같은 맵핑 조건을 충족한다면 정수에 대한 vector space가 될 수 있다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="11.398ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 5038 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mo" transform="translate(944.2, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1944.4, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(2991.2, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4269, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container><ul>
<li>어떠한 정수와 어떠한 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>의 Element를 곱했을 때 나오는 결과가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> set에 포함되어있는 경우</li>
</ul>
</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="11.505ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 5085 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(991.2, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1991.4, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(3038.2, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4316, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container><ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> set에 포함되어있는 두 element를 더했을 때 나오는 결과가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> set에 포함되어있는 경우</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<ul>
<li>Vector space <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>의 element는 편히 <strong>vector</strong>라고 부른다.</li>
<li>Vector space가 존재하는 정수 element는 <strong>scalar</strong>라고 부른다.<ul>
<li>사실 정수 뿐만이 아니라 다른 수의 체계 (e.g. 정수, 허수, 자연수…)도 사용 가능하다.</li>
</ul>
</li>
</ul>
<br>

<ul>
<li>이 때, 아래의 추가조건을 달성해야한다.<ul>
<li>0 vector 가 있어야한다 (i.e. 모든 vector에 대해서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="9.109ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 4026 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(722.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(1722.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(2485.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3541, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container> 라는 계산이 가능해야한다). (i.e. <strong>Null space</strong>)</li>
<li>모든 vector에 대해 상응하는 역수가 있어야한다 (i.e. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.629ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5582 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(707.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(1707.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(2096.4, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2874.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(3359.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4026.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(5082, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>이 가능해야한다). (i.e. <strong>Inverse element</strong>)</li>
<li>모든 vector에 대해 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="13.983ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 6180.4 665"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(707.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(1707.4, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(2701.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3757, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(4695.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(5695.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>가 성립해야한다. (i.e. <strong>Commutative</strong>)</li>
<li>모든 vector에 대해 상응하는 항등수가 있어야한다. (i.e. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="7.977ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 3526 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(1222.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(1985.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3041, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>) (i.e. <strong>Neutral element</strong>)</li>
<li>모든 vector와 어떠한 scalar 값 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>에 대해, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.334ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8545.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(529, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(918, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(1625.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2625.4, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(3341.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4008.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(5064, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(5593, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(6300.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7300.4, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(7829.4, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>가 성립해야한다 (i.e. <strong>Distributivity</strong>)</li>
<li>모든 vector와 어떠한 두개의 scalar 값 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>에 대해, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="14.701ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6498 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(529, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(918, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(1347, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(1832, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2498.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3554.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3943.6, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(4694.8, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(5195, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(5624, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(6013, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>가 성립해야한다 (i.e. <strong>Scalar associativity</strong>).</li>
<li>모든 vector와 어떠한 두개의 scalar 값 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>에 대해, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="17.935ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7927.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mo" transform="translate(1140.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2140.4, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(2569.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(2958.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(3721.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4777, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(5306, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(6013.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7013.4, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(7442.4, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>가 성립해야한다 (i.e. <strong>Scalar distributivity</strong>)</li>
</ul>
</li>
</ul>
<br>

<h2 id="Linear-transformation-정의"><a href="#Linear-transformation-정의" class="headerlink" title="Linear transformation 정의"></a>Linear transformation 정의</h2><ul>
<li><strong>Vector space간의 맵핑 관계는 Linear transformation으로 표현된다</strong>.</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> vector space에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container> vector space로의 변환 관계를 표현하는 함수인 T가 있다.<ul>
<li>수식으로는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="11.109ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 4910.1 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(981.8, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(1537.6, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(2584.3, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3862.1, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container> 로 표현된다.</li>
<li>T가 linear transformation이기 위해선 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="35.088ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 15508.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(704, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1093, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="msub" transform="translate(2025.6, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3136.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(4136.6, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="msub" transform="translate(5069.1, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5957.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6624.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(7680.2, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mi" transform="translate(8612.8, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(9316.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(9705.8, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(10594.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(11205.5, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(12205.8, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(13138.3, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(13842.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(14231.3, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(15119.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 조건이 충족되어야한다.</li>
</ul>
</li>
</ul>
<br>

<h2 id="Vector-subspace-정의"><a href="#Vector-subspace-정의" class="headerlink" title="Vector subspace 정의"></a>Vector subspace 정의</h2><ul>
<li>Vector space <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>에 속한 vector subspace <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g></g></g></svg></mjx-container>도 하나의 vector space이다.</li>
<li>Vector subspace의 존재함에는 몇가지 조건이 충족되어야한다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g></g></g></svg></mjx-container> vector subspace 내부에서도 <strong>addition 및 scalar multiplication에 대한 closure</strong>가 있어야한다.<ul>
<li>즉, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g></g></g></svg></mjx-container> 내부의 모든 vector 사이의 addition 및 scalar multiplication 연산은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g></g></g></svg></mjx-container> 내부의 또다른 vector가 되어야한다는 것이다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<h2 id="Linear-transformation과-Vector-subspace의-관계"><a href="#Linear-transformation과-Vector-subspace의-관계" class="headerlink" title="Linear transformation과 Vector subspace의 관계"></a>Linear transformation과 Vector subspace의 관계</h2><ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>로의 linear transformation <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>가 있다면… (i.e. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="11.109ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 4910.1 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(981.8, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(1537.6, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(2584.3, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3862.1, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>)<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>의 <strong>kernel</strong>은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="27.973ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12364.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(521, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(987, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1438, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1827, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(2531, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3197.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(4253.6, 0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mi" transform="translate(4753.6, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(5516.3, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(6461.1, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(7507.9, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(8063.7, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(8767.7, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9156.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(9641.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(10308.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(11364.2, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(11864.2, 0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container>이다</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>의 <strong>image</strong>는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="54.542ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 24107.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504, 0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1382, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1771, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(2475, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3141.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(4197.6, 0)"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mi" transform="translate(4697.6, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(5691.3, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(6636.1, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(7961.9, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mtext" transform="translate(8517.7, 0)"><path data-c="A0" d=""></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(250, 0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(639, 0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1195, 0)"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1639, 0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2031, 0)"></path><path data-c="20" d="" transform="translate(2475, 0)"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2725, 0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(3169, 0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(3697, 0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(3975, 0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(4369, 0)"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(4758, 0)"></path><path data-c="20" d="" transform="translate(5152, 0)"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(5402, 0)"></path><path data-c="A0" d="" transform="translate(5902, 0)"></path></g><g data-mml-node="mi" transform="translate(14669.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(15432.4, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(16377.2, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mtext" transform="translate(17146.2, 0)"><path data-c="A0" d=""></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(250, 0)"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(972, 0)"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1250, 0)"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1639, 0)"></path><path data-c="A0" d="" transform="translate(2195, 0)"></path></g><g data-mml-node="mi" transform="translate(19591.2, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(20295.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(20684.2, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(21169.2, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(21836, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(22891.8, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(23607.8, 0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></li>
</ul>
</li>
<li>아래 이미지에서 보이는 것과 같이,<ul>
<li>Kernel은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>의 subspace이다.</li>
<li>Image는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>의 subspace이다.</li>
</ul>
</li>
</ul>
<img src="/20210114-graduate-school-math-linear-algebra/kernel_image.png" class="" title="Kernel and image">

<br>

<hr>
<h1 id="기저-벡터-Bases-차원-Dimension-선형-변환-Linear-transformation-을-매트릭스로-표현하기"><a href="#기저-벡터-Bases-차원-Dimension-선형-변환-Linear-transformation-을-매트릭스로-표현하기" class="headerlink" title="기저 벡터 (Bases), 차원 (Dimension), 선형 변환 (Linear transformation)을 매트릭스로 표현하기"></a>기저 벡터 (Bases), 차원 (Dimension), 선형 변환 (Linear transformation)을 매트릭스로 표현하기</h1><ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> vector space의 <strong>기저 벡터</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.981ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4853.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1277.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1722.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3060.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3505.6, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4464.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>은 <strong>scalar scaling을 통해</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container><strong>의 모든 벡터를 표현</strong>할 수 있다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="20.858ex" height="1.676ex" role="img" focusable="false" viewBox="0 -583 9219.1 740.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(762.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1818.6, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="msub" transform="translate(2751.1, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mn" transform="translate(485, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3861.9, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(4862.1, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(6256.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(7256.6, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(529, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(8259.8, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></li>
</ul>
</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container><strong>의 차원은 기저 벡터의 수와 동일</strong>하다.</li>
<li>모든 기저벡터는 동일한 수의 element를 가지고 있다.<ul>
<li>예시로, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.633ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 722 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g></g></g></svg></mjx-container>의 기저벡터는 보통 $\{ (1,0,…,0), (0,1,0,…,0), … , (0, …, 0, 1) \}</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>4. Maths</category>
        <category>대학원 들어가기 전 알아야할 수학 시리즈</category>
      </categories>
      <tags>
        <tag>Maths</tag>
        <tag>Linear algebra</tag>
      </tags>
  </entry>
  <entry>
    <title>QIRA 디버거</title>
    <url>/20210115-qira/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/eGl6kpSajag" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p><strong>아이폰 탈옥</strong>과 <strong>PlayStation 탈옥</strong> 등 천재해커라고 불렸고, 현재는 <strong>Comma.ai</strong>라는 회사를 만들어 1천달러로 기존의 자동차를 자율주행차를 바꿀 수 있는 OpenPilot이라는 시스템을 만들고 있는 <strong>George Hotz</strong>의 발표이다.</p>
<p>발표 페이스가 무진장 빠르고… 스타일 자체가 ‘난 천재다!’라고 얘기하는 느낌이다 ㅋㅋ.</p>
<p>발표에서는 본인이 주장하는 ‘Timeless debugger’인 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dlb2hvdC9xaXJh">QIRA<i class="fa fa-external-link-alt"></i></span>를 소개한다.</p>
<br>

<hr>
<h1 id="gdb-vs-QIRA-Why-QIRA"><a href="#gdb-vs-QIRA-Why-QIRA" class="headerlink" title="gdb vs QIRA. Why QIRA?"></a>gdb vs QIRA. Why QIRA?</h1><ul>
<li><p><a href="https://www.gnu.org/s/gdb/"><strong>gdb</strong></a><strong>는 쒯이다</strong>.</p>
<ul>
<li>70년도에 만든 디버거를 왜 아직까지 쓰고있나?</li>
<li>70년도에 만든 패러다임을 왜 지금까지 쓰고있나?</li>
</ul>
</li>
<li><p>gdb는 <span class="exturl" data-url="aHR0cHM6Ly9saW51eC5kaWUubmV0L21hbi8zL2JhY2t0cmFjZQ==">backtrace<i class="fa fa-external-link-alt"></i></span> (i.e. Call stack을 되짚어보는 기능) 기능을 지원한다.</p>
<ul>
<li>하지만 정확한 위치에서 breakpoint를 찍지 못해서 프로그램을 아마 10번은 돌려야 할 것이다.<ul>
<li>왜 우리는 정확한 위치에 breakpoint를 찍지 못하는가? </li>
<li>Crash report를 보면 대충 어디서 프로그램이 터졌는지 의심가는 부분은 있지만, 정확한 코드 라인을 알지 못하기 때문이다.</li>
</ul>
</li>
<li>Breakpoint를 실수로 지나치면 돌아갈 방법이 없다.<ul>
<li>그러면 breakpoint를 좀 더 앞단의 코드로 옮기고 프로그램을 다시 시작해서 테스트 해야한다.<ul>
<li>Crash report가 몇백-몇천개씩 몰려들고있으면 ‘breakpoint’를 찾는 작업만으로 시간을 다 갖다버린다.</li>
</ul>
</li>
<li>뭐… gdb에서 rewind 같은 기능을 만들고있긴 하지만, 우리가 원하는건 그게 아니다.</li>
</ul>
</li>
</ul>
</li>
<li><p>우리가 원하는건 <strong>Timeless debugging</strong>이다.</p>
<ul>
<li>.NET, Objective-C, Swift와 같이 다이나믹한 디버거들도 많지만, 우리가 원하는건 <strong>binary 프로그램 디버깅</strong>이다.<ul>
<li>(Binary 프로그램 디버깅을 한다는건… 리버스 엔지니어링도 된다는게 아닐까…?)</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvSW50ZXJhY3RpdmVfRGlzYXNzZW1ibGVy">IDA<i class="fa fa-external-link-alt"></i></span>나 <span class="exturl" data-url="aHR0cHM6Ly9rby53aWtpcGVkaWEub3JnL3dpa2kvT2JqZHVtcA==">objdump<i class="fa fa-external-link-alt"></i></span>도 binary 프로그램이 작동하는 방식을 이해하기 위한 좋은 선택이다.</li>
</ul>
</li>
</ul>
<br>

<hr>
<h1 id="QIRA의-근원"><a href="#QIRA의-근원" class="headerlink" title="QIRA의 근원"></a>QIRA의 근원</h1><ul>
<li>QIRA의 메인 아이디어는 Git과 같은 version control에서 왔다.<ul>
<li>Git의 작동방식을 보면… 파일마다 버전 정보, 수정 내역, 수정한 사람의 정보 등등이 전부 기록되어있다.</li>
<li>이 방식을 그대로 가져와서… 파일이 아니라 <strong>메모리와 레지스터에 적용</strong>하면 어떨까? <ul>
<li>Instruction을 commit과 비슷한 개념으로 생각해서 말이다.</li>
<li>예를 들어, MOVE(r0,1) 과 같은 instruction이 있었다면, r0라는 메모리에 1 값을 넣었다는 정보를 저장할 수 있겠다.</li>
</ul>
</li>
<li>위의 아이디어로 벌써 디버거의 구상은 끝났다.<ul>
<li>Trace를 잘 할 수 있는 UI만 잘 만들면 된다.</li>
<li>gdb처럼 breakpoint 하나 놓쳤다고 프로그램을 다시 실행하게 만들지만 말자.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<hr>
<h1 id="QIRA의-특징"><a href="#QIRA의-특징" class="headerlink" title="QIRA의 특징"></a>QIRA의 특징</h1><img src="/20210115-qira/qira.png" class="" title="QIRA">

<ul>
<li>그렇게 만들어진 QIRA가 이것이다.<ul>
<li>좌상단에는 위에서 아래 순서로 Program trace가 나열되어있다.<ul>
<li>바로 밑에는 레지스터들이 있다.</li>
</ul>
</li>
<li>우측에는 IDA와 비슷한 방식으로 그래프 형태의 Program flow가 보인다.</li>
<li>좌하단에는 메모리 주소마다 저장된 hex값을 보여준다.</li>
</ul>
</li>
</ul>
<br>

<ul>
<li>QIRA의 장점은 아래와 같다.<ul>
<li><a href="https://security.stackexchange.com/questions/129499/what-does-eip-stand-for"><strong>EIP</strong></a>를 확실하게 알 수 있다 (i.e. 함수의 순서를 정확하게 알 수 있다).</li>
<li>런타임에서 <strong>변수의 타입</strong>을 알 수 있다.<ul>
<li>Static한 환경에서는 거의 절대로 변수의 타입을 알 수 없다 (메모리에서 변수의 시작점과 끝을 알 수 없기 때문)</li>
<li>QIRA는 Dynamic 트레이싱을 통해, 언제 얼만큼의 메모리가 접근하고 할당되는지 알기 때문에 변수의 타입을 추론할 수 있다.</li>
</ul>
</li>
<li>QIRA는 프로그램 전체의 history를 저장한다.<ul>
<li>gdb는 프로그램을 실행하면서만 trace를 읽을 수 있다.</li>
<li>QIRA는 trace의 로컬 복사본을 저장하기 때문에, <strong>프로그램이 끝나도 trace 분석</strong>이 가능하다.<ul>
<li>즉, 다시 분석하려고 또 프로그램을 킬 필요가 없다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<ul>
<li>오버헤드가 너무 클 것 같다는 생각이 들 수 있다.<ul>
<li>하지만 요즘 컴퓨터는 짱짱 좋아서 모바일에서도 빠르게 코드가 돌아간다.</li>
<li>데스크탑에서 QIRA를 돌린다? 절대로 리소스가 부족할리가 없다.</li>
</ul>
</li>
</ul>
<hr>
<p>George Hotz가 이야기하길, 본인이 기업들의 프로그램 분석 대회에서 상금을 타갈 수 있던데에는 QIRA가 큰 공헌을 했다고 한다.<br>다른 사람들보다 10배는 더 빠르게 binary 프로그램을 분석할 수 있다는건 큰 능력이다.</p>
<p>카네기 멜론 대학에서는 비슷한 프로젝트로 현재 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0JpbmFyeUFuYWx5c2lzUGxhdGZvcm0vYmFw">bap<i class="fa fa-external-link-alt"></i></span>이라는 시스템을 만들고있다고 한다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>QIRA</tag>
        <tag>geohot</tag>
        <tag>George Hotz</tag>
      </tags>
  </entry>
  <entry>
    <title>자율주행 자동차 만들기 - 2019년도 온라인 스터디 링크</title>
    <url>/20210119-autonomous-driving/</url>
    <content><![CDATA[<p>2019년 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3I=">SLAM KR 커뮤니티<i class="fa fa-external-link-alt"></i></span>에서 진행한 자율주행 스터디에 참여하였다.<br><span class="exturl" data-url="aHR0cDovL2Fjb3JucHViLmNvLmtyL2Jvb2svYXV0b25vbW91cy12ZWhpY2xlLXN5c3RlbXM=">‘자율 주행 자동차 만들기’<i class="fa fa-external-link-alt"></i></span>라는 책을 기반으로 함께 공부하였다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vcGxheWxpc3Q/bGlzdD1QTHViVXF1aXFOUWRPWVlVcUpGMVllQWNEcUZKMHNyeVVG">Youtube 재생목록<i class="fa fa-external-link-alt"></i></span>을 통해 영상을 확인할 수 있다.<br>스터디 내용 목차는 다음과 같다.</p>
<ul>
<li>자율주행 개요</li>
<li>자율주행을 위한 로컬라이제이션 1 &amp; 2</li>
<li>자율주행을 위한 인지</li>
<li>딥러닝을 통한 자율주행의 인지 </li>
<li>예측 및 경로 계획</li>
<li>강화학습 기반의 계획 및 제어</li>
<li>결정, 계획, 제어</li>
<li>자율주행을 위한 클라이언트 시스템</li>
<li>자율주행을 위한 클라우드 플랫폼</li>
</ul>
<hr>
<h2 id="내-파트"><a href="#내-파트" class="headerlink" title="내 파트"></a>내 파트</h2><p>스터디는 참여인원들이 돌아가면서 특정 파트의 발표를 하는 방식으로 진행되었다.</p>
<p>내가 맡은 파트는 ‘자율주행을 위한 인지’ 파트였는데, 사실상 이 챕터의 목적은 ‘딥러닝을 위한 자율주행의 인지’ 방법을 위한 빌드업이였다.<br>발표를 준비하면서 ‘인지’의 목적은 주로 1. Dynamic 객체와 Static 객체를 분리하는 것, 2. 공간 정보에 semantic 정보를 association하는 것임을 알게 되었다.</p>
<p>내 발표의 주제에는 다음과 같은 토픽이 있었다.</p>
<ol>
<li>딥러닝 이전의 확률적 segmentation 기법들을 통한 이미지 해석</li>
<li>스테레오 비전 기반 geometric verification을 통한 dynamic object 분리,</li>
<li>Optical flow / Scene flow를 이용한 객체 트랙킹</li>
<li>기타 객체 트랙킹 기법들</li>
</ol>
<p>책에서는 이 내용들에 대해 깊게 들어가지 않는데, 이는 최신 딥러닝 기법들의 정확도가 월등히 뛰어나기에 위 방법론을 갈아치울 것이라고 저자들이 판단하였기 때문이다. 나도 안전성을 중요시하는 자율주행 기술 특성상, 딥러닝 기법이 언젠가는 Traditional 기법들을 갈아치울 것이라고 생각한다.</p>
<p>내가 관련 자료와 논문을 찾아봤을 때는 확실히 ‘이 방법이 잘되더라 - 제품을 만들자’라는 느낌보다는 ‘기술적으로 시도해봤다’의 느낌이 강했다. 특히 hand-crafted 방법론들이나 확률적 모델링 방법들은 조금이라도 어려운 환경들이 나타났을 때 굉장히 쉽게 무너졌다. 그런면에 있어서 딥러닝과 같은 Data-driven approach가 확실히 우위를 점하게 되었다. 물론 연산량에 있어서는 딥러닝 기법이 훨씬 많겠지만, GPGPU의 발전, 경량화/양자화 기술의 발전, 그리고 기타 자율주행용 프로세서 및 하드웨어 발전을 고려했을 때, 딥러닝 기법이 언젠가는 traditional 기법들을 갈아치울 것이라고 생각한다 (다만, 이 책에서 보여주는 딥러닝 기법들이 주가 될지는 잘 모르겠다…).</p>
<p>기존 방식들의 논문들을 보았을 때 느낀 점은, 어떠한 어려운 문제가 있을 때 엔지니어링의 관점에서 문제를 정확하게 define하고 하나씩 태클링하는 모습이 인상적이였다. 그에 비해 딥러닝 논문들에서는 대부분 ‘데이터를 모으고, 문제가 해결될때까지 아키텍처를 이런저런 방법으로 바꾸고, 하이퍼파라미터를 이런저런 방법으로 바꿨는데, 결과는 이렇게 잘된다’ 라는 모습이 보였다. 물론 딥러닝 논문들도 나름의 metric을 가지고 성능평가를 가졌지만, 문제의 본질을 자주 생각하게 되었던 것은 기존의 방식이라고 생각되었다.</p>
<hr>
<h2 id="배운-점"><a href="#배운-점" class="headerlink" title="배운 점"></a>배운 점</h2><p>왜 때문이였는지 기억은 잘 나지 않지만, 스터디 후반으로 갔을 때는 잘 참여하지 못했다.<br>아마 강화학습 부분과 아키텍처 부분이 낯설어서 쫒아가지 못해서였을 것이다.</p>
<p>스터디 초반 부에는 대신 열심히 집중할 수 있었는데, 자율주행에서 사용하는 다양한 센서들에 대해 빠르게 익힐 수 있던 점이 좋았다.<br>카메라, 라이다, 레이더, INS (IMU + GPS)와 같이 차에 탑재할 수 있는 시스템들, 그리고 실험에서 사용할 수 있는 다양한 GPS 시스템들 (e.g. RTK GPS)등에 대해 알 수 있게 되었다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Community</tag>
      </tags>
  </entry>
  <entry>
    <title>2020년 SLAM 이론 스터디 (SLAM DUNK Season 2) + 스터디 링크</title>
    <url>/20210120-slam-dunk-2/</url>
    <content><![CDATA[<p>2019년 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3I=">SLAM KR 커뮤니티<i class="fa fa-external-link-alt"></i></span>에서 진행한 SLAM 이론 스터디에 참여하였다.<br><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMdWJVcXVpcU5RZFBfSDZ1VW1VLTlmMHlfTGhlQTNIaWw=">Bonn 대학의 Cyrill Stachniss 교수님의 렉처<i class="fa fa-external-link-alt"></i></span>를 기반으로 함께 공부하였다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMdWJVcXVpcU5RZFBfSDZ1VW1VLTlmMHlfTGhlQTNIaWw=">Youtube 재생목록<i class="fa fa-external-link-alt"></i></span>을 통해 영상을 확인할 수 있다.<br>스터디 내용 목차는 다음과 같다.</p>
<ul>
<li><p><strong>Introduction to SLAM</strong></p>
</li>
<li><p>An Informal Introduction to Least Squares</p>
</li>
<li><p>Graph-Based SLAM using Pose Graph</p>
</li>
<li><p>Graph-Based SLAM with Landmarks</p>
</li>
<li><p>Robust Least Squares for Graph-Based SLAM</p>
</li>
<li><p>Hierarchical Pose Graphs for SLAM</p>
</li>
<li><p>The Basics of Bundle Adjustment</p>
</li>
<li><p>Numeric of the Bundle Adjustment</p>
</li>
<li><p>Iterative Closest Points</p>
</li>
<li><p>Bayes Filter</p>
</li>
<li><p>Kalman FIlter &amp; EKF</p>
</li>
<li><p>EKF SLAM</p>
</li>
<li><p>Particle Filter and Monte Carlo Localization</p>
</li>
<li><p>Occupancy Grid Maps</p>
</li>
<li><p>Epipolar Geometry Basics</p>
</li>
<li><p>Camera Parameters</p>
</li>
<li><p>Direct Linear Transform</p>
</li>
<li><p>Camera Calibration using Zhang’s Method</p>
</li>
<li><p>Projective-3-Point Algorithm using Grunert’s method</p>
</li>
<li><p><strong>RANSAC</strong></p>
</li>
<li><p>Direct Solutions for computing F and E matrix</p>
</li>
<li><p>Relative Orientation, Fundamental and Essential Matrix</p>
</li>
<li><p>Absolute Orientation Similarity Transformations</p>
</li>
<li><p>Iterative Solution for the Relative Orientation</p>
</li>
<li><p>Triangulation for Image Pairs</p>
</li>
<li><p>Orthophotos</p>
</li>
<li><p>–</p>
</li>
</ul>
<p>이번 스터디에서 내가 커버했던 내용은 “Introduction to SLAM”과 “RANSAC” 부분을 커버하였다.<br>“Introduction to SLAM”은 기본적인 백엔드의 개념과 Localization + Mapping 에 대해서 설명하였다.<br>“RANSAC” 발표는 RANSAC 자체 알고리즘이 워낙 심플하기도 하고, 너무 발표가 짧아서… PROSAC과 Lo-RANSAC 구현을 이전에 했는데, 그 때 관련해서 느낀 점에 대해 적었다. (그랬더니 영상이 30분으로 길어졌다 끙)</p>
<hr>
<p>이번 스터디의 반은 기본적인 SLAM의 백엔드, 나머지 반은 Visual-SLAM의 기초 이론들을 다룬다.<br>시작할 때만 해도 ‘초반 반은 이번에 제대로 듣고, 나머지 반은 조금 아니까 대충 들어도 되겠지’라고 생각했지만 완전히 잘못된 생각이였다 ㅋㅋ…<br>생각보다 후반부가 어려웠고 중요한 내용들이 많이 있어서 많이 배우는 계기가 되었다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Community</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual-SLAM developer roadmap [1] - 컴퓨터 비전 입문</title>
    <url>/20210121-visual-slam-roadmap-beginner/</url>
    <content><![CDATA[<p>원본 Github 링크: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L3Zpc3VhbC1zbGFtLXJvYWRtYXA=">Visual-SLAM roadmap repository<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="컴퓨터-비전-입문"><a href="#컴퓨터-비전-입문" class="headerlink" title="컴퓨터 비전 입문"></a>컴퓨터 비전 입문</h1><p><img src="https://raw.githubusercontent.com/changh95/visual-slam-roadmap/main/img/beginner.png" alt="컴퓨티 비전 입문"></p>
<p>기초적인 프로그래밍, 수학, 영상처리 개념을 익히는 단계입니다.<br>프로그래밍과 수학 공부를 우선 하신 후, 그 다음 영상처리를 공부하시는 것을 추천합니다.</p>
<br>

<hr>
<h2 id="프로그래밍"><a href="#프로그래밍" class="headerlink" title="프로그래밍"></a>프로그래밍</h2><p>Visual-SLAM을 하기 위해서는 <strong>C++ 언어</strong>를 익혀야합니다.<br>C++에서 간단한 pointer 사용과 class 설계 및 이해를 할 수 있으면 충분하다고 생각합니다.<br>C++이 처음이시라면 다음 링크를 참조하셔서 공부하시면 좋습니다.</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaW5mbGVhcm4uY29tL2NvdXJzZS9mb2xsb3dpbmctYy1wbHVz">홍정모의 따배씨<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMZ25RcFF0RlRPR1JNNTlzcjNuU0w4Qm1lTVpSOUdDSUE=">Modern C++ for Computer Vision<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMbHJBVGZCTlo5OGR1ZG5NNDh5ZkdVbGRxR0QwUzRGRmI=">Cherno C++<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS92TG5Qd3haZFc0WQ==">freecodecamp C++<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<br>

<hr>
<h2 id="수학"><a href="#수학" class="headerlink" title="수학"></a>수학</h2><p>고등학교 수학에서 요구하는 것 보다 아주 살짝 더 필요합니다.</p>
<ul>
<li><strong>확률과 통계 기초</strong><ul>
<li>우선 가우시안 분포까지만 이해하셔도 됩니다. SLAM에서는 가우시안 형태의 센서 데이터를 많이 사용합니다.</li>
</ul>
</li>
<li><strong>로그와 지수</strong><ul>
<li>추후 Lie algebra를 이해하기 위해 필요합니다. </li>
</ul>
</li>
<li><strong>선형대수 기초</strong><ul>
<li>공간에 대한 계산을 하기 위해 전반적인 이해가 필요합니다.<ul>
<li>깊게 알수록 더 좋습니다.</li>
</ul>
</li>
<li>국문 강의는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMMTI3VDJadTc2RnVWTXExVVFuWnY5U0ctR0ZJZFpmTGc=">이상엽의 선형대수 강의<i class="fa fa-external-link-alt"></i></span>를 추천합니다.</li>
<li>영문 강의는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMRTdEREQ5MTAxMEJDNTFGOA==">Gilbert Strang 선형대수 강의<i class="fa fa-external-link-alt"></i></span> 를 추천합니다.</li>
<li>또는 <span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo0MDAwLzIwMjEwMTE0LWdyYWR1YXRlLXNjaG9vbC1tYXRoLWxpbmVhci1hbGdlYnJhLw==">여기<i class="fa fa-external-link-alt"></i></span>에도 정리가 되어있습니다.</li>
</ul>
</li>
<li><strong>미적분 기초</strong><ul>
<li>최적화 계산과 근사치 계산에서 필요합니다. 편미분, Jacobian 등은 지금 알면 좋습니다. Taylor expansion은 꼭 알고 가셔야합니다.</li>
</ul>
</li>
</ul>
<br>

<hr>
<h2 id="영상처리"><a href="#영상처리" class="headerlink" title="영상처리"></a>영상처리</h2><p>이미지를 통해 정보를 추출하는 방법들을 공부합니다.</p>
<h3 id="Projective-geometry"><a href="#Projective-geometry" class="headerlink" title="Projective geometry"></a>Projective geometry</h3><p>아마 입문 단계에서 제일 어려운 부분일 겁니다.</p>
<ul>
<li><strong>Image projection</strong>에 대해 이해해야합니다.<ul>
<li>3D scene이 2D 이미지로 투영되는 과정에 대해 이해합니다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS91SEFwRHFILThVRQ==">Stachniss 교수님의 렉처<i class="fa fa-external-link-alt"></i></span>를 추천합니다.<ul>
<li>이를 통해 Visual-SLAM에서 다루는 데이터에 대해 이해할 수 있습니다.</li>
</ul>
</li>
</ul>
</li>
<li><strong>3D 공간에서의 강체 운동 (rigid body motion) 및 공간 변환 관계</strong>를 이해합니다.<ul>
<li>Homogeneous coordinates, homogeneous transformation (i.e. 4x4 매트릭스)에 대해 익숙해져야합니다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9NUWRtMFpfZ05jdw==">Stachniss 교수님의 렉처<i class="fa fa-external-link-alt"></i></span>를 추천합니다.<ul>
<li>이를 통해 카메라의 움직임, 물체의 움직임을 수식적으로 표현할 수 있게 됩니다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="카메라-이해하기"><a href="#카메라-이해하기" class="headerlink" title="카메라 이해하기"></a>카메라 이해하기</h3><p>Visual-SLAM에서 사용되는 카메라의 특성에 대해 이해합니다.</p>
<ul>
<li><strong>렌즈의 특성</strong>에 대해 이해합니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWF0aHdvcmtzLmNvbS9oZWxwL3Zpc2lvbi91Zy9jYW1lcmEtY2FsaWJyYXRpb24uaHRtbA==">Mathworks 페이지<i class="fa fa-external-link-alt"></i></span>에 설명이 잘 되어있습니다.</li>
</ul>
</li>
<li><strong>카메라 센서</strong>에 대해 이해합니다.<ul>
<li>다양한 조명에서 기본적인 카메라 셋팅을 할 수 있어야합니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9SWDlEdElaTEhiZz90PTE4OQ==">사진 튜토리얼 - 1,2,3편<i class="fa fa-external-link-alt"></i></span>을 보시면 좋습니다.</li>
</ul>
</li>
<li>다양한 카메라 셋팅 (stereo / RGB-D)등을 이해합니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9oS3NkZldGQWpRWQ==">SLAM, KR 스터디 발표<i class="fa fa-external-link-alt"></i></span>를 추천합니다.</li>
</ul>
</li>
<li>두개의 2D 이미지로부터 3D 공간을 이해하는 epipolar geometry 방법을 이해합니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9jTGVGLUtOSGd3VQ==">Stachniss 교수님의 렉처<i class="fa fa-external-link-alt"></i></span>를 추천합니다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="이미지-데이터-이해하기"><a href="#이미지-데이터-이해하기" class="headerlink" title="이미지 데이터 이해하기"></a>이미지 데이터 이해하기</h3><ul>
<li><strong>이미지 데이터의 특성</strong>에 대해 이해합니다.</li>
<li><strong>SLAM에서 왜 grayscale 이미지를 사용하는지</strong> 이해합니다.<ul>
<li>Grayscale 이미지에서 유용한 정보를 뽑아내는 다양한 방법을 이해합니다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>Visual-SLAM 공부 로드맵</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Github</tag>
        <tag>Roadmap</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual-SLAM developer roadmap [0] - 로드맵 개요</title>
    <url>/20210121-visual-slam-roadmap/</url>
    <content><![CDATA[<p>원본 Github 링크: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L3Zpc3VhbC1zbGFtLXJvYWRtYXA=">Visual-SLAM roadmap repository<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="목적"><a href="#목적" class="headerlink" title="목적"></a>목적</h1><ul>
<li>‘저희는_SLAM_마스터가_될겁니다’ 톡방에서 ‘어디서부터 Visual-SLAM 공부를 시작해야할지 모르겠다’ 라는 질문이 많아서.</li>
<li>내가 거쳐갔던 논문들의 순서를 잊지 않기 위해서.</li>
</ul>
<hr>
<h1 id="순서"><a href="#순서" class="headerlink" title="순서"></a>순서</h1><ol>
<li>컴퓨터 비전 입문</li>
<li>SLAM 이해하기</li>
</ol>
<p>여기서부터는 원하는 stream으로 갈라져서…</p>
<ol start="3">
<li>Monocular Visual-SLAM</li>
<li>Stereo Visual-SLAM</li>
<li>Visual-inertial odometry (VIO) / Visual-inertial system (VINS)</li>
<li>RGB-D Visual-SLAM</li>
<li>Visual-LiDAR Fusion</li>
<li>Collaborative SLAM</li>
<li>Deep SLAM / Localization</li>
</ol>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>Visual-SLAM 공부 로드맵</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Github</tag>
        <tag>Roadmap</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM 마스터 오픈카톡방 500명 기념!</title>
    <url>/20210122-slam-master-500/</url>
    <content><![CDATA[<p>“저희는_SLAM_마스터가_될겁니다” 톡방 링크: <span class="exturl" data-url="aHR0cHM6Ly9vcGVuLmtha2FvLmNvbS9vL2c4VDVreExi">https://open.kakao.com/o/g8T5kxLb<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="SLAM-마스터"><a href="#SLAM-마스터" class="headerlink" title="SLAM 마스터"></a>SLAM 마스터</h2><p>2019년 11월, ORB-SLAM 스터디를 진행하면서 오픈카톡방을 개설하었습니다!<br>2~3달의 단기 스터디로 기획되었던 스터디였기에, 간단한 토론과 Q&amp;A를 위한 소통의 장으로 사용하기 위해 만들었어요.<br>만드는 김에 이름도 재미있게 + 포부있게 지었구요.</p>
<blockquote>
<p>이번 스터디가 끝나고 나면 SLAM 마스터가 되있을거야! … 같은 느낌</p>
</blockquote>
<p>하지만 ORB-SLAM 스터디는 코로나로 인해 무산되었습니다 (ㅠㅠ…)<br>한동안은 ‘코로나가 잠잠해지면, 다시 스터디를 열거야!’ 라는 생각으로 방을 유지했었어요.<br>하지만 점점 톡방에 참여해주시는 인원들이 늘어났고 SLAM에 대한 다양한 이야기가 오갔습니다.</p>
<hr>
<h2 id="지금의-톡방"><a href="#지금의-톡방" class="headerlink" title="지금의 톡방"></a>지금의 톡방</h2><img src="/20210122-slam-master-500/kakao.jpg" class="" title="Kakao">

<p>1년하고 조금의 시간이 흐른 지금, <strong>저희는 SLAM 마스터가 되겠다는 목표</strong>를 이뤘을까요?<br>일단 저는 마스터랑은 거리가 먼 것 같습니다 (먼산…)</p>
<p>현재 저희 톡방은 하나의 <strong>커뮤니티</strong>가 되었다고 생각합니다.<br>500명의 인원이 SLAM에 대해 토론을 하고, 업계 동향을 이야기하고, SLAM 관련 포지션의 구인/구직을 합니다.<br>저는 이러한 작용이 결국 <strong>SLAM 기술의 진입장벽을 낮추는데에 큰 도움</strong>을 주고 있다고 생각합니다.</p>
<blockquote>
<p>SLAM 공부하다가 막히면, 커뮤니티가 도와줄 수 있습니다.<br>SLAM 관련 종사자에게 질문이 있다면, 커뮤니티가 도와줄 수 있습니다.<br>SLAM 관련 연구를 혼자하기 버겁다면, 커뮤니티가 도와줄 수 있습니다.</p>
</blockquote>
<p><strong>SLAM은 정말 매력적이고 장래성이 높은 기술</strong>이라고 생각합니다.<br>그리고 개인적으로 생각보다 그렇게 진입장벽이 높은 기술은 아니라고 생각합니다. (진입장벽이 높아보이는 이유는, 자료를 찾기 어려워서라고 생각합니다)</p>
<p>이런 기술은 혼자만 하기에는 너무 아깝습니다 ㅎㅎ<br><strong>500명을 넘어서, 미래에는 더 많은 분들과 함께 SLAM 했으면 좋겠습니다!</strong> <span class="github-emoji" alias="smile_cat" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f638.png?v8">😸</span></p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Community</tag>
        <tag>SLAM Master</tag>
      </tags>
  </entry>
  <entry>
    <title>입문 Visual-SLAM 14강 번역 + 스터디 링크</title>
    <url>/20210123-slam-book-translation/</url>
    <content><![CDATA[<img src="/20210123-slam-book-translation/slambook.png" class="" title="slambook">

<p>2018년 SLAM을 잘 모르던 때에 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3I=">SLAM KR 커뮤니티<i class="fa fa-external-link-alt"></i></span>의 훌륭하신 분들과 함께 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dhb3hpYW5nMTIvc2xhbWJvb2s=">‘입문 Visual SLAM 14강’<i class="fa fa-external-link-alt"></i></span> 책을 번역하였습니다.<br>번역이 끝나고, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMdWJVcXVpcU5RZE9UTm9jbVdDU1drOVphV2hWN3ViQ0Q=">Visual-SLAM 스터디<i class="fa fa-external-link-alt"></i></span>도 참여하였습니다.</p>
<ul>
<li>1장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xWEtJM0RVV19LbG83R0NkNDNMRUZaZ09rSFpGOEE1YmFWMDNOem1paWY5NC9lZGl0">선행지식<i class="fa fa-external-link-alt"></i></span></li>
<li>2장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xVjhQelktdEZlelBILWZGRF9FRDRxSWp6TGhlZVdVcENjOWtqZUhHT1hody9lZGl0">SLAM과의 첫 만남<i class="fa fa-external-link-alt"></i></span></li>
<li>3장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xMGdKc0ZXR0FpTmFGYzV6YThJSlRaSFUzSkZZSHpTdHZMTmVMVkt5cnBVQS9lZGl0">3차원 공간 강체 변환<i class="fa fa-external-link-alt"></i></span></li>
<li>4장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xaWNQalV5VDNuUHZqWjFPVk10V3A5YWZVdHVKNGdYTEpMLWV4N0E5RnBOcy9lZGl0">Lie 군과 Lie 대수<i class="fa fa-external-link-alt"></i></span> </li>
<li>5장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xV1hxcXRVRjJMN3Q2djktcUM2elhGNDhNdW9nMlk3LWxFYzZvalhIYVRIWS9lZGl0">카메라 및 이미지<i class="fa fa-external-link-alt"></i></span> </li>
<li>6장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xcElZbEwzY1ZmOWdYVDlPN3BfNUg1YXowSFlrQUx1aHJuckJRZVpJMEhVdy9lZGl0">비선형 최적화<i class="fa fa-external-link-alt"></i></span> </li>
<li>7장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xbFdFZTVwSVVTTWh2ZnNxVEk0Qjd6ZjVKZGttS25VR1NsOEo4bU1kZHdfRS9lZGl0">Visual Odometry 1<i class="fa fa-external-link-alt"></i></span></li>
<li>8장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xUnJ0Z21KaG00VjNzVGJTRWJ0aW81NU51UDhEQkxWYVpLMGZPUGVkUWtxMC9lZGl0">Visual Odometry 2<i class="fa fa-external-link-alt"></i></span> </li>
<li>9장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xb0VNcVF2WXpBLWdzcmxSc05MWmxzaEhYQmJaMXNyeHBKdS1QMW9SVHVBUS9lZGl0">실습: Frontend 디자인<i class="fa fa-external-link-alt"></i></span> </li>
<li>10장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xWGxvQjRGd1VYaEVxME5SUFg2bG1vbTdtT3hYMmVRaTBuQTEzM0xFamNNcy9lZGl0">Backend 1<i class="fa fa-external-link-alt"></i></span> </li>
<li>11장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xczNrRzJRRzhxRUlpZTFacERxWEJQOVRWWXdLSkp0SHYySEhBRFl3b2xkdy9lZGl0">Backend 2<i class="fa fa-external-link-alt"></i></span> </li>
<li>12장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xMXJxN242NWNpZEhCaEViV25ZTUJia3VBWW95elJPeGZXTi0yNFlJRUkwRS9lZGl0">루프 탐지<i class="fa fa-external-link-alt"></i></span> </li>
<li>13장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xMkRsdnE0dDZwMExDSXM1Wjc2VVRQQ3ZxQmFsUHdzaHVlOElPZjZ1dkZNay9lZGl0">Mapping<i class="fa fa-external-link-alt"></i></span> </li>
<li>14장 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xUWFUb3M0QWdqd21hSXo1VThHUnRzeGR4b2sxMG5EMjJWR3UxZWJmdlNYSS9lZGl0">SLAM: 현재와 미래<i class="fa fa-external-link-alt"></i></span> </li>
<li><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xbXVDWXZoZUVTOGFGRU5JSWNzOEdEVnhSQ1I1dFNxYm9lbjhRdWVRMG1uNC9lZGl0">부록<i class="fa fa-external-link-alt"></i></span> </li>
<li><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vZG9jdW1lbnQvZC8xcmVzR1JBMEtsUDZzMER3QUpLTmE4MUlFMmhtZXl3WURUZjJPcjBXZG9yQS9lZGl0">참고문헌<i class="fa fa-external-link-alt"></i></span> </li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Community</tag>
      </tags>
  </entry>
  <entry>
    <title>Graphcore IPU로 Bundle adjustment / SLAM 하기!</title>
    <url>/20210128-gbp-poplar/</url>
    <content><![CDATA[<h1 id="IPU로-BA-SLAM"><a href="#IPU로-BA-SLAM" class="headerlink" title="IPU로 BA / SLAM?"></a>IPU로 BA / SLAM?</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuZ3JhcGhjb3JlLmFpLw==">Graphcore<i class="fa fa-external-link-alt"></i></span>는 영국의 반도체 회사로, AI 계산에 특화된 프로세서인 IPU를 개발한다. 현재 단계의 SLAM은 아직 완전 AI라고 할 수 없지만, 영국 Imperial College London의 Andrew Davison 교수님은 IPU와 같은 구조의 프로세서가 Spatial AI를 만들 수 있을 것이라고 굳게 믿고 계신다. IPU에 대한 설명은 이 <a href="https://changh95.github.io/20201226-CVPR-2020-SLAM-workshop-Davison/">링크</a>를 참고하자.</p>
<p>이에 대해 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMTQxMzk=">FutureMapping 2: Gaussian Belief Propagation for Spatial AI<i class="fa fa-external-link-alt"></i></span>라는 논문을 통해 Bundle adjustment를 Gaussian belief propagation 형태로 풀은 선행 연구가 있었다. GBP 방식은 효과적이었으나 CPU에서는 효율적이지 않아 ceres-solver, g2o, GTSAM 등에서는 현재 지원하고 있지 않은 기능이다. 하지만 IPU의 특별한 구조 덕분에 GBP 방식을 실제로 사용할 수 있을 것 같은 가능성이 보였다.</p>
<p>이 논문이 발표되고 1년의 시간이 지나 개선된 IPU 제품이 출시되었으며, 2020년 CVPR에서 해당 장비를 이용하여 CPU에서의 ceres-solver 라이브러리의 성능을 뛰어넘은 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDMxMzQ=">Bundle adjustment on a graph processor<i class="fa fa-external-link-alt"></i></span> 논문이 발표되었다. 아래는 관련 영상이다.</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/TqeN8aQNgd0" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<hr>
<h1 id="코드"><a href="#코드" class="headerlink" title="코드"></a>코드</h1><p>이 논문에 꽤나 감명받아있던 때에, 국내 클라우드 업체인 <span class="exturl" data-url="aHR0cHM6Ly93d3cubWVnYXpvbmUuY29tLw==">메가존 클라우드<i class="fa fa-external-link-alt"></i></span>에서 Graphcore IPU 서버를 수입하고 테스터들을 모집하고 있었다. 아무래도 IPU 칩은 주로 클라우드 기반 딥러닝 학습에 초점을 두고 있다보니 모집인원은 딥러닝에 집중되어있었지만, 감사하게도 지인분께서 IPU를 사용해볼 수 있게 메가존 클라우드에 나를 소개해주셨다. 나를 포함한 3명이 첫 라운드로 하나의 서버를 할당받았고, 메가존 클라우드 측에서도 TensorFlow 환경과 PyTorch 환경을 미리 설정해주시고 Q&amp;A 채널을 만들어주시는 등 지원을 아낌없이 해주셨다. 물론 나는 딥러닝을 할 것이 아니기 때문에, BA / SLAM 코드를 따로 끌어왔어야했다.</p>
<p>다행히도 Bundle adjustment on a graph processor 코드는 저자의 Github에 올라와있었다. 지원받은 기기는 M2000 IPU 속 일정 부분이였는데 (POD64와 성능이 비슷한? 또는 동일한? 것으로 알고 있다), 논문에서 구현된 내용을 돌리기에는 충분했을 것이다. ssh로 서버에 접속 후 git clone으로 끌어와서 돌려봤는데, 런타임 에러가 있었다. IPU를 다루게 해주는 프레임워크인 Poplar SDK 코드가 업데이트 되면서 바뀐건가? 했는데 그건 아닌 것 같았다. 어쩐지 조금 불신이 생겼지만, 해당 내용을 수정하고 저자의 repo에 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2pvZWFvcnRpei9nYnAtcG9wbGFyL3B1bGwvMQ==">PR<i class="fa fa-external-link-alt"></i></span>을 넣었다.</p>
<img src="/20210128-gbp-poplar/ipu-running.png" class="" title="BA">

<p>그리고 코드를 실행해보았다. 우선 첫번째로 느낀 점은 <strong>데이터 로딩이 무지하게 오래 걸린다는 점</strong>이다. 거의 <strong>20~30초</strong>는 데이터로딩에 걸린 것 같다 (즉, 이 상태로 20~30초를 기다린다). 이게 이렇게 되는게 맞는건가…? 아직도 솔직히 의문이긴 하다.</p>
<img src="/20210128-gbp-poplar/gbp-poplar.gif" class="" title="BA-2">

<p>그리고는 위와 같이 코드가 돌아갔다. 이렇게 돌아가면서 느낀 점은 <strong>여전히 너무 느리다는 것</strong>이다. ‘설마 iteration 도는거를 보기 쉽게 하려고 std::chrono 같은걸 끼얹었나?’ 라는 생각도 들어서 코드를 뜯어봤지만 그런건 없었다. 분명 논문에서는 250ms에 BA가 끝났다는데, 내가 돌린 코드에서는 20초가 걸렸다 ㅋㅋㅋ 이런 결과를 보면 IPU에 대한 신뢰도가 사라질 것이다. 왜냐하면 CPU에서 ceres-solver가 도는 모습을 log로 찍으면 눈에 보이지도 않기 때문이다.</p>
<hr>
<h1 id="저자와의-컨택"><a href="#저자와의-컨택" class="headerlink" title="저자와의 컨택"></a>저자와의 컨택</h1><p>저자와 여러 이메일을 주고 받았다.</p>
<p>나는 저자에게 ‘내가 돌린 코드에서는 20초가 나오는데, 논문에서 나오는 40ms는 어떻게 측정한 것인지?’ 라는 질문을 던졌다. 이에 대해 저자는 <strong>reprojection error가 1.5 pixel보다 작은 수치로 수렴하는 시간이 40 ms</strong>라고 하였다. 논문에서는 동일한 조건으로 CPU에서 ceres-solver로 계산 했을 때 1450ms 로 나왔다고 이야기하였다.</p>
<p>내가 코드를 돌렸을 때 ~350번 프레임 쯤에서 에러치가 1.5 pixel보다 낮아졌다. 저자이 따르면 0번째 프레임부터 여기까지의 시간을 재는 것이 맞다고 한다.</p>
<p>하지만 현재의 코드는 매 iteration마다 <code>engine.run</code> 함수가 들어가있고, 이는 반복 연산에 효율적인 방법은 아니다. 그렇기에 오버헤드가 많이 발생하여 훨씬 느리게 보이는 것이라고 저자는 이야기한다. </p>
<p>저자는** <code>repeat()</code>함수를 사용해서 350 iteration을 하나의 <code>engine.run</code>으로 넣으면 문제가 해결될 것**이라고 했고, 추후에 코드를 수정해 공유해주겠다고 했지만… 아쉽게도 나는 IPU 테스트 기간이 끝나버렸다 ㅜㅜㅜ 저자를 믿는 수 밖에… </p>
<p>저자의 말로는 <strong>다음 달 쯤에 논문이 또 나온다고 한다</strong>!!! 신나버려!</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Graphcore</tag>
        <tag>IPU</tag>
        <tag>Bundle adjustment</tag>
      </tags>
  </entry>
  <entry>
    <title>자료구조와 알고리즘 이해</title>
    <url>/20210214-data-structures-and-algorithms/</url>
    <content><![CDATA[<h1 id="왜-자료구조와-알고리즘을-배워야하는가"><a href="#왜-자료구조와-알고리즘을-배워야하는가" class="headerlink" title="왜 자료구조와 알고리즘을 배워야하는가?"></a>왜 자료구조와 알고리즘을 배워야하는가?</h1><p>자료구조: 효율적인 데이터 접근 및 수정 방법을 제공<br>알고리즘: 문제를 풀기 위한 효율적이고 빠른 방법을 제공</p>
<p>자료구조와 알고리즘을 알고 있다면 문제를 효율적인 방법으로 풀 수 있음.<br>자료구조와 알고리즘을 알지 못하는 프로그래머는 효율적인 방법으로 풀 수 없으며, 왜 본인의 코드가 비효율적인지 알지 못함.</p>
<br>

<hr>
<h1 id="알고리즘-효율성의-척도-Big-O"><a href="#알고리즘-효율성의-척도-Big-O" class="headerlink" title="알고리즘 효율성의 척도 - Big O"></a>알고리즘 효율성의 척도 - Big O</h1><p>Array에 담긴 모든 숫자들의 합을 구하는 함수를 짰다고 해보자.</p>
<p><code>number_list</code>에 10개의 값이 있다면, 이 함수가 끝나기 위해서는 10개의 값을 모두 탐색해야한다. 20개의 값이 있다면 20개의 값을 모두 탐색해야한다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_sum</span>(<span class="params">number_list</span>):</span></span><br><span class="line">    total_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> number_list:</span><br><span class="line">        total_sum += num</span><br><span class="line">    <span class="keyword">return</span> total_sum</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src="/20210214-data-structures-and-algorithms/sum.jpg" class="" title="find_sum">

<p>이번에는 array의 첫번째 값만 읽는 함수를 짰다고 해보자.</p>
<p>이번에는 <code>number_list</code>의 크기와 상관없이 무조건 첫번째 값만 읽으면 된다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first_num</span>(<span class="params">number_list</span>):</span></span><br><span class="line">    <span class="keyword">return</span> number_list[<span class="number">0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src="/20210214-data-structures-and-algorithms/first_elem.jpg" class="" title="first_num">


<p>(물론 이 두 함수의 목적은 달랐지만) 이 두 함수는 다른 <strong>Time complexity</strong>를 가지고 있다. Time complexity는 <strong>인풋 데이터의 크기가 커짐에따라 바뀌는 계산량의 관계</strong>를 뜻한다. 우리는 이를 <strong>Big O notation</strong>을 통해 표현한다.</p>
<p>Big O notation에는 다음과 같은 종류가 있다. (효율적인거 부터 비효율적인 순서대로)</p>
<ul>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 500 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="4.626ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2044.7 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1444.7, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="6.36ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2811.3 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(766.7, 0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(2044.7, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(2211.3, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.27ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1003.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.204ex" height="1.528ex" role="img" focusable="false" viewBox="0 -675.5 974.3 675.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500, 363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>)</li>
<li>O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.645ex" role="img" focusable="false" viewBox="0 -716 878 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z"></path></g></g></g></svg></mjx-container>)</li>
</ul>
<p>방금 전 <code>find_sum()</code> 함수는 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>), <code>first_num()</code> 함수는 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 500 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>)이였다. </p>
<p>실제로 인풋 데이터의 크기인 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>값이 올라가면서 얼마나 데이터를 탐색해야하는지 비교를 해보면 알고리즘의 효율성에 대해 좀 더 직관적으로 볼 수 있다. n의 수가 작을 때에는 큰 효과를 보지 못하지만, n의 크기가 점점 커지면서 효과는 기하급수적으로 커진다. n = 5인 경우와 n = 50인 경우를 비교해보자.</p>
<img src="/20210214-data-structures-and-algorithms/time_complexity.jpg" class="" title="time_complexity">


<p>Big O notation을 사용하여 알고리즘을 비교할 때 우리는 상수 값은 보통 포함시키지 않는다. O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이나 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.489ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 1100 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이나 둘 다 결국 linear하기 때문에, 다른 계수와 비교하는게 아닌 이상 큰 차이를 만들지 않기 때문이다.</p>
<p>Big O notation은 <strong>Space complexity</strong>를 표현하는데에도 사용 가능하다. Space complexity는 <strong>인풋 데이터의 크기가 커짐에 따라 바뀌는 데이터 저장 공간 크기의 관계</strong>를 뜻한다.</p>
<br>

<hr>
<h1 id="Logarithm-Binary-search-이진탐색"><a href="#Logarithm-Binary-search-이진탐색" class="headerlink" title="Logarithm (+ Binary search 이진탐색)"></a>Logarithm (+ Binary search 이진탐색)</h1><p>Log는 수학에서 Exponential의 반대되는 개념이라고 볼 수 있다. Log가 갑자기 왜 알고리즘 / 자료구조에 나오는지 이해하기 어려울 수 있다. 이해를 돕기 위한 예시를 한번 들어보자.</p>
<p>종이 사전에서 단어를 찾을 때를 생각해보자. 가장 간단한 방법으로는 사전의 첫장부터 모든 단어 하나하나 살펴가며 내가 찾는 단어인지 비교하는 방법이 있다. 이 방법은 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 걸릴 것이며, 종이사전이 10만개의 단어를 가진다면 10만번의 탐색을 해야할 것이다. 하지만 사람이라면 이렇게 하지 않는다. </p>
<img src="/20210214-data-structures-and-algorithms/linear_search.jpg" class="" title="linear_search">

<p>좀 더 좋은 방법은, 종이사전의 반씩 쪼개는 방법이다. 내가 찾는 단어가 종이사전의 첫 50%에 있는가, 아니면 뒤 50%에 있는가? 첫 50%에 있다면 뒤 50%는 버리고, 첫 50%에서 또 반을 갈라서 고른다. 이 과정을 단어를 찾을 때 까지 반복한다. 이 방법은 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="4.626ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2044.7 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1444.7, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 걸리며, 종이사전이 10만개의 단어를 가질 경우 약 17번 정도의 탐색 끝에 단어를 찾을 수 있다. 또, 이 경우 종이사전의 크기가 20만개로 늘어난다고 해도 탐색은 1번만 더 하면 된다.</p>
<img src="/20210214-data-structures-and-algorithms/bst.jpg" class="" title="bst">

<p>이 방법은 컴퓨터 과학에서는 <strong>Binary search</strong>라고 한다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params">arr, target</span>):</span></span><br><span class="line">    start, end = <span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> start &lt;= end:</span><br><span class="line">        mid = (start+end)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> target == arr[mid]:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> target &lt; arr[mid]:</span><br><span class="line">            end = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            start = mid + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Binary search는 효율적이지만, 데이터가 사전에 <strong>정렬</strong>되어있어야한다.</p>
<br>

<hr>
<h1 id="Sorting-정렬"><a href="#Sorting-정렬" class="headerlink" title="Sorting (정렬)"></a>Sorting (정렬)</h1><p>Sorting을 구현하는데에 가장 쉬운 방법은 <strong>selection sort</strong>이다. 이 방법은 모든 데이터를 나머지의 모든 데이터와 비교하며 정렬을 수행한다. 1. 모든 데이터를, 2. 모든 나머지 데이터들과 비교를 해야하니, O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="4.852ex" height="1.077ex" role="img" focusable="false" viewBox="0 -465 2144.4 476"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(1544.4, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 되며, 이는 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.27ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1003.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>)이 된다.</p>
<img src="/20210214-data-structures-and-algorithms/selection_sort.jpg" class="" title="selection_sort">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span>(<span class="params">arr</span>)</span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">i</span> <span class="title">in</span> <span class="title">range</span>(<span class="params"><span class="built_in">len</span>(<span class="params">arr</span>) -<span class="number">1</span></span>):</span></span><br><span class="line">        min_index = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(arr)-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> arr[j] &lt; arr[min_index]:</span><br><span class="line">                min_index = j</span><br><span class="line"></span><br><span class="line">        arr[i], arr[min_index] = arr[min_index], arr[i]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>좀 더 효율적인 방법에는 divide &amp; conquer 방식인 <strong>Merge sort</strong>가 있다 (코드는 <span class="exturl" data-url="aHR0cHM6Ly9yYXRzZ28uZ2l0aHViLmlvL2RhdGElMjBzdHJ1Y3R1cmUmYWxnb3JpdGhtLzIwMTcvMTAvMDMvbWVyZ2Vzb3J0Lw==">ratsgo님 블로그<i class="fa fa-external-link-alt"></i></span>를 참조했습니다). Merge sort는 array는 재귀적으로 반으로 계속 쪼갠 후, 쪼개진 값들을 constant time으로 비교하여 sort를 진행한다. 총 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="4.626ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2044.7 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(1444.7, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>개의 쪼갬 작업이 있고, 2개의 정렬된 리스트를 병합하는데에는 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)만큼의 계산이 필요하므로, 총 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="8.12ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 3589.1 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(1544.4, 0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(2822.4, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(2989.1, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)의 복잡도를 가진다.</p>
<p>부분적으로 sort하는게 아닌 이상 보통 merge sort가 많이 사용되며, O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="6.36ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2811.3 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(766.7, 0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(2044.7, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(2211.3, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)을 sorting 의 복잡도로 생각한다.</p>
<img src="/20210214-data-structures-and-algorithms/merge_sort.png" class="" title="merge sort">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span>(<span class="params"><span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span></span><br><span class="line">    mid = <span class="built_in">len</span>(<span class="built_in">list</span>) // <span class="number">2</span></span><br><span class="line">    leftList = <span class="built_in">list</span>[:mid]</span><br><span class="line">    rightList = <span class="built_in">list</span>[mid:]</span><br><span class="line">    leftList = merge_sort(leftList)</span><br><span class="line">    rightList = merge_sort(rightList)</span><br><span class="line">    <span class="keyword">return</span> merge(leftList, rightList)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span>(<span class="params">left, right</span>):</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(left) &gt; <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(right) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(left) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(right) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> left[<span class="number">0</span>] &lt;= right[<span class="number">0</span>]:</span><br><span class="line">                result.append(left[<span class="number">0</span>])</span><br><span class="line">                left = left[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result.append(right[<span class="number">0</span>])</span><br><span class="line">                right = right[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(left) &gt; <span class="number">0</span>:</span><br><span class="line">            result.append(left[<span class="number">0</span>])</span><br><span class="line">            left = left[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(right) &gt; <span class="number">0</span>:</span><br><span class="line">            result.append(right[<span class="number">0</span>])</span><br><span class="line">            right = right[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>유명한 sorting 방식중에는 Quick sort 도 있다. 평균 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="6.36ex" height="2.036ex" role="img" focusable="false" viewBox="0 -694 2811.3 900"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(766.7, 0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(2044.7, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mi" transform="translate(2211.3, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)의 복잡도를 가지지만, worst case (이미 sorting된 케이스)에서는 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.27ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 1003.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>)를 가진다.</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/XE4VP_8Y0BU" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<br>

<hr>
<h1 id="Arrays-Linked-List"><a href="#Arrays-Linked-List" class="headerlink" title="Arrays, Linked List"></a>Arrays, Linked List</h1><p><strong>Array</strong>는 가장 간단한 자료구조라고 볼 수 있다. 데이터들을 이어지게 하나로 쭉 늘어놓은 형태를 뜻하며, 생성 시 크기를 정한다. 10개의 크기를 가진 array에 11번째 데이터를 넣는 것은 불가능하다.</p>
<img src="/20210214-data-structures-and-algorithms/array.jpg" class="" title="array">

<p><strong>Linked list</strong>는 크기를 원하는대로 늘이고 줄일 수 있는 자료구조이다. 기본적으로 여러개의 node를 연결해둔 형태를 가지고 있고, 하나의 node에는 저장하고 싶은 데이터와 다음 node로의 메모리 주소 값을 저장한다. 쉽게 이해하려면, 하나의 하이퍼링크를 타고 갔더니 또 다른 하이퍼링크가 나오고, 그걸 타고 갔더니 또 하이퍼링크가 나오고, 또 타고 갔더니 또 나오는 형태로 보면 된다. 그러다가 나타나는 다음 node로의 메모리 주소가 NULL이라면 그것이 linked list의 마지막이라고 볼 수 있다.</p>
<img src="/20210214-data-structures-and-algorithms/linked_list.jpg" class="" title="linked list">

<p>Linked list에 새로운 값을 추가하려면 마지막 node에 다음 주소값만 적어주면 되기 때문에 constant time (i.e. O(1))이라고 볼 수 있다. 지우는 것도 하나의 주소값만 지워주면 되기 때문에 역시 O(1)이다. 이처럼 빠르게 보이지만, linked list는 특정 index의 값을 불러오기 위해서 모든 데이터를 거쳐야하기 때문에 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 소요된다. </p>
<p>이렇게 어떤 특정 index 값을 불러오는 것은 array가 O(1)로 더 빠르다. 하지만 array의 경우, 대부분의 프로그래밍 언어는 확장성을 추가한 <code>std::vector</code> 등의 형태로 구현하는데 (확장성이 없는 <code>std::array</code>도 있음), 이는 생성시 정해둔 크기를 초과하여 데이터를 추가할 때 기존의 크기의 2배가 되는 메모리를 새로 할당하고 기존의 데이터를 모두 복사해야한다. 이 경우 복사하는데에 시간이 많이 소요되지만, 언제 나타날 지 모르기 때문에 우리는 **평균 O(1)**을 가진다고 한다.</p>
<br>

<hr>
<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><p><strong>Tree</strong>는 하나의 node가 다음 n개의 node로의 메모리 주소를 가지고 있다. 이는 linked list가 1개의 다음 node로의 메모리 주소를 가지는 점과 다르다고 볼 수 있다. 어떠한 tree node가 다음 2개의 node로의 메모리 주소를 가진다면 binary tree가 된다. 다음 node는 또 그 다음 2개의 메모리 주소를 가지는데, 이는 tree가 재귀적인 특성을 가지기 때문에 subtree라고 부르기도 한다. 다음 node로의 주소가 없는 경우는 leaf node라고 부른다.</p>
<p><strong>Binary search tree</strong>는 자주 사용되는 Tree 구조로써, 하나의 node가 2개의 child node를 가지며, 좌측 child node는 더 작은숫자, 우측 child node는 더 크거나 같은 숫자를 가진다. Binary search tree에서 어떠한 값의 데이터를 찾을 때는 내가 현재 위차한 node보다 크거나 작은지만 보면 된다. 모든 branch의 높이가 같은 것은 아니며, 최악의 경우 branch의 높이만큼 이동해야한다. 그렇기 때문에 binary search tree는 1. 탐색, 2. 데이터 추가, 3. 데이터 삭제에 모두 O(H)만큼 소요되고, Height는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="4.208ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 1860 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1260, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>과 비례하기 때문에 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="4.208ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 1860 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1260, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 된다. </p>
<img src="/20210214-data-structures-and-algorithms/bst2.jpg" class="" title="binary search tree">

<p>종종 sorting이 완료된 값을 binary search tree로 넣을 때 (e.g. [1,2,3,4]) binary search tree의 법칙을 따랐음에도 linked list처럼 구현될 수 있다 (2는 1보다 크기 때문에 우측에, 3은 2와 1보다 크기 때문에 2의 우측에, 4는 3과 2와 1보다 크기 때문에 3의 우측에). 이렇게 되면 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 되어버려 비효율적이게 되니, 자동으로 좌/우를 밸런싱해서 tree height가 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="4.208ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 1860 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1260, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)으로 수렴하게 해주는 red-black tree와 같은 기법을 사용하면 좋다. </p>
<img src="/20210214-data-structures-and-algorithms/balanced_tree.jpg" class="" title="balanced tree">

<p><strong>Heap, 또는 Priority Queue</strong>는 parent node가 child node보다 더 중요하거나 동일한 우선도를 가지는 경우를 뜻한다. Heap의 root node는 항상 제일 높은 우선도를 가지는 값이다. Min heap의 경우 최소값이 root node로 가고, max heap의 경우 최대값이 root node로 간다. Root node의 값을 얻기위해서는 값을 1개만 보면 되기 때문에 O(1)이 소요된다. Heap에 새로운 값을 추가하거나 뺄 때는 tree의 가장 아랫단 빈 곳에 임의로 추가하고, tree를 올라가면서 priority를 비교하며 위치를 교환하는 방식이다. 이 방식은 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="4.208ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 1860 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1260, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)이 소요된다. 이 방식의 단점은 root node를 제외한 나머지 node들이 sorting이 되지 않아서 나머지 정보를 사용하기 어렵다는 점이며, 이 때문에 탐색을 할 경우 unsorted array의 탐색과 똑같은 O(<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>)만큼의 시간이 걸린다. 그렇기에 보통 root node만 중요할 때 이 자료구조를 사용하는 편이 많다.</p>
<br>

<hr>
<h1 id="Depth-first-search-vs-Breadth-first-search-깊이탐색-vs-너비탐색"><a href="#Depth-first-search-vs-Breadth-first-search-깊이탐색-vs-너비탐색" class="headerlink" title="Depth-first search vs Breadth-first search (깊이탐색 vs 너비탐색)"></a>Depth-first search vs Breadth-first search (깊이탐색 vs 너비탐색)</h1><p>Tree를 탐색하는데에는 두가지 방법이 있다. <strong>Depth-first search</strong>와 <strong>Breadth-first search</strong>이다. Tree를 찾는데에 처음부터 최대한 깊게 들어가서 최하레벨 element부터 찾는게 depth-first, 아니면 최대한 상위레벨에서 하나씩 보면서 천천히 들어가는게 breadth-first라고 볼 수 있다.</p>
<img src="/20210214-data-structures-and-algorithms/depth_breadth.jpg" class="" title="depth vs breadth">

<p>Depth-first는 <strong>stack</strong>이라는 list 자료구조 형태로 구현된다. Stack은 2가지 작업을 할 수 있는데, list의 끝단에 새 값을 추가 (i.e. push)하거나 값을 지우는 (i.e. pop) 작업을 할 수 있다. 이러한 방식을 <strong>LIFO</strong> - Last in, first out이라고 한다.</p>
<p>Breadth-first는 <strong>queue</strong>라는 자료구조 형태로 구현된다. Queue는 list 끝단에 새 값을 추가 (i.e. enqueue)와 list 최앞단 값을 지우는 (i.e. dequeue) 작업을 할 수 있다. 이러한 방식을 <strong>FIFO</strong> - first in, first out라고 한다.</p>
<br>

<hr>
<h1 id="Graphs-그래프"><a href="#Graphs-그래프" class="headerlink" title="Graphs (그래프)"></a>Graphs (그래프)</h1><p>Graph는 데이터 값들을 node처럼 다루고, 이들을 edge로 연결해둔 형태를 뜻한다. 모든 tree는 graph지만, 모든 graph가 tree는 아니다. A-&gt;B 형태로 ‘A에서 B’를 구현할 수도 있고, A-B 형태로 ‘A에서 B 또는 B에서 A’를 구현할 수도 있다. Edge를 구현하는 방법 중에는 Adjacency list와 Adjacency matrix 가 유명하다. </p>
<p>Graph는 node간의 거리를 표현하여 <strong>최단거리 계산</strong>을 하는데에 효과적이다. </p>
<p>예를 들어, 내가 인맥을 통해 JYP 박진영을 알기위해 인맥의 몇단계를 건너야하는지를 생각해보자. 내가 우리 팀장님을 통해 업계 사람 4명을 거쳐 JYP를 알게되는 루트가 있다고 해보자. 그리고 내가 JYP 팬클럽에 가입한 친구를 통해 2명만 거쳐서 JYP를 알게되는 루트가 있다고 보자. 이 경우 최단거리로 내가 JYP와 이어진 거리는 나-친구-친구지인-JYP 이기 때문에 3이다. </p>
<img src="/20210214-data-structures-and-algorithms/jyp.jpg" class="" title="jyp">

<p>종종 graph의 edge에는 weight가 있을 수 있다. 예를 들어서, 내가 출근을 하려고 하는데 버스를 탈 경우 한번에 20분이 걸려서 도착하고, 지하철을 탈 경우 한번 갈아타서 8분 + 8분이 걸린다고 해보자. 이 경우에는 Dijkstra 알고리즘 등으로 weight를 고려해서 풀 수 있다. </p>
<img src="/20210214-data-structures-and-algorithms/dijkstra.jpg" class="" title="dijkstra">

<br>

<hr>
<h1 id="Hash-map-해쉬맵"><a href="#Hash-map-해쉬맵" class="headerlink" title="Hash map (해쉬맵)"></a>Hash map (해쉬맵)</h1><p><strong>Hash map</strong>은 기존의 array구조에 key-value 구조를 추가함으로써, 데이터를 추가/삭제/탐색하는데에 평균 constant time O(1)로 데이터를 탐색할 수 있는 방법이다. Hash map은 hash function을 통해 key값에 대한 <em>유일한</em> hash code 값을 만들어낸다. 이 hash code 값을 index로 사용해서 value 값들을 저장해둘 수 있다. 당연하게도, 동일한 key 값에 대해 hash code는 동일하게 나와야한다.</p>
<img src="/20210214-data-structures-and-algorithms/hash_map.png" class="" title="hash map">

<p>위에 <em>유일한</em> hash code라고 적으며 강조를 했는데, 이는 종종 다른 key 값들로 hash code를 만들었는데, 동일한 hash code가 만들어지는 경우이다. 이 경우 동일한 hash code에 여러 value 값을 저장하는데, 이 때 linked list 형태로 저장해야하기 때문에 (linked list는 O(n)을 가지고 있으므로), O(1)으로 값을 받지 못하게 된다. 이 현상을 hash collision이라고 한다. 이 때문에 O(1)을 유지하고 싶으면 hash function을 잘 설계해야하며, 어쩔 수 없이 hash collision이 나타나게되면 O(1)이 유지가 되지 않으므로 ‘평균 O(1)’이라고 하는 것이다. 파이썬의 Dictionary, C++의 std::unordered_map이 hash map을 사용한다.</p>
<hr>
<p>출처: <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9vejljRXFGeW5IVQ==">Tren Black 영상 - Data Structures and Algorithms in 15 Minutes<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Data structures</tag>
        <tag>Programming</tag>
        <tag>Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>SW 기술면접 준비</title>
    <url>/20210216-SW-tech-interview-prep/</url>
    <content><![CDATA[<h1 id="Process-vs-Thread"><a href="#Process-vs-Thread" class="headerlink" title="Process vs Thread"></a>Process vs Thread</h1><ul>
<li>Program: 작업을 수행하기 위해 실행가능한 파일</li>
<li>Process: 실행되고 있는 Program<ul>
<li>Code | Heap | Register, counter, stack을 가지고 있음.</li>
</ul>
</li>
<li>Thread: Process 내부의 실행 단위<ul>
<li>하나의 process 안에 여러개의 thread가 있을 수 있고, 모든 thread는 code, data, heap을 공유.</li>
<li>Thread마다 register, counter, stack을 따로 가지고 있음.</li>
</ul>
</li>
</ul>
<h2 id="여러개의-작업을-할-때-여러개의-process를-쓰는게-좋은가-아니면-여러개의-thread를-쓰는게-좋은가"><a href="#여러개의-작업을-할-때-여러개의-process를-쓰는게-좋은가-아니면-여러개의-thread를-쓰는게-좋은가" class="headerlink" title="여러개의 작업을 할 때 여러개의 process를 쓰는게 좋은가? 아니면 여러개의 thread를 쓰는게 좋은가?"></a>여러개의 작업을 할 때 여러개의 process를 쓰는게 좋은가? 아니면 여러개의 thread를 쓰는게 좋은가?</h2><ul>
<li>Process를 새로 생성하거나 제거하는 것이 Thread를 새로 생성하거나 제거하는 것 보다 오래 걸림.</li>
<li>Process간의 context switching이 Thread간의 context switching보다 오래 걸림.<ul>
<li>Context switching: 현재 작업이 사용하는 CPU의 자원을 process control block에 저장하고, 기존의 하던 작업을 로드함. 이로써 작업 변환이 가능.</li>
</ul>
</li>
</ul>
<h1 id="Multi-processing-vs-Multi-threading"><a href="#Multi-processing-vs-Multi-threading" class="headerlink" title="Multi-processing vs Multi-threading"></a>Multi-processing vs Multi-threading</h1><ul>
<li>Multi-processing : 하나의 작업을 위해 여러개의 process를 한번에 돌리는 것.<ul>
<li>여러개의 CPU가 필요함.</li>
<li>Socket 통신과 같은 Process들간의 데이터 통신이 가능.</li>
<li>Process 중 하나가 죽어도 나머지는 계속 돌아감. (안정적?)</li>
<li>Context switching으로 여러개의 process가 동시에 돌아가는 것 처럼 보이게함.</li>
</ul>
</li>
<li>Multi-threading : 하나의 작업을 위해 여러개의 thread를 한번에 돌리는 것.<ul>
<li>하나의 process에 대해 여러 thread가 동시에 수행.</li>
<li>Context switching으로 여러개의 thread가 동시에 돌아가는 것 처럼 보이게함.</li>
</ul>
</li>
</ul>
<h1 id="Concurrency-vs-Parallelism"><a href="#Concurrency-vs-Parallelism" class="headerlink" title="Concurrency vs Parallelism"></a>Concurrency vs Parallelism</h1><ul>
<li>Concurrency: 다수의 작업을 동시에 진행하는 것. 하지만 정말로 어떠한 시점에서 여러개의 작업이 동시에 처리되고 있지 않아도 된다.<ul>
<li>하나의 작업을 진행하다가 context switching을 해서 다른 작업을 하다가, 다시 돌아와서 또 하고…</li>
</ul>
</li>
<li>Paralellism: 다수의 작업을 <em>동시에</em> 진행하는 것. 어떠한 시점에서 여러개의 작업이 동시에 처리되고 있어야함.<ul>
<li>1번 작업과 2번 작업이 각각 CPU 하나씩 할당해서 동시에 처리되는 느낌.</li>
</ul>
</li>
</ul>
<h1 id="Semaphore-vs-Mutex"><a href="#Semaphore-vs-Mutex" class="headerlink" title="Semaphore vs Mutex"></a>Semaphore vs Mutex</h1><ul>
<li>Semaphore : 공유된 자원의 데이터를 여러 process가 접근하는 것을 막는 것<ul>
<li>wait / signal 이 있는데, process가 resource를 점유하고 있는지 아닌지를 알려준다. <ul>
<li>Couting semaphore - n/0 시그널. 0이면 resource가 점유중, n이면 n개만큼의 process가 추가로 점유 가능.</li>
<li>Binary semaphore - 1/0 시그널. 1이면 resource가 오픈, 0이면 resource가 점유중. </li>
</ul>
</li>
</ul>
</li>
<li>Mutex : 공유된 자원의 데이터를 여러 thread가 접근하는 것을 막는 것<ul>
<li>Process가 resource를 점유하면 mutex object의 lock 권한을 가지게 된다. </li>
</ul>
</li>
</ul>
<h1 id="Overriding-vs-Overloading"><a href="#Overriding-vs-Overloading" class="headerlink" title="Overriding vs Overloading"></a>Overriding vs Overloading</h1><ul>
<li>Overriding: 부모 클래스가 가지고 있는 메소드를 자식 클래스가 재정의해서 사용하는 것.<ul>
<li>메소드의 이름/매개변수/반환형이 같은 상속 받은 메소드를 덮어쓰는 것 (부모 클래스의 메소드는 무시하고 자식 클래스가 필요로하는 메소드를 추가하여 사용하는 것)</li>
</ul>
</li>
<li>Overloading: 같은 이름의 메소드를 여러개 정의하되, 매개 변수의 유형과 개수를 달리하여 다양한 유형의 호출에 응답하는 것.</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Programming</tag>
        <tag>Concurrency</tag>
        <tag>Parallelism</tag>
        <tag>Semaphore</tag>
        <tag>Mutex</tag>
        <tag>Thread</tag>
        <tag>Process</tag>
      </tags>
  </entry>
  <entry>
    <title>50가지 CV/SLAM 기술면접 질문 리스트</title>
    <url>/20210221-50-CV-SLAM-interview-questions/</url>
    <content><![CDATA[<ol>
<li><a href="https://changh95.github.io/20210222-image-projection/">Image projection 과정에 대해서 설명해주세요. Intrisic matrix와 Extrinsic matrix도 설명해주세요.</a></li>
<li><a href="https://changh95.github.io/20210222-mono-stereo-rgbd/">Monocular / Stereo / RGB-D SLAM의 특징을 설명해주세요. 각각 방식의 장점과 단점은 무엇인가요?</a></li>
<li><a href="https://changh95.github.io/20210228-AD-sensors/">SLAM에서 쓸 수 있는 센서에는 어떤게 있을까요? 알고있는 센서들 각각의 작동방법과 특징과 장단점을 이야기하세요. 이들을 함께 쓰려면 어떻게 해야할까요?</a></li>
<li><a href="https://changh95.github.io/20210313-ransac/">RANSAC에 대해 설명해주세요. RANSAC의 장점과 단점도 설명해주세요. 다른 Outlier removal 기법 중 아는 방식이 있다면 설명해주세요</a></li>
<li><a href="https://changh95.github.io/20210313-ba/">Bundle adjustment에 대해서 설명해주세요</a></li>
<li><a href="https://changh95.github.io/20210314-nonlinear-optimisation/">Gradient descent 방식, Newton-Raphson 방식, Gauss-Newton 방식, Levenberg-Marquardt 방식 최적화 기법은 어떻게 다른가요?</a></li>
<li><a href="https://changh95.github.io/20210816-tightly-loosely-coupled">Tightly-coupled 방식과 Loosely-coupled 방식의 차이를 설명해주세요</a>.</li>
<li><a href="https://changh95.github.io/20210817-vslam-vs-vio/">Visual SLAM과 Visual-Inertial Odometry는 무슨 차이가 있나요?</a></li>
<li>Loop closure가 무엇인가요? 어떻게 하는건가요?</li>
<li>Kalman filter와 Particle filter에 대해서 설명해주세요. Kalman filter와 Extended Kalman filter의 차이도 함께 설명해주세요.</li>
<li>Fundamental matrix 와 Essential matrix가 무엇인가요? Epipolar constraint는 무엇인가요?</li>
<li>실내 SLAM과 실외 SLAM은 어떤점들이 다를까요?</li>
<li>Rotation matrix, Quaternion, Axis-angle, Euler angle의 차이점을 설명해주세요</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.077ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2686 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645, 0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(1408, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1797, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(2297, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.079ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2687 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(1409, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1798, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(2298, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>는 어떻게 다르나요?</li>
<li>Lie Group과 Lie Algebra는 왜 쓰이나요?</li>
<li>PTAM, ORB-SLAM, SVO의 차이점을 이야기해주세요.</li>
<li>Fundamental matrix와 Essential matrix의 degree of freedom은 각각 몇개인가요? 5/7/8 point 알고리즘을 설명해주세요.</li>
<li>카메라와 IMU 캘리브레이션은 어떻게 하나요?</li>
<li>Keypoint detector와 Keypoint descriptor의 다른 점을 설명하세요. Keypoint tracking은 어떻게 수행하나요?</li>
<li>Feature의 invariance는 어떤 것을 의미하나요?</li>
<li>Homography matrix가 무엇인가요?</li>
<li>Perspective-n-Point 문제가 무엇인가요?</li>
<li>Inverse depth parameterization이 무엇인가요?</li>
<li>Feature-based method와 Direct method의 다른 점을 설명하고, 두 방식의 장단점을 설명하세요. Direct method와 Optical flow가 어떻게 다른지도 설명하세요.</li>
<li>Marginalization이 무엇인가요?</li>
<li>Floating-point descriptor와 Binary descriptor는 무슨 차이가 있나요?</li>
<li>IMU Pre-integration이 무엇인가요?</li>
<li>SLAM을 하는 도중 움직이는 물체가 있을 때 어떻게 해야할까요?</li>
<li>Graph optimization이 무엇이고, 왜 쓰나요?</li>
<li>Information matrix란 무엇인가요?</li>
<li>Sparse mapping과 Dense mapping의 차이와 장단점을 설명하세요.</li>
<li>Extended Kalman filter와 Bundle adjustment의 차이를 설명하세요.</li>
<li>Line / Edge 추출 방식들을 설명하고 각각의 장단점을 설명하세요.</li>
<li>Structure-from-Motion과 Visual odometry는 어떻게 다른가요?</li>
<li>SLAM에서 Keyframe이 무엇일까요?</li>
<li>Scale drift는 왜 나타나는걸까요?</li>
<li>여러개의 map을 하나로 합치려면 어떻게 해야할까요?</li>
<li>Relocalization이란 무엇인가요?</li>
<li>BoW와 VLAD에 대해서 설명해주세요.</li>
<li>ICP (Iterative closest point)의 작동 방법을 설명해주세요.</li>
<li>Patch similarity를 구하는 방법 중 SSD, SAD, NCC를 비교해주세요.</li>
<li>Feature descriptor distance는 어떤 방식으로 구해지나요?</li>
<li>이미지에서 노이즈를 줄이는 방식에 대해 소개해주세요.</li>
<li>반복되는 패턴을 바라보고 있는 2장의 이미지가 있습니다. 이 때 두 이미지 사이의 정확한 변환관계를 어떻게 구하나요?</li>
<li>FLANN, LSH, Multi-probe LSH, kd-tree에 대해서 설명해주세요.</li>
<li>정확한 Feature matching을 만드는 전략에는 어떤 방법들이 있을까요?</li>
<li>Image pyramid에 대해서 설명해주세요.</li>
<li>MLE와 MAP는 어떤게 다른가요?</li>
<li>DLT (Direct Linear Transform)에 대해서 설명해주세요.</li>
<li>Visual-LiDAR fusion 방식에 대해서 아는대로 얘기해주세요.</li>
</ol>
<p>하나씩 풀어서 적어보도록 하자 <span class="github-emoji" alias="smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">😄</span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Interview</tag>
        <tag>Questions</tag>
      </tags>
  </entry>
  <entry>
    <title>Image projection 이란? Intrinsic / Extrinsic / Projection matrix란?</title>
    <url>/20210222-image-projection/</url>
    <content><![CDATA[<h1 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문:"></a>예상 면접 질문:</h1><ol>
<li>Image projection 과정에 대해서 설명해주세요</li>
<li>카메라는 어떻게 작동하나요?</li>
<li>3D 공간이 2D 이미지로 투영되는 방법에 대해 설명해주세요</li>
<li>Intrinsic / Extrinsic / Projection matrix에 대해 설명해주세요.</li>
</ol>
<h1 id="답"><a href="#답" class="headerlink" title="답"></a>답</h1><img src="/20210222-image-projection/projection1.png" class="" title="projection">

<img src="/20210222-image-projection/projection2.png" class="" title="projection2">

<p><strong>3D 공간의 정보를 2D 이미지로 투영하는 과정</strong>을 image projection이라고 합니다.</p>
<p>3D 공간 위의 어떠한 점를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>로 표현하고, 2D 이미지 위의 픽셀 위치를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.032ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 3550.1 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(850, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1294.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(1779.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2224.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(2724.3, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 로 표현하겠습니다. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="7.302ex" height="1.984ex" role="img" focusable="false" viewBox="0 -683 3227.3 877"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(852, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1296.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2059.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2504.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g></g></g></svg></mjx-container>는 World frame으로부터의 3D coordinate이고, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="3.397ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 1501.7 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(572, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1016.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container>는 Camera frame으로부터의 pixel coordinate를 의미합니다. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.032ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 3550.1 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(850, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1294.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(1779.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2224.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(2724.3, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 모두 밑에 1이 붙는데, 이 1은 homogeneous coordinate 표기법으로 위치를 표현하기 위함입니다. Homogeneous coordinate로 표현함으로써 3차원-2차원 변환 관계를 scale 차원 정보를 포함해서 계산하는 projective geometry를 통해 계산할 수 있게 되는데, 이 scale 차원 정보가 1일 때는 우리가 익숙한 euclidean space에서 계산할 수 있습니다.</p>
<p>카메라가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>의 3D 포인트를 바라보고 있을 때 카메라의 optical centre와 3D 포인트 사이의 직선을 그릴 수 있습니다. 이 직선은 image plane을 통과하며, 이는 3D 포인트가 2D 이미지에 투영될 수 있다는 것입니다. 투영을 수행하기 위해서는 우선 3D 포인트를 world coordinate frame 기준이 아닌 camera coordinate frame 기준에서 표현해야 하고, 이를 위해 world-&gt;camrea frame 변환이 필요합니다. 이를 위해 Camera coordinate frame과 World coordinate frame의 회전과 이동 (rotation / translation)에 대한 변환 관계를 계산해야하고, 이 변환 관계는 Euclidean space에서의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.079ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2687 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(1409, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1798, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(2298, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>로 표현하면 3x4 매트릭스 형태를 가집니다. (보통의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.079ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2687 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(1409, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1798, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(2298, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 매트릭스는 4x4 형태이지만, 수식을 간단하게 표현하기 위해 euclidean space로 만들어 아랫단의 [0, 0, 0, 1]을 제거하여 3x4 매트릭스로 표현하였습니다. [0, 0, 0, 1]을 붙혀 4x4로 계산해도 같은 결과가 나옵니다.) 이 3x4 매트릭스를 Extrinsic matrix라고 부르며, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 에 이 매트릭스를 곱하면, 카메라의 coordinate frame에서부터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 의 상대적인 R|t를 알 수 있습니다. </p>
<p>이후, <strong>Intrinsic matrix</strong>를 이용해서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.936ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5275.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1130, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1574.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(2337.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2782.3, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(3505.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3950, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4450, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>를 2차원 평면에 투영시킬 수 있습니다. Intrinsic matrix는 <strong>Focal Length</strong> (fx, fy)와 <strong>Principal point</strong> (cx, cy) 정보를 가지고 있습니다. Focal length는 normalized scale에서 pixel scale로 변환을 할 수 있게 해줍니다. cx와 cy는 보통 중앙 픽셀의 위치 값을 가지는데, 이는 coordinate frame을 optical centre (또는 이미지 중앙)이 아닌, 좌측 상단에 올려놓기 위함입니다. 하지만 카메라가 실제로 만들어지면서 완벽하게 중앙에 위치하지 않기 때문에, 몇픽셀의 차이는 있을 수 있습니다. Intrinsic matrix의 3번 row는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.664ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2945.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mn" transform="translate(278, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(778, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(1222.7, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(1722.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2167.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2667.3, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>인데, 이는 homogeneous coordinate를 맞추는 것이 아니고, 3차원→2차원 축소를 위함입니다.</p>
<p>이 계산이 끝나면 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.084ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 4457.1 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(747, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1319, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1763.7, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(2232.7, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(2717.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3162.3, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="msup" transform="translate(3631.3, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>이 결과 값으로 나오는데, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>는 homogeneous coordinates의 영향으로 나온 scale 값입니다. 우리는 euclidean space에서의 값을 원하기 때문에 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.99ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4415.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(691.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mo" transform="translate(1413.4, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(1691.4, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2263.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2708.1, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(3193.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3637.8, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(4137.8, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>로 normalize를 시켜주면 pixel coordinate 값, 즉 2차원의 좌표값이 나오게 됩니다.</p>
<p>전체적으로, 3차원 공간에 존재하는 하나의 점을 2차원 이미지의 픽셀 위치로 투영하였고, 이 과정을 image projection이라고 합니다. <strong>Projection matrix</strong>는 intrinsic matrix와 extrinsic matrix를 곱하여 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="30.019ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 13268.4 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(691.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mo" transform="translate(1413.4, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(1691.4, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2263.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2708.1, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(3193.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3637.8, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(4137.8, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(5241.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(6297.1, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(7270.4, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mo" transform="translate(7992.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(8270.6, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(9122.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(9567.2, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(10330.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(10774.9, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(11497.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(11942.6, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="msup" transform="translate(12442.6, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 형태로 바꾸었을 때의 매트릭스를 뜻합니다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Image projection</tag>
        <tag>Pinhole camera</tag>
      </tags>
  </entry>
  <entry>
    <title>Monocular, Stereo, RGB-D SLAM의 비교</title>
    <url>/20210222-mono-stereo-rgbd/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문:"></a>예상 면접 질문:</h2><ol>
<li>Monocular, Stereo, RGB-D SLAM의 차이점이 무엇인가요? (또는 각각의 특징, 장단점을 알려주세요)</li>
<li>Stereo나 RGB-D가 Monocular 방식에 대해 가지는 장점이 무엇일까요?</li>
</ol>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><h3 id="Monocular-SLAM"><a href="#Monocular-SLAM" class="headerlink" title="Monocular SLAM"></a>Monocular SLAM</h3><ul>
<li>카메라 1대를 사용하여 SLAM을 하는 것</li>
<li>최소 2개의 카메라를 사용하는 stereo 시스템이나, projector-camera 하드웨어를 사용하는 RGB-D 카메라보다 저렴함 (+ 전력 소비량도 작음).</li>
<li>Monocular camera 이미지 1장으로는 3D 공간을 up-to-scale로 추정할 수 있음 (i.e. 정확한 lambda 값을 모름).</li>
<li>Image projection, 또는 N-view geometry로도 up-to-scale로만 추정 가능함.<ul>
<li>정확한 scale 값을 모르기 때문에 <strong>Scale ambiguity</strong>가 있다고 할 수 있음.<ul>
<li>이미 알고있는 물체의 크기를 (i.e. Fiducial 마커) 대조하거나, 아니면 IMU 센서를 통해 취득한 모션 등으로 metric scale을 복원할 수 있음.</li>
<li>최근 딥러닝 기반 monocular depth estimation, 또는 single image monocular depth estimation 기술을 사용해 depth 정보를 추출하기도 함. </li>
</ul>
</li>
</ul>
</li>
<li>360 카메라나 어안렌즈와 같은 특수한 렌즈 구성을 사용하지 않는 이상 field of view가 작음.</li>
<li>Monocular SLAM의 연구 방향은 대체적으로 ‘<strong>하나의 카메라만으로 어느정도까지 정확하게 SLAM을 할 수 있는가</strong>‘에 초점을 둠.</li>
</ul>
<p> </p>
<hr>
<h3 id="Stereo-SLAM"><a href="#Stereo-SLAM" class="headerlink" title="Stereo SLAM"></a>Stereo SLAM</h3><ul>
<li><strong>Baseline</strong> 거리만큼 떨어진 카메라 2대를 사용하여 SLAM을 하는 것<ul>
<li>이 때, 카메라는 동시에 이미지를 취득해야함. (i.e. <strong>synchronised cameras</strong>)</li>
<li>Stereo를 계속 추가해서 붙여나가 multi-camera system도 만들 수 있음.</li>
</ul>
</li>
<li>Stereo 시스템은 한 프레임에 취득한 두 이미지간의 <strong>disparity</strong> 정보를 이용해서 픽셀마다 depth를 얻어낼 수 있음.<ul>
<li>이 Depth를 unproject함으로써 한 프레임의 데이터만으로도 metric scale을 가진 3D structure를 얻어낼 수 있음.<ul>
<li>Baseline의 길이가 길수록 더 먼 거리의 3D structure를 정확하게 잴 수 있음.</li>
<li>프레임간 모션 정보를 Monocular 방식이 사용하는 방법들을 (e.g. PnP) 사용해서 얻을 수도 있고, LiDAR나 RGB-D에서 사용하는 structure를 이용한 방법을 (e.g. ICP) 통해 얻을 수도 있음.</li>
</ul>
</li>
<li>모든 픽셀마다 disparity를 계산하고 depth를 계산하는데에 <strong>많은 계산량</strong>이 필요하며, 종종 실시간으로 계산하기 위해 GPU나 FGPA 등 가속장비를 사용함.</li>
</ul>
</li>
<li>카메라 간의 baseline 정보와 각각의 카메라의 렌즈 특성에 대한 <strong>calibration</strong>하는 작업이 대체적으로 굉장히 귀찮음.</li>
<li>Stereo SLAM이나 Multi-camera SLAM의 연구 방향은 대체적으로 ‘<strong>가장 정확한 SLAM을 만드는 것</strong>‘에 초점을 둠.</li>
</ul>
<p> </p>
<hr>
<h3 id="RGB-D-SLAM"><a href="#RGB-D-SLAM" class="headerlink" title="RGB-D SLAM"></a>RGB-D SLAM</h3><ul>
<li><strong>Structured light</strong> (i.e. 구조광)이나 <strong>Time-of-Flight</strong> (ToF) 센서를 이용한 카메라를 사용함.<ul>
<li>Strutured light는 임의의 패턴을 방출하여, 이미지 센서로 해당 패턴을 읽어서 Depth를 추출함.</li>
<li>ToF 센서는 빛을 방출하고 이미지 센서로 돌아오는 시간을 측정하여 거리를 추출함.</li>
</ul>
</li>
<li>센서가 depth 값을 얻어주기 때문에 직접적으로 depth 정보를 계산하지 않아도 됨<ul>
<li>쉽게 실시간 depth 데이터를 얻을 수 있음.</li>
</ul>
</li>
<li>취득한 Depth 데이터를 unproject함으로써 한 프레임의 데이터만으로도 실제 metric scale의 3D structure를 얻어낼 수 있음.</li>
<li>단점도 꽤 많음.<ul>
<li>길어도 ~20m 정도밖에 Depth 정보를 얻지 못함. 이 데이터도 노이즈가 상당함</li>
<li>Field of view가 작음</li>
<li>실외에서 사용하기 어려움 (적외선 파장이 햇빛에 포함된 적외선 파장과 겹침)</li>
</ul>
</li>
<li>RGB-D SLAM의 연구 방향은 대체적으로 ‘<strong>Dense SLAM/Mapping</strong>‘ 또는 ‘<strong>Depth 정보를 사용해서 어떤 새로운 기능을 만들수있을까</strong>‘에 초점을 둠.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Monocular SLAM</tag>
        <tag>Stereo SLAM</tag>
        <tag>RGB-D SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM에서 사용할 수 있는 센서의 종류</title>
    <url>/20210228-AD-sensors/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h1 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문:"></a>예상 면접 질문:</h1><ol>
<li>자율주행 (또는 드론, 또는 VR/AR)에서 쓸 수 있는 센서에는 어떤게 있을까요?</li>
<li>다양한 센서의 값들을 어떻게 섞을 수 있나요?</li>
<li>Camera와 LiDAR의 차이점을 설명해주세요.</li>
</ol>
<br>

<hr>
<h1 id="답"><a href="#답" class="headerlink" title="답"></a>답</h1><h2 id="센서의-종류"><a href="#센서의-종류" class="headerlink" title="센서의 종류"></a>센서의 종류</h2><ul>
<li>Preprioceptive sensors: 주변 환경의 상태와 관련없이 로봇 자기 자신의 state 값을 읽을 수 있는 센서<ul>
<li>IMU, Wheel encoder</li>
</ul>
</li>
<li>Exteroceptive sensors: 주변 환경에 대한 상태를 읽는 센서<ul>
<li>GNSS, Camera, LiDAR, RADAR, Ultrasonic</li>
</ul>
</li>
</ul>
<br>

<hr>
<h2 id="Preprioceptive-sensor-설명"><a href="#Preprioceptive-sensor-설명" class="headerlink" title="Preprioceptive sensor 설명"></a>Preprioceptive sensor 설명</h2><h3 id="IMU-Inertial-measurement-units"><a href="#IMU-Inertial-measurement-units" class="headerlink" title="IMU - Inertial measurement units"></a>IMU - Inertial measurement units</h3><ul>
<li>Linear accelerator (선형가속도 측정) 와 Angular gyroscope (각속도 측정)를 포함하는 센서</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTWFzcy1zcHJpbmctZGFtcGVyX21vZGVs">Spring-damper system<i class="fa fa-external-link-alt"></i></span>의 원리를 이용하여 만든 시스템.<ul>
<li>소형화를 위해 실제로 spring-damper 를 사용하는건 아님<ul>
<li>Optical system - 차량용 IMU</li>
<li>MEMS - 칩기반 IMU (스마트폰 탑재)</li>
</ul>
</li>
</ul>
</li>
<li>장점:<ul>
<li>저렴한 consumer grade 제품</li>
<li>높은 sensitivity</li>
<li>높은 FPS (100Hz, 200Hz, 400Hz, 800Hz 제품 등 다양하게 있음)</li>
</ul>
</li>
<li>단점:<ul>
<li>시간이 지남에 따라 bias에 의한 drift가 생김<ul>
<li>이러한 drift를 해소하기 위해 camera나 LiDAR를 함께 사용해서 drift 보정을 수행함.</li>
</ul>
</li>
<li>높은 정확도 + 낮은 drift를 가진 tactical grade IMU는 엄청 비쌈<ul>
<li>이런 IMU는 1시간에 1도 미만의 drift를 가짐.</li>
<li>하지만 가격이 억대임 ㄷㄷ </li>
</ul>
</li>
<li>낮은 가격의 consumer grade는 에러가 높고 drift도 높음.</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/imu.png" class="" title="imu">

<br>

<hr>
<h3 id="Wheel-encoder"><a href="#Wheel-encoder" class="headerlink" title="Wheel encoder"></a>Wheel encoder</h3><ul>
<li>바퀴의 회전량을 (RPM)과 이동량을 (바퀴의 회전량 * 바퀴의 둘레) 측정하는 센서<ul>
<li>brush, optical, magentic, inductive, capacitative… 만드는 방법은 여러가지임</li>
</ul>
</li>
<li>장점:<ul>
<li>센서가 쌈</li>
</ul>
</li>
<li>단점:<ul>
<li>Drift에 약함</li>
<li>바퀴가 헛도는 경우 잘못된 센서 값이 생길 수 있음</li>
<li>(바퀴의 회전량 x 바퀴의 둘레) 를 계산할때… 바퀴의 둘레가 사실 주행 중 자주 바뀜<ul>
<li>마찰열로 인해 타이어 팽창</li>
<li>바람빠짐 등등</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/wheel.png" class="" title="wheel">

<br>

<hr>
<h2 id="Exteroceptive-센서-종류"><a href="#Exteroceptive-센서-종류" class="headerlink" title="Exteroceptive 센서 종류"></a>Exteroceptive 센서 종류</h2><h3 id="GNSS-Global-navigation-satellite-system"><a href="#GNSS-Global-navigation-satellite-system" class="headerlink" title="GNSS - Global navigation satellite system"></a>GNSS - Global navigation satellite system</h3><ul>
<li>비콘 기반의 위치추정 센서.<ul>
<li>보통 1. 위치추정, 2. 루트 플래닝 작업에서 많이 사용함.</li>
<li>다수의 비콘에 대한 통신 시간에 대한 차이를 (i.e. time delays) 이용하여 비콘-로봇의 거리 값을 구하고, 삼각측량을 통해서 localization 수행.</li>
</ul>
</li>
<li>GPS (US), GLONASS (Russia), BeiDou (China), Gallileo (Europe) 등 각각의 나라/대륙에서 관리하는 시스템이 있음.</li>
<li>장점:<ul>
<li>싸고 구하기 쉬움</li>
</ul>
</li>
<li>단점:<ul>
<li>부정확함 (10~20m 정도의 위치 오차)<ul>
<li>Camera나 LiDAR를 사용해서 drift 보정을 하기도 함.</li>
</ul>
</li>
<li>RTK-GPS, DGPS, AGPS 등을 이용하면 오차가 cm 단위로 내려오면서 훨씬 정확해지나, 가격도 굉장히 높아짐</li>
<li>고층빌딩 사이에서 <span class="exturl" data-url="aHR0cHM6Ly9hcmd1c3RyYWNraW5nLnplbmRlc2suY29tL2hjL2VuLXVzL2FydGljbGVzLzMzMzc1NzAzNzY5Ni1HUFMtQWNjdXJhY3ktQm91bmNpbmctTXVsdGlwYXRoLQ==">multi-path 문제<i class="fa fa-external-link-alt"></i></span>로 인해 오차가 생길 수 있음</li>
<li>실내/지하에서는 신호를 아예 받을 수 없음<ul>
<li>이런 경우에는 카메라나 라이다를 사용한 map 기반 localization을 수행해야함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/gnss.png" class="" title="gnss">

<br>

<hr>
<h3 id="LiDAR-Light-detection-and-ranging-sensors"><a href="#LiDAR-Light-detection-and-ranging-sensors" class="headerlink" title="LiDAR - Light detection and ranging sensors"></a>LiDAR - Light detection and ranging sensors</h3><ul>
<li>적외선 레이저를 쏘아 반사되어 돌아오는 시간을 측량하여 거리를 잴 수 있는 센서<ul>
<li>시간을 측량하는데에는 3가지 방식이 있음;<ul>
<li>Pulse를 이용한 Time-of-flight</li>
<li>Phase shift</li>
<li>Frequency modulation</li>
</ul>
</li>
</ul>
</li>
<li>3D point cloud 형태로 데이터를 받기 때문에, 주변환경의 3D 구조를 바로 알 수 있음.<ul>
<li>Point cloud 를 얻는데에 2가지 방식이 있음:<ul>
<li>Scanning LiDAR (Rolling shutter 방식과 유사함)<ul>
<li>Mechanical Scanning<ul>
<li>Macro-mechanical</li>
<li>Micro-motio</li>
<li>Prisms</li>
</ul>
</li>
<li>Solid State Scanning<ul>
<li>MEMS</li>
<li>Electro-optical</li>
<li>Optical phased array</li>
</ul>
</li>
</ul>
</li>
<li>Flash LiDAR (Global shutter 방식과 유사함)</li>
</ul>
</li>
<li>(LiDAR 공부하기 좋은 자료 <span class="exturl" data-url="aHR0cHM6Ly93d3cudGkuY29tL2xpdC9tbC9zbHlwNjY1L3NseXA2NjUucGRmP3RzPTE2MTAxODM4Nzk0MDY=">#1<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cDovL3d3dy5lcG5jLmNvLmtyL25ld3MvYXJ0aWNsZVZpZXcuaHRtbD9pZHhubz04MjA5OQ==">#2<i class="fa fa-external-link-alt"></i></span>)</li>
</ul>
</li>
<li>장점:<ul>
<li>센서가 추정한 3D 구조가 타 센서에 비해 훨씬 정확한 편임.</li>
<li>센서마다 다르지만, 자율주행용 LiDAR의 경우 ~100m 정도 거리까지 커버가 가능함.</li>
<li>특정 적외선 파장을 사용하기 때문에 빛의 파장 간섭등이 잘 일어나지 않음<ul>
<li>(동일한 device를 여러곳에서 사용하는 경우 제외)</li>
</ul>
</li>
<li>Reflectance (i.e. 쏜 레이저와 돌아온 레이저의 신호 세기를 비교하여 반사량을 계산 가능) <ul>
<li>reflectance를 이용하여 자율주행에서는 차선이나 횡단보도 패턴을 읽을 수도 있음</li>
</ul>
</li>
</ul>
</li>
<li>단점:<ul>
<li>비쌈 (~몇천만원 단위)<ul>
<li>물론 가격은 지속적으로 낮아지고 있지만, 아직 카메라, RADAR, Ultrasonic 센서에 비해 비쌈.</li>
</ul>
</li>
<li>Resolution의 한계<ul>
<li>LiDAR 이미징 센서는 아직 카메라와 비교하였을 때 센서 resolution이 많이 부족함.</li>
</ul>
</li>
<li>날씨 영향을 받음<ul>
<li>눈이 오거나 비가 올때 레이저가 눈/비에 막히기 때문에 작동하기 어려움</li>
</ul>
</li>
<li>Solid-state의 경우 360도 방향을 커버하기 위해 여러개의 센서가 필요함</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/lidar.png" class="" title="lidar">
<img src="/20210228-AD-sensors/lidar_multipath.png" class="" title="multipath">

<br>

<hr>
<h3 id="RADAR-Radio-detection-and-ranging-sensor"><a href="#RADAR-Radio-detection-and-ranging-sensor" class="headerlink" title="RADAR - Radio detection and ranging sensor"></a>RADAR - Radio detection and ranging sensor</h3><ul>
<li>전파 (raidiowave)를 이용해서 주변 환경에 반사되어 돌아오는 전파를 측정하여 radial 거리를 재는 센서.<ul>
<li>돌아오는 전파의 각도 + 돌아오는데 걸리는 시간을 이용해서 주변 환경에 대한 거리를 추정할 수 있음.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRG9wcGxlcl9lZmZlY3Q=">Doppler 효과<i class="fa fa-external-link-alt"></i></span>를 이용해서 물체의 속도를 측정 가능.</li>
<li>전파의 종류를 바꿈으로써 near-range와 long-range를 정할 수 있음.</li>
</ul>
</li>
<li>장점:<ul>
<li>날씨의 영향을 잘 받지 않음</li>
<li>타 센서에서 얻지 못하는 ‘속도’ 값을 얻을 수 있음.</li>
</ul>
</li>
<li>단점:<ul>
<li>작은 물체들은 detection에 실패할 수 있음</li>
<li>LiDAR보다 resolution이 낮음</li>
<li>주변에 벽이나 바닥에 대해 multipath 문제가 있을 수 있음</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/radar.jpg" class="" title="Radar">

<br>

<hr>
<h3 id="Ultrasound"><a href="#Ultrasound" class="headerlink" title="Ultrasound"></a>Ultrasound</h3><ul>
<li>RADAR와 작동방식이 거의 동일하나, 초음파를 이용함.</li>
<li>장점:<ul>
<li>저렴함</li>
<li>Near-range에서 잘 작동함</li>
</ul>
</li>
<li>단점:<ul>
<li>Geometry를 잘 추정하는 편은 아님.<ul>
<li>다중 센서를 이용해서 geometry 추정을 할 수는 있지만… 좋은 편은 아님.</li>
</ul>
</li>
<li>노이즈가 많음</li>
</ul>
</li>
</ul>
<br>

<hr>
<h3 id="Camera"><a href="#Camera" class="headerlink" title="Camera"></a>Camera</h3><ul>
<li>이미징 센서를 통해 빛의 정보를 받아드리고, RGB 패턴에 대해 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRGVtb3NhaWNpbmc=">debayering<i class="fa fa-external-link-alt"></i></span>을 통해 이미지를 만듬.</li>
<li>필터를 사용하지 않으면 Monochrome 카메라, 또는 RGB가 아닌 다른 특정 파장으로 특수 카메라 (e.g. 열화상 카메라, 적외선 카메라)를 만들 수 있음.</li>
<li>장점:<ul>
<li>사람이 보는 시야와 가장 유사한 데이터를 뽑음</li>
<li>저렴한 센서임에도 texture가 충만한 dense한 정보를 가짐.</li>
<li>FPS가 높은 편 (30fps, 60fps)</li>
<li>렌즈를 바꿈으로써 시야각을 바꿀 수 있음</li>
</ul>
</li>
<li>단점:<ul>
<li>Depth 정보가 없음<ul>
<li>Stereovision을 통해 Depth를 구현할 수 있지만, 계산량이 적지 않음</li>
</ul>
</li>
<li>조명에 대한 영향을 많이 받음<ul>
<li>어두운 곳에서는 좋은 데이터가 많이 안들어옴</li>
<li>너무 밝거나 빛반사가 카메라에 바로 오는 경우 작동하지 않을 수 있음.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/camera.png" class="" title="camera">

<br>

<hr>
<h3 id="Microphones"><a href="#Microphones" class="headerlink" title="Microphones"></a>Microphones</h3><ul>
<li>공기의 진동을 transducer를 통해 전기 신호로 변환시켜 소리를 읽는 센서.</li>
<li>여러개의 마이크 신호를 조합하여 소리의 근원에 대한 위치 및 방향을 계산할 수 있음.</li>
<li>장점:<ul>
<li>유일하게 소리에 대한 정보를 사용하는 센서</li>
<li>가격이 저렴함</li>
</ul>
</li>
<li>단점:<ul>
<li>Geometry가 부정확함</li>
<li>잡음이 심함</li>
</ul>
</li>
</ul>
<img src="/20210228-AD-sensors/mic.png" class="" title="mic">

<br>

<hr>
<h2 id="센서를-섞는-방법"><a href="#센서를-섞는-방법" class="headerlink" title="센서를 섞는 방법"></a>센서를 섞는 방법</h2><p>센서 값을 섞는다는 것은, 다양한 센서 값들을 기반으로 ‘로봇과 주변 환경의 state를 추정’하는 작업이다.<br>이는 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTWF4aW11bV9hX3Bvc3RlcmlvcmlfZXN0aW1hdGlvbg==">Maximum-a-Posteriori estimation<i class="fa fa-external-link-alt"></i></span>과 유사하다.<br>MAP는 optimisation 방식으로 (e.g. least squares optimisation, bundle adjustment) 풀 수도 있고, filter 방식으로도 (e.g. Kalman filter, Particle filter) 풀 수 있다.</p>
<img src="/20210228-AD-sensors/bayes.png" class="" title="Bayes">

<p>Maximum-a-posteriori estimation을 진행하는데에는 Tightly-coupled 방식과 Loosely-coupled 방식이 있다.<br>Tightly-coupled vs Loosely-coupled 방식에 대해 이야기가 많지만, 두 방식 중 어떤 방식이 항상 더 좋다라고 단정할 수 없다.</p>
<p>Tightly-coupled 방식과 loosely-coupled 방식의 차이는 <a href="https://changh95.github.io/20210816-tightly-loosely-coupled/">다음 글</a>에서 소개한다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Sensors</tag>
        <tag>Camera</tag>
        <tag>LiDAR</tag>
        <tag>RADAR</tag>
        <tag>Ultrasonic</tag>
        <tag>IMU</tag>
        <tag>GPS</tag>
        <tag>GNSS</tag>
        <tag>Tightly-coupled</tag>
        <tag>Loosely-coupled</tag>
      </tags>
  </entry>
  <entry>
    <title>Bundle adjustment란?</title>
    <url>/20210313-ba/</url>
    <content><![CDATA[<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문"></a>예상 면접 질문</h2><ol>
<li>Bundle adjustment가 무엇인가요?</li>
<li>Bundle adjustment의 과정에 대해 설명해주세요.</li>
<li>Bundle adjustment에서 matrix sparsity가 중요한 이유가 무엇인가요?</li>
</ol>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><ul>
<li>Structure from Motion을 수행할 때, 다수의 프레임에 존재하는 Visual keypoint 들의 위치 (i.e. 2D pixel location)를 기반으로 추정할 수 있는 <strong>3D landmark position과 카메라 프레임간의 3D relative motion을 동시에 최적화</strong>하는 과정</li>
<li>SLAM에서는 다음과 같은 방법으로 사용한다.<ol>
<li>Sliding window optimisation<ul>
<li>실시간으로 SLAM 프로그램을 돌리면서, 마지막 N개의 keyframe 정보를 기반으로 Bundle adjustment를 수행하여 실시간으로 local map + pose를 보정하는 작업</li>
</ul>
</li>
<li>Loop closure<ul>
<li>실시간으로 SLAM을 돌리면서, Loop closure detection이 일어났을 때 Loop closure optimisation을 다른 쓰레드에서 비실시간으로 수행하여 loop에 대한 pose + global map을 보정하는 작업</li>
</ul>
</li>
<li>Global optimisation<ul>
<li>실시간으로 SLAM을 돌리면서, 종종 다른 쓰레드에서 비실시간으로 모든 map + pose를 보정하는 작업. (잘 안쓰임)</li>
</ul>
</li>
<li>Global optimisation as 후처리<ul>
<li>SLAM이 끝나고나서, 모든 map + pose를 비실시간으로 보정하는 작업.</li>
</ul>
</li>
</ol>
</li>
</ul>
<hr>
<img src="/20210313-ba/ba.png" class="" title="ba">

<ul>
<li>보통 closed-form 방식으로 initial guess 정보가 있음.<ul>
<li>Initialization 단계에서 Homography/Fundamental matrix를 이용하여 얻어낸 relative motion 정보 + 이후 PnP로 추가해내간 relative motion 정보.</li>
<li>Initialization 단계에서 초기 R|t로 얻어낸 3D landmark position 정보 + 이후 triangulation으로 추가해내간 3D landmark position 정보.</li>
<li>하지만 이러한 정보들은 어쩔 수 없이 error를 내포하고 있음.</li>
</ul>
</li>
<li>위의 error를 보정하기 위해 최적화를 진행함.<ul>
<li>3D landmark에서 카메라 coordinate의 원점까지의 직선을 ‘ray’로 표현할 수 있음.</li>
<li>이러한 ray가 다수 있을 경우 ‘bundle of ray’라고 표현함.</li>
<li>이 ‘<strong>bundle of ray’를 보정</strong> (i.e. adjust) 한다는 의미로 bundle adjustment라는 이름을 사용함.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Cost-function-reprojection-error"><a href="#Cost-function-reprojection-error" class="headerlink" title="Cost function - reprojection error"></a>Cost function - reprojection error</h3><img src="/20210313-ba/reprojection_error.png" class="" title="reprojection_error">
<img src="/20210313-ba/reprojection_error2.png" class="" title="reprojection_error2">

<ul>
<li>최적화에 대한 cost function은 <strong>reprojection error</strong>임.<ul>
<li>2D visual keypoint 위치가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>, 3D landmark 위치가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.699ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 751 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g></g></svg></mjx-container>, 3D camera pose가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container>, projection 과정이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g></svg></mjx-container>일 때, reprojection error는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="15.616ex" height="2.552ex" role="img" focusable="false" viewBox="0 -833.9 6902.2 1128.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(278, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(556, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1379.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2379.7, 0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2949.7, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(642, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(983.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1428, 0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(715, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(5386.7, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msup" transform="translate(5664.7, 0)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(278, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></g></svg></mjx-container>로 표현됨.</li>
<li>Projection 과정은 <a href="https://changh95.github.io/20210222-image-projection/">Image projection 과정</a>과 Distort 과정이 함께 들어가있음.</li>
</ul>
</li>
</ul>
<details>
  <summary> Projection 관련 수식 </summary>

<hr>
<ul>
<li><p>Image projection</p>
<img src="/20210313-ba/projection.png" class="" title="projection matrix">
</li>
<li><p>Radial / Tangential distortion</p>
<img src="/20210313-ba/radial.png" class="" title="radial distortion">
<img src="/20210313-ba/tangential.png" class="" title="tangential distortion">

</li>
</ul>
<hr>
</details>

<hr>
<ul>
<li>이러한 Image projection 과정은 Non-linear하기 때문에, <strong>Gauss-Newton</strong>이나 <strong>Levenberg-Marquardt optimisation</strong>과 같은 non-linear optimisation을 통해 최적화해야한다.<ul>
<li>Least-squares 형태로 최적화 문제를 만들고 모든 입력 데이터가 Gaussian 형태를 따른다는 전제를 만들면, 최적화 문제가 Maximum likelihood estimation이 된다.<ul>
<li>즉, 이 최적화 문제의 해가 statistically optimal하게 된다. </li>
</ul>
</li>
<li>Gauss-Newton이나 Levenberg-Marquardt 에 대한 자세한 내용은 <a href="https://changh95.github.io/20210314-nonlinear-optimisation/">이 글</a>에서 설명한다.</li>
<li>Gauss-Newton을 예시로 들면, iteration을 수행하기 위해 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="8.295ex" height="1.808ex" role="img" focusable="false" viewBox="0 -717 3666.6 799"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888, 0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(1332, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2181.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3237.6, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 문제를 풀어 iteration을 위한 이동치를 구한다.</li>
</ul>
</li>
</ul>
<img src="/20210313-ba/optimise.png" class="" title="optimisation">

<hr>
<h4 id="Parameter-갯수"><a href="#Parameter-갯수" class="headerlink" title="Parameter 갯수"></a>Parameter 갯수</h4><ul>
<li>우리가 구해야하는 parameter는 3D landmark position (3) + Extrinsic parameters (6) + Intrinsic parameters (5) + Scale factor (1), 1개의 3D landmark당 총 15개이다.<ul>
<li>물론 동일한 포인트를 다른 시점에서 보면 당연히 더 늘어난다.</li>
<li>아래 예시를 보면, parameter의 수는 엄청나게 커진다.</li>
</ul>
</li>
</ul>
<img src="/20210313-ba/observation_model.png" class="" title="observation_model">

<img src="/20210313-ba/params.png" class="" title="observation_model">
<h2 id="이미지-출처-Cyrill-Stachniss-교수님-렉처"><a href="#이미지-출처-Cyrill-Stachniss-교수님-렉처" class="headerlink" title="이미지 출처: Cyrill Stachniss 교수님 렉처"></a>이미지 출처: Cyrill Stachniss 교수님 렉처</h2><ul>
<li>Parameter의 수가 많으면, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="8.295ex" height="1.808ex" role="img" focusable="false" viewBox="0 -717 3666.6 799"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(888, 0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(1332, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2181.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3237.6, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 문제에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container> 매트릭스도, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 매트릭스도 엄청나게 커진다.<ul>
<li>위의 예제를 예시로 들면, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container> 매트릭스만 보았을 때 2.5TB의 용량을 차지한다.</li>
</ul>
</li>
<li>하지만 매트릭스의 대부분은 비어있다 (i.e. sparse matrix)<ul>
<li>왜냐하면 연결되어있는 Factor 끼리만 error를 측정할 수 있기 때문이다.</li>
</ul>
</li>
</ul>
<img src="/20210313-ba/sparsity.png" class="" title="sparsity">

<hr>
<h4 id="Sparsity"><a href="#Sparsity" class="headerlink" title="Sparsity"></a>Sparsity</h4><ul>
<li>2개의 Factor간의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.636ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 723 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(429, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 매트릭스는 sparse matrix 형태를 가진다.<ul>
<li>하지만 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 매트릭스 전체는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.636ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 723 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(429, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>들의 sum이기 때문에, dense matrix 형태를 가진다.</li>
</ul>
</li>
<li>2개의 Factor간의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.545ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1125 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 매트릭스 역시 sparse matrix 형태를 가진다.<ul>
<li>Dense matrix의 형태를 가지는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 매트릭스와는 다르게, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container> 매트릭스는 sparse matrix 형태를 가진다.</li>
</ul>
</li>
</ul>
<img src="/20210313-ba/bH.png" class="" title="bH">
<p>이미지 출처: AirLab Summer School SLAM backend 영상<br>(마지막 H 매트릭스 그림이 symmetric 해야하는데… 잘못 그렸다 :( )</p>
<hr>
<h4 id="Sparse-matrix-계산"><a href="#Sparse-matrix-계산" class="headerlink" title="Sparse matrix 계산"></a>Sparse matrix 계산</h4><ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container> matrix는 symmetric하고 positive-definite하다는 특성을 보인다.<ul>
<li>그렇기 때문에, 거대한 dense matrix의 inverse를 구하는것보다 decomposition하는 것이 훨씬 효율적이다.</li>
</ul>
</li>
<li>Symmetric하고 positive-definite한 매트릭스에 걸 수 있는 decomposition 방식에는 2가지가 있다.<ul>
<li>Cholesky decomposition</li>
<li>LU decomposition</li>
<li>보통 Cholesky decomposition이 LU decomposition 보다 약 2배정도 빠르기 때문에 이 방법을 많이 사용한다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>Optimisation</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Bundle adjustment</tag>
        <tag>Non-linear optimisation</tag>
        <tag>Gauss-Newton</tag>
        <tag>Levenberg-Marquardt</tag>
      </tags>
  </entry>
  <entry>
    <title>RANSAC이란? Robust-estimator란? M-estimator란?</title>
    <url>/20210313-ransac/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문"></a>예상 면접 질문</h2><ol>
<li>Outlier란 무엇인가요? 어떻게 제거할 수 있을까요?</li>
<li>RANSAC이란 무엇인가요? RANSAC의 작동 방식을 설명해주세요. RANSAC의 장단점도 설명해주세요.</li>
<li>RANSAC 외로 사용할 수 있는 outlier removal 기법들에는 무엇이 있나요?</li>
</ol>
<hr>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><h3 id="Outlier-데이터란"><a href="#Outlier-데이터란" class="headerlink" title="Outlier 데이터란?"></a>Outlier 데이터란?</h3><ul>
<li>컴퓨터 비전에서 우리는 Homography matrix, Fundamental matrix, Essential matrix 등 특정 model에 대한 정보를 계산해야함.<ul>
<li>이 때, 우리는 input 데이터로부터 (e.g. keypoint correspondence) model 정보를 계산함.</li>
</ul>
</li>
<li>Input 데이터에는 두가지 종류의 데이터가 섞여있음.<ul>
<li>Outlier 데이터 - 기하학적으로나 통계적으로 잘못된 데이터.</li>
<li>Inlier 데이터 - 기하학적으로나 통계적으로 올바른 데이터.</li>
<li>단순히 데이터만 봐서는 inlier / outlier 구분하기 어려움.</li>
</ul>
</li>
<li>Model parameter를 정확하게 계산하기 위해서는 Inlier 데이터만 사용해야함.<ul>
<li><strong>Inlier 데이터로만 model parameter를 계산하면 좋은 모델 파라미터가 나옴</strong>.</li>
<li>Outlier 데이터를 포함해서 model parmater를 계산하면 잘못된 모델 파라미터가 나옴.</li>
</ul>
</li>
<li>Outlier가 생길 수 있는 원인들<ul>
<li>조명 변화</li>
<li>부분적 가려짐</li>
<li>회전</li>
<li>모션 블러 등등…</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="Outlier-removal이란"><a href="#Outlier-removal이란" class="headerlink" title="Outlier removal이란?"></a>Outlier removal이란?</h3><ul>
<li>Model parameter를 계산하기 위해, 전체 데이터 중 inlier와 outlier를 구분하여 inlier 데이터만 계산에 사용해야함.<ul>
<li>특히, 컴퓨터 비전에서 사용하는 least-squares 최적화 방식은 outlier 데이터에 굉장히 취약함.</li>
</ul>
</li>
<li><strong>RANSAC</strong>, <strong>M-estimator</strong> 등 다양한 방법이 있음.<ul>
<li>[이번 글에서는 LkOS, Certifiable algorithms는 다루지 않음].</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h3 id="RANSAC이란"><a href="#RANSAC이란" class="headerlink" title="RANSAC이란?"></a>RANSAC이란?</h3><ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5jcy5haXQuYWMudGgvfm1kYWlsZXkvY3ZyZWFkaW5ncy9GaXNjaGxlci1SQU5TQUMucGRm">“Fischler &amp; Bolles 1981, Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography”<i class="fa fa-external-link-alt"></i></span> 논문에서 제안된 방식.</li>
<li><strong>RANdom SAmple Consensus</strong>를 줄여서 RANSAC.<ul>
<li>Random - 무작위하게</li>
<li>Sample - 데이터 샘플을 뽑아서 모델을 추정하고</li>
<li>Consensus - 모델에 대한 데이터의 합의도를 구해 정확한 모델임을 평가함.</li>
</ul>
</li>
<li>RANSAC은 하나의 프레임워크임 (C++에서 템플릿 클래스로 자주 구현함).<ul>
<li>Homography in RANSAC framework</li>
<li>F matrix in RANSAC framework</li>
<li>E matrix in RANSAC framework…</li>
</ul>
</li>
</ul>
<p> </p>
<h4 id="RANSAC-작동-방식"><a href="#RANSAC-작동-방식" class="headerlink" title="RANSAC 작동 방식"></a>RANSAC 작동 방식</h4><ol>
<li>전체 데이터로부터 무작위로 minimal set의 데이터를 추출</li>
<li>뽑은 데이터로 모델 추정</li>
<li>Score 측정<ul>
<li>If (현재 score &gt; 이전 score), Update score</li>
</ul>
</li>
<li>1로 돌아감.</li>
</ol>
<details>
  <summary> 예시: Homography in RANSAC framework </summary>

<hr>
<h5 id="예시-Homography-in-RANSAC-framework"><a href="#예시-Homography-in-RANSAC-framework" class="headerlink" title="예시: Homography in RANSAC framework"></a>예시: Homography in RANSAC framework</h5><ol>
<li>전체 데이터로부터 무작위로 4개의 데이터를 추출 (i.e. Homography matrix를 추정하는데 필요한 최소한의 데이터의 수는 4개)</li>
<li>뽑은 데이터로 Homography matrix 추정</li>
<li>해당 모델을 기반으로 전체 데이터에 대한 Reprojection error를 측정하고, error threshold를 넘지 않은 데이터의 수를 Score로 저장.<ul>
<li>If (현재 score &gt; 이전 score), Update score</li>
</ul>
</li>
<li>1로 돌아감.</li>
</ol>
<ul>
<li>100개의 input 데이터가 (i.e. keypoint correspondence) 있다고 해보자. 그리고 Error threshold는 3.0으로 셋팅했다고 해보자.</li>
<li>첫번째 루프에서 100개 데이터 중 4개를 무작위로 뽑는다.<ul>
<li>이 4개의 데이터를 기반으로 Homography matrix를 계산한다.</li>
<li>해당 Homography matrix를 모든 input 데이터에 적용한다. Keypoint correspondence이기 때문에, 1번 이미지로부터 나온 keypoint의 위치 값에 homography matrix를 곱하면 2번 이미지 위의 keypoint 위치를 추정할 것이다. 하지만 실제 위치와는 조금 차이가 있을 것인데 (i.e. reprojection error), 이 차이를 구하여 error threshold를 넘는지 확인한다.</li>
<li>계산을 해봤더니 10개의 데이터가 error treshold를 넘지 않았다고 해보자. 우리는 이 데이터의 갯수를 ‘inlier의 수’로 취급하며, 이 값을 우리는 ‘best model score’로 저장한다.</li>
</ul>
</li>
<li>두번째 루프를 시작한다. 다시 100개 데이터 중 4개를 무작위로 뽑는다.<ul>
<li>이 4개의 데이터를 기반으로 Homography matrix를 계산한다.</li>
<li>다시 해당 homography matrix를 모든 input 데이터에 적용하고 inlier의 수를 측정한다.</li>
<li>이번에는 50개의 inlier 데이터가 측정되었고, 지난 루프의 10개보다 더 많다. 즉, 이 모델은 지난 루프보다 더 정확한 모델이니, 이 값을 ‘best model score’로 저장한다.</li>
</ul>
</li>
<li>세번째 루프를 시작한다. 다시 100개 데이터 중 4개를 무작위로 뽑는다.<ul>
<li>이 4개의 데이터를 기반으로 Homography matrix를 계산한다.</li>
<li>다시 해당 homography matrix를 모든 input 데이터에 적용하고 inlier의 수를 측정한다.</li>
<li>이번에는 25개의 inlier 데이터가 측정되었고, 지난 루프의 50개보다 적다. 즉, 이 모델은 지난 루프보다 더 부정확한 모델이니, ‘best model score’는 변하지 않는다.</li>
</ul>
</li>
<li>이 과정을 반복한다.</li>
</ul>
<hr>
</details>

<p> </p>
<h4 id="RANSAC-공식"><a href="#RANSAC-공식" class="headerlink" title="RANSAC 공식"></a>RANSAC 공식</h4><ul>
<li>T: 샘플링의 횟수</li>
<li>p: 우리가 고른 데이터가 inlier인 확률</li>
<li>e: 전체 데이터의 inlier:outlier 비율</li>
<li>s: Minimal set을 만들기 위한 데이터의 갯수</li>
<li>p, e, s는 보통 우리가 고름.<ul>
<li>예시…</li>
<li>p = 0.99 (99%의 확률로 정확한 모델을 얻고싶다)</li>
<li>e = 0.50 (전체 데이터 중 inlier:outlier 비율이 50:50이다. [물론 절대로 이 값은 정확하게 알 수 없다.])</li>
<li>s = 3,4,5,7,8… (p3p? Homography? Nister E? 7-point F? 8-point F?)</li>
</ul>
</li>
</ul>
<img src="/20210313-ransac/ransac.png" class="" title="ransac">

<p> </p>
<h4 id="RANSAC의-장-단점"><a href="#RANSAC의-장-단점" class="headerlink" title="RANSAC의 장/단점"></a>RANSAC의 장/단점</h4><ul>
<li><p>장점:</p>
<ul>
<li>성공 시 Outlier를 효과적으로 걸러낼 수 있음.</li>
<li>대략적인 성공 시간을 알 수 있음 (i.e. X번의 iteration 후 Y%의 확률로 정확한 모델 추론~)</li>
<li>일찍 끝나면 그만큼 시간을 벌음</li>
<li>이해하기 쉬움</li>
</ul>
</li>
<li><p>단점:</p>
<ul>
<li>무작위 샘플링이라 매번 결과가 다름 (i.e. 동일한 input 데이터임에도, seed 값에 따라서 다른 결과가 나옴.)<ul>
<li>이 때문에 deterministic test를 할 때는 seed를 고정해야함.</li>
</ul>
</li>
<li>Outlier의 비중이 많아지면 돌아야하는 iteration 수가 급격하게 늘어남</li>
<li>실패할 경우 model 추론에 완전히 실패함</li>
<li>Threshold 값을 유저가 직접 튜닝해야함 (i.e. 유저의 경험에 의존함)</li>
<li>Multi-model 추론을 할 수 없음.</li>
</ul>
</li>
</ul>
<p> </p>
<h4 id="개량-RANSAC"><a href="#개량-RANSAC" class="headerlink" title="개량 RANSAC?"></a>개량 RANSAC?</h4><div class="video-container"><iframe src="https://www.youtube.com/embed/Q7FqV_bglHo" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p>@19:05초 부터…</p>
<p>[다른 글에서 좀 더 제대로 설명할게요 :)]</p>
<hr>
<h3 id="M-Estimator란"><a href="#M-Estimator란" class="headerlink" title="M-Estimator란?"></a>M-Estimator란?</h3><ul>
<li>컴퓨터 비전에서는 종종 Least-squares optimisation을 통해 Maximum-likelihood Estimation (MLE)를 수행하여 model parameter를 추정하기도 한다.<ul>
<li>Least-sqaures는 모든 데이터 포인트가 unbias한 노이즈를 가진다는 것을 전제로 삼는다.</li>
<li>하지만 bias가 포함된 데이터 포인트가 계산에 포함되면 least-squares 과정이 무너지게 된다.</li>
</ul>
</li>
<li>이러한 bias를 가진 데이터 포인트를 찾아내고 계산에서 제외하는 기술들을 robust estimation이라고 한다.<ul>
<li>이 중, kernel 기법을 사용하여 데이터 포인트들의 residual 값에 weight를 걸어 optimisation을 수행할 수 있게 해주는 기술이 M-estimator이다.</li>
</ul>
</li>
</ul>
<h4 id="사용하는-방법"><a href="#사용하는-방법" class="headerlink" title="사용하는 방법"></a>사용하는 방법</h4><ul>
<li>기존의 방식은 어떤 model parameter <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.113ex" xmlns="http://www.w3.org/2000/svg" width="2.717ex" height="1.708ex" role="img" focusable="false" viewBox="0 -705 1201 755"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g></g></g></svg></mjx-container>을 추정하고, 해당 모델의 정확도는 모든 데이터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.014ex" xmlns="http://www.w3.org/2000/svg" width="1.446ex" height="1.584ex" role="img" focusable="false" viewBox="0 -694 639 700"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41D" d="M351 686L442 690Q533 694 534 694H540V389Q540 327 540 253T539 163Q539 97 541 83T555 66Q569 62 596 62H609V31Q609 0 608 0Q588 0 510 -3T412 -6Q411 -6 411 16V38L401 31Q337 -6 265 -6Q159 -6 99 58T38 224Q38 265 51 303T92 375T165 429T272 449Q359 449 417 412V507V555Q417 597 415 607T402 620Q388 624 361 624H348V686H351ZM411 350Q362 399 291 399Q278 399 256 392T218 371Q195 351 189 320T182 238V221Q182 179 183 159T191 115T212 74Q241 46 288 46Q358 46 404 100L411 109V350Z"></path></g></g></g></g></svg></mjx-container>에 대해 residual <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>를 구해 평가하였다.<ul>
<li>수식으로 표현하면 다음과 같다: <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.71ex" xmlns="http://www.w3.org/2000/svg" width="22.127ex" height="2.596ex" role="img" focusable="false" viewBox="0 -833.9 9780.3 1147.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833, 0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1667, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g></g></g><g data-mml-node="munder" transform="translate(2732.9, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41D" d="M351 686L442 690Q533 694 534 694H540V389Q540 327 540 253T539 163Q539 97 541 83T555 66Q569 62 596 62H609V31Q609 0 608 0Q588 0 510 -3T412 -6Q411 -6 411 16V38L401 31Q337 -6 265 -6Q159 -6 99 58T38 224Q38 265 51 303T92 375T165 429T272 449Q359 449 417 412V507V555Q417 597 415 607T402 620Q388 624 361 624H348V686H351ZM411 350Q362 399 291 399Q278 399 256 392T218 371Q195 351 189 320T182 238V221Q182 179 183 159T191 115T212 74Q241 46 288 46Q358 46 404 100L411 109V350Z"></path></g></g><g data-mml-node="mo" transform="translate(639, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1306, 0)"><g data-mml-node="mi"><path data-c="44" d="M37 475Q19 475 19 487Q19 536 103 604T327 682H356Q386 683 408 683H419Q475 683 506 681T582 668T667 633Q766 571 766 450Q766 365 723 287T611 152T455 57T279 6Q248 1 160 0Q148 0 131 0T108 -1Q72 -1 72 11Q72 24 90 40T133 64L144 68L152 88Q247 328 272 587Q275 613 272 613Q272 613 269 613Q225 610 195 602T149 579T129 556T119 532Q118 530 116 525T113 518Q102 502 80 490T37 475ZM665 407Q665 596 412 613Q403 614 383 614Q370 614 370 612Q370 598 363 542T323 357T242 103L228 69H265Q391 73 481 119Q536 148 575 188T633 268T658 338T665 392V407Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(5474.2, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(5925.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6314.2, 0)"><g data-mml-node="mi"><path data-c="1D41D" d="M351 686L442 690Q533 694 534 694H540V389Q540 327 540 253T539 163Q539 97 541 83T555 66Q569 62 596 62H609V31Q609 0 608 0Q588 0 510 -3T412 -6Q411 -6 411 16V38L401 31Q337 -6 265 -6Q159 -6 99 58T38 224Q38 265 51 303T92 375T165 429T272 449Q359 449 417 412V507V555Q417 597 415 607T402 620Q388 624 361 624H348V686H351ZM411 350Q362 399 291 399Q278 399 256 392T218 371Q195 351 189 320T182 238V221Q182 179 183 159T191 115T212 74Q241 46 288 46Q358 46 404 100L411 109V350Z"></path></g></g><g data-mml-node="mo" transform="translate(7231, 0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7786.8, 0)"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g><g data-mml-node="msup" transform="translate(8987.8, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></g></svg></mjx-container></li>
<li>(하지만 이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 값은 bias를 가진 데이터에서는 1. 큰 값을 가지며, 2. 패턴을 가지며 나타났고, 이는 optimisation을 어렵게 하였다).</li>
</ul>
</li>
<li>M-estimator는 사전에 정해둔 커널의 모양을 따라, residual <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 값에 따른 weight를 새로 지정한다.<ul>
<li>수식으로 표현하면 다음과 같다. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.71ex" xmlns="http://www.w3.org/2000/svg" width="25.057ex" height="2.596ex" role="img" focusable="false" viewBox="0 -833.9 11075.3 1147.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833, 0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1667, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g></g></g><g data-mml-node="munder" transform="translate(2732.9, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41D" d="M351 686L442 690Q533 694 534 694H540V389Q540 327 540 253T539 163Q539 97 541 83T555 66Q569 62 596 62H609V31Q609 0 608 0Q588 0 510 -3T412 -6Q411 -6 411 16V38L401 31Q337 -6 265 -6Q159 -6 99 58T38 224Q38 265 51 303T92 375T165 429T272 449Q359 449 417 412V507V555Q417 597 415 607T402 620Q388 624 361 624H348V686H351ZM411 350Q362 399 291 399Q278 399 256 392T218 371Q195 351 189 320T182 238V221Q182 179 183 159T191 115T212 74Q241 46 288 46Q358 46 404 100L411 109V350Z"></path></g></g><g data-mml-node="mo" transform="translate(639, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1306, 0)"><g data-mml-node="mi"><path data-c="44" d="M37 475Q19 475 19 487Q19 536 103 604T327 682H356Q386 683 408 683H419Q475 683 506 681T582 668T667 633Q766 571 766 450Q766 365 723 287T611 152T455 57T279 6Q248 1 160 0Q148 0 131 0T108 -1Q72 -1 72 11Q72 24 90 40T133 64L144 68L152 88Q247 328 272 587Q275 613 272 613Q272 613 269 613Q225 610 195 602T149 579T129 556T119 532Q118 530 116 525T113 518Q102 502 80 490T37 475ZM665 407Q665 596 412 613Q403 614 383 614Q370 614 370 612Q370 598 363 542T323 357T242 103L228 69H265Q391 73 481 119Q536 148 575 188T633 268T658 338T665 392V407Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(5474.2, 0)"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="mo" transform="translate(5991.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6380.2, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(6831.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7220.2, 0)"><g data-mml-node="mi"><path data-c="1D41D" d="M351 686L442 690Q533 694 534 694H540V389Q540 327 540 253T539 163Q539 97 541 83T555 66Q569 62 596 62H609V31Q609 0 608 0Q588 0 510 -3T412 -6Q411 -6 411 16V38L401 31Q337 -6 265 -6Q159 -6 99 58T38 224Q38 265 51 303T92 375T165 429T272 449Q359 449 417 412V507V555Q417 597 415 607T402 620Q388 624 361 624H348V686H351ZM411 350Q362 399 291 399Q278 399 256 392T218 371Q195 351 189 320T182 238V221Q182 179 183 159T191 115T212 74Q241 46 288 46Q358 46 404 100L411 109V350Z"></path></g></g><g data-mml-node="mo" transform="translate(8137, 0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8692.8, 0)"><g data-mml-node="mi"><path data-c="4D" d="M28 9Q28 37 43 63T73 90Q77 90 83 84T103 70T141 57H146Q162 57 178 79T222 167Q266 279 295 371T334 513T349 598T358 651T371 677Q397 705 432 705Q442 705 445 699T452 666Q453 661 453 659Q475 538 509 405T568 207L574 192Q581 178 587 164T594 150Q596 150 635 189T693 248Q765 324 863 438T1024 626T1089 701Q1093 705 1100 705Q1111 705 1111 682Q1111 675 1108 660T1099 611T1086 540Q1041 277 1041 144Q1041 98 1044 75T1050 48T1059 42Q1064 41 1075 46Q1102 61 1121 61Q1137 61 1137 50Q1137 28 1087 0T1000 -29Q983 -29 972 -23T955 -9T945 16T942 45T941 83V96Q941 158 952 256T974 422L985 489Q984 489 939 436T821 300T698 164Q665 128 620 85T568 37Q564 34 558 34Q550 34 546 37T535 54Q512 91 496 127T450 259T389 498L384 518Q349 367 294 223T198 15Q155 -50 117 -50Q87 -50 61 -35T30 -6Q28 2 28 9Z"></path></g></g><g data-mml-node="msup" transform="translate(9893.8, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(10686.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
</li>
<li>커널의 모양은 <span class="exturl" data-url="aHR0cDovL3d3dy1zb3AuaW5yaWEuZnIvb2R5c3NlZS9zb2Z0d2FyZS9vbGRfcm9ib3R2aXMvVHV0b3JpYWwtRXN0aW0vbm9kZTI0Lmh0bWw=">이 링크<i class="fa fa-external-link-alt"></i></span>, 그리고 아래 그림에도 표현되어있다.<ul>
<li>아무런 kernel도 사용하지 않은 pure least-squares의 경우, residual 값은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.933ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 854.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(451, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> 형태로 계산된다.<ul>
<li>이는 전체다수의 residual이 어떠한 optimal 값을 향하고 있어도, 어떤 높은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>값을 가진 outlier 데이터가 나타난다면 전체적인 optimisation 방향은 이 outlier의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 값을 줄이기 위한 방향으로 흘러간다는 것이다 (이것은 좋지 않다).</li>
</ul>
</li>
<li>Hard-redescender로 잘 알려진 Tukey나 Gemen-McClure kernel의 경우, 특정 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 값 이상부터는 constant로 바꿔버린다.<ul>
<li>이는, 특정 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 값 이상부터는 모든 값들이 outlier로 표현되며, 단일 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>값이 높다고 해도 전체적인 optimisation 방향에 크게 영향을 끼치지 못한다. 전체적인 optimisation의 방향이 바뀌기 위해서는 전체다수의 residual이 특정한 방향에 optimal 값이 있다고 동의할 때만 가능하다. </li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20210313-ransac/m-estimator.png" class="" title="m-estimator">

]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>RANSAC</tag>
        <tag>Robust estimator</tag>
        <tag>M-estimator</tag>
        <tag>PROSAC</tag>
        <tag>Lo-RANSAC</tag>
        <tag>Huber</tag>
        <tag>Tukey</tag>
        <tag>MAXCON</tag>
      </tags>
  </entry>
  <entry>
    <title>비선형 최적화 Non-linear optimisation - Gradient descent, Newton-Raphson, Gauss-Newton, Levenberg-Marquardt</title>
    <url>/20210314-nonlinear-optimisation/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문"></a>예상 면접 질문</h2><ol>
<li>Gradient descent 방식, Newton-Raphson 방식, Gauss-Newton 방식, Levenberg-Marquardt 방식 최적화 기법은 어떻게 다른가요?</li>
</ol>
<hr>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><ul>
<li>Gradient descent, Newton-Raphson, Gauss-Newton, Levenberg-Marquardt 최적화 방법들은 모두 continuous한 manifold위에서 optimal한 model parameter값을 찾기 위해 iterative하게 최적화하는 기법이다.</li>
<li>Iterative 하게 최적화하기 때문에 Initial value가 필요하다.</li>
<li>네가지 방법 모두 convex한 manifold 위에서 최적화 하는것을 전제로 두고 있다.<ul>
<li>하지만 실제로 non-convex한 환경에 적용되었을 때 local minima에 빠질 수 있는데, 종종 특별한 테크닉을 사용하여 local minima에 빠지는 것을 방지하기도 한다. (e.g. Momentum)</li>
</ul>
</li>
<li>SLAM에서 많이 쓰이는 방식은 Gauss-Newton, Levenberg-Marquardt 방식이다. 종종 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUG93ZWxsJTI3c19kb2dfbGVnX21ldGhvZA==">Dogleg 방식<i class="fa fa-external-link-alt"></i></span>을 쓰기도 한다.</li>
</ul>
<p> </p>
<hr>
<h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><ul>
<li>Gradient descent방식은 딥러닝에서 많이 사용하는 기법으로써, manifold위 loss function을 최적화하기 위해 사용한다.<ul>
<li>Loss function은 L1 norm, L2 norm 및 다양한 norm을 사용할 수 있다.</li>
<li>Initial point는 zero-initialization, random initalization, He initialization 등 여러가지 방법이 있다. (He init은 잘 모른다…)</li>
</ul>
</li>
<li>작동 방식은 아래와 같다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 975.6 607.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>에서 시작하여, 우선 해당 위치의 derivative (i.e. Jacobian)을 구한다.</li>
<li>해당 Jacobian 매트릭스의 - 방향에 사전에 정의해둔 learning rate를 곱한 후, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 975.6 607.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>에 더한다.<ul>
<li>i.e. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="16.169ex" height="2.093ex" role="img" focusable="false" viewBox="0 -717 7146.5 925"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2171.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3227.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4440.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(5440.5, 0)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mi" transform="translate(6080.5, 0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(6524.5, 0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g></g></g></svg></mjx-container></li>
<li>이를 통해, Manifold에서 에러가 작아지는 방향으로 움직일 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>단순히 Gradient descent 방식만 가지고는 처음 만나는 minima를 global minima라고 인식하게 된다.<ul>
<li>즉, non-convex한 상황에는 local minima에 빠지기 쉽다.</li>
<li>이를 타파하기 위해 딥러닝에서는 <span class="exturl" data-url="aHR0cHM6Ly9saWdodC10cmVlLnRpc3RvcnkuY29tLzE0MA==">momentum<i class="fa fa-external-link-alt"></i></span> 등 다양한 기법들이 있다.</li>
</ul>
</li>
</ul>
<img src="/20210314-nonlinear-optimisation/gd.png" class="" title="Gradient descent">
<p>이미지 출처: <span class="exturl" data-url="aHR0cHM6Ly93d3cua2RudWdnZXRzLmNvbS8yMDE4LzA2L2ludHVpdGl2ZS1pbnRyb2R1Y3Rpb24tZ3JhZGllbnQtZGVzY2VudC5odG1s">KDnuggets Article By Keshav Dhandhania and Savan Visalpara<i class="fa fa-external-link-alt"></i></span>.</p>
<p> </p>
<hr>
<h3 id="Newton-Raphson"><a href="#Newton-Raphson" class="headerlink" title="Newton-Raphson"></a>Newton-Raphson</h3><ul>
<li>Newton-Raphson 방식은 꽤 빠르게 해에 접근할 수 있는 방식이다.</li>
<li>작동 방식은 아래와 같다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="17.831ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 7881.4 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1378, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2227.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3283.5, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4552, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5552.2, 0)"><g data-mml-node="mrow" transform="translate(325.2, 516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mrow" transform="translate(550, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1435.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(603, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(847.5, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1435.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><rect width="2089.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 975.6 607.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>의 지점에서 1차 미분 값을 이용하여 접선 방정식을 만든다.</li>
<li>이 접선 방정식의 해를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="4.285ex" height="1.471ex" role="img" focusable="false" viewBox="0 -442 1894.1 650"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>로 삼는다.</li>
<li>Converge 할 때 까지 루프.</li>
</ul>
</li>
<li>Newton-Raphson 방식에는 치명적인 단점이 두개 있다.<ul>
<li>첫째는, 2차 미분 값이 0이 되는 지점에서는 diverge 해버린다는 점.</li>
<li>둘째는, Local minimum이나 local maximum에서는 값이 진동 (i.e. oscillate)한다는 점이다.</li>
</ul>
</li>
</ul>
<img src="/20210314-nonlinear-optimisation/Newton-Raphson_Method.png" class="" title="Newton-Raphson">

<p>이미지 출처: <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLm5hdmVyLmNvbS9Qb3N0Vmlldy5uaG4/YmxvZ0lkPXNlb2tob28mbG9nTm89MjIxMjkwOTM1OTk4JnBhcmVudENhdGVnb3J5Tm89JmNhdGVnb3J5Tm89NjMmdmlld0RhdGU9JmlzU2hvd1BvcHVsYXJQb3N0cz10cnVlJmZyb209c2VhcmNo">네이버 블로그: 도마뱀님 - C언어)Newton-Raphson Method<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h3 id="Gauss-Newton"><a href="#Gauss-Newton" class="headerlink" title="Gauss-Newton"></a>Gauss-Newton</h3><ul>
<li><p>SLAM에서 많이 사용하는 Gauss-Newton 방식으로 설명하겠습니다.</p>
</li>
<li><p>입력 데이터가 Gaussian 분포를 따르고 있다는 전제 하에, Least-squares 최적화 과정은 Maximum likelihood estimation으로 변환될 수 있다.</p>
<ul>
<li>즉, 통계적으로 optimal한 해를 구할 수 있다.<img src="/20210314-nonlinear-optimisation/gaussian.jpeg" class="" title="Gaussian">
</li>
</ul>
</li>
<li><p>한번에 Optimal한 해는 구하기 어려우며, 올바른 방향으로 iterative하게 최적화해야한다.</p>
</li>
<li><p>Non-linear한 manifold 위에서 최적화 방향을 잡기 위해 1차 미분을 수행한다. 1차 미분은 Taylor expansion으로 근사한다.</p>
<img src="/20210314-nonlinear-optimisation/derivative.jpeg" class="" title="first derivative - taylor expansion">
</li>
<li><p>1차 미분 후 정리하면 2차 방정식 형태로 정리될 수 있다.</p>
</li>
<li><p>여기서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.661ex" xmlns="http://www.w3.org/2000/svg" width="6.348ex" height="2.565ex" role="img" focusable="false" viewBox="0 -841.7 2805.7 1133.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mi" transform="translate(686.9, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="TeXAtom" transform="translate(555, -284.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mi" transform="translate(1234.7, 0)"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"></path></g><g data-mml-node="msub" transform="translate(1956.7, 0)"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="TeXAtom" transform="translate(555, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>는 Hessian matrix로 근사된다. </p>
<img src="/20210314-nonlinear-optimisation/quadratic.jpeg" class="" title="Gradient descent">
</li>
<li><p>2차 방정식은 미분한 결과가 0일 때, 최소값 또는 최대값을 얻을 수 있다. Convex인 형태이기 때문에 최소값을 얻을 수 있다.</p>
</li>
<li><p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.299ex" height="1.647ex" role="img" focusable="false" viewBox="0 -717 1016 728"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(444, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>를 푸는 방법에는 여러가지 방법이 있다.</p>
<ul>
<li>Parameter의 수가 많지 않으면 Inverse matrix를 구할 수 있다.</li>
<li>Parameter의 수가 많다면, Cholesky decomposition이나 LU decomposition을 사용할 수 있다.<img src="/20210314-nonlinear-optimisation/optimal.jpeg" class="" title="Gradient descent">
</li>
</ul>
</li>
<li><p>이러한 형태로 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="2.299ex" height="1.647ex" role="img" focusable="false" viewBox="0 -717 1016 728"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(444, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>를 얻고나면, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.471ex" xmlns="http://www.w3.org/2000/svg" width="4.285ex" height="1.471ex" role="img" focusable="false" viewBox="0 -442 1894.1 650"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>을 얻을 수 있다.</p>
<ul>
<li>이 후, iteration을 계속하면 된다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h3 id="Levenberg-Marquardt"><a href="#Levenberg-Marquardt" class="headerlink" title="Levenberg-Marquardt"></a>Levenberg-Marquardt</h3><ul>
<li>Gauss-Newton 방식에 trust region을 추가한 방식이 Levenberg-Marquardt 방식이다.</li>
<li>추가중…</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Non-linear optimisation</tag>
        <tag>Gauss-Newton</tag>
        <tag>Levenberg-Marquardt</tag>
        <tag>Gradient descent</tag>
        <tag>Newtwon-Raphson</tag>
      </tags>
  </entry>
  <entry>
    <title>Professional C++ - C++ 기초 정리 (Ch1, Part 1)</title>
    <url>/20210321-cpp-basics/</url>
    <content><![CDATA[<h2 id="1-1-1-완전-기초-Hello-World-내부에-봐야하는-것"><a href="#1-1-1-완전-기초-Hello-World-내부에-봐야하는-것" class="headerlink" title="1.1.1 완전 기초 - Hello World 내부에 봐야하는 것"></a>1.1.1 완전 기초 - Hello World 내부에 봐야하는 것</h2><ul>
<li>주석</li>
<li>전처리 지시자 (Pre-processor)<ul>
<li><code>#include [file]</code></li>
<li><code>#define [key] [value]</code></li>
<li><code>#ifdef [key], #endif</code></li>
<li><code>#ifndef [key], #endif</code></li>
<li><code>#pragma [xyz]</code></li>
</ul>
</li>
<li>main() 함수<ul>
<li><code>int main(int argc, char* argv[])</code></li>
</ul>
</li>
<li>I/O 스트림<ul>
<li><code>std::cout</code>, <code>std::cerr</code></li>
<li><code>\n</code>, <code>\r</code>, <code>\t</code>, <code>\\</code>, <code>\"</code></li>
<li><code>std::cin</code></li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// A simple hello world program</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Hello world!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-1-2-Namespace"><a href="#1-1-2-Namespace" class="headerlink" title="1.1.2 Namespace"></a>1.1.2 Namespace</h2><ul>
<li><code>using namespace std;</code>는 쓰지 말자</li>
<li>헤더파일 안에서는 using 쓰지 말자<ul>
<li>그러면 헤더파일을 include하는 모든 파일에서 using문으로 지정한 방식으로 호출해야한다.</li>
</ul>
</li>
<li>C++17에서는 <strong>Nested namespace</strong>를 사용할 수 있다.</li>
<li>Namespace alias도 잘 쓰자!</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> Library{</span><br><span class="line">    <span class="keyword">namespace</span> Module{</span><br><span class="line">        <span class="keyword">namespace</span> Submodule{</span><br><span class="line">            <span class="comment">// ... </span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> Library::Module::Submodule{</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// namespace alias</span></span><br><span class="line"><span class="keyword">namespace</span> MySubmodule = Library::Module::SubModule;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-1-3-Literals"><a href="#1-1-3-Literals" class="headerlink" title="1.1.3 Literals"></a>1.1.3 Literals</h2><ul>
<li>십진수, 8진수, 16진수, 이진수</li>
<li>신기하게 자릿수도 적을 수 있음 ㅋㅋ (e.g. 123’456’789 )</li>
</ul>
<p> </p>
<h2 id="1-1-4-변수"><a href="#1-1-4-변수" class="headerlink" title="1.1.4 변수"></a>1.1.4 변수</h2><ul>
<li>signed int, short, long, long long</li>
<li>unsigned int, short, long, long long</li>
<li>float, double, long double</li>
<li>char, char16_t, char32_t, wchar_t</li>
<li>bool</li>
<li>(C++17) <strong>std::byte</strong><ul>
<li>기존에 쓰던 char나 unsigned char 대신 쓸 수 있음</li>
</ul>
</li>
<li>캐스팅 방식에는 3가지 방법이 있음. <strong><code>static_cast&lt;&gt;</code>를 많이 쓰자!</strong></li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">int</span> i1 = (<span class="keyword">int</span>)myVal;</span><br><span class="line"><span class="keyword">int</span> i1 = <span class="keyword">int</span>(myVal);</span><br><span class="line"><span class="keyword">int</span> i1 = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(myVal);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-1-5-연산자"><a href="#1-1-5-연산자" class="headerlink" title="1.1.5 연산자"></a>1.1.5 연산자</h2><ul>
<li>=, !, +, -, *, /</li>
<li>% Modulus, 나머지 연산자</li>
<li>++ 사전증가/사후증가, – 사전감소,사후감소</li>
<li>+=, -=, *=, /=, %=</li>
<li>&amp;, &amp;= AND 연산</li>
<li>|, |= OR 연산</li>
<li>&lt;&lt;, &gt;&gt;, &lt;&lt;=, =&gt;&gt; Bitwise shift</li>
<li>^, ^= XOR 연산</li>
</ul>
<p> </p>
<h2 id="1-1-6-타입"><a href="#1-1-6-타입" class="headerlink" title="1.1.6 타입"></a>1.1.6 타입</h2><ul>
<li>Enum type<ul>
<li>Enum type의 값에 int를 더해버리면 완전 다르게 될 수 있다.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> TypeA = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> TypeB = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> TypeC = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> TypeD = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// same as ...</span></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Type</span> {</span> TypeA, TypeB, TypeC, TypeD };</span><br></pre></td></tr></table></figure>

<ul>
<li>그러니 <strong>Strong type</strong> (또는 Type-safe하게) 만들자!<ul>
<li><strong>Enum class</strong></li>
<li>이 방식을 쓰는 것이 1. 안전하고, 2. 이해하기 더 쉽다.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="keyword">class</span> <span class="title">Type</span> :</span> <span class="keyword">unsigned</span> <span class="keyword">long</span></span><br><span class="line">{</span><br><span class="line">    TypeA = <span class="number">1</span>,</span><br><span class="line">    TypeB,</span><br><span class="line">    TypeC = <span class="number">10</span>,</span><br><span class="line">    TypeD</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="comment">// use it as</span></span><br><span class="line"><span class="keyword">if</span> (Type::TypeA == <span class="number">1</span>) {...}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>Struct<ul>
<li>미니 class 처럼 쓰면 된다. 대신 private 항목이 없는…</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Employee</span>{</span></span><br><span class="line">    <span class="keyword">char</span> mNameInitial;</span><br><span class="line">    <span class="keyword">char</span> mSurnameInitial;</span><br><span class="line">    <span class="keyword">int</span> mEmployeeNumber;</span><br><span class="line">    <span class="keyword">int</span> mSalary;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// use as</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    Employee employee;</span><br><span class="line">    employee.mNameInitial = <span class="string">"H"</span>;</span><br><span class="line">    employee.mSurnameInitial = <span class="string">"C"</span>;</span><br><span class="line">    employee.mEmployeeNumber = <span class="number">100</span>;</span><br><span class="line">    employee.mSalary = <span class="number">80'000'000</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-1-7-조건문"><a href="#1-1-7-조건문" class="headerlink" title="1.1.7 조건문"></a>1.1.7 조건문</h2><ul>
<li>if/else문<ul>
<li><strong>(C++17) Initialiser를 써서 if문 만들기</strong></li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// C++17 Initilaiser if/else</span></span><br><span class="line"><span class="keyword">if</span> (Employee employee = GetEmployee(); employee.salary &gt; <span class="number">1000</span>) {...}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>Switch 문<ul>
<li><strong>Fallthrough model</strong>도 있음<ul>
<li>버그에 취약하기때문에 C++17 방식을 사용하면 좋음</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (cases){</span><br><span class="line">    <span class="keyword">case</span> A:</span><br><span class="line">        <span class="comment">// case a</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> B:</span><br><span class="line">        <span class="comment">// case b</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="comment">// Error message</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Fallthrough model with C++17 initializer + [[fallthrough]]</span></span><br><span class="line"><span class="keyword">switch</span> (Case <span class="keyword">case</span>){</span><br><span class="line">    <span class="keyword">case</span> A:</span><br><span class="line">        <span class="comment">// case a</span></span><br><span class="line">        [[fallthrough]];</span><br><span class="line">    <span class="keyword">case</span> B:</span><br><span class="line">        <span class="comment">// case b</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> C:</span><br><span class="line">        <span class="comment">// case c</span></span><br><span class="line">        [[fallthrough]];</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="comment">// Error message</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>조건 연산자<ul>
<li><code>std::cout &lt;&lt; ((i&gt;2) ? "i is higher than 2" &gt; "i is lower than 2");</code> </li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="1-1-9-함수"><a href="#1-1-9-함수" class="headerlink" title="1.1.9 함수"></a>1.1.9 함수</h2><ul>
<li>함수 선언 - 헤더파일</li>
<li>함수 구현 - 소스파일</li>
<li>함수 리턴 형에 <code>auto</code> 키워드 사용 가능.</li>
<li><code>__func__</code>에는 현재 함수 이름이 담겨있다.</li>
</ul>
<p> </p>
<h2 id="1-1-10-C-스타일-배열"><a href="#1-1-10-C-스타일-배열" class="headerlink" title="1.1.10 C 스타일 배열"></a>1.1.10 C 스타일 배열</h2><ul>
<li>C 스타일 배열은 별로 안 좋아함…</li>
<li><code>int myArray[3] = {0};</code>처럼 만들 수 있음<ul>
<li>2차원은 <code>int myArray[3][3] = {0};</code>으로 만듬.</li>
</ul>
</li>
<li>(C++17)<code>int arraySize = std::size(myArray)</code> <ul>
<li>C++17를 지원하지 않으면 <code>unsigned int arraySize = sizeof(myArray) / sizeof(myArray[0])</code>;</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="1-1-11-std-array"><a href="#1-1-11-std-array" class="headerlink" title="1.1.11 std::array"></a>1.1.11 std::array</h2><ul>
<li>C 스타일 배열이지만 iterator 쓰기 쉽게 만든 것</li>
<li><code>std::array&lt;int,3&gt; arr = {9,8,7};</code></li>
</ul>
<p> </p>
<h2 id="1-1-12-std-vector"><a href="#1-1-12-std-vector" class="headerlink" title="1.1.12 std::vector"></a>1.1.12 std::vector</h2><ul>
<li>동적 컨테이너</li>
<li><code>std::vector&lt;int&gt; vec = {11, 12};</code></li>
</ul>
<p> </p>
<h2 id="1-1-13-Structural-binding"><a href="#1-1-13-Structural-binding" class="headerlink" title="1.1.13 Structural binding"></a>1.1.13 Structural binding</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="keyword">int</span>,3&gt; values = {<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>};</span><br><span class="line"><span class="keyword">auto</span> [x,y,z] = values;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-1-14-반복문"><a href="#1-1-14-반복문" class="headerlink" title="1.1.14 반복문"></a>1.1.14 반복문</h2><ul>
<li>for</li>
<li>range based for</li>
<li>while</li>
<li>do-while</li>
</ul>
<p> </p>
<h2 id="1-1-15-Initalizer-list"><a href="#1-1-15-Initalizer-list" class="headerlink" title="1.1.15 Initalizer list"></a>1.1.15 Initalizer list</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;initializer_list&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a = makeSum({<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>});</span><br><span class="line"><span class="keyword">int</span> a = makeSum({<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>});</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">makeSum</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">initializer_list</span>&lt;<span class="keyword">int</span>&gt; lst)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> total =<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> value: lst){</span><br><span class="line">        total += value;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> total;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>Professional C++ 책</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Missing Semesters (2020) 1강 The Shell</title>
    <url>/20210322-missing-semester-shell/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/Z56Jmr9Z34Q" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><ul>
<li>One of the primary ways of interacting with the computer</li>
<li>Visual interfaces are limited<ul>
<li>You can only use the functions that the buttons provide.</li>
</ul>
</li>
<li>Most platforms provide some sort of shells<ul>
<li>Windows - Powershell</li>
<li>Linux - Bash shell</li>
<li>MacOS - Bash shell</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Shell-prompt"><a href="#Shell-prompt" class="headerlink" title="Shell prompt"></a>Shell prompt</h2><ul>
<li>We can customise our shell prompts</li>
<li>We get to write shell commands on the shell prompts<ul>
<li>We can execute a program with arugments</li>
<li>Examples:<ul>
<li><code>date</code> prints today’s date</li>
<li><code>echo</code> prints out the arguments given<ul>
<li>arguments are something that’s separated by a blank space<ul>
<li><code>echo hello</code>.</li>
<li><code>echo "Hello world"</code></li>
<li><code>echo Hello\ world</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>How does the shell know what these programs are?<ul>
<li>Your OS comes with some built-in programs, stored on your file system.</li>
<li><strong>Environment variable</strong><ul>
<li>Things that are set when we start the terminal.<ul>
<li>e.g. Where is the home directory? What is my user name?</li>
</ul>
</li>
<li><strong>Path variable</strong><ul>
<li><code>echo $Path</code> will show all the directories that the shell will search for programs.</li>
<li>When we try to run a program on terminal, bash will search through all the paths stated in the path variables for the file/program that matches with the program we typed onto the terminal.<ul>
<li>e.g. Bash will search for a file called <code>echo</code> throughout all paths stored in the path variable.</li>
<li>If I’m not sure which <code>echo</code> I am running, then I can use <code>which echo</code> command to find out what program I am running.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Navigating-file-systems-using-programs"><a href="#Navigating-file-systems-using-programs" class="headerlink" title="Navigating file systems + using programs"></a>Navigating file systems + using programs</h2><ul>
<li>UNIX systems use <code>/</code> as the root directory. Windows doesn’t.</li>
<li>Use <code>pwd</code> to print my working directory.</li>
<li>Use <strong><code>cd</code> to change my current working directory.</strong><ul>
<li>We can use relative paths to navigate around system</li>
<li><code>cd /</code> - Absolute path for root directory</li>
<li><code>cd ..</code>. <code>cd ./home</code>, <code>cd ../../../../../</code></li>
<li>Use <code>cd ~</code> to go to home directory (i.e. <code>/home/username/</code>).</li>
<li>Use <code>cd -</code> to go back to the directory you were at.</li>
</ul>
</li>
<li>Use <strong><code>ls</code> to see what files and directories are in the current working directory.</strong></li>
<li>Use <code>program_name --help</code> to print out what flags and arguments we can use for the program.<ul>
<li><code>[]</code> parameters are optional parameters</li>
<li><code>...</code> means multiple of them.</li>
</ul>
</li>
<li><code>man program_name</code> will show the program manual</li>
<li><code>mv</code> will move files</li>
<li><code>cp</code> will copy files</li>
<li><code>rm</code> will remove files<ul>
<li><code>rm -r</code> will remove file/directory in recursive manner</li>
<li><code>rmdir</code> will remove an empty directory</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Permissions"><a href="#Permissions" class="headerlink" title="Permissions"></a>Permissions</h2><ul>
<li>Use <code>ls -l</code> to view files and directories in long listing format.<ul>
<li>The first few letters tell us about many things</li>
<li>If the first letter is <code>d</code>, it’s a directory. Otherwise it’s a file.</li>
<li>The rest of the letters mean the permissions granted for each user groups:<ul>
<li>The owner of the file</li>
<li>The group that owns this file</li>
<li>Anyone else</li>
</ul>
</li>
<li><strong><code>r</code> - read permission</strong></li>
<li><strong><code>w</code> - write permission</strong></li>
<li><strong><code>x</code> - execute permission</strong></li>
<li><code>-</code> - Don’t have permission</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h2><ul>
<li>Every program has 2 primary streams by default<ul>
<li>Input stream</li>
<li>Output stream</li>
</ul>
</li>
<li><strong><code>&lt;</code> or <code>&gt;</code> will rewire the streams</strong><ul>
<li><code>&lt;</code> rewire the input of the program to be the contents of a file</li>
<li><code>&gt;</code> rewire the output of the program into a file.</li>
<li>e.g. <code>echo hello &gt; hello.txt</code> will save ‘hello’ to hello.txt. Nothing will print, but the content of hello.txt will be hello. We can check this by <code>cat hello.txt</code>. <code>cat</code> will print the contents of a file.</li>
<li>We can also use <code>cat &lt; hello.txt</code></li>
<li>We can copy contents of a file without using <code>cp</code>, by doing <code>cat &lt; hello.txt &gt; hello2.txt</code>. This will print the contents of ‘hello.txt’ and output it to ‘hello2.txt’.</li>
</ul>
</li>
<li><code>&gt;&gt;</code> will append the output.<ul>
<li><code>cat &lt; hello.txt &gt; hello2.txt</code>, then <code>cat &lt; hello.txt &gt;&gt; hello2.txt</code> will print <code>hello\n hello</code>.</li>
</ul>
</li>
<li>We can use <code>|</code> to make more sophisticated streams.<ul>
<li><code>|</code> will take the output of the left side’s program, to be the input of the right side’s program.</li>
<li>e.g. <code>ls -l / | tail -n1</code>. <code>ls -l /</code> will print the files in root directory in long format. <code>tail -n1</code> will read data and output the last line. Overall, this code will output the last line of the <code>ls -l /</code>.</li>
<li>e.g. <code>curl --head --silent google.com | grep --ignore-case content-length | cut --delimiter=' ' -f2</code>. The first part will grab the information of ‘google.com’. The second part uses <code>grep</code> command to extract the information with the header ‘content-length’. The third part then cuts the string by a space, and then outputs the second field (which is the value of the content-length of google.com). </li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Root-user-mode"><a href="#Root-user-mode" class="headerlink" title="Root user mode"></a>Root user mode</h2><ul>
<li>Use <strong><code>sudo</code> to activate superuser mode</strong>.<ul>
<li>‘do as superuser’.</li>
<li>You don’t want to run as superuse all the time. Often critical failures mas superuse may lead to complete system failure.</li>
</ul>
</li>
<li>Use <code>cd /sys</code><ul>
<li>These files are not actual files in your system.</li>
<li>These are various kernels parameters (i.e. the core of your computer)</li>
<li>e.g. <code>cd /sys/backlight/intel_backlight/</code>, <code>cat brightness</code> to show the current brightness of the screen. We can use <code>sudo su</code> to activate root user mode, and then use <code>sudo echo 500 &gt; brightness</code> to modify the brightness.<ul>
<li><strong>When running as a superuser (i.e. root user), we will see <code>#</code> on the terminal.</strong><ul>
<li>We use <code>exit</code> to exit the root user mode.</li>
</ul>
</li>
<li><strong>When not running as a super user, we see <code>$</code> on the terminal.</strong></li>
</ul>
</li>
<li>Running <code>echo 1000 | sudo tee brightness</code> will work without being a root user. The <code>tee</code> command will write the input data to a file, but also will print the data at the same time.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="How-to-open-a-file"><a href="#How-to-open-a-file" class="headerlink" title="How to open a file"></a>How to open a file</h2><ul>
<li>Use <code>xdg-open</code> to open a file in Linux. This will use an appropriate program to open a file.</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>MIT Missing Semesters 강의 정리</category>
      </categories>
      <tags>
        <tag>Missing Semesters</tag>
        <tag>MIT</tag>
        <tag>Shell Script</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title>Missing Semesters (2020) 2강 Shell Tools and Scripting</title>
    <url>/20210322-missing-semesters-shell-tools-and-scripting/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/kgII-YWo3Zw" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<h2 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h2><ul>
<li><strong><code>foo=bar</code> to assign a variable</strong></li>
<li><strong><code>echo $foo</code> to access and print the variable</strong></li>
<li><strong>Spaces are important</strong> - we separate arguments with spaces.<ul>
<li>i.e. <code>foo = bar</code> does not work.</li>
</ul>
</li>
<li>Strings?<ul>
<li><code>echo "Hello"</code> and <code>echo 'World'</code></li>
<li>The difference in double quotes and single quotes are…<ul>
<li><code>echo "Value is $foo"</code> prints ‘Value is bar’</li>
<li><code>echo 'Value is $foo'</code> prints ‘Value is $foo’</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h2><ul>
<li>We can make funcions by <code>.sh</code> files.<ul>
<li>e.g. Inside the <code>mcd.sh</code> file, we have…</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">mcd</span></span>() {</span><br><span class="line">    mkdir -p <span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">    <span class="built_in">cd</span> <span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<ul>
<li>This function is like mv + cd.</li>
<li><strong><code>$1</code> is a special variable, similar to argv.</strong><ul>
<li>It means the first argument.</li>
<li>Similarily, <code>$2</code>, <code>$3</code>… they will be second and third arguments, up to 9th.</li>
<li><code>$0</code> is reserved for the name of the script.</li>
</ul>
</li>
<li>We can use <strong><code>source mcd.sh</code> to load the function into the shell.</strong><ul>
<li>We can now use <code>mcd test</code> command to create a directory called test and move into it.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Special-commands"><a href="#Special-commands" class="headerlink" title="Special commands"></a>Special commands</h2><ul>
<li><code>!!</code> gives the last command given to the shell.<ul>
<li>If I used <code>mkdir ~/Downloads/folder1</code> in the previous comment, I can then use <code>sudo !!</code> to represent <code>sudo mkdir ~/Downloads/folder1</code>.</li>
</ul>
</li>
<li><code>$_</code> gives the last argument from the previous command… like below.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rmdir <span class="built_in">test</span></span><br><span class="line">mkdir <span class="variable">$_</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$_</span></span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="Using-error-codes"><a href="#Using-error-codes" class="headerlink" title="Using error codes"></a>Using error codes</h2><ul>
<li><strong><code>$?</code> gives the error code</strong> from the previous command.<ul>
<li><code>echo "Hello"</code>, then <code>echo $?</code> will print 0, since there is no error (i.e. error code 0).</li>
<li><code>grep foobar mdc.sh</code>, then <code>echo $?</code> will print 1. This is because there is no string ‘foobar’ in the mcd.sh, so the error code 1 is given.</li>
<li><code>true</code>, then <code>echo $?</code> will always have error code 0.</li>
<li><code>false</code>, then <code>echo $?</code> will always have error code 1.</li>
<li><strong>We can use this like <code>false || echo "Oops fail"</code>. Bash will run the first command, and if it fails, then we will run the second command.</strong><ul>
<li>i.e. <code>true || echo "This will not be printed"</code>.</li>
</ul>
</li>
<li><strong>We can also use this <code>true &amp;&amp; echo "This works well"</code>. Bash will run the first command, and if it suceeds, then we will run the second command.</strong></li>
</ul>
</li>
<li><strong>We can use <code>;</code> to run two functions and concatenate the results too.</strong><ul>
<li>e.g. <code>false ; echo "This will print anyways"</code>.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Using-functions-and-variables-together"><a href="#Using-functions-and-variables-together" class="headerlink" title="Using functions and variables together"></a>Using functions and variables together</h2><ul>
<li><code>foo=$(pwd)</code> will store the results of pwd into foo.<ul>
<li>We can access it by <code>echo $foo</code></li>
<li>We can also use it like <code>echo "We are in $(pwd)"</code></li>
</ul>
</li>
<li>We can concatenate the outputs of two functions like<ul>
<li><code>cat &lt;(ls) &lt;(ls ..)</code>.</li>
<li>We execute the function, and then get the output as a temporary file to feed into the function next to it.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Example-function"><a href="#Example-function" class="headerlink" title="Example function"></a>Example function</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Staring program at <span class="subst">$(date)</span>"</span> <span class="comment"># Date will be substituted</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Running program <span class="variable">$0</span> with <span class="variable">$#</span> arguments with pid $$"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> <span class="string">"<span class="variable">$@</span>"</span>; <span class="keyword">do</span></span><br><span class="line">    grep foobar <span class="string">"<span class="variable">$file</span>"</span> &gt; /dev/null 2&gt; /dev/null</span><br><span class="line">    <span class="comment"># When pattern is not found, grep has exit status 1</span></span><br><span class="line">    <span class="comment"># We redirect STDOUT and STDERR to a null register since we do not care about them</span></span><br><span class="line"></span><br><span class="line">    ,<span class="keyword">if</span> [[ :$?<span class="string">" -ne 0 ]]; then</span></span><br><span class="line"><span class="string">        echo "</span>File <span class="variable">$file</span> does not have any foobar, adding one<span class="string">"</span></span><br><span class="line"><span class="string">        echo "</span><span class="comment"># foobar" &gt;&gt; "$file"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><code>date</code> is a built-in bash function.</li>
<li><code>$0</code> is the name of the script we are running</li>
<li><code>$#</code> is the number of arguments we are giving to this program.</li>
<li><code>$$</code> is the process ID of this command.</li>
<li><code>$@</code> puts all the arguments.<ul>
<li>If we do not know how many arguments are there (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.946ex" role="img" focusable="false" viewBox="0 -666 778 860"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g></g></g></svg></mjx-container>2, $3…), then we can use <code>$@</code> to put all the arguments.</li>
</ul>
</li>
<li>In a for loop, we make the arguments into <code>file</code> variable. (i.e. We are running a loop over every argument given. Maybe the arguments are file names)</li>
<li><code>grep foobar "$file"</code> means we are going to try to find ‘foobar’ in the file (i.e. arguments)</li>
<li><code>&gt;</code> is sending the output of a program to a file.</li>
<li><code>/dev/null</code> is a special space in UNIX that gets discarded regardless of how much we put it in.</li>
<li><code>2&gt;</code> is sending the STDERR of a program to a file.</li>
<li><code>$?</code> gives the error code of the previous command.</li>
<li><code>-ne</code> is a comparison operator of ‘not equal’. (Similar to !=)</li>
<li><code>"$?" -ne 0</code> will try to see if the error code is 0 (i.e. foobar was not found)<ul>
<li>Then, it will copy a comment of <code># foobar</code> into the file.</li>
</ul>
</li>
<li><code>fi</code> is maybe the end of if statement?</li>
<li><code>done</code> is maybe the end of the script? </li>
</ul>
<p> </p>
<h2 id="Globbing"><a href="#Globbing" class="headerlink" title="Globbing"></a>Globbing</h2><ul>
<li><code>*.sh</code> <strong>Globbing</strong> all files with the same extension ‘.sh’.</li>
<li>Say there are ‘project1’, ‘project2’, ‘project42’ in the directory. <ul>
<li>Using <code>ls project?</code> will give ‘project1’ and ‘project2’ as it only suggests possibilities with 1 single character.</li>
</ul>
</li>
<li>**Curly braces **are power tools.<ul>
<li>When running <code>convert image.png image.jpg</code>, we can do it as <code>convert image.{png, jpg}</code></li>
<li><code>touch foo{,1,2,10}</code> will mean <code>touch foo, foo1, foo2, foo10</code>.</li>
<li><code>touch project{1,2}/src/test/test{1,2,3}.py</code> will be very useful in automation.</li>
<li><code>mkrdir foo bar</code>, then <code>touch {foo,bar}/{a...j}</code> will be useful as well.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Shell-tricks-Python"><a href="#Shell-tricks-Python" class="headerlink" title="Shell tricks - Python"></a>Shell tricks - Python</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/local/bin/python</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">for</span> arg <span class="keyword">in</span> <span class="built_in">reversed</span>(sys.argv[<span class="number">1</span>:]):</span><br><span class="line">    print(arg)</span><br></pre></td></tr></table></figure>

<ul>
<li>The first line <code>#!/usr/local/bin/python</code> will allow the user to run the script like <code>./scripy.py a1 a2 a3</code> instead of the normal way <code>python3 ./script.py a1 a2 a3</code>.<ul>
<li>The downside of this method is that different environments might have different directories of saving python.</li>
<li>So in this case, <strong><code>#!/usr/bin/env python</code> is actually more useful.</strong></li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Debugging-shells"><a href="#Debugging-shells" class="headerlink" title="Debugging shells"></a>Debugging shells</h2><ul>
<li><code>shellcheck example.sh</code>.</li>
</ul>
<p> </p>
<h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><ul>
<li><code>find . -name src -type d</code> will recursively go through the current directory to find a directory called ‘src’.</li>
<li><code>find . -path '**/test/*.py' -type f</code> will recursively go through the current directory to find a python file under the test folder.</li>
<li><code>find . -mtime -1</code> will find the files that has been ‘modified’(i.e. m-times) by 1 time.</li>
<li><code>find . -name "*.tmp" -exec rm {} \;</code> will find all the files with ‘.tmp’ extension and remove them.<ul>
<li>On the shell, it will look like nothing happened, but when you check with <code>echo $?</code>, you will find that it had exit code 0.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h2><ul>
<li><code>locate some_string</code> will do something similar to find.<ul>
<li>However, locate is much faster as it searches through the index that the UNIX system built already (it is very similar to database approach), so it’s much faster than brute-force search method of ‘find’.</li>
</ul>
</li>
<li><code>updatedb</code> command will update this database.</li>
</ul>
<p> </p>
<h2 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h2><ul>
<li><code>grep foobar example.sh</code> will search for the contents of the file. <ul>
<li>This is different from ‘find’ or ‘locate’, as these are for directories.</li>
</ul>
</li>
<li><code>grep -R foobar .</code> will recursively search for the string amongst all of the files in the directory.</li>
</ul>
<p> </p>
<h2 id="rg-ripgrep"><a href="#rg-ripgrep" class="headerlink" title="rg - ripgrep"></a>rg - ripgrep</h2><ul>
<li>ripgrep is an installed package - <code>sudo apt install ripgrep</code><ul>
<li>Just a better alternative to ‘grep’, as it has colour coding, unicode supports, fast recursive search with respect to gitignore etc.</li>
</ul>
</li>
<li><code>rg "import requests" -t py -C 5 ~/scratch</code> will find all the python files that uses ‘import reuqests’. </li>
<li><code>rg -u --files-without-match "^#\!" -t sh</code><ul>
<li><code>-u</code> means don’t ignore hidden files</li>
<li><code>--files-without-match</code> means I want to print the files that do NOT match with the pattern. (This is quite hard with grep)</li>
<li><code>"^#\!"</code> is a regex saying that the beginning of a line has a ‘#’ and ‘!’.</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Searching-command-history"><a href="#Searching-command-history" class="headerlink" title="Searching command history"></a>Searching command history</h2><ul>
<li><code>history</code> will print the recent commands</li>
<li><code>history 1</code> will print all the commands from the beginning of the time.</li>
<li><code>history 1 | grep convert</code> will print the times when we used ‘convert’ function.</li>
</ul>
<p> </p>
<h2 id="Navigation"><a href="#Navigation" class="headerlink" title="Navigation"></a>Navigation</h2><ul>
<li><code>tree</code> builds a directory tree easily</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>MIT Missing Semesters 강의 정리</category>
      </categories>
      <tags>
        <tag>Missing Semesters</tag>
        <tag>MIT</tag>
        <tag>Shell Script</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title>Cracking the coding interview 노트 - 기술면접 준비</title>
    <url>/20210323-cracking-the-coding-interview-notes1/</url>
    <content><![CDATA[<h2 id="기본적으로-알아야하는-것들"><a href="#기본적으로-알아야하는-것들" class="headerlink" title="기본적으로 알아야하는 것들"></a>기본적으로 알아야하는 것들</h2><ul>
<li>자료구조<ul>
<li>Linked lists</li>
<li>Trees, Tries &amp; Graphs</li>
<li>Stacks &amp; Queues</li>
<li>Heaps</li>
<li>Vectors / ArrayLists</li>
<li><strong>Hash tables</strong></li>
</ul>
</li>
<li>알고리즘<ul>
<li>Breadth-First Search</li>
<li>Depth-First Search</li>
<li>Binary Search</li>
<li>Merge Sort</li>
<li>Quick Sort</li>
</ul>
</li>
<li>컨셉<ul>
<li>Bit Manipulation</li>
<li>Memory (Stack vs Heap)</li>
<li>Recursion</li>
<li>Dynamic Programming</li>
<li>Big O Time &amp; Space</li>
</ul>
</li>
</ul>
<p>Practice implementing the data structures and algorithms (first on paper, then on a computer).</p>
<p> </p>
<h2 id="문제를-푸는-방법"><a href="#문제를-푸는-방법" class="headerlink" title="문제를 푸는 방법"></a>문제를 푸는 방법</h2><p>문제를 풀면서 계속 이야기해야한다. 면접관은 우리가 어떻게 문제에 접근하는지 보고싶어한다.</p>
<ol>
<li>Listen<ul>
<li>문제가 정의하는 것들을 자세히 들여다보기.<ul>
<li><strong>Mentally record any unique information in the problem</strong></li>
</ul>
</li>
<li>보통 Optimal code를 작성하기 위해 모든 정보를 사용해야함.</li>
</ul>
</li>
<li>Example<ul>
<li>Example을 하나 만들어보자.<ul>
<li>보통 Example은 심플하고, 특수 케이스가 아니여야한다.</li>
<li>Example은 specific하고 적당히 커야한다.</li>
</ul>
</li>
<li>디버깅을 해보자.</li>
</ul>
</li>
<li>Brute force<ul>
<li>최대한 빠르게 brute-force 코드를 작성하자.</li>
<li>효율적인 코드가 아니여도 된다! 일단 간단한걸 만들고 그 다음에 optimise한다.<ul>
<li>Time &amp; space complexity가 어떻게 되는지 알아보고, 더 좋은 방법을 모색한다.</li>
</ul>
</li>
</ul>
</li>
<li>Optimise<ul>
<li><strong>BUD Optimisation</strong>을 사용해서 최적화된 코드를 짜자.<ul>
<li>Bottlenecks</li>
<li>Unnecessary Work</li>
<li>Duplicated Work</li>
</ul>
</li>
<li>문제에서 쓰지 않은 내용이 있는가 확인하자.</li>
<li>Example 케이스에 대해 풀어보자.<ul>
<li>풀린다면, 새로운 Example 케이스도 만들어서 풀어보자!</li>
</ul>
</li>
<li>틀린 케이스로 한번 풀어보자. 이 이슈를 해결할 수 있는가?</li>
<li>Precomputation이 있는가? (e.g. sorting이 이미 되어있다던지…)<ul>
<li>이 정보를 잘 이용하면 훨씬 효율적이게 코드를 짤 수 있다.</li>
</ul>
</li>
<li>Time vs space tradeoff를 고려하자. Hash table은 굉장히 좋은 툴이다!</li>
</ul>
</li>
<li>Walk through<ul>
<li>Optimal solution이 만들어졌다면, 한번 머릿속으로 돌려보면서 모든 디테일이 확실하게 커버되는지 확인하자.</li>
<li>Whiteboard coding도 좋다!</li>
</ul>
</li>
<li>Implement<ul>
<li>깔끔한 코드를 작성하자!</li>
<li>Modularisation은 필수다.</li>
<li>Error check를 까먹지 말자! 필요하다면 TODO로 놔두고, 대신 말로 어떤걸 체크할지 이야기하자.</li>
<li>Variable name은 깔끔하게 하자. </li>
</ul>
</li>
<li>Test<ul>
<li>‘Submit’하기 전에 무조건 테스트는 꼭 하자!</li>
<li>Conceptual test. Code review하듯이 코드를 다시 읽자.</li>
<li>Non-standard code를 잡아내자.</li>
<li>자주 틀리는 곳들 - recursion, integer divison, binary tree의 null node등을 체크하자.</li>
<li>작은 테스트 케이스를 돌려보자.</li>
<li>특수 케이스 코드를 돌려보자. </li>
</ul>
</li>
</ol>
<p> </p>
<h2 id="최적화-기법-BUD-찾기"><a href="#최적화-기법-BUD-찾기" class="headerlink" title="최적화 기법 - BUD 찾기"></a>최적화 기법 - BUD 찾기</h2>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>Cracking the coding interview</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Missing Semesters (2020) 3강 Vim</title>
    <url>/20210323-missing-semesters-vim/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/a6Q8Na575qc" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<h2 id="Vim-modes"><a href="#Vim-modes" class="headerlink" title="Vim modes"></a>Vim modes</h2><ul>
<li><strong>Normal mode</strong><ul>
<li>목적: Navigation</li>
<li>다른 mode에서 <code>esc</code>를 눌러서 이동</li>
</ul>
</li>
<li><strong>Insert mode</strong><ul>
<li>Normal mode에서 <code>i</code>를 눌러서 이동</li>
<li>목적: Typing text into the text buffer</li>
</ul>
</li>
<li><strong>Replace mode</strong><ul>
<li>Normal mode에서 <code>r</code>을 눌러서 이동</li>
</ul>
</li>
<li><strong>Visual mode</strong><ul>
<li>Normal mode에서 <code>v</code>를 눌러서 이동</li>
<li>Visual line<ul>
<li>Normal mode에서 <code>shift + v</code>를 눌러서 이동</li>
</ul>
</li>
<li>Visual block<ul>
<li>Normal mode에서 <code>ctrl + v</code>를 눌러서 이동</li>
</ul>
</li>
</ul>
</li>
<li><strong>Command-line mode</strong><ul>
<li>Normal mode에서 <code>:</code>을 눌러서 이동</li>
</ul>
</li>
</ul>
<p>이것을 보면 <code>esc</code>를 많이 누르게 될 것을 알고 있다. <code>esc</code>키를 누르기 위해 왼손 새끼손가락을 쭉 올리는거보다, 종종 캡스락 키에 <code>esc</code> 키를 바인딩 하기도 한다.</p>
<h2 id="Open-file"><a href="#Open-file" class="headerlink" title="Open file"></a>Open file</h2><ul>
<li><code>vim</code>을 눌러서 vim을 키고 끌 수 있다.</li>
<li><code>vim file.md</code>를 써서 파일을 열 수 있다.</li>
<li>vim에서 파일을 읽거나 저장하거나, vim을 종료하려고 할 때는 command-line mode를 사용한다.<ul>
<li>Normal mode에서 <code>:</code>를 눌러서 command-line mode로 와야한다.</li>
<li><code>:quit</code>을 누르면 끌 수 있다.</li>
<li><code>:q</code>는 짧은 버전이다.</li>
<li><code>:w</code>를 통해 write, 즉 저장할 수 있다.</li>
<li><code>:help xyz</code>를 눌러서 xyz에 대한 document를 볼 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Buffers-tabs-windows"><a href="#Buffers-tabs-windows" class="headerlink" title="Buffers, tabs, windows"></a>Buffers, tabs, windows</h2><ul>
<li>VSCode는 동시에 여러개의 파일을 열 수 있다.<ul>
<li>Vim 도 가능하다. 여러개의 buffer를 열 수 있다.</li>
</ul>
</li>
<li>기존의 에디터 프로그램들에서 사용하는 개념들은 vim에서는 다음과 같이 맵핑된다.<ul>
<li>File -&gt; buffer</li>
<li>Tab -&gt; Tab</li>
<li>Window -&gt; Window</li>
</ul>
</li>
<li>Vim도 여러개의 buffer (i.e.file)을 열 수 있고, 여러개의 탭을 열 수 있고, 여러개의 윈도우를 사용할 수 있다.</li>
<li>Buffer는 윈도우 없이 여러개를 열 수 있다.</li>
<li>Window를 열기 위해서는 <code>:sp</code> 커맨드를 사용한다.<ul>
<li><code>:q</code> 또는 <code>:quit</code>을 사용해서 윈도우를 닫는다.</li>
<li><code>:qa</code> (i.e. quit all)을 사용해서 모든 윈도우를 한번에 닫을 수 있다.</li>
</ul>
</li>
<li><code>tabnew</code>를 사용하여 새 tab을 열 수 있다.<ul>
<li>하나의 tab에는 여러 윈도우를 띄울 수 있다. </li>
</ul>
</li>
</ul>
<h2 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h2><ul>
<li>Navigate around buffer in normal modeS?<ul>
<li><code>hjkl</code> buttons to move cursors<ul>
<li>Instead of <code>hhhhh</code>, we can also do <code>5h</code>.</li>
</ul>
</li>
<li><code>w</code> and <code>b</code> to move cursor by words<ul>
<li><code>e</code> to move cursor to the end of the word</li>
</ul>
</li>
<li><code>0</code> to move by line<ul>
<li><code>$</code> to move to the end of a line</li>
<li><code>^</code> to move to first non-empty character of the line</li>
</ul>
</li>
<li><code>ctrl + U</code> moves up the page, <code>ctrl + D</code> moves down the page.</li>
<li><code>G</code> moves to the bottom of the page, <code>gg</code> moves to the top of the page</li>
<li><code>L</code> moves your cursor to the bottom of the screen, <code>M</code> to the middle of the screen, <code>H</code> to the top of the screen</li>
</ul>
</li>
<li>Find words<ul>
<li><code>fo</code> to find the first ‘o’.</li>
<li><code>Fo</code> to find the first ‘o’ backwards.</li>
<li><code>to</code> to find the first ‘o’, but place the cursor right before it.</li>
<li><code>To</code> to find the fisrt ‘o’, but place the cursor right before it.</li>
<li><code>/word</code> to search for a word</li>
</ul>
</li>
<li>Editing<ul>
<li><code>o</code> in normal mode will automatically activate insert mode after making a new line</li>
<li><code>O</code> in normal mode will automatically activate insert mode after making a new line above the cursor.</li>
<li><code>dw</code> to delete a word<ul>
<li>We can do <code>7dw</code> to delete 7 words.</li>
</ul>
</li>
<li><code>de</code> to delete to end of a word</li>
<li><code>dd</code> to delete a line. Repeating the command affects the entire line.</li>
<li><code>ce</code> to change to end of a word. It deletes the content to end of a word and automatically activate insert mode.</li>
<li><code>cc</code> to change a line. Repeating the command affects the entire line.</li>
<li><code>x</code> deletes a character</li>
<li><code>ra</code> replace a character by ‘a’</li>
<li><code>u</code> Undo action</li>
<li><code>R</code> redo action</li>
<li><code>y</code> to copy (yank), and <code>p</code> to paste.<ul>
<li><code>yy</code> to copy a line</li>
<li><code>yw</code> to copy a word</li>
<li>To select a block of text, we need to enter the visual mode. Use <code>v</code> to enter the visual mode. Use <code>V</code> to enter the visual line mode. Use <code>ctrl + V</code> to enter the visual block mode.<ul>
<li>Use <code>hjkl</code> to navigate cursor and select the block of text</li>
<li>then, use <code>y</code> to copy the block of text and return to normal mode.</li>
</ul>
</li>
</ul>
</li>
<li><code>~</code> to change the case</li>
</ul>
</li>
<li>Modifer commands <ul>
<li>Use <code>ci(</code> or <code>ci[</code> to change the contents inside the brackets (i for inside)</li>
<li>Use <code>%</code> to jump back and forth the parentheses.</li>
<li>Use <code>da(</code> to delete the entire content + the parentheses. (a for around)</li>
</ul>
</li>
<li>Repeat command<ul>
<li><code>.</code> to repeat the previous command.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>MIT Missing Semesters 강의 정리</category>
      </categories>
      <tags>
        <tag>Missing Semesters</tag>
        <tag>MIT</tag>
        <tag>Bash</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title>Ctrl과 Caps lock 위치 바꾸기 (Windows / Linux)</title>
    <url>/20210325-ctrl-capslock-swap/</url>
    <content><![CDATA[<h2 id="바꾸는-이유"><a href="#바꾸는-이유" class="headerlink" title="바꾸는 이유"></a>바꾸는 이유</h2><ul>
<li>주 키보드 중 하나가 해피해킹이라서 (i.e. ctrl이 자체적으로 caps lock위치에 있음)<ul>
<li>이걸 쓰다가 보통의 키보드나 노트북을 쓸 때 항상 헷갈림</li>
</ul>
</li>
<li>Ctrl+C, Ctrl+V, Ctrl+W 모두 캡스락 위치가 손목이 더 편하다.</li>
</ul>
<p> </p>
<hr>
<h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9qb2huaGFsbGVyLmNvbS91c2VmdWwtc3R1ZmYvZGlzYWJsZS1jYXBzLWxvY2s=">링크<i class="fa fa-external-link-alt"></i></span>로 들어간다.</li>
<li><code>Swap Caps Lock and the left Control key</code>의 파일을 다운로드하고, 레지스트리 등록.</li>
</ul>
<p> </p>
<hr>
<h2 id="Linux-Ubuntu"><a href="#Linux-Ubuntu" class="headerlink" title="Linux (Ubuntu)"></a>Linux (Ubuntu)</h2><ul>
<li><code>gnome-tweak-tools</code> 다운로드.<ul>
<li>i.e. <code>sudo apt install gnome-tweak-tools</code></li>
</ul>
</li>
<li>메뉴에서 ‘Tweaks’ 실행<ul>
<li>그 후, Keyboard &amp; Mouse &gt; Keyboard &gt; Additional Layout Options &gt; Ctrl position로 탐색</li>
<li><code>Swap ctrl and caps lock</code> 체크.</li>
</ul>
</li>
</ul>
<img src="/20210325-ctrl-capslock-swap/swap.png" class="" title="swap">

<blockquote>
<p>중대 Update!<br>노트북에 해피해킹을 연결하면, 해피해킹 키보드의 ctrl 버튼이 caps lock으로 맵핑되어버린다 ㄷㄷ… 해피해킹을 노트북에도 연결하는 경우에는 ‘Caps lock as ctrl’ 옵션을 하는게 더 편하다.</p>
</blockquote>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>Missing Semesters (2020) 4강 Data Wrangling</title>
    <url>/20210325-missing-semester-data-wranglnig/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/sz_dsktIjt4" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<h2 id="Data-wrangling"><a href="#Data-wrangling" class="headerlink" title="Data wrangling?"></a>Data wrangling?</h2><ul>
<li>Changing the format of data<ul>
<li>Image? Graph? YAML? JSON?</li>
<li>Using <code>|</code> operator in bash is already a form of data wrangling!</li>
</ul>
</li>
</ul>
<h2 id="Get-log-data-and-do-something-with-it"><a href="#Get-log-data-and-do-something-with-it" class="headerlink" title="Get log data and do something with it"></a>Get log data and do something with it</h2><ul>
<li>To do data wrangling, we need some data.<ul>
<li>In this example, the instructor pulled some log data from remote server and did some data wrangling on it.</li>
<li><code>ssh</code> to control servers remotely</li>
<li><code>grep</code> to grab content</li>
<li><code>less</code> to fit the loooong data into a single page, instead of flooding lines.</li>
<li><code>ssh '...... | grep "Disconneced from"' &gt; ssh.log</code> to grab anything that contains ‘Disconnected from” in the line and save it to ‘ssh.log’ file.</li>
<li><code>cat ssh.log | less</code> to view the log in a pager.</li>
</ul>
</li>
<li><code>sed</code> is a stream editor that enables you make changes to the contents of the stream.<ul>
<li><code>cat ssh.log | sed 's/.*Disconnected from//'</code> will grab the contents from the ‘ssh.log’, and apply <code>sed</code>‘s regular expression (regex).<ul>
<li>Here, we have the ‘user name’ data that we want to get, after the ‘Disconnected from…’ string.</li>
<li><code>s</code> is an expression to substitute the string. It takes 2 arguments, the first one being search, and the second one being the replacement string.</li>
<li><code>s/ aaa / bbb /</code> will replace the ‘aaa’ string into ‘bbb’ string.</li>
<li><code>.</code> means ‘any single character’</li>
<li><code>*</code> means ‘zero or more of this character’</li>
<li>So <code>.*</code> means ‘zero or more of any of the following characters’… of ‘Disconnected from’</li>
</ul>
</li>
<li><code>echo 'aba' | sed 's/[ab]//'</code> will return ‘ba’.<ul>
<li>This means ‘replace either a or b that appears first time with an empty space’.</li>
<li><code>[ab]</code> means either a or b.</li>
<li>So <code>echo 'bba' | sed 's/[ab]//'</code> will return ‘ba’ as well.</li>
<li><code>echo 'aba' | sed 's/[ab]//g'</code> will return nothing (i.e. removed all a and b). The <code>g</code> will do the action as many times it matches.</li>
</ul>
</li>
<li><code>echo 'abcaba' | sed -E 's/(ab)*//g'</code> will return ‘ca’.<ul>
<li>means ‘I want zero or more of the string ‘ab’ and replace it with empty space’.<ul>
<li>‘ab’c’ab’a.</li>
</ul>
</li>
<li>So it’s not ‘either a or b’. It will be looking for ‘ab’</li>
<li><code>-E</code> is to allow usage of old-fashioned regex. </li>
</ul>
</li>
<li><code>echo 'abcababc' | sed -E 's/(ab|bc)*//g'</code> will return ‘cc’.<ul>
<li>Although we stated to remove ‘ab’ or ‘bc’, the c-characters still remain.</li>
<li>This is because in the ‘abc’, the ‘ab’ is already removed, so when we try to look for ‘bc’, there is only ‘c’ left in the string so we could not find ‘bc’.</li>
</ul>
</li>
<li></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>MIT Missing Semesters 강의 정리</category>
      </categories>
      <tags>
        <tag>Missing Semesters</tag>
        <tag>MIT</tag>
        <tag>Bash</tag>
        <tag>Data Wrangling</tag>
      </tags>
  </entry>
  <entry>
    <title>리눅스 터미널 업그레이드 - Alacritty + zsh + prezto + tmux + (NeoVim)!</title>
    <url>/20210326-alacritty-tmux-zsh-prezto/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9kZXYudG8vaGxhcHBhL215LXdlYi1kZXZlbG9wbWVudC1lbnZpcm9ubWVudC1hbGFjcml0dHktdG11eC1uZW92aW0tNHBkMg==">참조한 링크<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="Alacritty"><a href="#Alacritty" class="headerlink" title="Alacritty"></a>Alacritty</h2><img src="/20210326-alacritty-tmux-zsh-prezto/alacritty.png" class="" title="Alacritty">

<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FsYWNyaXR0eS9hbGFjcml0dHk=">Alacritty<i class="fa fa-external-link-alt"></i></span>는 Rust로 짜여진 OpenGL 가속을 사용하는 터미널 에뮬레이터다.</p>
<p>기존의 bash terminal을 놔두고 alacritty를 선택한 이유는?</p>
<ul>
<li>OpenGL 가속으로 인해 로깅이 훨씬 빠르다고 한다.<ul>
<li>실제로 <code>tree</code>를 찍어보면 조금 더 빠르다.</li>
</ul>
</li>
<li>색깔을 바꿀 수 있다!<ul>
<li>커스터마이징은 꼭 해야지 :)</li>
</ul>
</li>
</ul>
<p>설치 가이드를 쫒아서 하다보니 금방 실행할 수 있었다.</p>
<p>설치 과정에서 눈여겨 본 점은:</p>
<ul>
<li>rustup 컴파일러를 설치해야한다는 점</li>
<li><code>cc</code>가 gcc에 연결되어야하는 점.<ul>
<li>이게 되어있지 않다면 빌드 과정 중에 <code>linker not found 'cc'</code> 같은 에러가 나타난다.</li>
<li>그럴 때는 <code>cc --version</code>을 쳐서 확인해보자.<ul>
<li>에러가 뜬다면, gcc를 다시 설치해야한다.</li>
<li>간단한게 <code>sudo apt remove gcc</code>, 그 후 <code>sudo apt install gcc</code>를 수행해서 gcc를 다시 설치한다.</li>
<li>Alacritty build는 <code>cargo clean</code>으로 클린 빌드를하고, 다시 <code>cargo build --release</code>로 빌드하자. 내 경우는 gcc를 다시 깔고 클린빌드 후 다시 빌드하니 해결되었다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>폰트 설정을 기대해서 JetBrains Mono 폰트를 써보려고 했는데 깨진다 :(</p>
<p> </p>
<hr>
<h2 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29obXl6c2gvb2hteXpzaC93aWtpL0luc3RhbGxpbmctWlNI">zsh<i class="fa fa-external-link-alt"></i></span>는 Ubuntu Linux에서 기본으로 제공하는 bash 쉘의 기능을 포함하여 편리한 기능이 여러가지 추가되어 있는 Shell 환경이다.</p>
<p><code>sudo apt install zsh</code>로 설치하였다.<br>그 후, 기본 shell을 zsh로 변경하기 위해 <code>chsh -s $(which zsh)</code>를 수행했다.</p>
<p>보통 많이들 <span class="exturl" data-url="aHR0cHM6Ly9vaG15ei5zaC8=">‘oh-my-zsh’<i class="fa fa-external-link-alt"></i></span> 많이 설치하는데, 나는 다른 zsh 프레임워크를 설치했다. 이유는 다음 섹션에서 설명한다.</p>
<p> </p>
<hr>
<h2 id="prezto"><a href="#prezto" class="headerlink" title="prezto"></a>prezto</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvcmluLWlvbmVzY3UvcHJlenRv">prezto<i class="fa fa-external-link-alt"></i></span>는 zsh 프레임워크이다.</p>
<p>oh-my-zsh를 쓰지않고 prezto를 사용하게 된 것은 순전히 <a href="">Missing Semesters 강의</a> 때문이다 ㅋㅋ 유명한 프레임워크인만큼 거의 대부분의 기능들은 oh-my-zsh와 비슷하게 제공될 것이라고 생각했고, 대신 디자인이 꽤 맘에 들었기 때문이다.</p>
<p>module은 아직 많이 깔지는 않았다.<br>아마 아래의 것들을 사용하지않을까 싶다</p>
<ul>
<li>autosuggestions</li>
<li>docker</li>
<li>environment</li>
<li>git</li>
<li>history</li>
<li>python</li>
<li>ssh</li>
<li>tmux</li>
</ul>
<p> </p>
<hr>
<h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RtdXgvdG11eC93aWtp">tmux<i class="fa fa-external-link-alt"></i></span>는 터미널 창을 여러개로 쪼개서 쓸 수 있게 해준다.</p>
<p>사실 아직 많이 익숙하지는 않다.<br>튜토리얼을 보고 익숙해지려는 중</p>
<p> </p>
<hr>
<h2 id="Neovim"><a href="#Neovim" class="headerlink" title="Neovim"></a>Neovim</h2><p>도입할지 말지 고민중인 에디터.<br>vscode를 버려야 할 이유가 있을까?<br>굳이 버리는건 아닌가? 흐음</p>
<p> </p>
<hr>
<h2 id="결과물"><a href="#결과물" class="headerlink" title="결과물"></a>결과물</h2><img src="/20210326-alacritty-tmux-zsh-prezto/result.png" class="" title="result">
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>Terminal</tag>
        <tag>zsh</tag>
        <tag>Alacritty</tag>
        <tag>prezto</tag>
        <tag>vim</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux tmux 설정 방법 (쉽게 쓰기)</title>
    <url>/20210326-tmux/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/0eCHCrYMQIw?t=514" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h2 id="설정"><a href="#설정" class="headerlink" title="설정"></a>설정</h2><p>위 영상을 참고해서 tmux 를 설정했다.<br>사용한 <code>.tmux.conf</code>는 다음과 같다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split windows like vim</span></span><br><span class="line"><span class="comment"># vim's definition of a horizontal/vertical split is reversed from tmux's</span></span><br><span class="line"><span class="built_in">bind</span> s split-window -v</span><br><span class="line"><span class="built_in">bind</span> v split-window -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># move around panes with hjkl, as one would in vim after pressing ctrl-w</span></span><br><span class="line"><span class="built_in">bind</span> h run-shell -b <span class="string">'tmux-keep-zoom L'</span></span><br><span class="line"><span class="built_in">bind</span> j run-shell -b <span class="string">'tmux-keep-zoom D'</span></span><br><span class="line"><span class="built_in">bind</span> k run-shell -b <span class="string">'tmux-keep-zoom U'</span></span><br><span class="line"><span class="built_in">bind</span> l run-shell -b <span class="string">'tmux-keep-zoom R'</span></span><br><span class="line">unbind z</span><br><span class="line"><span class="built_in">bind</span> z run-shell -b <span class="string">'tmux-resize-screen'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vi-style controls for copy mode</span></span><br><span class="line">setw -g mode-keys vi</span><br><span class="line"><span class="built_in">set</span> -g base-index 1</span><br><span class="line">setw -g pane-base-index 1</span><br><span class="line">set-option -g prefix C-a</span><br><span class="line">unbind-key C-b</span><br><span class="line">bind-key C-a send-prefix</span><br><span class="line">setw -g mode-keys vi</span><br><span class="line"><span class="built_in">set</span> -g display-time 1</span><br></pre></td></tr></table></figure>

<ul>
<li><code>ctrl + a</code> + <code>v</code>를 누르면 화면이 좌우로 쪼개진다</li>
<li><code>ctrl + a</code> + <code>h</code>를 누르면 화면이 상하로 쪼개진다</li>
<li>Vim처럼 <code>ctrl + a</code> + <code>hjkl</code>을 사용해서 pane간 이동을 할 수 있다.</li>
<li>Session간 컨트롤은 다음과 같이 할 수 있다.<ul>
<li><code>tmux new -s &lt;session name&gt;</code>을 사용해서 새 session을 만들 수 있다.</li>
<li><code>tmux ls</code>를 사용해 모든 session들을 볼 수 있다.</li>
<li><code>exit</code>을 사용해 하나의 session을 종료할 수 있다.</li>
<li><code>tmux kill-server</code>로 모든 session을 종료할 수 있다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>tmux</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>MLOps - From Model-centric to Data-centric AI (Andrew Ng 발표)</title>
    <url>/20210327-Andrew-Ng-MLOps-talk/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/06-AZXmwHjo" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<p>(Andrew Ng 교수님 최고… 상냥보스…)</p>
<h2 id="How-to-improve-AI-systems"><a href="#How-to-improve-AI-systems" class="headerlink" title="How to improve AI systems"></a>How to improve AI systems</h2><ul>
<li>AI systems = Code (Model) + Data<ul>
<li>Model도 중요하고 Data도 중요하다.</li>
</ul>
</li>
<li>성능을 높이려면 두가지 방법이 있다.<ul>
<li><ol>
<li>Model을 개선하기</li>
</ol>
<ul>
<li>SOTA 모델을 사용하기</li>
<li>Hyperparameter 튜닝</li>
</ul>
</li>
<li><ol start="2">
<li>Data를 개선하기</li>
</ol>
<ul>
<li>데이터 품질 개선하기</li>
</ul>
</li>
<li>Baseline 모델이 있을 때, 성능을 더 높여야한다면 어떤 방법을 먼저 사용하겠는가?<ul>
<li>교수님 팀의 실험으로는 <strong>Data를 개선하는 것이 제일 효과적이였다고 한다</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>


<p> </p>
<hr>
<h2 id="Data-is-food-for-AI"><a href="#Data-is-food-for-AI" class="headerlink" title="Data is food for AI"></a>Data is food for AI</h2><ul>
<li>데이터 준비 과정 vs 데이터 사용 과정 (80:20)<ul>
<li>데이터는 준비하는 과정이 훨씬 더 오래 걸린다.</li>
<li>모델이 데이터를 직접적으로 사용하는 부분은 전체 프로세스에서 굉장히 작은 부분이다.<ul>
<li>하지만 99%의 AI연구는 이 분야에 집중한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Lifecycle-of-an-ML-project"><a href="#Lifecycle-of-an-ML-project" class="headerlink" title="Lifecycle of an ML project"></a>Lifecycle of an ML project</h2>

<ul>
<li>Scope project<ul>
<li>문제를 구체화한다.</li>
</ul>
</li>
<li>Collect data<ul>
<li>데이터의 label이 제대로 되어있는가?<ul>
<li>데이터 label이 정확한지 어떻게 알 수 있는가? 어떻게 평가할 것인가? </li>
<li>어떤 특정 labelling 방식을 정하고, 모든 데이터셋에서 이 방식을 유지해야한다. 그렇지 않으면 데이터에 노이즈가 생긴다.</li>
</ul>
</li>
<li><strong>데이터 품질을 일정하게 유지</strong>해야한다.<ul>
<li>이것은 <strong>systematic하게 만들자! == MLOps</strong></li>
<li>데이터 labelling을 해주는 인력에게 샘플을 요구하자. Consistency를 확인해야한다.</li>
</ul>
</li>
</ul>
</li>
<li>Train data<ul>
<li>트레이닝</li>
<li>Error analysis</li>
<li>개선점 찾기<ul>
<li>Model-centric? Data-centric?</li>
</ul>
</li>
</ul>
</li>
<li>Deploy in production<ul>
<li>Systematically check for concept drift / data drift<ul>
<li>고객이 원하는 수준은 항상 높아짐</li>
</ul>
</li>
<li>Flow data back to retrain / update model regularly</li>
</ul>
</li>
</ul>
<img src="/20210327-Andrew-Ng-MLOps-talk/label.png" class="" title="Label">
<img src="/20210327-Andrew-Ng-MLOps-talk/label2.png" class="" title="Label2">

<p> </p>
<hr>
<h2 id="Model-centric-vs-Data-centric"><a href="#Model-centric-vs-Data-centric" class="headerlink" title="Model-centric vs Data-centric"></a>Model-centric vs Data-centric</h2><ul>
<li><strong>Model-centric view</strong><ul>
<li>데이터는 모을 수 있는 만큼 모은다.</li>
<li>데이터 내부에 있는 노이즈에 잘 대응할 수 있는 모델을 만든다.<ul>
<li>데이터는 고정하고, 지속적으로 모델을 개선해나간다.</li>
</ul>
</li>
<li>모델 튜닝을 어떻게 해야 더 좋은 성능을 얻을 수 있을까?</li>
</ul>
</li>
<li><strong>Data-centric view</strong><ul>
<li>여러 툴을 이용해서 <strong>데이터 품질을 높인다</strong>.</li>
<li>이 경우, <strong>여러 모델을 동일한 데이터로 학습</strong>할 수 있다.<ul>
<li>대신 모델을 고정하고, 데이터를 지속적으로 개선한다.</li>
</ul>
</li>
<li>데이터를 어떻게 개선해야 (e.g. 데이터 추가, augmentation, labelling) 더 좋은 성능을 얻을 수 있을까?<ul>
<li>Augmentation, 데이터 추가 - Input x를 바꾸는 방법</li>
<li>Give more consistent definition for labels - Labels y를 바꾸는 방법  </li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Clean-vs-Noisy-data"><a href="#Clean-vs-Noisy-data" class="headerlink" title="Clean vs Noisy data"></a>Clean vs Noisy data</h2><ul>
<li>Clean + Consistent한 데이터는 중요하다!</li>
</ul>
<img src="/20210327-Andrew-Ng-MLOps-talk/clean.png" class="" title="Clean">

<blockquote>
<p>예시 1: 500개의 데이터가 있는데, 그 중 12%는 노이즈가 끼어있다. 이 때 뉴럴넷의 성능을 높이는 방법에는 두가지가 있다: 1. 노이즈를 제거하기, 2. 새로운 데이터 500개를 얻어낸다 (i.e. 트레이닝 데이터를 2배로 늘린다). 어떤 방식이 더 좋을까?</p>
</blockquote>
<p>데이터셋에서 노이즈를 제거함으로써 얻는 이득을 트레이닝 데이터를 키우는 방식으로 얻어내려면, 거의 3배의 데이터를 모아야한다. </p>
<p>이미 큰 데이터셋을 쓰고 있다면… 더 많은 데이터를 모으기는 쉽지 않다 :)</p>
<p>작은 데이터셋을 쓰고 있다면 데이터 품질과 툴은 더욱더 중요해진다.</p>
<img src="/20210327-Andrew-Ng-MLOps-talk/cleannoisy.png" class="" title="Clean vs noisy">

<p> </p>
<hr>
<h2 id="MLOps"><a href="#MLOps" class="headerlink" title="MLOps"></a>MLOps</h2><img src="/20210327-Andrew-Ng-MLOps-talk/mlops.png" class="" title="mlops">

<ul>
<li>AI software has a more iterative development process, compared to the traditional software development.</li>
</ul>
<p> </p>
<hr>
<blockquote>
<p>From ‘big data’ to ‘good data’</p>
</blockquote>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>MLOps</tag>
        <tag>Andrew Ng</tag>
      </tags>
  </entry>
  <entry>
    <title>로봇미래전략컨퍼런스 2021 - 로봇 대항해 시대 (김상배 교수님 발표)</title>
    <url>/20210327-RFSC-Kim-The-Age-of-Robot-Exploration/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/CfKTEqzPDls?t=4610" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p>1:18:00 시작.</p>
<p> </p>
<hr>
<h2 id="4차-산업혁명과-로봇"><a href="#4차-산업혁명과-로봇" class="headerlink" title="4차 산업혁명과 로봇"></a>4차 산업혁명과 로봇</h2><ul>
<li>1차 산업혁명 - 증기기관 (동력)</li>
<li>2차 산업혁명 - 전기 (어디서든 쓸 수 있는 에너지, 동력)</li>
<li>3차 산업혁명 - 컴퓨터, 인터넷 (단순 정신노동자동화, 수많은 양의 정보)</li>
<li>4차 산업혁명 - 지능, 정보, 동력의 조합? (로봇 -&gt; 복잡한 노동 자동화)</li>
</ul>
<p> </p>
<hr>
<h2 id="3차-산업혁명-vs-4차-산업혁명"><a href="#3차-산업혁명-vs-4차-산업혁명" class="headerlink" title="3차 산업혁명 vs 4차 산업혁명"></a>3차 산업혁명 vs 4차 산업혁명</h2><ul>
<li>3차 산업혁명은 정보에 집중<ul>
<li>스마트폰 앱 등은 정보를 효율적으로 보낼 수 있는 매체</li>
</ul>
</li>
<li>4차 산업혁명은 물리적인 서비스에 집중<ul>
<li>고령화 사회?</li>
<li>3차 산업혁명에서도 물리적인 서비스를 했다.<ul>
<li>다만 엄청 단순하게 함.</li>
<li>공장형 로봇<ul>
<li>굉장히 정밀하고 정확하고 효율적이다.</li>
<li>하지만 localization만 잘 하는 것일 뿐…<ul>
<li>운동지능이 없음 (Dynamic physical interaction)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="지금까지의-로봇"><a href="#지금까지의-로봇" class="headerlink" title="지금까지의 로봇"></a>지금까지의 로봇</h2><ul>
<li>공장용 로봇<ul>
<li>Not designed to touch</li>
<li>Collision이 있으면 안됨 (충격 흡수도 잘 못함)</li>
<li>바닥에 고정되어있음</li>
<li>정밀한 동작만 하면 잘 작동함</li>
</ul>
</li>
<li>실제 환경<ul>
<li>로봇이 움직임<ul>
<li>균형잡기?</li>
</ul>
</li>
<li>주변 환경이 복잡하고 계속 변화함<ul>
<li>공장용 로봇에 적용된 방식으로는 커버할 수 없음.</li>
</ul>
</li>
<li>정밀한 위치제어 + 힘조절이 필요.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Proprioceptive-actuation"><a href="#Proprioceptive-actuation" class="headerlink" title="Proprioceptive actuation"></a>Proprioceptive actuation</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/_Yxy8HQVkhg" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<ul>
<li>공장로봇처럼 딱딱하지 않고, 충격을 훕수할 수 있음</li>
<li>4족 로봇이 움직이는데에는 이런 방식이 필수적임<ul>
<li>뛰어다니고 걸어다니는 로봇에는 이 기술이 필수</li>
<li>MIT Cheetah<ul>
<li>Boston dynamics에서는 유압으로 구현되던것이 Cheetah 로봇에서는 전기모터로 구현됨<ul>
<li>유압은 가격 + Maintenance + 소형화 문제 등이 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="MIT-Cheetahs"><a href="#MIT-Cheetahs" class="headerlink" title="MIT Cheetahs"></a>MIT Cheetahs</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/QZ1DaQgg3lE" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/xNeZWP5Mx9s" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<ul>
<li>로봇의 발이 땅에 닿아있을때는 힘조절을 함</li>
<li>로봇의 발이 땅에서 떨어졌을때는 위치조절을 함</li>
</ul>
<h2 id="Teleoperation-robot"><a href="#Teleoperation-robot" class="headerlink" title="Teleoperation robot"></a>Teleoperation robot</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/p8ozov_xymM" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/IUXHKhB8FZE" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<ul>
<li>사람이 갈 수 없는 곳에 로봇을 대신 투입</li>
<li>Virtual magnet<ul>
<li> “좋은 센서와 하드웨어가 만나면 간단한 task는 굉장히 쉽게 해결할 수 있다”.</li>
</ul>
</li>
</ul>
<h2 id="인공지능에-대한-오류"><a href="#인공지능에-대한-오류" class="headerlink" title="인공지능에 대한 오류"></a>인공지능에 대한 오류</h2><ul>
<li>사람은 기술의 난이도를 사람의 능력에 빗대서 판단한다.<ul>
<li>미니치타로 백플립하는거보다 제자리에서 걷는게 100배 더 어려움</li>
</ul>
</li>
<li>사람은 로봇을 볼 때도 의인화를 하게 프로그램 되어있다.</li>
<li>우리의 사고는 우리의 언어에 강력하게 연결되어있고 모든 기술에 관한 의사소통은 언어로 한다.<ul>
<li>물리적인 지능에 대한 이해도가 부족.</li>
<li>e.g. 물리적인 지능은 보통 무의식에서 일어난다? 우리가 생각하면서 젓가락질을 바르지는 않는다. </li>
</ul>
</li>
</ul>
<blockquote>
<p>대부분의 사람이 간과하는 부분의 기술을 개발하면 앞서나갈수있다.</p>
</blockquote>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.4 Robotics</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>MIT</tag>
        <tag>Prof Sangbae Kim</tag>
      </tags>
  </entry>
  <entry>
    <title>Professional C++ - C++의 객체지향 언어 특성 소개 + 프로그램 만들기 (Ch1, Part 3-6)</title>
    <url>/20210328-cpp-basics-3/</url>
    <content><![CDATA[<h2 id="1-3-1-클래스-정의"><a href="#1-3-1-클래스-정의" class="headerlink" title="1.3.1 클래스 정의"></a>1.3.1 클래스 정의</h2><ul>
<li>Class: 객체의 특성을 정의<ul>
<li>Class 선언 코드는 <strong>헤더파일</strong> (.h) 파일에 작성</li>
<li>Class 구현 코드는 <strong>소스파일</strong> (.cpp, .cc) 파일에 작성</li>
</ul>
</li>
<li><strong>Data member</strong>와 <strong>method</strong> 구현 필요<ul>
<li><strong>public</strong>, <strong>protected</strong>, <strong>private</strong>으로 구분됨.<ul>
<li>public: 클래스 외부에서 접근 가능</li>
<li>protected: 클래스 상속 관계시 접근 가능 </li>
<li>private: 클래스 내부에서만 접근 가능<ul>
<li>외부에서 접근하게 하려면 getter / setter 함수를 만들면 됨.</li>
</ul>
</li>
</ul>
</li>
<li>Data member는 보통 <code>m</code>을 앞에 붙여서 표시함. (data <strong>m</strong>ember)</li>
</ul>
</li>
<li>Class를 생성하는 <strong>생성자</strong> (constructor)가 필요.<ul>
<li><strong>default constructor</strong>, <strong>copy constructor</strong>, <strong>parameterized constructor</strong>가 있음.<ul>
<li>default: 기본 생성자</li>
<li>copy constructor: 동일한 class의 다른 instance의 내용을 기반으로 새로운 instance를 만드는 것. 이 때, 포인터의 값들은 깊은 복사가 되야한다. (<span class="exturl" data-url="aHR0cHM6Ly9qYWVodW4yODQxLmdpdGh1Yi5pby8yMDE5LzAxLzEzL2phdmEtb2JqZWN0LWNvcHkv">참조 링크<i class="fa fa-external-link-alt"></i></span>)</li>
<li>parameterized constructor: 특정한 방식으로 class를 초기화 하는 방법</li>
</ul>
</li>
<li><strong>member initialiser list를 쓰는것이 깔끔하고 보기 좋음</strong>.</li>
</ul>
</li>
<li>Class를 파괴하는 <strong>소멸자</strong> (destructor)도 필요.<ul>
<li>Raw pointer를 사용했으면 destructor에서 포인터 해제를 해줘야함.</li>
<li>외부 API를 통해서 포인터 작업을 했으면 그것 역시 해제를 해줘야함 (e.g. OpenGL context, GPU 메모리…)</li>
<li>이런게 없으면 안적어도 됨.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">someClass</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> VecI = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;;</span><br><span class="line"></span><br><span class="line">    someClass() = <span class="keyword">default</span>; <span class="comment">// default constructor. 안 적어도 되지만, default를 적어주면 좀 더 확실하게 표현 가능.</span></span><br><span class="line">    someClass(<span class="keyword">const</span> someClass&amp; some1) { mPtrInt = <span class="keyword">new</span> <span class="keyword">int</span>(*some1.mPtrInt); }; <span class="comment">// Copy constructor. 포인터의 deep copy를 함</span></span><br><span class="line">    someClass(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> str; <span class="keyword">const</span> VecI&amp; vec): mStr(str), mVecInt(vec) {}; <span class="comment">// parameterised constructor + member initialiser list</span></span><br><span class="line"></span><br><span class="line">    ~someClass() { <span class="keyword">delete</span> mPtrInt; };</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">foo2</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">foo3</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setStr</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; inputStr)</span> </span>{ mStr = inputStr; };</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setVecInt</span><span class="params">(<span class="keyword">const</span> VecI&amp; inputVec)</span> </span>{ mVecInt = inputVec; };</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">getStr</span><span class="params">()</span> </span>{ <span class="keyword">return</span> mStr } <span class="keyword">const</span>;</span><br><span class="line">    <span class="function">VecI <span class="title">getVecInt</span><span class="params">()</span> </span>{ <span class="keyword">return</span> mVecInt } <span class="keyword">const</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> mStr;</span><br><span class="line">    VecI mVecInt;</span><br><span class="line">    <span class="keyword">int</span>* mPtrInt;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-3-2-클래스-사용하기"><a href="#1-3-2-클래스-사용하기" class="headerlink" title="1.3.2 클래스 사용하기"></a>1.3.2 클래스 사용하기</h2><ul>
<li>Class의 instance는 두가지 방법으로 만들 수 있음<ul>
<li>Stack</li>
<li>Heap</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">someClass</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="comment">// 위에 있던거...</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// Stack 기반</span></span><br><span class="line">    <span class="keyword">auto</span> instance = someClass();</span><br><span class="line">    <span class="keyword">auto</span> str = instance.getStr();</span><br><span class="line">    <span class="keyword">auto</span> vec = instance.getVecInt();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Heap 기반</span></span><br><span class="line">    <span class="keyword">auto</span> instance_p = <span class="built_in">std</span>::make_unique&lt;someClass&gt;();</span><br><span class="line">    <span class="keyword">auto</span> str2 = instance_p-&gt;getStr();</span><br><span class="line">    <span class="keyword">auto</span> vec2 = instance_p-&gt;getVecInt();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 제발 쓰지 말았으면 하는 Heap 기반 방식</span></span><br><span class="line">    <span class="keyword">auto</span> instance_NEVER_DO_THIS = <span class="keyword">new</span> someClass();</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    <span class="keyword">delete</span> instance_NEVER_DO_THIS;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-4-Uniform-초기화"><a href="#1-4-Uniform-초기화" class="headerlink" title="1.4 Uniform 초기화"></a>1.4 Uniform 초기화</h2><ul>
<li>종종 실수로 함수에 변수가 들어가면서 타입이 축소화 될 때가 있다.<ul>
<li>e.g. double -&gt; int</li>
<li>이 경우에는 정보가 손실되고, 컴파일러 에러 없이 예기치 못한 에러가 생길 수 있다. (최신 컴파일러는 왠만하면 경고 주는듯)</li>
</ul>
</li>
<li>안전하게 하는 방법은 uniform initalization이다.<ul>
<li>Class 초기화를 할 때 유용하다.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">int</span> i )</span></span>{ <span class="comment">/*... */</span> };</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> pi = <span class="number">3.141592654</span>;</span><br><span class="line">    foo(<span class="number">3.14</span>); <span class="comment">// 3.14가 3으로 줄어들을 수 있음</span></span><br><span class="line">    foo({<span class="number">3.14</span>}); <span class="comment">// 3으로 줄어들지 않고 컴파일러 에러를 뱉음</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>Professional C++ 책</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Professional C++ - C++ 고급 기능 (?) (Ch1, Part 2)</title>
    <url>/20210328-cpp-basics2/</url>
    <content><![CDATA[<h2 id="1-2-2-C-String"><a href="#1-2-2-C-String" class="headerlink" title="1.2.2 C++ String"></a>1.2.2 C++ String</h2><p> </p>
<h2 id="1-2-2-Pointer-Dynamic-memory-allocation"><a href="#1-2-2-Pointer-Dynamic-memory-allocation" class="headerlink" title="1.2.2 Pointer + Dynamic memory allocation"></a>1.2.2 Pointer + Dynamic memory allocation</h2><ul>
<li><strong>Stack</strong><ul>
<li>함수마다 독립적인 메모리 공간 제공</li>
<li>함수가 끝나면 자동으로 메모리 할당 해제</li>
</ul>
</li>
<li><strong>Heap</strong><ul>
<li>Stack과 독립적인 메모리 공간</li>
<li>함수가 끝나도 계속 저장하고 싶으면 Heap에 저장</li>
<li>스마트 포인터가 아닌 이상 자동으로 할당 해제 불가능</li>
</ul>
</li>
<li>Raw pointer<ul>
<li>C에서 사용하는 malloc/free 대신 new/delete 사용.</li>
<li>개인적으로 코딩테스트용 아니면 잘 안쓰는 편…</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 기본 구조</span></span><br><span class="line"><span class="keyword">int</span>* intPointer;</span><br><span class="line"><span class="keyword">int</span>* intPointer2 = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">auto</span> intPointer = <span class="keyword">new</span> <span class="keyword">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 동적 배열 할당</span></span><br><span class="line"><span class="keyword">int</span> arraySize = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span>* myVariableSizedARray = <span class="keyword">new</span> <span class="keyword">int</span>[ArraySize];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 참조 + 역참조</span></span><br><span class="line"><span class="keyword">int</span> i = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">int</span>* intPointer = &amp;i;</span><br><span class="line">*intPointer = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 구조체 리턴</span></span><br><span class="line">Employee* employee = getEmployee(); </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; (*employee).salary &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">Employee* employee = getEmployee(); </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; employee-&gt;salary &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">// 이 방식이 좀 더 읽기 쉬운듯</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Smart pointer</strong><ul>
<li><strong>std::unique_ptr</strong><ul>
<li>scope를 벗어나거나 삭제되면 메모리 자동 해제<ul>
<li>return, 또는 exception에서 이득</li>
</ul>
</li>
</ul>
</li>
<li><strong>std::shared_ptr</strong><ul>
<li>Reference counter가 0이 되면 메모리 해제</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 기본</span></span><br><span class="line"><span class="keyword">auto</span> employee_unique = <span class="built_in">std</span>::make_unique&lt;Employee&gt;(); <span class="comment">// unique_ptr</span></span><br><span class="line"><span class="keyword">auto</span> employee_shared = <span class="built_in">std</span>::make_shared&lt;Employee&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 배열</span></span><br><span class="line"><span class="keyword">auto</span> employees = <span class="built_in">std</span>::make_unique&lt;Employee[]&gt;(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 구조체 멤버 변수 호출</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; employee-&gt;salary &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">// raw pointer랑 동일</span></span><br></pre></td></tr></table></figure>

<h2 id="1-2-3-const"><a href="#1-2-3-const" class="headerlink" title="1.2.3 const"></a>1.2.3 const</h2><ul>
<li>C에서는 상수 값을 <code>#define</code>으로 사용했었음.</li>
<li>C++에서는 <code>const</code>를 사용함.<ul>
<li><code>#define</code>은 보통 하드웨어 또는 빌드 configuration이 아닌 이상 잘 안씀.</li>
</ul>
</li>
<li>값이 변경되지 않도록 보장하는 작업.<ul>
<li>실수로 변경하는 경우만 보호해줌.</li>
</ul>
</li>
<li>함수 매개변수로 const를 사용해서, 함수가 실행되는 도중 정보가 의도치 않게 변경되는 것으로부터 보호함.</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> kPi = <span class="number">3.141592654</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> path = <span class="string">"../../file.exe"</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; someString)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  someString = <span class="string">"New string!"</span>; <span class="comment">// 여기서 컴파일 에러 발생</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-2-4-레퍼런스"><a href="#1-2-4-레퍼런스" class="headerlink" title="1.2.4 레퍼런스"></a>1.2.4 레퍼런스</h2><ul>
<li>메모리를 새로 할당하지 않고 기존 변수에 새 이름을 지정할 수 있음.<ul>
<li><code>int x = 100; int&amp; refVariable = x;</code></li>
</ul>
</li>
<li>‘<strong>Pass by reference</strong>‘는 함수를 만들 때 고려해야하는 중요한 포인트임.<ul>
<li>일반적으로 함수에 변수를 전달할 때는 pass by value를 사용함.<ul>
<li>이 방식은 기존 변수를 copy한 후에 함수에 전달 함.<ul>
<li>원본이 손상되지 않음</li>
</ul>
</li>
<li>즉, copy에 대한 계산이 필요없어짐.</li>
<li>또 원본을 수정할 수 있음.</li>
</ul>
</li>
</ul>
</li>
<li>종종 큰 구조체를 넘기는 경우 1. 원본을 수정하고 싶지 않으면서, 2. 복사에 시간이 너무 오래걸리는 경우가 있음.<ul>
<li>그럴 때는 ‘<strong>Pass by const reference</strong>‘를 사용하면 됨.</li>
<li>복사를 하지 않고 reference를 넘기면서, const로 보호처리를 하는 것임.</li>
<li>container의 형태를 하고 있는 것들은 왠만하면 다 reference를 걸고 넘기는 것이 좋음.<ul>
<li>primitives는 그냥 넘겨도 무방함.</li>
<li><strong>이미지</strong> 같이 큰 구조체를 넘긴다? <strong>백프로 reference걸고 넘겨야함</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">passByValue</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  ... <span class="comment">// i, j, k의 복사본이 들어옴.</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">passByReference</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span>&amp; inputString)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  inputString = <span class="string">"String updated"</span>; <span class="comment">// 원본 inputString의 값이 업데이트 됨</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">passByConstReference</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; inputString)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  inputString = <span class="string">"String updated?"</span>; <span class="comment">// 컴파일러 에러</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-2-5-Exception"><a href="#1-2-5-Exception" class="headerlink" title="1.2.5 Exception"></a>1.2.5 Exception</h2><ul>
<li>C++는 잘못된 메모리에 접근해도 에러를 내지 않는다.</li>
<li>이런 의도치 않은 실수를 예방하기 위해 exception을 사용한다<ul>
<li>Exception을 사용해서 프로그램이 터질 때 버그리포트를 만들게 할 수 있다.</li>
</ul>
</li>
<li><code>throw</code>, 또는 <code>try</code> &amp; <code>catch</code>가 많이 사용된다.<ul>
<li>하지만 이걸 쓰지 않고 <code>assert</code>로 통일해서 쓰는 회사들도 있다.</li>
<li>assert는 debug 모드일 때 버그를 확인할 수 있다.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">processImage</span><span class="params">(<span class="keyword">const</span> Image&amp; img)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">if</span> (img.empty())</span><br><span class="line">    <span class="keyword">throw</span> <span class="built_in">std</span>::invalid_arguemnt(<span class="string">"Input image is empty. Cannot process empty images"</span>);</span><br><span class="line"></span><br><span class="line">  ... <span class="comment">// some image processing</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span></span><br><span class="line">{</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; foo(<span class="number">0</span>,<span class="number">0</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; </span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; foo(<span class="number">10</span>,<span class="number">0</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; </span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; foo(<span class="number">100</span>,<span class="number">0</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; </span><br><span class="line">}</span><br><span class="line"><span class="keyword">catch</span> (<span class="keyword">const</span> <span class="built_in">std</span>::invalid_argument&amp; exception)</span><br><span class="line">{</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Exception"</span> &lt;&lt; exception.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="1-2-6-Type-inferencing"><a href="#1-2-6-Type-inferencing" class="headerlink" title="1.2.6 Type inferencing"></a>1.2.6 Type inferencing</h2><ul>
<li><code>auto</code> 키워드<ul>
<li>어디서 쓰나?<ul>
<li>자동으로 함수의 리턴 타입 추론</li>
<li>Structural binding</li>
<li>lambda 함수 등에서 쓰임</li>
</ul>
</li>
<li>되도록이면 auto 쓰기!<ul>
<li>처음에 type이 어떤 형태인지 명시적으로 표현할 때만 제대로 표현해주고, 그 후에는 변수 이름을 적당히 만들어줘서 모두 auto로 돌리는게 좋다.</li>
<li>개인적으로 함수를 만들 때 리턴 타입에는 auto를 안쓰는 편.</li>
<li><code>std::make_unique&lt;&gt;()</code>나 <code>std::make_shared&lt;&gt;()</code>를 쓸 때 좌변에는 무조건 auto를 쓰는 편. 왜냐하면 오른쪽에 어차피 타입이 적혀있기 때문.<ul>
<li>가독성 부스트</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><code>decltype</code> 키워드<ul>
<li>인수로 지정한 표현식의 타입을 알아냄.</li>
<li>const를 삭제하지 않음.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">foo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> str = <span class="string">"some string"</span>;</span><br><span class="line">  <span class="keyword">return</span> str;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">constFoo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> str = <span class="string">"some const string"</span>;</span><br><span class="line">  <span class="keyword">return</span> str;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="comment">// auto 쓰기</span></span><br><span class="line">  <span class="keyword">auto</span> str1 = foo();</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">auto</span> str2 = constFoo(); <span class="comment">// 여기서 const 중요!! const를 return하는데 그냥 auto만 써버리면 copy가 일어남.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// decltype</span></span><br><span class="line">  <span class="keyword">int</span> x = <span class="number">123</span>;</span><br><span class="line">  <span class="keyword">decltype</span>(x) y = <span class="number">456</span>; <span class="comment">// x의 타입 가져오기</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">decltype</span>(constFoo()) str3 = constFoo(); <span class="comment">// const std::string 리턴 타입을 전부 추론함.</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>Professional C++ 책</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Professional C++ - C++ 컨테이너 개요 (Ch17, Part 1)</title>
    <url>/20210328-cpp-containers-iterators/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>STL에 있는 컨테이너를 이용하면 C 스타일 배열을 사용하지 않아도 된다.</li>
<li>아래와 같은 컨테이너가 있다.<ul>
<li>순차 컨테이너<ul>
<li><code>std::vector</code></li>
<li><code>std::deque</code></li>
<li><code>std::list</code></li>
<li><code>std::forward_list</code></li>
<li><code>std::array</code></li>
</ul>
</li>
<li>연관 컨테이너<ul>
<li><code>std::map</code></li>
<li><code>std::multimap</code></li>
<li><code>std::set</code></li>
<li><code>std::multiset</code></li>
</ul>
</li>
<li>비정렬 연관 컨테이너 (Hash table)<ul>
<li><code>std::unordered_map</code></li>
<li><code>std::unordered_multimap</code></li>
<li><code>std::unordered_set</code></li>
<li><code>std::unordered_multiset</code></li>
</ul>
</li>
<li>컨테이너 어댑터<ul>
<li><code>std::queue</code></li>
<li><code>std::priority_queue</code></li>
<li><code>std::stack</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="17-1-1-원소에-대한-요구사항"><a href="#17-1-1-원소에-대한-요구사항" class="headerlink" title="17.1.1 원소에 대한 요구사항"></a>17.1.1 원소에 대한 요구사항</h2><ul>
<li>STL 컨테이너 특징<ul>
<li>STL 컨테이너는 기본적으로 value semantics (i.e. pass by value)를 통해 원소를 생성한다.<ul>
<li>즉, 원소의 복제본을 저장한다.</li>
</ul>
</li>
<li>원소에 대한 레퍼런스나 포인터를 사용할 수도 있다. (i.e. reference semantics, pass by pointer)<ul>
<li>컨테이너에 <code>std::reference_wrapper</code>를 사용해도 좋다.</li>
<li><code>std::unique_ptr</code> 또는 <code>std::shared_ptr</code>를 쓰는 것이 좋다.</li>
</ul>
</li>
</ul>
</li>
<li>Allocator<ul>
<li>STL 컨테이너는 allocator를 통해 원소에 대한 메모리를 할당하거나 해제할 수 있다.</li>
</ul>
</li>
<li>Comparator<ul>
<li><code>std::map</code>과 같은 일부 컨테이너는 comparator를 쓸 수 있다.</li>
</ul>
</li>
<li>Allocator + Comparator를 사용하는 컨테이너의 원소가 갖춰야할 요구사항은 다음과 같다.<ul>
<li>Default constructor</li>
<li>Copy constructor</li>
<li>Move constructor</li>
<li>Destructor</li>
<li>Assignment operator</li>
<li>Move assignment operator</li>
<li>Operator ==</li>
<li>Operator &lt;</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>Professional C++ 책</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 3월 SLAM 뉴스</title>
    <url>/20210329-mar-2021-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="이번-달-내가-관심가지는-논문들"><a href="#이번-달-내가-관심가지는-논문들" class="headerlink" title="이번 달 내가 관심가지는 논문들"></a>이번 달 내가 관심가지는 논문들</h2><details>
  <summary> iMAP: Implicit Mapping and Positioning in Real-Time </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDMuMTIzNTI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Andrew Davison 교수님 랩실 연구</li>
<li>Multi-layer perceptron으로 scene representation을 표현?</li>
<li>Unobserved holes도 자동으로 채울 수 있다?</li>
</ul>
</details>

<details>
  <summary> Tangent Space Backpropagation for 3D Transformation Groups </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMTIwMzIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.007ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2213 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(935, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1324, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(1824, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.007ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2213 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(935, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1324, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mo" transform="translate(1824, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> space를 backprop 할 수 있다는건, Gauss-Newton / Levenberg-Marquardt와 같은 iterative weighted least squares 없이 한번에 좋은 값을 얻어낼 수 있도록 학습할 수 있다는 것일까?</li>
</ul>
</details>

<details>
  <summary> Avoiding Degeneracy for Monocular Visual SLAM with Point and Line Features </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDE1MDEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>명현 교수님 랩실</li>
<li>Point + Line을 이용하는건 꼭 봐야한다!</li>
</ul>
</details>

<details>
  <summary> Visual Place Recognition using LiDAR Intensity Information </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDk2MDUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Grisetti &amp; Stachniss 교수님 랩실</li>
<li>Visual localization 연구도 굉장히 오래됬고, LiDAR intensity 이미지도 쓴지 오래됬는데 이런 연구가 이제서야 나타나게 된건 왜 때문일까?</li>
</ul>
</details>

<details>
  <summary> Probabilistic two-stage detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDc0NjEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>CenterNet v2</li>
<li>Two-stage detector인데 이렇게 빠르다고? ㄷㄷ</li>
</ul>
</details>

<details>
  <summary> Back to the Feature: Learning Robust Camera Localization from Pixels to Pose </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDkyMTMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Paul-Edouard Sarlin의 새 논문.</li>
<li>Victor Larsson, Marc Pollefeys, Torsten Sattler, Vincent Lepetit등 유명 연구자 다수 참여. </li>
<li>현재 visual localization 연구의 최앞단이 아닐까?</li>
<li>이전의 HFNet의 경우는 사전에 만들어둔 SfM 모델에 대해 Global feature (e.g. NetVLAD)와 local feature (e.g. SuperPoint) 등을 이용해서 pose 값을 생성했다.<ul>
<li>이 연구에서는 robust feature를 아직 사용하지 못했다는 단점이 있었다.</li>
</ul>
</li>
<li>이번 연구에서는 사전에 만들어둔 SfM 모델에 대해 global feature를 사용해서 reference 이미지를 찾고, local feature는 multi-scale로 뽑아서 robust한 feature를 뽑아 셋을 전부 비교한다.<ul>
<li>비교하고나서 나온 loss를 기반으로 local feature를 뽑는 CNN 네트워크를 학습하는 것 같다.</li>
</ul>
</li>
<li>제대로 각잡고 읽어봐야지…</li>
</ul>
</details>

<details>
  <summary> Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDgyOTIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Tat-Jun Chin, Ian Reid, Anders Eriksson의 연구</li>
<li>Rotation averaging을 아직 실제로 써본 적이 많이 없어서 익숙하지 않다.</li>
<li>이 논문을 통해서 조금 익숙해질 필요가 있다.</li>
</ul>
</details>

<details>
  <summary> NeRF−−: Neural Radiance Fields Without Known Camera Parameters </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDcwNjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Victor Prisacariu 교수님 랩실</li>
<li>요즘 인기가 많은 NeRF 계열 논문이지만 조금 다른 맥락을 가지고 있다.<ul>
<li>기존의 방식들은 NeRF의 학습 과정 중 intrinsic / extrinsic 값들이 제공된다는 전제를 가지고 있다.</li>
<li>이번 논문은 RGB 이미지만을 가지고 NeRF 모델을 학습하고, joint estimation을 통해서 camera parameter를 구할 수 있다는 것을 보여준다.<ul>
<li>물론 이 camera parameter들을 가지고 다시 novel view를 만들 수 있다.</li>
<li>근데 Joint estimation…? 이거 완전 슬램인듯 ㄷㄷ</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Visual SLAM in Dynamic Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly96YWd1YW4udW5pemFyLmVzL3JlY29yZC8xMDA2NzIvZmlsZXMvVEVTSVMtMjAyMS0wODMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>DynaSLAM의 저자인 Berta Bescos의 박사 졸업 논문</li>
</ul>
</details>

<details>
  <summary> GVINS: Tightly Coupled GNSS-Visual-Inertial for Smooth and Consistent State Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDc4OTkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>VINS-Mono의 Shaojie Shen의 새 연구</li>
<li>Visual + Inertial + GNSS 라면 자율주행 및 실외 드론에서 쓰기 좋을 것 같다.</li>
</ul>
</details>

<details>
  <summary> Consensus Maximisation Using Influences of Monotone Boolean Functions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDQyMDAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Consensus Maximisation은 읽어야한다…! 이게 미래다 ㅋㅋ</li>
</ul>
</details>

<details>
  <summary> Self-supervised Geometric Perception </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDMuMDMxMTQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Luca Carlone + Vladlen Koltun</li>
<li>아직 감이 잘 잡히지 않는 논문. 하지만 이 두 교수님이 적으신거면 분명 뭔가 있다.</li>
</ul>
</details>

<details>
  <summary> Square Root Bundle Adjustment for Large-Scale Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDE4NDMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Usenko, Cremers 교수님</li>
</ul>
</details>


<details>
  <summary> Holistic 3D Scene Understanding from a Single Image with Implicit Representation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDMuMDY0MjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>모션 블러란? 모션 블러를 없애는 방법은?</title>
    <url>/20210329-motion-blur/</url>
    <content><![CDATA[<blockquote>
<p>“나는 배경 지식은 알고싶지 않아! 곧바로 결과를 보고싶어!” 라고 생각하신다면 스크롤을 쭉 내리셔서 4번 챕터의 비디오를 봐주세요! 스마트폰으로 블러 최소화 하는 방법도 밑으로 스크롤을 쭉 내려서 3번 챕터로 가주세요! :) </p>
</blockquote>
<p> </p>
<h2 id="모션-블러란"><a href="#모션-블러란" class="headerlink" title="모션 블러란?"></a>모션 블러란?</h2><p>모션 블러는 영상기기로 영상을 취득하면서, 피사체가 움직이면서 잔상이 남는 영상 아티팩트이다…라고 하면 어려우니까, 사진 찍을 때 <strong>움직여서 번지는걸 모션 블러</strong>라고 한다 :)</p>
<p>모션 블러는 기존의 사진 및 영상 촬영에서는 물체의 자연스러운 움직임을 표현하기 위해 꼭 들어가야하는 요소이다.</p>
<p>그러나, <strong>로보틱스 비전</strong>와 <strong>딥러닝 비전</strong> 등 영상을 분석해서 주위 환경에 대해 이해하고 정보를 추출하려고 하는 목적을 지닐 때에 <strong>모션 블러는 굉장히 골칫거리</strong>가 될 수 있다. 비-딥러닝 방식의 영상처리이던지, CNN 딥러닝의 컨벌루션 방식이라던지, 영상을 분석하기 위해서는 점과 선 등의 low-level feature를 조합해서 high-level 지식을 추출해야하는데, 모션 블러는 이 점과 선을 흐리게 하기 때문에 high-level 지식을 추출하기 더 어려워진다. 그러므로, <strong>모션 블러는 가능하면 최대한 피해야한다!</strong></p>
<img src="/20210329-motion-blur/Untitled.png" class="" title="goddamnit">

<p>디텍션 + 트랙킹을 수행할 때, 여러가지 급격한 이미지 변화를 나타내는 이유 중 몇개를 보여주는 이미지입니다. (d) Abrupt motions는 갑작스러운 움직임에 피사체 및 주위 환경이 흐려져 보이는 것을 뜻 하는데, 이는 모션 블러 효과도 포함된다고 볼 수 있습니다. </p>
<p>이 이미지는 ICCV 2015년 “Hierarchical Convolutional Features for Visual Tracking” 이라는 자료에서 발췌했습니다.</p>
<p> </p>
<hr>
<h2 id="모션-블러는-어떻게-지우는-것인가-소프트웨어"><a href="#모션-블러는-어떻게-지우는-것인가-소프트웨어" class="headerlink" title="모션 블러는 어떻게 지우는 것인가? (소프트웨어)"></a>모션 블러는 어떻게 지우는 것인가? (소프트웨어)</h2><p>모션 블러 효과를 지워주는 연구가 없던것은 아니다. 영상 속 픽셀들에 대해 수학적 방법으로 ‘이 픽셀은 모션 블러 효과를 받은 것인가?’를 판별해서, 블러 효과를 지워보려는 연구가 많이 있었다. 하지만 효과적으로 사용할 수 있는 방법은 결국 나타나지 않았다. </p>
<p>첫번째 이유는 <strong>정확도가 많이 높지 않았다</strong>. 모션 블러가 있는 이미지만을 가지고 모션 블러가 전혀 없는 깨끗한 이미지를 만들어내기는 수학적으로 쉽지 않다. 어떤 방법으로 모션 블러를 인식하고, 어떤 방법으로 지우는게 효과적이고, 어떤 방법으로 변환된 이미지에서 깨끗한 모양을 복원할까? 아무리 많은 데이터를 모으고 좋은 모델을 만들어도 이게 가능할까? 모션 블러 제거라는 문제는 단순히 더러운 데이터에서 깨끗한 데이터를 검출해내는게 아니라, 깨끗한 데이터의 정보가 거의 존재하지 않는 데이터에서 깨끗한 데이터를 추출하는 과정이다. 모션 블러가 조금만 있다면 할만 할 수도 있겠다. 근데 모션블러가 이미지를 거의 뒤덮으면 어떻게 풀어야할까?</p>
<p>두번째 이유는 <strong>속도가 굉장히 느렸다</strong>. 모션 블러를 인식해본다고 하자. 어떠한 픽셀을 기점으로 주위에 픽셀값들을 하나하나 비교해보면서 블러가 있는지 없는지를 확인해봐야한다. X 방향으로도 확인하고, Y 방향으로도 확인한다. 모션 블러를 지우는 연구를 제대로 들여다보지는 않았지만, 아마 추측으로는 gradient가 굉장히 뭉뜽그려진 부분을 찾으려고 하지 않을까? focus가 좋지 않거나, 형태 자체가 뭉뜽그려진 값을 준다면 모션블러와 비슷하게 보일수도 있으니, 모션 블러만을 제대로 분류하는 알고리즘도 들어갈 것이다. 모션 블러가 조금만 있는 이미지라면 계산량이 작을 수도 있겠다. 반대로, 모션블러가 엄청나게 들어간 이미지는 더 계산이 많이 필요할 수도 있겠다. 이미지마다 연산량이 그렇게 많이 차이가 나면, 동영상에서 매 프레임마다 계산을 하려고 할 때 한 프레임은 빠르게 계산하고 다음 프레임은 엄청나게 느리게 처리가 되면서, 전체적으로 엄청나게 버벅거릴 수도 있겠다.</p>
<p> </p>
<hr>
<h2 id="진짜-모션-블러는-어떻게-지우는거지-하드웨어"><a href="#진짜-모션-블러는-어떻게-지우는거지-하드웨어" class="headerlink" title="진짜 모션 블러는 어떻게 지우는거지? (하드웨어)"></a>진짜 모션 블러는 어떻게 지우는거지? (하드웨어)</h2><p>소프트웨어만으로는 모션 블러를 지우기 쉽지 않다. 그러면 우리는 언제든지 모션 블러를 달고 살아야하는가? ㅠㅠ</p>
<p>아니다!</p>
<p>우리가 사용하는 <strong>대부분의 영상처리 알고리즘들은 ‘조금의 모션 블러’에는 강건하게 계산을 수행할 수 있다</strong>. 또, <strong>잘 알려진 특정 알고리즘 등을 통해서 이미지를 조금 변형시켜 모션 블러 효과를 무시</strong>할 수도 있다. 우리의 목적은 모션 블러를 100% 없애는게 아니라, 이 알고리즘들이 내가 얻은 영상에서 사용할 수 있게 하는 것이다 :)</p>
<p>소프트웨어에서 심한 모션 블러를 잡을 수 없다면, 하드웨어에서 잡아볼 수 있겠다. 다만 소프트웨어로 처리하는데 필요한 지식과 하드웨어로 처리하는데 필요한 지식은 서로 굉장히 다르다. <strong>하드웨어 쪽은 알고리즘에 대한 지식보다는 광학과 신호처리 쪽으로의 지식이 필요</strong>하므로, 컴퓨터 비전 / 딥러닝의 소프트웨어만을 하시던 분들은 조금 생소하실 수 도 있다.</p>
<p>모션 블러가 왜 나타나는지 이해하기 위해서는 카메라가 어떻게 작동하는지 이해해야한다. 카메라가 작동하는 원리는 아래와 같다</p>
<p> </p>
<h3 id="2-1-카메라의-원리"><a href="#2-1-카메라의-원리" class="headerlink" title="2.1 카메라의 원리"></a>2.1 카메라의 원리</h3><ol>
<li><p>셔터가 열리고, 이미지 센서가 빛에 노출된다.</p>
<img src="/20210329-motion-blur/Untitled_1.png" width="500" height="200" alt="lol" title="lol2">
</li>
<li><p>빛이 센서에 닿으면서, <strong>센서의 픽셀마다 받아드린 빛의 양 만큼의 신호를 모은다</strong>.</p>
<img src="/20210329-motion-blur/Untitled_2.png" width="500" height="200" alt="lol" title="lol2">
</li>
<li><p>셔터가 닫히면서, 더 이상 이미지 센서가 빛에 노출되지 않는다.</p>
<img src="/20210329-motion-blur/Untitled_3.png" width="500" height="200" alt="lol" title="lol2">
</li>
<li><p><strong>지금까지 모아온 빛 신호를 디지털 신호로 바꾸고</strong>, 2D 이미지를 만든다.</p>
<img src="/20210329-motion-blur/Untitled_4.png" width="500" height="200" alt="lol" title="lol2">
</li>
<li><p>1로 돌아가서 새 사진을 찍는다.</p>
</li>
</ol>
<p>간단히 요약하면, <strong>이미지 센서가 빛에 노출된 동안 얻은 정보를 재구성하면 영상이 나온다</strong>. 그러면, <strong>이미지 센서가 빛에 노출되는 동안 이 빛에 아무런 변화가 없으면 그것이 스틸샷</strong>이겠고, <strong>모션 블러는 이미지 센서가 빛에 노출된 동안 생긴 변화</strong>가 되겠다.</p>
<p> </p>
<h3 id="모션블러의-원리"><a href="#모션블러의-원리" class="headerlink" title="모션블러의 원리"></a>모션블러의 원리</h3><p>이미지 센서는 빛에 특정 시간동안 노출되어있어야 한다. 이 노출되어있는 시간을 우리는 <strong>Exposure, 또는 Exposure time</strong>이라고 한다. </p>
<p>Exposure를 받고있는 도중에 피사체가 움직여버렸다고 생각해보자. <strong>움직이기 전과 움직이기 시작한 후에 들어오는 빛이 다를것이다</strong>. 움직이기 전의 빛을 받던 픽셀은 더 이상 빛을 받지 못하게 되고, 움직이면서 새로운 픽셀이 빛을 받게 된다. 이렇게 <strong>(이전에 빛을 받던 픽셀 + 새로 빛을 받는 픽셀)만큼의 픽셀들이 피사체에 대한 정보를 담게 된다</strong>. 하지만, 충분한 시간동안 <strong>충분한 양의 빛이 담기지 않기 때문에 약한 신호로 측정</strong>되고, 이 때문에 <strong>투명해보이는 효과</strong>가 나타난다.</p>
<img src="/20210329-motion-blur/Untitled_5.png" width="750" height="500" alt="lol" title="lol2">

<p>그렇다면, Exposure 중에만 피사체가 움직이지 않으면 모션 블러가 없겠다! </p>
<p>하지만 대부분 피사체들은 연속적으로 움직이고, exposure 중에만 멈춰주는 착한 피사체는 없기 때문에… Exposure 중에 번지는 효과를 최소화하기 위해서는 <strong>Exposure를 줄이면 되겠다</strong>. 10의 Exposure에서 10 픽셀의 번짐 (모션 블러)를 일으킨다면, 1의 Exposure에는 1픽셀의 번짐만이 있을 것이다. 모션 블러의 효과가 10 픽셀에서 1픽셀로 줄었다!</p>
<p>그리고 아주 다행히도, <strong>괜찮은 웹캠, 스마트폰, 디지털 카메라에서도 요즘은 exposure를 조정할 수 있다!</strong></p>
<p> </p>
<h3 id="ISO-값-조정하기"><a href="#ISO-값-조정하기" class="headerlink" title="ISO 값 조정하기"></a>ISO 값 조정하기</h3><p>Exposure를 줄이면 모션 블러를 줄일 수 있지만, 다른 문제가 하나 생긴다. Exposure를 10→1로 줄인다면 모션 블러도 10→1로 줄어든다. 그러나, 빛을 받는 시간이 줄어드는 것이라… 받아드리는 빛의 양도 10→1로 줄어든다. 이 뜻은 <strong>Exposure를 줄인만큼 영상이 어두워진다는 것이다</strong>. <strong>이론적으론 모션 블러를 최소화하다가 하마터면 시꺼면 화면을 볼 수도 있다</strong>.</p>
<p>다행히도, 디지털 영상 촬영에서는 <strong>ISO</strong>라는 파라미터가 있다. 이것은 <strong>받아드린 빛 신호를 증폭기 (amplifier)를 이용해서 신호를 증폭</strong>시키는 것이다. </p>
<p>10의 신호를 받았는데, 나는 100을 원한다? 그러면 신호를 10배 올릴 수 있는 ISO 값을 설정하면 된다. 1의 신호를 받았는데, 나는 100을 원한다? 그러면 신호를 100배 올릴 수 있는 ISO 값을 설정하면 된다. </p>
<p>그러면 모션 블러를 최소화하려면, <strong>Exposure를 줄이고, ISO 값을 높이면 되겠다</strong>.</p>
<p>다만 <strong>ISO 값은 높이면 높일수록 노이즈가 커진다</strong>. 이미지 센서 특성 상 항상 어느정도의 노이즈가 들어가는데, 그 값을도 함께 증폭이 되기 때문이다. <strong>이 때 생기는 노이즈는 피할 수 없다</strong>. 최소화 하려면 이미지 센서 사이즈를 큰거를 쓰거나, 가우시안 블러 등으로 노이즈를 없애면 되지만… 이 부분은 <strong>다른 글에서 다루기로 한다</strong>.</p>
<img src="/20210329-motion-blur/Untitled_6.png" class="" title="ISO and noise">

<p> </p>
<hr>
<h2 id="스마트폰에서-모션블러-지우기"><a href="#스마트폰에서-모션블러-지우기" class="headerlink" title="스마트폰에서 모션블러 지우기"></a>스마트폰에서 모션블러 지우기</h2><p>스마트폰에서는 다행히도 Exposure 값을 바꿀 수 있다. 사용한 스마트폰은 삼성 갤럭시 S7 Edge이다.</p>
<p>기존의 카메라 앱에 들어가서, 왼쪽으로 슬라이드하면 **’프로 모드’**가 있다.</p>
<p><strong>프로 모드에서는 Exposure, ISO 등 여러 값들을 사용자가 원하는대로 바꿀 수 있다.</strong></p>
<p>아이폰, 다른 안드로이드 스마트폰에서도 비슷한 기능을 찾아 들어가면 된다.</p>
<img src="/20210329-motion-blur/pro.png" class="" title="pro1">

<p>비교를 위해서 Exposure를 1/5정도로 가능한 높이고 손을 가볍게 흔들면서 사진을 찍었다. 엄청난 양의 모션 블러가 보인다. 그에 비해, 뒤에 움직이지 않는 물체들은 선명하게 촬영되었다.</p>
<p>(아래 사진은 블러가 가득하다)</p>
<img src="/20210329-motion-blur/blur.jpg" width="350" height="500" alt="lol" title="lol2">

<p>이제 Exposure를 줄여보기로 한다. 약 1/250 정도로 줄여보았다. </p>
<p>이렇게 되서 모션 블러와 함께 피사체가 함께 사라졌다. 우리가 원하는건 피사체는 남고 블러만 사라지는거니까, ISO를 높여보기로 한다.</p>
<p>(이미지가 전체적으로 어두워졌다. 이미지가 로딩이 안되는게 아니다! 사진 오른쪽 위를 보면 빛을 강하게 발산하는 컴퓨터 화면은 보인다.)</p>
<img src="/20210329-motion-blur/before_fix.jpg" width="350" height="500" alt="lol" title="lol2">

<p>ISO를 높이고, 가볍게 손을 흔들면서 다시 촬영해보았다.</p>
<p>오른쪽의 사진이 방금 ISO를 높인 후 찍힌사진이다. </p>
<p>사진만 봐서는 손을 움직이지 않고 찍은것 처럼 보이나, 실제로는 아까전에 찍은 왼쪽의 블러 이미지를 얻을 때와 똑같이 움직인 사진이다. </p>
<img src="/20210329-motion-blur/comparison.png" class="" title="comp">


<p>모션 블러가 없어진건 정말 다행이다! 하지만 ISO를 높이면서 이미지에 노이즈가 생겼다.</p>
<p>지금 나오는 두개의 이미지는 각각 높은 ISO에서 얻어진 이미지와, 기존의 낮은 ISO에서 얻어진 이미지이다. <strong>높은 ISO에서 얻어진 이미지는 노이즈가 생겼다</strong>. 사진을 찍는 사람들은 이걸 보고 사진의 grain (질감)이라고 하고 이를 통해 특정한 느낌을 표현하려고 하지만, 로보틱스 비전이나 딥러닝 비전을 하는 우리에게는 전혀 필요 없다. 깔끔한 이미지가 최고다!</p>
<img src="/20210329-motion-blur/comp2.png" class="" title="comp2">

<p> </p>
<h2 id="머신-비전-카메라-Scientific-Camera로-모션-블러-제거하기"><a href="#머신-비전-카메라-Scientific-Camera로-모션-블러-제거하기" class="headerlink" title="머신 비전 카메라 / Scientific Camera로 모션 블러 제거하기"></a>머신 비전 카메라 / Scientific Camera로 모션 블러 제거하기</h2><p>위의 스마트폰과 같은 방식으로 진행되었다.</p>
<p>FLIR사의 Chameleon 3 Monochrome 카메라를 이용해서 촬영되었고, Spinnaker Evaluation S/W를 사용했다.</p>
<p>머신 비전 카메라들은 Exposure time와 ISO를 훨씬 더 정확하게 조정할 수 있다. FLIR 사의 경우에는 ISO를 Gain이라는 단어로 표현한다. 머신 비전 회사마다 명칭을 다르게 부르는 경우가 있으니, 이 점 유의해야한다. 보통 Exposure와 ISO는 오토모드로 고정이 되어있어서, 이 락을 먼저 풀어줘야한다.</p>
<p>첫번째 영상은 레퍼런스 영상이다.</p>
<p>약 이정도의 속도로 손을 열심히 흔들어서 모션 블러를 만들어보려고 했다. </p>
<img src="/20210329-motion-blur/1.gif" width="1080" height="720" alt="[alternative_text]" title="[이예에~~1]">

<p>2번째 영상에서는 기본 세팅에서 잡혀있는 Exposure 값으로 촬영을 해보았다.</p>
<p>기본 세팅이 워낙 잘 잡혀있어서 모션 블러가 스마트폰 만큼 크진 않지만, 모션블러가 없지는 않다.</p>
<p>이정도 모션 블러라면, 데이터셋이 충분하지 않은 딥러닝 비전은 검출에 실패할 수도 있고, 로보틱스 비전의 경우 피쳐 검출이 부정확할 수 있다.</p>
<img src="/20210329-motion-blur/2.gif" width="1080" height="720" alt="[alternative_text]" title="[이예에~~2]">

<p>세번째 영상에서는 Exposure 값을 줄이고, 밝아질 때 까지 Gain을 높여보았다.</p>
<p>빠르게 움직이는 피사체임에도 불구하고, 모션 블러가 없는 영상을 얻을 수 있었다.</p>
<p>그러나, 영상 썸네일에서 보이듯이 전체적으로 영상에 노이즈가 생긴 것을 확인할 수 있다.</p>
<img src="/20210329-motion-blur/3.gif" width="1080" height="720" alt="[alternative_text]" title="[이예에~~3]">

<hr>
<h2 id="참고자료-소스"><a href="#참고자료-소스" class="headerlink" title="참고자료 + 소스"></a>참고자료 + 소스</h2><ul>
<li><p>ICCV 2015 - “Hierarchical Convolutional Features for Visual Tracking”</p>
<p>  <span class="exturl" data-url="aHR0cHM6Ly9wZGZzLnNlbWFudGljc2Nob2xhci5vcmcvOWVkYS82NmY4NTUxZTIwZmNhM2IxZjkyNzU3ODFlOTU1NWVhNTAyM2QucGRm">링크<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>카메라 센서 이미지를 가져온 원본 비디오</p>
<p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1Cc1hJUWtoc3hFTQ==">How Camera Sensors Pick up Light with John Greengo<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>ISO 노이즈 값 설명하는 그래프를 가져온 웹사이트</p>
<p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuZGF2ZW1vcnJvd3Bob3RvZ3JhcGh5LmNvbS9jYW1lcmEtc2Vuc29yLXNpemUtZ3VpZGUjRHluYW1pY19SYW5nZV9JU09fU2Vuc29yX1NpemU=">Camera Sensor Size Photography Guide [Updated 2019]<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
        <category>카메라 기초 시리즈</category>
      </categories>
      <tags>
        <tag>Camera</tag>
        <tag>Imaging</tag>
        <tag>Motion blur</tag>
      </tags>
  </entry>
  <entry>
    <title>메르센 트위스터 (std::mt19937 / std::mersenne_twister_engine)</title>
    <url>/20210330-mersenne-twister/</url>
    <content><![CDATA[<blockquote>
<p>std::mt19337과 std::mersenne_twister_engine은 C++에서 제공하는 random number generator이다. 기존에 많이 사용되던 rand의 대체제로 쓸 수 있다.</p>
</blockquote>
<p> </p>
<hr>
<h2 id="왜-rand-대신-써야하는가"><a href="#왜-rand-대신-써야하는가" class="headerlink" title="왜 rand 대신 써야하는가?"></a>왜 rand 대신 써야하는가?</h2><p>rand의 문제점은 아래와 같다.</p>
<ul>
<li>생성된 random number의 분포가 그리 고르지 않다.</li>
<li>반복 주기는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.844ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1257.1 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg></mjx-container>로 다른 pseudo random number generator 에 비해 상대적으로 짧다.</li>
<li>전역 함수이기 때문에 프로그램 전체에서 시드를 공유한다.</li>
</ul>
<p>시뮬레이션 계산을 할 때 random number에 bias가 있다면 그 시뮬레이션 계산은 믿을 수 없다.</p>
<p>Mersenne Twister는 pseudorandom number generator로써 rand의  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.844ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1257.1 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg></mjx-container> 보다 <strong>훨씬 긴 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="9.141ex" height="2.088ex" role="img" focusable="false" viewBox="0 -841 4040.2 923"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(500, 0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1000, 0)"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(1500, 0)"></path><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" transform="translate(2000, 0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2540, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(3540.2, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>의 주기</strong>를 가지고 있다. Pseudorandom number generator이기 때문에 완전 random한 결과를 주는 것은 아니지만, 주기가 굉장히 길기 때문에 작은 샘플링 수에서는 충분히 random하다고 판단할 수 있다 (긴 주기가 random-ness를 보장하진 않지만, 해당 테스트에서 rand의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.844ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1257.1 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg></mjx-container> 주기는 random의 형태가 무너지는 문제가 나타난다). </p>
<p>이에 대한 증거로 Diehard 테스트와 다수의 TestU01 테스트를 통과했다.</p>
<p> </p>
<hr>
<h2 id="Mersenne-Twister는-무엇인가"><a href="#Mersenne-Twister는-무엇인가" class="headerlink" title="Mersenne Twister는 무엇인가?"></a>Mersenne Twister는 무엇인가?</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/b/b5/Mersenne_Twister_visualisation.svg" alt="https://upload.wikimedia.org/wikipedia/commons/b/b5/Mersenne_Twister_visualisation.svg"></p>
<p>많은 휴리스틱 알고리즘을 이용해서 random number를 생성한다.</p>
<p>주기가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="9.141ex" height="2.088ex" role="img" focusable="false" viewBox="0 -841 4040.2 923"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(500, 0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1000, 0)"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(1500, 0)"></path><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" transform="translate(2000, 0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2540, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(3540.2, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 인 이유는 624개의 integer (32-bit) ⇒ <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="16.466ex" height="1.717ex" role="img" focusable="false" viewBox="0 -677 7278 759"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1000, 0)"></path></g><g data-mml-node="mo" transform="translate(1722.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(2444.4, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g><g data-mml-node="mo" transform="translate(3722.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(4778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(500, 0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1000, 0)"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(1500, 0)"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(2000, 0)"></path></g></g></g></svg></mjx-container> 이기 때문이다.</p>
<p>C++에는 <random>에 구현되어있으며 아래와 같은 구성을 가지고 있다.</random></p>
<ul>
<li>커스텀 Mersenne Twister를 만들 때 사용되는 std::mersenne_twister_engine</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">UIntType</span>,</span></span><br><span class="line"><span class="class">    <span class="title">size_t</span> <span class="title">w</span>, <span class="title">size_t</span> <span class="title">n</span>, <span class="title">size_t</span> <span class="title">m</span>, <span class="title">size_t</span> <span class="title">r</span>,</span></span><br><span class="line"><span class="class">    <span class="title">UIntType</span> <span class="title">a</span>, <span class="title">size_t</span> <span class="title">u</span>, <span class="title">UIntType</span> <span class="title">d</span>, <span class="title">size_t</span> <span class="title">s</span>,</span></span><br><span class="line"><span class="class">    <span class="title">UIntType</span> <span class="title">b</span>, <span class="title">size_t</span> <span class="title">t</span>,</span></span><br><span class="line"><span class="class">    <span class="title">UIntType</span> <span class="title">c</span>, <span class="title">size_t</span> <span class="title">l</span>, <span class="title">UIntType</span> <span class="title">f</span></span></span><br><span class="line"><span class="class">&gt;</span> <span class="class"><span class="keyword">class</span> <span class="title">mersenne_twister_engine</span>;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>1998년도에 나온 32-bit MT를 구현하는 std::mt19937</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> mersenne_twister_engine&lt;<span class="keyword">unsigned</span> <span class="keyword">int</span>, <span class="number">32</span>, <span class="number">624</span>, <span class="number">397</span>, <span class="number">31</span>, <span class="number">0x9908b0df</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">0xffffffff</span>, <span class="number">7</span>, <span class="number">0x9d2c5680</span>, <span class="number">15</span>, <span class="number">0xefc60000</span>, <span class="number">18</span>, <span class="number">1812433253</span>&gt; mt19937;</span><br></pre></td></tr></table></figure>

<ul>
<li>2000년도에 나온 64-bit MT를 구현하는 std::mt19937_64</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> mersenne_twister_engine&lt;<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>, <span class="number">64</span>, <span class="number">312</span>, <span class="number">156</span>, <span class="number">31</span>,</span><br><span class="line">    <span class="number">0xb5026f5aa96619e9</span>ULL, <span class="number">29</span>,</span><br><span class="line">    <span class="number">0x5555555555555555</span>ULL, <span class="number">17</span>,</span><br><span class="line">    <span class="number">0x71d67fffeda60000</span>ULL, <span class="number">37</span>,</span><br><span class="line">    <span class="number">0xfff7eee000000000</span>ULL, <span class="number">43</span>,</span><br><span class="line">    <span class="number">6364136223846793005U</span>LL&gt; mt19937_64;</span><br></pre></td></tr></table></figure>

<p> </p>
<blockquote>
<p>이론에 관심 있으신 분들은 오리지널 논문을 참조바랍니다.<br><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS8xMC4xMTQ1LzI3Mjk5MS4yNzI5OTU=">“Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator”<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p> </p>
<hr>
<h2 id="Mersenne-Twister의-장점"><a href="#Mersenne-Twister의-장점" class="headerlink" title="Mersenne Twister의 장점"></a>Mersenne Twister의 장점</h2><ul>
<li>C++에 이미 구현되어있고 라이센스가 풀려있다.</li>
<li>Stats 테스트를 많이 통과했다 (e.g. Diehard test, many of TestU01 test)</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="9.141ex" height="2.088ex" role="img" focusable="false" viewBox="0 -841 4040.2 923"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(500, 0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1000, 0)"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(1500, 0)"></path><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" transform="translate(2000, 0)"></path></g></g></g><g data-mml-node="mo" transform="translate(2540, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(3540.2, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 의 주기로 왠만하면 random-ness를 유지해준다.</li>
<li>distribution이 일정한 편이다.</li>
<li>빠르다! → 2017년에 나온 논문에 따르면 True random 방식의 SIMD 구현보다 20배 빠르게 64-bit floating point random number를 생성한다고 한다.</li>
</ul>
<h2 id="Mersenne-Twister의-단점"><a href="#Mersenne-Twister의-단점" class="headerlink" title="Mersenne Twister의 단점"></a>Mersenne Twister의 단점</h2><ul>
<li>State vector가 큰 편이다 (2.5 kB)<ul>
<li>임베디드에서 문제가 될 수 있는데, 이는 TinyMT를 사용해서 해결할 수도 있다.</li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5tYXRoLnNjaS5oaXJvc2hpbWEtdS5hYy5qcC9+bS1tYXQvTVQvVElOWU1UL2luZGV4Lmh0bWw=">Tiny Mersenne Twister (TinyMT)<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>SIMD 버전을 쓰지 않으면 다른 방법들에 비해서 빠른건 아니다.<ul>
<li>SIMD 버전은 SFMT가 2배 더 빠르다고 한다. 하지만 Intel SSE2가 있어야한다.</li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5tYXRoLnNjaS5oaXJvc2hpbWEtdS5hYy5qcC9+bS1tYXQvTVQvU0ZNVC8=">SIMD-oriented Fast Mersenne Twister (SFMT)<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>암호학적으로 안전한 방법이 아니다. std::mt19937의 경우, 624개의 random number generation 값을 보면, 추후 어떤 값들이 나올지 전부 추측이 가능하다.<ul>
<li>하지만 실험용으로는 전혀 문제없다 😈</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="내가-사용하는-방법"><a href="#내가-사용하는-방법" class="headerlink" title="내가 사용하는 방법"></a>내가 사용하는 방법</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;random&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::random_device randDev;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::mt19937 <span class="title">mtGenerator</span><span class="params">(randDev())</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ... //</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int32_t</span>&gt; someData;</span><br><span class="line"><span class="built_in">std</span>::shuffle(someData.begin(), someData.end(), mtGenerator); <span class="comment">// 랜덤 셔플</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>mt19937을 이용해서 랜덤 데이터 셔플을 한다던지, RANSAC을 돌린다던지, random seed 를 만든다던지 할 때 쓰면 좋다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>SW</tag>
        <tag>Random number generator</tag>
      </tags>
  </entry>
  <entry>
    <title>C++을 배워야하는 이유</title>
    <url>/20210330-why-cpp/</url>
    <content><![CDATA[<p>아랫 글은 Youtube 채널 The Cherno의 “Welcome to C++”를 번역하고, 부분부분 저의 의견을 넣었습니다.</p>
<p> </p>
<h2 id="C-의-특성"><a href="#C-의-특성" class="headerlink" title="C++의 특성"></a>C++의 특성</h2><p>C++는 80년대부터 사용된 오래된 프로그래밍 언어이다. 많은 사람들이 C++가 오래된 만큼 최신 언어에 뒤쳐지고 그만큼 느릴 것이라고 생각하기도 하는데, 이는 대단히 잘못된 생각이다.</p>
<p>C++는 사실 빠른 연산이 필요한 게임 엔진, 그래픽스 엔진, 실시간 시스템, 서버 백엔드 등에 사용되며, 특정 분야에서는 C++ 외 다른 언어를 권장하지 않을 만큼 특성이 뚜렷한 프로그래밍 언어이다. 예를 들어, 게임 엔진의 3대장인 Unity, Unreal, Frostbite 엔진 모두 C++로 작성되어있다.</p>
<p>컴퓨터가 C++ 코드를 이해하기 위해서는 우선 컴파일러를 통해 machine code로 변환해야한다. 이 Machine code라는 것은 사람이 읽을 수 있는 형태의 코드가 아닌, 프로그램을 실행하려고 하는 하드웨어 플랫폼의 프로세서가 읽을 수 있는 형태의 코드이다. Machine code는 프로세서에 직접적으로 접근하여 논리연산을 실행한다. </p>
<p>이 때, C++ 코드는 사람이 생각하는 것과 비슷한 <strong>high-level 명령어</strong>부터 machine code 수준의 <strong>low-level 명령어</strong>를 모두 가지고 있기 때문에, 프로그래머의 실력에 따라 low-level 명령어들로 <strong>논리연산을 하나하나 설계함으로써 굉장히 효율적인 연산</strong>을 할 수 있다. High-level 명령어만 사용할 수 있는 언어들은 이를 수행할 수 없다.</p>
<p>C++를 잘 사용하여 CPU에게 비효율 적인 명령을 하지 않음으로써 우리는 진정으로 필요한 연산만을 할 수 있고, 이를 통해 빠른 연산을 할 수 있는 것이다.</p>
<p> </p>
<hr>
<h2 id="C-을-어디에-사용할-수-있을까"><a href="#C-을-어디에-사용할-수-있을까" class="headerlink" title="C++을 어디에 사용할 수 있을까?"></a>C++을 어디에 사용할 수 있을까?</h2><p>그러면 어떤 환경에서 C++ 프로그래밍을 할 수 있는걸까?</p>
<p>사실 컴파일러만 있다면 거의 모든 환경에서 C++ 프로그래밍을 할 수 있다. </p>
<ul>
<li>Windows, Mac, Linux와 같은 데스크탑 운영체제에서부터,</li>
<li>iOS, Android와 같은 모바일 운영체제도,</li>
<li>ChromeOS, Xbox, PS3 와 같은 게임 콘솔 플랫폼에서도 C++ 코드를 실행할 수 있다.</li>
<li>프로세서의 종류도 x86, x64, ARM 등 컴파일러만 있다면 가리지 않는다.</li>
</ul>
<p>이처럼 많은 플랫폼들에서 사용가능하기 때문에 C++는 다양한 플랫폼 지원이 필요할 때 특히나 더 빛을 발한다. 컴파일러가 존재하는 한, 나의 C++ 코드는 모든 플랫폼에서 Native하게, 효율적이게 실행할 수 있는 것이다. </p>
<p> </p>
<hr>
<h2 id="다른-언어가-아닌-C-를-사용하는-이유"><a href="#다른-언어가-아닌-C-를-사용하는-이유" class="headerlink" title="다른 언어가 아닌 C++를 사용하는 이유"></a>다른 언어가 아닌 C++를 사용하는 이유</h2><p>물론 다양한 플랫폼을 지원하는 다른 언어들도 있다. 하지만 C++는 80년대에서부터 개발되어왔고, 많은 양의 소스코드와 튜토리얼들이 이미 만들어져있다.</p>
<p>다른 유명한 언어 중 Java나 C#이 가끔 C++와 비교되곤 하는데, 이 방식들은 Virtual Machine (VM)을 이용하는 방식으로 C++와 다른 방식이다. VM 방식은 컴파일러를 통해 intermediate 언어로 우선 변환이 되고, 실제로 프로그램을 실행시킬 때는 VM이 interpretation 형태로 한줄한줄 읽어주는 형태로 진행된다.</p>
<p>VM 방식과 C++ 방식의 차이를 예를 들자면…</p>
<p>내가 영어로 책을 적어서 출판했는데, 독일에 이 책을 팔고싶다고 해보자. 독일의 독자들은 독일어로만 이해할 수 있기 때문에, 내 책을 팔 때 실시간 번역가를 동행시킨다고 해보자. 그러면 독일인 독자는 책을 읽고 싶으면 이 번역가에게 부탁을 해서, 번역가가 영어로 된 책을 읽고 (컴파일러 변환) 독일어로 번역을 해주면 그제서야 한줄한줄 내용을 이해하게 되는 것 (interpretation) 이다. 이에 반해, C++의 방식은 아예 처음부터 책을 독일어로 출판함으로써, 독일인 독자 (Platform)이 Native하게 읽을 수 있게 만들어 주는 방식이다. 당연히 후자의 방식이 책의 내용을 이해하는데에 (코드를 실행하는데에) 훨씬 효율적일 수 밖에 없다.</p>
<p> </p>
<hr>
<h2 id="C-이-그러면-항상-무조건-더-빠른가"><a href="#C-이-그러면-항상-무조건-더-빠른가" class="headerlink" title="C++이 그러면 항상 무조건 더 빠른가?"></a>C++이 그러면 항상 무조건 더 빠른가?</h2><p>C++ 코드의 native 한 특성 때문에, 모든 C++ 코드가 항상 빠르다는 생각은 버려야한다. </p>
<p>잘 짠 C++ 코드는 당연히 빠를것이다. 하지만 잘 짜지 못한 C++ 코드는 빠르지 않을 것이며, 심지어는 Java나 C# 같은 VM 방식의 코드에 비해 더 느릴 수 있다. VM 방식은 컴파일러를 통하면서 어느정도의 코드 최적화를 통해 속도를 얻어낼 수 있는 방면에, C++ 코드는 그러한 방식이 없을 수 있다. 즉<strong>빠르고 좋은 성능을 얻으려면, 우리가 그러한 성능을 낼 수 있는 C++ 코드를 짜는 방법에 대해 공부해야한다.</strong></p>
<p> </p>
<hr>
<h2 id="컴퓨터-비전에-C-가-사용되는-이유"><a href="#컴퓨터-비전에-C-가-사용되는-이유" class="headerlink" title="컴퓨터 비전에 C++가 사용되는 이유"></a>컴퓨터 비전에 C++가 사용되는 이유</h2><p>컴퓨터 비전에서는 크게 C/C++/Python이 사용된다.</p>
<p><strong>개인적으로 컴퓨터 비전을 할 때는 C++를 배우는 것을 강력하게 추천하며, 딥러닝을 공부할 경우 Python도 함께 배우는 것 역시 추천한다.</strong></p>
<h3 id="C-vs-C"><a href="#C-vs-C" class="headerlink" title="C vs C++"></a>C vs C++</h3><p>C는 임베디드 환경에서 사용이 되기도 하는데, 이러한 환경에서 하드웨어는 C와 밖에 통신을 못 하는 경우가 많다. 다만, 임베디드가 아닌 환경에서는 굳이 C를 써야할 이유는 없다.</p>
<h3 id="Python-vs-C"><a href="#Python-vs-C" class="headerlink" title="Python vs C++"></a>Python vs C++</h3><p>Python은 굉장히 high-level 언어이기 때문에 C++보다 훨씬 간결하고 쉽게 프로그래밍을 할 수 있다는 장점이 있다. Python의 모토가 “Life is short, you need python”인데, 그만큼 프로그래머가 쉽게 프로그래밍을 하면서 시간을 절약할 수 있다는 의미를 가지고 있다. 하지만 Python은 interpretation 언어이며 low-level optimisation이 불가능하기 때문에 프로그램의 시간은 절약할 수 없다는 단점을 가지고 있다.</p>
<p>Python의 단점 중 하나는 High-level 언어이기 때문에, 하드웨어 서포트 등이 좋지 않다는 점이다. Python은 이러한 단점을 C/C++로 구현된 코드를 Wrapper로 감싸서 사용함으로써 극복하는데, 사실 C/C++ 을 사용했으면 겪지 않을 문제라고 볼 수 있다.</p>
<p>Python은 여러가지 Data science, Data visualisation 관련 서포트가 많고, 또 무엇보다 투탑 딥러닝 관련 프레임워크인 TensorFlow와 PyTorch를 Python으로 사용할 수 있기 때문에 딥러닝에서는 필수적인 언어로 여겨진다. C++ 유저를 위해 TensorFlow와 PyTorch의 C++ API가 개발되었지만, 트레이닝 단계에서 굳이 C++로 할 필요가 있나, 라는 생각이 들기도 한다 (물론 large-scale training에서는 의미가 있을 수도 있지만, 학생의 입장에서는 크게 의미가 없을 수 있다). 하지만, 학습된 딥러닝 모델로 빠르게 inference 성능을 내야할 때는 optimisation이 필요한데, 이 떄부터 Python의 한계가 나타나기 시작하면서 ONNX, TensorRT 등의 라이브러리를 통해 C++로 변환하여 사용한다.</p>
<p>Python의 마지막 단점은 Multi-threading이 어렵다는 점이다. Multi-threading을 쓴다는 것은, 병렬처리를 통해 반복적인 계산을 더욱 빠르게 진행하겠다는 의미를 가진다. 결국 빠름을 추구하겠다는 이야기인데, optimise가 되지 않는 코드들을 병렬처리를 함으로써 성능을 조금이라도 끌어올리기보단, C++을 통해 제대로 optimise하면서 여러개의 쓰레드를 빠르게 돌리는게 더 큰 성능을 얻을 수 있다. 실제로 SLAM과 같이 다중 쓰레드를 이용하는 실시간 컴퓨터 비전 프로그램 중에서 Python을 이용하는 경우는 Toy example을 제외하고는 없다.</p>
<h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><hr>
<div class="video-container"><iframe src="https://www.youtube.com/embed/18c3MTX0PK0" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>SW</tag>
      </tags>
  </entry>
  <entry>
    <title>R2D2 - Repeatable and Reliable Detector and Descriptor (NAVER LABS Europe)</title>
    <url>/20210405-R2D2/</url>
    <content><![CDATA[<p>… 블로그 아티클은 중요한 내용은 이야기하지 않는 것 같다. 논문을 따로 리뷰해야할듯.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ldXJvcGUubmF2ZXJsYWJzLmNvbS9ibG9nL3IyZDItcmVwZWF0YWJsZS1hbmQtcmVsaWFibGUtZGV0ZWN0b3ItYW5kLWRlc2NyaXB0b3Iv">원본 글 링크<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL25hdmVyL3IyZDI=">Github code<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="R2D2의-주-목적"><a href="#R2D2의-주-목적" class="headerlink" title="R2D2의 주 목적"></a>R2D2의 주 목적</h2><p>Combining keypoint reliability in an image as part of the keypoint detection problem significantly improves feature matching.</p>
<p> </p>
<hr>
<h2 id="과거의-Handcrafted-keypoints"><a href="#과거의-Handcrafted-keypoints" class="headerlink" title="과거의 Handcrafted keypoints"></a>과거의 Handcrafted keypoints</h2><ul>
<li>사람이 직접 디자인<ul>
<li>Edge나 corner같은 ‘salient region’을 찾도록 만듬. (괄호를 친 이유는, 실제로는 salient하지 않기 때문에…)</li>
</ul>
</li>
<li>Learning-based를 사용해서 만든 keypoint들은 ‘interesting keypoint’를 학습함.<ul>
<li>참조하면 좋은 논문 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDcuMTAyNTQ=">“From handcrafted to deep local features”<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Salient-region-Interesting-keypoint"><a href="#Salient-region-Interesting-keypoint" class="headerlink" title="Salient region? Interesting keypoint?"></a>Salient region? Interesting keypoint?</h2><ul>
<li>과거에는 ‘repeatable’한 키포인트를 찾았음.<ul>
<li>i.e. 물체를 바라보는 각도가 달라져도 동일한 위치에서 검출되는 키포인트들.</li>
<li>보통 ‘detector’가 잘 찾을 수 있는 것을 repeatable이라고 함.</li>
</ul>
</li>
<li>하지만 실제로는 ‘reliable’한 것도 중요함.<ul>
<li>i.e. 여러 이미지에서 동일한 포인트로 매칭할 수 있어야함.</li>
<li>보통 ‘descriptor’가 잘 찾을 수 있는 것을 reliable이라고 함.</li>
</ul>
</li>
<li>좋은 키포인트들은 reliable + repeatable 함.</li>
</ul>
<p> </p>
<hr>
<h2 id="R2D2"><a href="#R2D2" class="headerlink" title="R2D2"></a>R2D2</h2><ul>
<li>Reliability를 하나의 지표로 삼아서 keypoint detector를 ranking loss 기반으로 학습함.</li>
<li>HPatches, Aachen day-night 데이터셋 등에서 잘 되는 것을 입증함.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>NAVER LABS</tag>
        <tag>Visual localization</tag>
        <tag>Local feature</tag>
      </tags>
  </entry>
  <entry>
    <title>Kapture + CVPR2020 Visual localization challenge (NAVER LABS Europe)</title>
    <url>/20210405-naver-labs-europe-kapture/</url>
    <content><![CDATA[<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ldXJvcGUubmF2ZXJsYWJzLmNvbS9ibG9nL2thcHR1cmUv">kapture – A unified data format to facilitate visual localization and structure from motion.아티클 원본<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ldXJvcGUubmF2ZXJsYWJzLmNvbS9ibG9nL29uZS1tZXRob2Qtb25lLXBpcGVsaW5lLW5hdmVyLWxhYnMtZXVyb3BlLXJhbmtzLWhpZ2gtYWNyb3NzLXRocmVlLXZpc3VhbC1sb2NhbGl6YXRpb24tY2hhbGxlbmdlcy1hdC1jdnByLTIwMjAv">One method, one pipeline: NAVER LABS Europe ranks high across three visual localization challenges at CVPR 2020 아티클 원본<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<hr>
<h2 id="Kapture"><a href="#Kapture" class="headerlink" title="Kapture"></a>Kapture</h2><ul>
<li>많은 Visual localization 데이터셋들이 다른 데이터 format을 가지고 있다.</li>
<li><strong>Kapture</strong>는<ol>
<li>이런 데이터를 호환되게 convert 시켜줄 수 있다.</li>
</ol>
<ul>
<li>Sensor parameters (i.e. intrinsic/extrinsics)</li>
<li>Raw sensor data (i.e. camera images / LiDAR data)</li>
<li>Other sensor data (GPS/WiFi)</li>
<li>Intermediate data<ul>
<li>2d local features</li>
<li>2D-2D match</li>
<li>global feature</li>
<li>3D map</li>
</ul>
</li>
</ul>
<ol start="2">
<li>다양한 pipeline을 제공한다.</li>
</ol>
<ul>
<li>COLMAP + SIFT</li>
<li>R2D2 + AP-GeM</li>
</ul>
<ol start="3">
<li>Visual localization 실험을 위한 데이터셋을 준비해놨다.</li>
</ol>
<ul>
<li>ECCV 2020 Visual localization challenge에 쓰인 데이터셋 모두 포함<ul>
<li>Aachen Day-Night</li>
<li>Inloc</li>
<li>RobotCar Seasons</li>
<li>Extended CMU-Seasons</li>
<li>SILDa</li>
<li>Time of Day</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Visual localization 입문을 쉽게 만들어주는 소프트웨어 정도로 생각하면 좋을 것 같다.</p>
<p> </p>
<hr>
<h2 id="CVPR-2020-workshop"><a href="#CVPR-2020-workshop" class="headerlink" title="CVPR 2020 workshop"></a>CVPR 2020 workshop</h2><ul>
<li>CVPR 2020의 Joint Workshop on Long-Term Visual Localization, Visual Odometry and Geometric and Learning-based SLAM에서는 3가지 visual localization 챌린지를 열었다.<ol>
<li>자율주행 자동차에서 visual localization하기</li>
<li>Handheld device에서 visual localization하기</li>
<li>Long-term localization을 위한 local feature 찾기.</li>
</ol>
</li>
<li>NAVER LABS Europe팀은 1번 챌린지에서 1등, 2번 챌린지에서 4등, 3번 챌린지에서 2등을 했다고 한다.</li>
</ul>
<ul>
<li>사용한 기술은 APGeM이라는 image retrieval 기술과 R2D2라는 local feature이다.</li>
<li>이 둘을 Kapture에 담은 visual localization pipeline을 이용했다.</li>
</ul>
<p> </p>
<h3 id="사용한-기술-RD2D-APGeM"><a href="#사용한-기술-RD2D-APGeM" class="headerlink" title="사용한 기술 - RD2D + APGeM"></a>사용한 기술 - RD2D + APGeM</h3><ul>
<li>글의 초반은 <a href="changh95.github.io/20210405-naver-labs-europe-kapture">이전 글</a>과 겹치는 내용이 많아 생략한다.</li>
</ul>
<ul>
<li>R2D2<ul>
<li>Sparse keypoint detector &amp; descriptor</li>
<li>Synthetic image 기반으로 학습됨.</li>
<li>Detection과 Description을 동시에 추론 가능.<ul>
<li>Keypoint reliability와 repeatability를 따로 계산함.</li>
</ul>
</li>
</ul>
</li>
<li>너무 큰 Large-scale인 경우에는 3D reconstruction을 할 수 없음. 그러므로 image retrieval을 이용해서 place recognition 방식을 사용함.</li>
<li>APGeM<ul>
<li>Generalized meanpooling (GeM) 레이어를 이용해서 feature map을 정해진 길이의 컴팩트한 형태로 바꿈.</li>
<li>Mean average precision (mAP) 값을 이용해서 모델을 학습함. (Metric learning)<ul>
<li>Google Landmakrs dataset으로 학습.</li>
</ul>
</li>
</ul>
</li>
<li>COLMAP을 이용해서 structure-from-motion (SfM)을 수행하고, geometric verification을 수행.</li>
</ul>
<ul>
<li>파이프라인을 보면…<ul>
<li>처음에 Training images + poses를 기반으로 APGeM을 적용한다.</li>
<li>이렇게 뽑은 similar 이미지들에 R2D2를 적용하여 keypoint를 뽑고 매칭을 한다.</li>
<li>COLMAP으로 SfM을 수행해서 3D point cloud를 만든다.</li>
<li>Query image에 APGeM을 적용한다.</li>
<li>이렇게 뽑은 similar image에 존재하는 2D-3D correspondence를 만들어서, query-reference-3Dmap correspondence를 만들고, geometric verification (i.e. PnP+RANSAC)으로 pose 를 추정한다.</li>
</ul>
</li>
</ul>
<ul>
<li>조금 이해가 안가는 부분은, training images는 어떻게 결정하는지이다. APGeM으로 Top k개의 similar image들을 뽑은거를 training image라고 한걸까?</li>
</ul>
<p> </p>
<h3 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h3><ul>
<li>이미지에 다양한 요소가 포함되어있음에도 굉장히 잘 generalize했다.<ul>
<li>time of day</li>
<li>season of the year</li>
<li>Outdate reference representation</li>
<li>Occlusion</li>
<li>Motion blur</li>
<li>Extreme viewpoint change</li>
<li>Low texture area</li>
</ul>
</li>
<li>1등을 한 챌린지인 ‘자율주행에서의 visual localization’에서는 motion sequence가 주어지면서, 이로부터 정확한 위치를 찾을 수 있었다.</li>
<li>4등을 한 챌린지인 ‘Handheld 기기에서의 visual localization’에서는 1장의 query image로부터 정확한 위치를 찾는 것이였다.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>NAVER LABS</tag>
        <tag>Visual localization</tag>
        <tag>Kapture</tag>
      </tags>
  </entry>
  <entry>
    <title>Methods for visual localization (NAVER LABS Europe)</title>
    <url>/20210405-naver-labs-visloc/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9ldXJvcGUubmF2ZXJsYWJzLmNvbS9ibG9nL21ldGhvZHMtZm9yLXZpc3VhbC1sb2NhbGl6YXRpb24v">원본 글 링크<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h2 id="Visual-localization"><a href="#Visual-localization" class="headerlink" title="Visual localization"></a>Visual localization</h2><ul>
<li>이미지로부터 현재 위치를 추정하는 기술.</li>
<li>로보틱스, 자율주행, 증강현실</li>
<li>기존에 사용하던 방법: GNSS<ul>
<li>GNSS는 실내에서 사용 불가능</li>
<li>오차 범위가 미터단위로 큼</li>
<li>방향 정보가 없음<ul>
<li>GPS와 Compass 정보를 합성하여 1~2m 정도의 위치 오차와 10도 정도의 방향 오차로 현재 위치를 추정할 수 있으나, 로보틱스/자율주행/증강현실에서 요구하는 정확도에 훨씬 못 미침.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="어려운-이유"><a href="#어려운-이유" class="headerlink" title="어려운 이유"></a>어려운 이유</h3><ul>
<li>Visual localization을 통해 실내와 실외 환경에서 cm 단위로 위치를 추정할 수 있어야함.<ul>
<li>실외: 자율주행</li>
<li>실내: 로보틱스, 증강현실, 자율주행 (GNSS가 닿지 않는 주차장 등)</li>
<li>사전에 모아둔 reference 이미지들과 내가 현재 보고 있는 query 이미지들간의 correspondence를 이용하여 현재 위치를 추정 가능함.<ul>
<li>이러한 ‘visual map’ 정보는 3D reconstruction을 통해 생성 가능함.  </li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>기술적 어려움:<ul>
<li>조명 변화.<ul>
<li>Visual map이 낮에 찍은 사진들로 이뤄졌다면, 밤에 찍은 query 이미지로 위치를 추정할 수 있을까?</li>
</ul>
</li>
<li>동적 객체<ul>
<li>Visual map을 만들 때 움직이는 객체가 포함되어있었다면?</li>
</ul>
</li>
<li>날씨/계절 변화</li>
<li>가려짐 / 부분적 가려짐</li>
<li>시점의 차이</li>
</ul>
</li>
</ul>
<ul>
<li>다양한 파이프라인이 제안 됨.</li>
<li>다양한 데이터셋이 제안됨.<ul>
<li>데이터셋마다 포맷이 많이 다름.</li>
<li>NAVER LABS에서 만든 Kapture를 통해 포맷을 하나로 통합함.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="기술-오버뷰"><a href="#기술-오버뷰" class="headerlink" title="기술 오버뷰"></a>기술 오버뷰</h2><ol>
<li>Structure-based<ul>
<li>오래된 방법</li>
<li>3D reconstruction으로 point cloud를 생성하고, local feture matching을 통해 query image와 3d map같의 correspondence를 찾는 작업.</li>
</ul>
</li>
<li>Structure-based + Image retrieval<ul>
<li>Image retrieval 등을 사용해서 search space를 줄일 수 있음.</li>
</ul>
</li>
<li>Pose interpolation<ul>
<li>3D reconstruction을 수행하지 않고, reference image로부터 pose interpolation을 함.</li>
</ul>
</li>
<li>Relative pose estimation<ul>
<li>3D reconstruction을 수행하지 않고, reference image로부터 query image에 대한 relative pose estimation을 함.</li>
</ul>
</li>
<li>Scene poitn regression<ul>
<li>딥러닝을 사용하여 2D pixel location과 3D point의 correspondence를 구함.</li>
<li>Structured-based 방식과 비슷하지만, 딥러닝을 사용함.</li>
<li>학습 과정에서 3D reconstruction을 사용하기도 함.</li>
</ul>
</li>
<li>Absolute pose regression<ul>
<li>End-to-end로 이미지-&gt;pose regression을 수행함. </li>
</ul>
</li>
</ol>
<ul>
<li>각각의 방법들은 generalization과 정확도가 많이 다름.</li>
</ul>
<img src="/20210405-naver-labs-visloc/visual_localization_methods.png" class="" title="methods">
<img src="/20210405-naver-labs-visloc/comparison.png" class="" title="comparison">


<p> </p>
<hr>
<h2 id="Structure-based-methods"><a href="#Structure-based-methods" class="headerlink" title="Structure-based methods"></a>Structure-based methods</h2><ul>
<li>3D reconstruction을 사용하면 정확도가 높아진다.<ul>
<li>하지만 종종 3D reconstruction을 할 수 없거나, 3D map을 유지보수하기 어려운 상황이 있다.</li>
</ul>
</li>
</ul>
<ul>
<li>3D reconstruction은 Structure-from-Motion(SfM)으로 수행한다.<ul>
<li>SfM은 여러개의 이미지로부터 local feature correspondence를 기반으로 reconstruction을 수행한다.</li>
<li>Local feature correspondence는 feature descriptor들간의 거리를 기반으로 만들어진다.<ul>
<li>하지만 이 feature descriptor들은 사실 visual localization을 위해 만들어진 것이 아니기 때문에 여러 단점을 가지고 있다.</li>
<li>낮/밤, 계절, 날씨를 구분하지 못한다는 것들이다.</li>
<li>최근에는 딥러닝 기반으로 학습한 local feature를 사용하기도 한다.</li>
</ul>
</li>
<li>3D reconstruction은 local feature들간의 2D-2D correspondence를 통해 생성된다.</li>
</ul>
</li>
</ul>
<ul>
<li>Query 이미지의 위치를 계산하기 위해서는, query 이미지와 3D map간의 2D-3D correspondence를 descriptor matching으로 찾고, perspective-n-point (PnP) solver + RANSAC을 사용해서 위치 정보를 구한다.<ul>
<li>Large-scale map의 경우 3D map이 엄청나게 커지게 되며 query 이미지와 3D map에 대한 correspondence를 탐색할 때 엄청난 계산량을 필요로 한다.<ul>
<li>Aachen-Day-Night 데이터셋의 경우, 3D point는 약 70만개 ~ 250만개 정도 된다고 한다 (셋팅에 따라…).</li>
</ul>
</li>
<li>Search space를 줄이기 위해서는 image retrieval이 사용된다.<ul>
<li>비슷한 이미지를 가진 곳에서만 correspondence를 찾아보는 전략이다.</li>
</ul>
</li>
<li>Global map에서 correspondence를 찾는 방법이 아닌, retrieved image들로 local map을 만들어서 그 안에서 correspondence를 구할 수도 있다.<ul>
<li>이 경우 우리는 global map을 유지보수하지 않아도 된다.</li>
<li>하지만 retrieved image들로부터 local map을 항상 만들 수 있다는 보장이 없다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Image-retrieval-based-methods"><a href="#Image-retrieval-based-methods" class="headerlink" title="Image retrieval-based methods"></a>Image retrieval-based methods</h2><ul>
<li>Image retrieval-based 방식을 쓰려면 우선 데이터셋에 해당 이미지에 대한 위치정보 label이 필요하다.<ul>
<li>GPS 정보나 6DOF 위치 정보로 보통 되어있다.</li>
</ul>
</li>
<li>Image retrieval은 DB로부터 가장 비슷한 이미지를 찾는다.<ul>
<li>종종 re-ranking step이 함께 사용되기도 한다.</li>
<li>‘가장 비슷한 이미지’에 대한 criteria는 보통 2가지가 있다.<ol>
<li>Landmark retrieval - Query와 retrieved 이미지가 동일한 landmark를 많이 가지고 있을 때</li>
<li>Geolocalization / Place recognition - Query와 retrieved 이미지가 비슷한 위치에서 찍었을 때</li>
</ol>
</li>
</ul>
</li>
</ul>
<ul>
<li>예전에는 image retrieval을 위해 bag-of-visual-words를 사용했다.</li>
<li>하지만 최근에는 딥러닝으로 좀 더 high-level semantics를 포함하는 정보를 사용한다.<ul>
<li>Ranking loss를 사용해서 retrieval 작업에 특화된 네트워크를 만들기도 한다.</li>
</ul>
</li>
</ul>
<ul>
<li>Image retrieval을 사용해서 얻는 이점은 2가지이다.<ol>
<li>Structure-based 방식을 사용할 때 search space 줄이기</li>
<li>(Structure가 없을 때) Direct localization으로 pose를 구하기<ul>
<li>Nearest neighbour image나 k개의 이미지들의 interpolation으로 구할 수 있음.</li>
</ul>
</li>
</ol>
</li>
</ul>
<ul>
<li>Camera intrinsic이 있다면 relative pose를 구할 수도 있다.<ul>
<li>Retrieved image의 absolute pose도 안다면, query image의 absolute pose도 계산할 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Pose-regression-based-method"><a href="#Pose-regression-based-method" class="headerlink" title="Pose regression-based method"></a>Pose regression-based method</h2><ul>
<li>CNN을 이용해서 RGB-&gt;pose를 계산하는 end-to-end 방식<ul>
<li>Low-level feature에 pose estimation을 위한 정보가 들어있다는 가정을 가짐.</li>
<li>Transfer learning을 쓸 수 있다고 함?</li>
</ul>
</li>
<li>PoseNet의 경우, VGGNet/ResNet의 CNN 레이어와 FC 레이어를 사용해서 6D pose를 regress함. (기존의 VGGNet/ResNet은 FC 레이어 대신 softmax 레이어를 가지고 있음)</li>
</ul>
<ul>
<li>Pose regression 방식의 장점:<ul>
<li>Feature engineering이 필요없다.<ul>
<li>딥러닝이 알아서 robust feature를 학습함 (weather, viewpoint, lighting etc)</li>
</ul>
</li>
<li>Structure-based 방식보다 메모리를 훨씬 적게 사용한다.<ul>
<li>모델은 해봤자 50mb. SfM 포인트 클라우드는 Gb 단위.</li>
</ul>
</li>
<li>Transfer learning을 사용해서 중간 크기의 데이터셋에서 잘 쓸 수 있다.</li>
</ul>
</li>
<li>단점:<ul>
<li>좀 부정확한 편.</li>
</ul>
</li>
<li>PoseNet의 정확도를 높이기 위해 다음과 같은 방식이 시도되었다.<ul>
<li>새로운 loss function 사용</li>
<li>LSTM 레이어 추가</li>
<li>추가 센서 사용</li>
<li>Absolute + relative pose를 동시에 학습 - VLocNet<ul>
<li>거기에 semantic segmentation도 같이 수행 - VLocNet++</li>
</ul>
</li>
</ul>
</li>
<li>이러한 방식은 결국 scene에 대한 정보를 compression하는 것에 지나지 않는다.<ul>
<li>즉, generalize하기 어렵다.</li>
</ul>
</li>
</ul>
<ul>
<li>최근에는 hybrid pose learning 방식이 적용되고 있다.<ul>
<li>Learning은 좋은 2D-3D correspondence를 학습하는데에 집중하고, 기존의 structure-based + image retrieval 방식을 사용하던 파이프라인에서 집중하던 geometrical constraint를 그대로 사용하는 방식이 있다.</li>
<li>하지만 이러한 방식은 pose accuracy를 높여주나, 새로운 scene에 generalize되지 않는다.</li>
</ul>
</li>
</ul>
<ul>
<li>최근의 SANet의 경우 camera localization을 위한 scene agnostic neural architecture를 제안했다고 한다. (?)<ul>
<li>Model parameter와 scene이 독립적이라고 한다 (?)</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h2><ul>
<li>Geometric 방식이 아직 end-to-end 방식보다 좋다.<ul>
<li>하지만 최고의 성능을 내기 위해서는 geometric 방식에서 몇가지 모듈을 딥러닝 솔루션으로 바꾸는게 좋다.</li>
<li>예를 들어, local / global feature extractor는 바꾸는게 좋다.<ul>
<li>Handcrafted 방식들에 비해 좀 더 다양한 variation에 강인하기 때문이다.</li>
</ul>
</li>
</ul>
</li>
<li>Pure retrieval 기법은 rough location을 얻는데에 굉장히 좋은 솔루션이 될 수 있다.<ul>
<li>빠르고 계산량이 적기 때문이다.</li>
</ul>
</li>
<li>Regression pose generalization 분야는 아직 연구가 더 필요하다.</li>
<li>Semantic 정보를 사용하는 시도가 아직 크게 없다.<ul>
<li>이러한 시도는 local feature match의 성능을 높이거나</li>
<li>특정 object를 localization에 사용할 수 있다.<ul>
<li>반대로, 특정 object를 인식해서 localization에서 제외할 수 있다 (움직이는 물체들).</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="볼만한-논문들"><a href="#볼만한-논문들" class="headerlink" title="볼만한 논문들"></a>볼만한 논문들</h2><p>Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. In CVPR, 2016.</p>
<p>Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. DSAC – differentiable RANSAC for camera localization. In CVPR, 2017.</p>
<p>Eric Brachmann and Carsten Rother. Learning less is more – 6d camera localization via 3d surface regression. In CVPR, 2018.</p>
<p>Eric Brachmann and Carsten Rother. Expert sample consensus applied to camera re-localization. In ICCV, 2019.</p>
<p>Eric Brachmann and Carsten Rother. Visual camera re-localization from rgb and rgb-d images using dsac, arXiv, 2020.</p>
<p>(Brachmann 최고…)</p>
<p>Gabriela Csurka, Christopher R. Dance, and Martin Humenberger. From Handcrafted to Deep Local Invariant Features. arXiv, 1807.10254, 2018.</p>
<p>Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperPoint: Self-Supervised Interest Point Detection and Description. In CVPR Workshops, 2018.</p>
<p>Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net: a Trainable CNN for Joint Description and Detection of Local Features. In CVPR, 2019.</p>
<p>Martin Humenberger, Yohann Cabon, Nicolas Guerin, Julien Morat, Jerome Revaud, Philippe Rerole, Noe Pion, Cesar de Souza, Vincent Leroy, and Gabriela Csurka. Robust Image Retrieval-based Visual Localization using Kapture. arXiv:2007.13867, 2020.</p>
<p>Revaud Jerome, Philippe Weinzaepfel, Cesar De Souza, and Martin Humenberger.R2D2: Reliable and Repeatable Detectors and Descriptors. In NeurIPS, 2019.</p>
<p>Alex Kendall, Matthew Grimes, and Roberto Cipolla. PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. In ICCV, 2015.</p>
<p>Alex Kendall and Roberto Cipolla. Geometric Loss Functions for Camera Pose Regression with Deep Learning. In CVPR, 2017.</p>
<p>Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, and Juho Kannala. Hierarchical scene coordinate classification and regression for visual localization. In CVPR, 2020.</p>
<p>Liu Liu, Hongdong Li, and Yuchao Dai. Efficient Global 2D-3D Matching for CameraLocalization in a Large-Scale 3D Map. In ICCV, 2017.</p>
<p>Noé Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler. Benchmarking Image Retrieval for Visual Localization. In 3DV, 2020.</p>
<p>Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From Coarse to Fine: Robust Hierarchical Localization at Large Scale. In CVPR, 2019.</p>
<p>Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning Feature Matching with Graph Neural Networks. In CVPR, 2020.</p>
<p>Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6DoF Outdoor Visual Localization in Changing Conditions. In CVPR, 2018.</p>
<p>Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura Leal-Taixe. Understanding the Limitations of CNN-based Absolute Camera Pose Regression. In CVPR, 2019.</p>
<p>Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion Revisited. In CVPR, 2016.</p>
<p>Johannes Lutz Schonberger, Hans Hardmeier, Torsten Sattler, and Marc Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In CVPR, 2017.</p>
<p>Matthias Schorghuber, Daniel Steininger, Yohann Cabon, Martin Humenberger, and Margrit Gelautz. Slamantic-leveraging semantics to improve vslam in dynamic environments. In ICCV Workshops, 2019.</p>
<p>H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic, T. Pajdla, and A. Torii. InLoc: Indoor Visual Localization with Dense Matching and View Synthesis. PAMI, 2019.</p>
<p>Philippe Weinzaepfel, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. Visual localization by learning objects-of-interest dense match regression. In CVPR, June 2019.</p>
<p>Luwei Yang, Ziqian Bai, Chengzhou Tang, Honghua Li, Yasutaka Furukawa, and Ping Tan. Sanet: Scene agnostic network for camera localization. In ECCV, 2019.</p>
<p>Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura Leal-Taixe. To Learn or not to Learn: Visual Localization from Essential Matrices. ICRA, 2020.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>NAVER LABS</tag>
        <tag>Visual localization</tag>
      </tags>
  </entry>
  <entry>
    <title>3DGV 2021 - Representations and Computational Patterns in Spatial AI (Prof. Andrew Davison)</title>
    <url>/20210407-3dgv-andrew-davison/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/ERCqnJnLH9Y" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p>토크의 상당 부분은 <a href="https://changh95.github.io/20201226-CVPR-2020-SLAM-workshop-Davison/">이전 세미나</a>와 내용이 겹칠 것으로 예상된다. 이번 글에서는 새로 업데이트 된 내용만 커버하기로…</p>
<hr>
<h2 id="Spatial-AI-Perception-for-Embodied-Intelligence"><a href="#Spatial-AI-Perception-for-Embodied-Intelligence" class="headerlink" title="Spatial AI - Perception for Embodied Intelligence"></a>Spatial AI - Perception for Embodied Intelligence</h2><ul>
<li>To enable embodied AI, a Spatial AI system should build <strong>a persistent and understandable scene representation</strong> which is close to metric 3D geometry, at least locally.<ul>
<li>Map 처럼 생겼다던가…</li>
<li>쉽게 update 가능한다던가…</li>
<li>Planning 같은게 쉽게 만들어졌다던가…</li>
</ul>
</li>
<li>Generality…<ul>
<li>다른 도메인에 사용되더라도 Spatial AI는 공통적으로 가지는 특성이 있을 것이다.<ul>
<li>드론, 로봇, AR… 상관 없다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Rearrangement-A-Challenge-for-Embodied-AI"><a href="#Rearrangement-A-Challenge-for-Embodied-AI" class="headerlink" title="Rearrangement: A Challenge for Embodied AI"></a>Rearrangement: A Challenge for Embodied AI</h2><ul>
<li>다양한 분야의 전문가들과 함께 만든 discussion 논문.</li>
<li><strong>“Spatial perception, navigation, interaction을 위해 어떤 challenge를 푸는게 좋을까?”에 대한 전문가들의 답변.</strong></li>
<li>여러가지 질문들<ul>
<li>하나의 dominant task를 수행하기 위해 subtask는 어떻게 알아낼 것인가?</li>
<li>각각의 agent가 얼마나 task를 잘 수행하고 있는지 알 수 있을까?<ul>
<li>Task specification, settings, evaluation</li>
<li>로봇이 방을 치우는 task라면, tidy-state라는 것은 무엇인가?<ul>
<li>정확한 3D coordinate을 줄까?</li>
<li>대충 이미지 하나를 던져줄까?</li>
<li>Exploration을 통해 알아서 학습하게 할까? (강화학습?)</li>
</ul>
</li>
<li>다양한 ‘정확도’로 방이 치워질 수 있는데, 이 때 ‘방이 깔끔해진 정도’를 어떤 metric으로 결정해야할까?<ul>
<li>테이블 위에 접시를 놓고, 접시 위에 칼을 놔야한다면…</li>
<li>접시는 테이블 위 아무데나 있어도 되지만, 칼은 더 좁은 공간인 접시 위에 있어야한다. 이 경우 어떻게 score를 매길 것인가?</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>교수님이 이 연구에서 관심을 가지셨던것은 ‘<strong>어떤 방식으로 scene representation을 만들어야 real-time, persistent, updatable할 수 있을까?</strong>‘ 라고 하셨다.</li>
<li>이런 문제를 풀기 위해서 end-to-end로 푼다는 것은 아직 상상도 못하겠다.<ul>
<li>어떠한 intermediate representation이 꼭 필요할 것이라고 생각한다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="RLBench"><a href="#RLBench" class="headerlink" title="RLBench"></a>RLBench</h2><ul>
<li>Rearrangement 논문에서 참고된 시뮬레이션 소프트웨어.<ul>
<li>Robot learning을 위해 만듬.</li>
</ul>
</li>
<li>Prof Andrew Davison은 원래 벤치마크를 별로 안좋아하심<ul>
<li>근데 이건 교수님께서 직접 만드셨네…? ㄷㄷ 뭐지</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Creating-map-representations…"><a href="#Creating-map-representations…" class="headerlink" title="Creating map representations…"></a>Creating map representations…</h2><ul>
<li><a href="https://changh95.github.io/20201226-CVPR-2020-SLAM-workshop-Davison/">이전 세미나</a>와 내용이 많이 겹침</li>
<li>MonoSLAM</li>
<li>ElasticFusion</li>
<li>SemanticFusion</li>
<li>Semantic SLAM Computation Graph</li>
<li>SLAM meets deep learing<ul>
<li>End-to-End algorithm &lt;-&gt; Fully human-designed algorithm</li>
</ul>
</li>
<li>CodeSLAM, SceneCode, DeepFactors</li>
</ul>
<p> </p>
<hr>
<h2 id="iMAP-Implicit-Mapping-and-Positioning-in-Real-Time"><a href="#iMAP-Implicit-Mapping-and-Positioning-in-Real-Time" class="headerlink" title="iMAP - Implicit Mapping and Positioning in Real-Time"></a>iMAP - Implicit Mapping and Positioning in Real-Time</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/c-zkKGArl5Y" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<ul>
<li>Uses <strong>implicit neural representations</strong>, like how <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDg5MzQ=">NeRF<i class="fa fa-external-link-alt"></i></span> is using the thing<ul>
<li>하지만 이 논문에서는 RGB-D를 사용하기 때문에 훨씬 쉽다고 교수님이 얘기하심.</li>
</ul>
</li>
<li>Multi-layer perceptron 내부에서 처리되는 값들만을 가지고 실시간으로 scene representation을 표현.<ul>
<li>사실상 <strong>실시간으로 multi-layer perceptron을 학습</strong>하면서 값들을 사용하는 것.</li>
<li><strong>xyz coordinate을 인풋으로 주면, scene속에 있는 어떤 3D point의 occupancy와 colour 값을 아웃풋으로 내뱉는 네트워크</strong>임.</li>
</ul>
</li>
<li>카메라가 어디있는지 알고 있고, 센서 인풋으로 raw colour + depth 이미지가 들어올 때, 우리는 <strong>MLP 네트워크가 내뱉는 아웃풋이 raw colour + depth 이미지에 매칭되게 optimise</strong>할 수 있다.<ul>
<li>즉, RGB+Depth 이미지는 optimizing cost function을 위해서만 사용되고, 실제로 SLAM 계산에는 사용되지 않음? </li>
</ul>
</li>
<li>이러한 결과물을 SLAM 시스템으로 변환하기 위해서는, RGB+Depth render 값들이 어떠한 하나의 map representation으로 수렴할 수 있어야한다.<ul>
<li>여기서 <strong>Keyframe 구조</strong>를 사용해서 맵을 딴다.</li>
<li>교수님께서는 PTAM을 예시로 들면서 tracking / mapping이 쓰레드로 따로 도는 것을 설명하셨다.</li>
<li>영상에서 빨간색 피라미드 형태로 나타나는 것이 keyframe이다.</li>
</ul>
</li>
<li>Keyframe은 DB를 만들어서 관리한다.<ul>
<li>Mapping thread에서는 이전에 취득한 키프레임과 가장 최신의 프레임을 이용해서 정보를 공유한다.</li>
<li>모든 키프레임을 다 쓰는거는 아니다. 그러면 너무 느릴것이다.</li>
<li>**’가장 유용한 키프레임’**을 고르고, 그 후 그 키프레임에서 **’가장 유용한 point’**를 뽑아 최적화한다.<ul>
<li>이런 방식으로 <strong>계산해야할 포인트의 수를 줄여서 실시간 작동</strong>을 구현할 수 있었다.</li>
<li>2Hz (ㅋㅋ 겨우 실시간)으로 실시간 맵핑을 할 수 있었다.</li>
</ul>
</li>
</ul>
</li>
<li>진짜 신기한건, 아직 보지도 않은 공간의 데이터도 대충 채워넣을 수 있다는 것이다.<ul>
<li>딥러닝 없이 RGB-D SLAM으로 했을 경우에는, 보지 못한 공간은 구멍이 숭숭 뚫려있다.</li>
</ul>
</li>
<li>결국에 그냥 SLAM한거랑 뭐가 다른거지…? 라는 생각이 들긴했지만,<ul>
<li>교수님 말씀으로는 ‘<strong>하나의 네트워크를 가지고 tracking + mapping을 다 할 수 있는 neural representation을 만든 것이다</strong>‘ 라고 하심</li>
<li>그리고 그 네트워크는 <strong>단지 1mb의 작은 모델</strong>이다. (호우,,,)</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Object-based-SLAM"><a href="#Object-based-SLAM" class="headerlink" title="Object-based SLAM"></a>Object-based SLAM</h2><ul>
<li><a href="https://changh95.github.io/20201226-CVPR-2020-SLAM-workshop-Davison/">이전 세미나</a>와 내용이 많이 겹침</li>
<li>SLAM++</li>
<li>MoreFusion</li>
<li>Fusion++</li>
<li>NodeSLAM</li>
</ul>
<p> </p>
<hr>
<h2 id="Processor-for-Spatial-AI"><a href="#Processor-for-Spatial-AI" class="headerlink" title="Processor for Spatial AI"></a>Processor for Spatial AI</h2><ul>
<li><a href="https://changh95.github.io/20201226-CVPR-2020-SLAM-workshop-Davison/">이전 세미나</a>와 내용이 많이 겹침</li>
<li>SLAMBench</li>
<li>Graphcore IPU</li>
<li>Futuremapping</li>
<li>Bundle Adjustment on a Graph Processor </li>
</ul>
<p> </p>
<hr>
<h2 id="Research-plans-for-Gaussian-Belief-Propagation-amp-Factorized-Computation"><a href="#Research-plans-for-Gaussian-Belief-Propagation-amp-Factorized-Computation" class="headerlink" title="Research plans for Gaussian Belief Propagation & Factorized Computation"></a>Research plans for Gaussian Belief Propagation &amp; Factorized Computation</h2><ul>
<li>Dynamic, self-abstracting graph map: Object-based SLAM?</li>
<li>General dense front-end image processing<ul>
<li>Unifying flow estimation and segmentation</li>
<li>Smart cameras</li>
</ul>
</li>
<li>Multi-device distributed mapping<ul>
<li>Towards a robot web?</li>
</ul>
</li>
<li>GBP learning<ul>
<li>Continual, self-supervised convergent</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="질문"><a href="#질문" class="headerlink" title="질문"></a>질문</h2><p>Q. iMAP에서 implicit neural representation을 쓴 이유는 무엇인가요? 그냥 mesh에 optimisation하면 안되나요?<br>A. Mesh optimisation은 굉장히 어렵다. Topology 자체가 굉장히 복잡한데, 그 위에 새로운 measurement가 생길때마다 editing을 해야한다면 끔찍하다. 솔직히 어떻게 해야할지 모르겠다. 3D representation에는 여러 방법이 있고 각각의 장단점이 있다.</p>
<p>Q. iMap을 large-scale로 가져가려면 어떤 난관이 있을까요?<br>A. Large-scale을 고려하고 실험한적은 아직 없다. 아마 더 큰 네트워크가 필요하지 않을까? Reprsentation을 더 풍부하게 하고 싶다면 네트워크는 커져야할것이다.</p>
<p>Q. iMap의 경우 online learning이 된다는 것이 굉장히 특이하고 신기하지만, 기존의 Deep SLAM 방법론들은 미리 학습해온 모델을 사용하는 경우가 많습니다. 이 두가지를 혼용할 경우 정확도는 당연히 떨어지게 될 것으로 보이고, 이는 두가지 방법론이 적대적으로 보이기도 하는데, 어떻게 이 두가지 연구방법론을 유지하면서 정확도/성능을 높일 생각이신가요? (Stefan Leutenegger 교수님 질문 ㄷㄷ, Davison 교수님 당황 ㅋㅋ)<br>A. 이 방법론 둘 다 해야한다. 하지만 우리는 두 방법이 각각 장단점을 가지고 있다는 것을 안다. Pre-learned된 방법들은 적당히 좋은 성능을 보이지만, 새로운 환경에 가져갔을 때 항상 성능이 애매해지는 것을 볼 수 있다. iMAP 같은 방법은 새로운 환경에서도 바닥부터 새로 트레이닝하면서 좋은 결과를 만들 수 있지만, 매번 새롭게 트레이닝 해야한다는게 매우 귀찮을 것이다. 미래의 로봇은 사실 이 두개를 동시에 다 해야할것이다. 로봇을 샀으면 키자마자 뭔가를 해야하지 않을까? (ㅋㅋ) 하지만 새로운 환경인만큼 새로운 무언가를 배우긴 해야할 것이다. 예를 들어 기본적인 맵핑 알고리즘들은 탑재했지만, semantic 정보를 한 10~30분정도 학습하는건 어떨까? Bayesian의 생각으로 이야기할 때, 결국 <strong>우리는 새로운 것을 배우는 방법</strong>과 <strong>이전에 배운 것 (prior)을 잘 활용하는 방법들</strong>을 <strong>잘 밸런스</strong> 할 수 있지 않을까 생각한다.</p>
<p>Q. 이 두 방법을 밸런스해야할까요? 아니면 두개를 병렬처리해야할까요?? (ㄷㄷㄷ)<br>A. </p>
<p>Q. Unified representation이 정말 필요할까요?<br>A. 당연히 상황에 따라서 여러 파라미터가 조정되어야한다. Rearrange task의 경우에는 semantic class / instance도 잘 알아야할 것이고, 정확도도 중요할 것이다. 단순히 장애물을 회피하는 로봇이라면 굳이 그런걸 알 필요는 없다. Manipulation 로봇이라면 mm 단위의 정확도가 필요할수도 있고, 아닐수도 있다 (suction cup같은걸 쓰면 mm단위는 필요없다). Representation에 대해 연구하면서 여러가지 고민을 새롭게 하게 된다.</p>
<p>Q. iMAP이 맵 하나를 implicit representation에 넣는 것은 잘 보았다. Object-level SLAM을 할 때도 implicit representation이 될 것 같은가? 아니면 graph라던지 explicit representation밖에 없는것인가? (Angela Dai 교수님 질문)<br>A. 아직 잘 모르겠다. 하지만 iMAP을 만들면서 본 것이, 매 프레임마다 맵 자체가 굉장히 흔들리는 것을 볼 수 있다. 이 흔들림은 사실 SGD 등을 통해 네트워크를 최적화하면서 모델 weight가 바뀌는데, 이 때 특정 object (map안에 있는 축구공이라던지)를 자세히 보면, 이 흔들림이 object단위로 균일하게 흔들리는 것을 보았다. 아마 이 object에 대한 weight 파라미터는 그리 많지 않을 것이다. 나는 이를 통해 네트워크 안에 implicit한 형태로 object에 대한 representation이 들어있다고 보고있다. 이러한 representation을 code로 뽑아낸다던지… 등은 아직 잘 모르겠다.</p>
<p>Q. 그렇다면 몇개의 파라미터만 바꿔서 object를 다룰 수 있게 된다면…  Object를 마음대로 옮긴다던지, 또는 scene deformation을 만들어서 loop closure라던지 구현할 수 있을까요? 아무래도 SLAM 특성 상 이런 방향으로 가고싶을 것 같은데요.(Stefan Leutenegger 교수님 질문)<br>A. 나도 이런 방향으로 연구하고싶다 (ㅋㅋ) 몇개의 파라미터만 조정해서 loop closure를 만들 수 있을까? 그게 될지는 잘 모르겠다. 진짜 잘 모르겠음 (이 말 하시고 고민에 빠지심 ㅋㅋ)</p>
<p>Q. Rearrangement를 할 때, indirect한 effect가 있다면 이것은 implicit representation이 되야할까? explicit representation이 되야할까?<br>A. </p>
<p>Q. CAD-model 없이 Object-level SLAM이 가능할까요?<br>A. 어후… 어려울거같다. Class만 가지고 모든 instance의 geometry를 추정할 수 있을까? 사람은 가방을 보았을 때 어디에 포켓이 있고 어디에 지퍼가 있고 등등 잘 알고있지만, 딥러닝은 아직 그정도까지 깊게 이해하고 있지는 않는 것 같다. 간단한 컵/접시 모양은 할 수 있겠다만, 이것 이상의 복잡한 물체들은 아직 많이 어렵다고 본다.<br>코멘트: 그게 된다면 그게 Spatial AI일듯 ㅋㅋ (Angela Dai교수님 ㅋㅋ)</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Deep SLAM</tag>
        <tag>Rearrangement</tag>
        <tag>Embodied AI</tag>
        <tag>Spatial AI</tag>
        <tag>RLBench</tag>
        <tag>iMAP</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAMANTIC – Leveraging semantics to improve VSLAM in dynamic environments (NAVER LABS Europe)</title>
    <url>/20210408-slamantic-blog/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9ldXJvcGUubmF2ZXJsYWJzLmNvbS9ibG9nL3NsYW1hbnRpYy1sZXZlcmFnaW5nLXNlbWFudGljcy10by1pbXByb3ZlLXZzbGFtLWluLWR5bmFtaWMtZW52aXJvbm1lbnRzLTIv">원본 글 링크<i class="fa fa-external-link-alt"></i></span></p>
<p>네이버랩스 유럽의 블로그를 통해 Visual localization 기술을 공부하다가 SLAMANTIC에 대한 블로그 아티클을 보게 되었다. 서울에서 열린 ICCV 2019 학회에서 이 연구를 처음 보았지만, 당시에는 잘 이해하지 못해서 그냥 ‘오 딥슬램!’ 정도로 생각하고 넘어갔었다.</p>
<p>최근에는 semantic 정보를 정말 잘 활용할 수 있는 SLAM이 필요하겠다라는 생각이 들었는데, 블로그 탐방을 하다가 이 논문을 보게 되면서 다시 관심을 가지게 되었다.</p>
<p> </p>
<hr>
<h2 id="Visual-SLAM"><a href="#Visual-SLAM" class="headerlink" title="Visual SLAM"></a>Visual SLAM</h2><ul>
<li>SLAM은 원래 static한 환경을 전제로 하며, 연속된 측량을 통해 map을 계속 이어가는 방식임.<ul>
<li>여러 observation을 기반으로 triangulation을 하며 3D measurement를 만들어감.</li>
</ul>
</li>
<li>측량중에 환경이 움직여버리다면 (i.e. 움직이는 물체가 있다면), static한 환경이라는 전제가 깨져버리기 때문에 SLAM이 잘 되지 않음.<ul>
<li>움직여버린다면 triangulation이 잘 되지 않음.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="SLAMANTIC"><a href="#SLAMANTIC" class="headerlink" title="SLAMANTIC"></a>SLAMANTIC</h2><ul>
<li>Semantic segmentation을 통해서 각각의 object class마다 (i.e. car, person, building etc) 생겨난 map point에 confidence measure와 detection consistency를 계산한다.</li>
<li>높은 confidence를 가진 point를 이용해서 낮은 confidence를 가진 point에 대해 verification을 수행하고, 최종적으로 pose와 map point를 계산한다.</li>
</ul>
<p> </p>
<hr>
<p>아 이거도 블로그 글이 너무 짧아서 논문으로 제대로 읽어야겠네</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>NAVER LABS</tag>
        <tag>SLAMANTIC</tag>
        <tag>Semantic SLAM</tag>
        <tag>Visual SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>관심있는 GTC 2021 토크</title>
    <url>/20210415-gtc-2021-lists/</url>
    <content><![CDATA[<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RpdmUlMjBpbnRvJTIwRGVlcCUyMExlYXJuaW5nJTNBJTIwQ29kZSUyMFNpZGUtYnktU2lkZSUyMHdpdGglMjBNWE5ldCUyQyUyMFB5VG9yY2glMkMlMjBhbmQlMjBUZW5zb3JGbG93JTIwJTIwJTVCUzMxNjkyJTVELzFfdG5yZDlobDcvMjA0Njc4MDUz">Dive into Deep Learning: Code Side-by-Side with MXNet, PyTorch, and TensorFlow [S31692]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RlZXAlMjBMZWFybmluZyUyMERlbXlzdGlmaWVkJTIwJTVCUzMxODQzJTVELzFfNXZ1NGdtdGc=">Deep Learning Demystified [S31843]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0pldHNvbiUyMDEwMSUzQSUyMCUyMExlYXJuaW5nJTIwRWRnZSUyMEFJJTIwRnVuZGFtZW50YWxzJTIwJTVCUzMyNzAwJTVELzFfbjNrNWZ6dWM=">Jetson 101: Learning Edge AI Fundamentals [S32700]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL01vZGVybiUyMEFydGlmaWNpYWwlMjBJbnRlbGxpZ2VuY2UlMjAxOTgwcy0yMDIxJTIwYW5kJTIwQmV5b25kJTIwJTVCRTMzMjcyJTVELzFfdDN0aGI0c3g=">Modern Artificial Intelligence 1980s-2021 and Beyond [E33272]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RoZSUyMEVuZXJneS1CYXNlZCUyMHZpZXclMjBvZiUyMFNlbGYtU3VwZXJ2aXNlZCUyMExlYXJuaW5nJTIwJTVCUzMzMjY4JTVELzFfODA5a2pkeHE=">The Energy-Based view of Self-Supervised Learning [S33268]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RyYW5zZmVyJTIwTGVhcm5pbmclMjBUb29sa2l0JTIwYW5kJTIwRGVlcFN0cmVhbSUyMFNESyUyMGZvciUyMFZpc2lvbiUyMEFJX0ludGVsbGlnZW50JTIwVmlkZW8lMjBBbmFseXRpY3MlMjAlNUJDV0VTMTEyNyU1RC8xX3ZuNGprNXc4">Transfer Learning Toolkit and DeepStream SDK for Vision AI/Intelligent Video Analytics [CWES1127]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0FjY2VsZXJhdGUlMjBEZWVwJTIwTGVhcm5pbmclMjBJbmZlcmVuY2UlMjB3aXRoJTIwVGVuc29yUlQlMjA4LjAlMjAlNUJTMzE4NzYlNUQvMV9yaGh2NWFpcS8yMDQ2NzgwNzM=">Accelerate Deep Learning Inference with TensorRT 8.0 [S31876]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RlbnNvclJUJTIwUXVpY2slMjBTdGFydCUyMEd1aWRlJTIwJTVCUzMxODI4JTVELzFfOGViemRmMTE=">TensorRT Quick Start Guide [S31828]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0tlcmFzJTIwYW5kJTIwVGVuc29yRmxvdyUzQSUyMFRoZSUyME5leHQlMjBGaXZlJTIwWWVhcnMlMjAlNUJTMzE5MjUlNUQvMV9hNGxqanV5eS8yMDQ2NzgwNTM=">Keras and TensorFlow: The Next Five Years [S31925]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RoZSUyMFN0YXRlJTIwb2YlMjBQeVRvcmNoJTIwJTVCUzMxMjIzJTVELzFfdTVlMmQ3ZmkvMjA0Njc4MDUz">The State of PyTorch [S31223]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1B5VG9yY2glMjBQZXJmb3JtYW5jZSUyMFR1bmluZyUyMEd1aWRlJTIwJTVCUzMxODMxJTVELzFfbmN0ZGM4c3kvMjA0Njc4MDUz">PyTorch Performance Tuning Guide [S31831]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0tPUkVBJTIwQUklMjBEZXZlbG9wZXIlMjBNZWV0dXAlMjAlNUJTRTIzMzMlNUQvMV94dm5vejI2Zw==">KOREA AI Developer Meetup [SE2333]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RlZXAlMjBMZWFybmluZyUyQyUyME1hY2hpbmUlMjBMZWFybmluZyUyQyUyMGFuZCUyMERhdGElMjBTY2llbmNlJTIwJTVCQ1dFUzE5NjYlNUQvMV81OWEwZTkwcg==">Deep Learning, Machine Learning, and Data Science [CWES1966]<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="CUDA-GPU-Acceleration"><a href="#CUDA-GPU-Acceleration" class="headerlink" title="CUDA / GPU Acceleration"></a>CUDA / GPU Acceleration</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1plcm8lMjB0byUyMEdQVSUyMEhlcm8lMjB3aXRoJTIwT3BlbkFDQyUyMCU1QlMzMTgxNiU1RC8xXzd3anh4NGZq">Zero to GPU Hero with OpenACC [S31816]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL011bHRpLUdQVSUyMFByb2dyYW1taW5nJTIwd2l0aCUyMENVREElMkMlMjBHUFVEaXJlY3QlMkMlMjBOQ0NMJTJDJTIwTlZTSE1FTSUyQyUyMGFuZCUyME1QSSUyMCU1QkNXRVMxMDg0JTVELzFfMWUwMmpoeno=">Multi-GPU Programming with CUDA, GPUDirect, NCCL, NVSHMEM, and MPI [CWES1084]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0Z1dHVyZSUyMG9mJTIwU3RhbmRhcmQlMjBhbmQlMjBDVURBJTIwQyUyQiUyQiUyMCU1QkNXRVMxODAyJTVELzFfYnF5cG11MjM=">Future of Standard and CUDA C++ [CWES1802]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RlbnNvciUyMENvcmUtQWNjZWxlcmF0ZWQlMjBNYXRoJTIwTGlicmFyaWVzJTIwZm9yJTIwRGVuc2UlMjBhbmQlMjBTcGFyc2UlMjBMaW5lYXIlMjBBbGdlYnJhJTIwaW4lMjBBSSUyMGFuZCUyMEhQQyUyMCU1QkNXRVMxMDk4JTVELzFfejZlcHQxa3k=">Tensor Core-Accelerated Math Libraries for Dense and Sparse Linear Algebra in AI and HPC [CWES1098]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RocnVzdCUyQyUyMENVQiUyQyUyMGFuZCUyMGxpYmN1JTJCJTJCJTIwVXNlcidzJTIwRm9ydW0lMjAlNUJDV0VTMTgwMSU1RC8xX293dXo5bTFq">Thrust, CUB, and libcu++ User’s Forum [CWES1801]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0luc2lkZSUyME5WQyUyQiUyQiUyMGFuZCUyME5WRk9SVFJBTiUyMCU1QlMzMTM1OCU1RC8xX2h3Mm1tN2hu">Inside NVC++ and NVFORTRAN [S31358]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0NVREElM0ElMjBOZXclMjBGZWF0dXJlcyUyMGFuZCUyMEJleW9uZCUyMCU1QlMzMTg1NyU1RC8xX2hmdHVxNGMz">CUDA: New Features and Beyond [S31857]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1JlY2VudCUyMERldmVsb3BtZW50cyUyMGluJTIwTlZJRElBJTIwTWF0aCUyMExpYnJhcmllcyUyMCU1QlMzMTc1NCU1RC8xX3poNTg1Znp3">Recent Developments in NVIDIA Math Libraries [S31754]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RoZSUyME5WSURJQSUyMEMlMkIlMkIlMjBTdGFuZGFyZCUyMExpYnJhcnklMjAlNUJTMzEzNTklNUQvMV9jejhvMW40ag==">The NVIDIA C++ Standard Library [S31359]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1ByZXNlbnQlMjBhbmQlMjBGdXR1cmUlMjBvZiUyMEFjY2VsZXJhdGVkJTIwQ29tcHV0aW5nJTIwUHJvZ3JhbW1pbmclMjBBcHByb2FjaGVzJTIwJTVCUzMxMTQ2JTVELzFfZXkxYTd0MXE=">Present and Future of Accelerated Computing Programming Approaches [S31146]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0l0JUUyJTgwJTk5cyUyMEFsaXZlJTNBJTIwQ1VEQSUyMGluJTIwVmlzdWFsJTIwU3R1ZGlvJTIwQ29kZSElMjAlNUJTMzE4ODQlNUQvMV9nZWllNmgxMQ==">It’s Alive: CUDA in Visual Studio Code! [S31884]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0dQVSUyMFBlcmZvcm1hbmNlJTIwQW5hbHlzaXMlMjBhbmQlMjBPcHRpbWl6YXRpb24lMjAlNUJDV0VTMTk2OSU1RC8xX3E5MnFweDgy">GPU Performance Analysis and Optimization [CWES1969]<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Industry-trend"><a href="#Industry-trend" class="headerlink" title="Industry trend"></a>Industry trend</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0ElMjBWaXNpb24lMjBmb3IlMjB0aGUlMjBOZXh0JTIwRGVjYWRlJTIwb2YlMjBDb21wdXRpbmclMjAlNUJTMzE5MDAlNUQvMV96ZWY4czdrbQ==">A Vision for the Next Decade of Computing [S31900]<i class="fa fa-external-link-alt"></i></span><ul>
<li>ARM architecture와 딥러닝</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0FuJTIwT3ZlcnZpZXclMjBvZiUyME5WSURJQSUyMENsb3VkWFIlMjAlNUJTMzIwMDAlNUQvMV8xdTFzZmVidC8yMDQ2Nzg0MDM=">An Overview of NVIDIA CloudXR [S32000]<i class="fa fa-external-link-alt"></i></span><ul>
<li>VR/AR</li>
</ul>
</li>
</ul>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RlZXAlMjBMZWFybmluZyUyMGluJTIwUm9ib3RpYyUyMEF1dG9tYXRpb24lMjBhbmQlMjBXYXJlaG91c2UlMjBMb2dpc3RpY3MlMjAlNUJTMzIxMzklNUQvMV81anFmYzJjYQ==">Deep Learning in Robotic Automation and Warehouse Logistics [S32139]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0FsbCUyMFRoaW5ncyUyMEpldHNvbiUyMCU1QkNXRVMxMTM0JTVELzFfaTdxaXcxM3I=">All Things Jetson [CWES1134]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1JlcHJlc2VudGF0aW9uJTIwTGVhcm5pbmclMjBmb3IlMjBBdXRvbm9tb3VzJTIwUm9ib3RzJTIwJTVCUzMxODc1JTVELzFfZ2Njc3Fubzg=">Representation Learning for Autonomous Robots [S31875]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0ludHJvZHVjdGlvbiUyMHRvJTIwR2Vvc3BhdGlhbCUyMEludGVsbGlnZW5jZSUzQSUyMFByZXNlbnQlMjBhbmQlMjBGdXR1cmUlMjAoUHJlc2VudGVkJTIwYnklMjBVU0dJRiklMjAlNUJTUzMyOTkzJTVELzFfeGU5OW4zNXQ=">Introduction to Geospatial Intelligence: Present and Future (Presented by USGIF) [SS32993]<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Automotive"><a href="#Automotive" class="headerlink" title="Automotive"></a>Automotive</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL01hcHBpbmclMjBhbmQlMjBMb2NhbGl6YXRpb24lMjB3aXRoJTIwRFJJVkUlMjBBViUyMCU1QlNFMzA2OCU1RC8xX3J1OGo2eTNp">Mapping and Localization with DRIVE AV [SE3068]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL05vJTIwRHJpdmVyJTIwTm8lMjBQcm9ibGVtLiUyMFRoZSUyMEtvcmVhbiUyMFN0YXJ0dXBzJTIwQmVoaW5kJTIwdGhlJTIwU2VsZi1Ecml2aW5nJTIwQ2FyJTIwUmV2b2x1dGlvbi4lMjAlNUJTMzI2NTAlNUQvMV93eGQ0ZDA5MS8yMDQ2Nzg2MTM=">No Driver? No Problem. The Korean Startups Behind the Self-Driving Car Revolution. [S32650]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RyaXZpbmclMjB0aGUlMjBSb2JvdGF4aSUyMFJldm9sdXRpb24lMjB3aXRoJTIwWm9veCUyMENUTyUyMEplc3NlJTIwTGV2aW5zb24lMjAlNUJTMzI0NDUlNUQvMV8zZzI3ZWg0cQ==">Driving the Robotaxi Revolution with Zoox CTO Jesse Levinson [S32445]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0ElMjBGdXR1cmUlMjB3aXRoJTIwU2VsZi1Ecml2aW5nJTIwVmVoaWNsZXMlMjAlNUJTMzIwOTIlNUQvMV83bW9qeTVmeA==">A Future with Self-Driving Vehicles [S32092]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1RyYW5zZm9ybWluZyUyMHRoZSUyMEF1dG9tb3RpdmUlMjBGdXR1cmUlM0ElMjBJbm5vdmF0aW9ucyUyMGluJTIwRGlnaXRhbGl6YXRpb24lMkMlMjBFbGVjdHJpZmljYXRpb24lMkMlMjBhbmQlMjBTdXN0YWluYWJpbGl0eSUyMCU1QkUzMjUyMCU1RC8xX2NtcHZjNjBn">Transforming the Automotive Future: Innovations in Digitalization, Electrification, and Sustainability [E32520]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1BlcmNlcHRpb24lMjBEZXZlbG9wbWVudCUyMGZvciUyMEF1dG9ub21vdXMlMjBWZWhpY2xlcyUyMCU1QkNXRVMxOTY0JTVELzFfd3VyZ2lpcTQ=">Perception Development for Autonomous Vehicles [CWES1964]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL05WSURJQSUyME9tbml2ZXJzZSUyMCUyQiUyMERyaXZlU2ltJTIwZm9yJTIwU3ludGhldGljJTIwRGF0YSUyMEdlbmVyYXRpb24lMjAlNUJTMzIwNDIlNUQvMV9jazQ3cTRrag==">NVIDIA Omniverse + DriveSim for Synthetic Data Generation [S32042]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0RldmVsb3BpbmclMjBhJTIwRnVsbHklMjBBdXRvbm9tb3VzJTIwVmVoaWNsZSUyMGZyb20lMjB0aGUlMjBHcm91bmQlMjBVcCUyMCU1QlMzMzIzMyU1RC8xX2kwYjQ3YnhwLzIwNDY3ODQ0Mw==">Developing a Fully Autonomous Vehicle from the Ground Up [S33233]<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL0dQVS1CYXNlZCUyMFJlYWwtVGltZSUyMEluZmVyZW5jZSUyMGZvciUyMEF1dG9ub21vdXMlMjBWZWhpY2xlcyUyMCU1QlMzMTc1NiU1RC8xX2Z3eGg0OHFnLzIwNDY3ODQ0Mw==">GPU-Based Real-Time Inference for Autonomous Vehicles [S31756]<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Healthcare"><a href="#Healthcare" class="headerlink" title="Healthcare"></a>Healthcare</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndGMyMS5ldmVudC5udmlkaWEuY29tL21lZGlhL1N1cmdpY2FsJTIwVmlzaW9uJTNBJTIwVW5kZXJzdGFuZGluZyUyMFN1cmdpY2FsJTIwU2NlbmVzJTIwVXNpbmclMjBDb21wdXRlciUyMFZpc2lvbiUyMCU1QkUzMTkyMSU1RC8xX2d2dGhsZ3Nq">Surgical Vision: Understanding Surgical Scenes Using Computer Vision [E31921]<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>NVIDIA</tag>
        <tag>GTC</tag>
      </tags>
  </entry>
  <entry>
    <title>CV / 이력서 작성 요령</title>
    <url>/20210528-work-history/</url>
    <content><![CDATA[<h2 id="이력서는-왜-잘-써야-하는가"><a href="#이력서는-왜-잘-써야-하는가" class="headerlink" title="이력서는 왜 잘 써야 하는가?"></a>이력서는 왜 잘 써야 하는가?</h2><p>이런 당연한걸 왜 묻는걸까? </p>
<p>당연히 지원/취직/이직에 성공하기 위해서이다.</p>
<p>좋게 써야지 나를 잘 봐줄 것이 아닌가?</p>
<img src="/20210528-work-history/ez.jpeg" class="" title="ez">

<p>그렇다면 조금 다르게 질문을 해보자.</p>
<blockquote>
<p><strong>‘좋은 이력서는 지원 과정에서 어떤 긍정적인 효과를 불러오는가?’</strong></p>
</blockquote>
<p>이 질문에도 똑같이 ‘합격’이라는 좋은 효과를 불러온다, 라고 할 수 있겠다.</p>
<p>하지만 좋은 이력서 하나만으로는 모든 면접과정을 통과할 수는 없다.</p>
<p>또, 반대로 이력서가 안좋은데 합격하는 경우 역시 없다.</p>
<p>그렇기에 이번 글은 <strong>이력서의 목적을 이해</strong>하고, 이에 맞춰 <strong>좋은 이력서를 적기 위한 전략</strong>에 대해 알아본다.</p>
<p> </p>
<hr>
<h2 id="서류전형-통과를-위해-이력서-전략"><a href="#서류전형-통과를-위해-이력서-전략" class="headerlink" title="서류전형 통과를 위해 이력서 전략"></a>서류전형 통과를 위해 이력서 전략</h2><p>이력서의 목적에는 두가지가 있으며 각각의 중요도는 다음과 같다.</p>
<ol>
<li>서류전형 통과 (70%)</li>
<li>면접전형 중 질문거리 제공 (30%)</li>
</ol>
<p>이력서의 가장 중요한 역할은 <strong>서류전형을 통과</strong>하는 것이다. 서류전형을 통과하지 못하면 아무리 그 뒤의 1차/2차 면접을 준비한들 의미가 없다.</p>
<p>우선 알아야할 것은, 회사의 규모에 따라서 이력서를 검토하는 방식이 다르다는 것이다.</p>
<p><strong>소규모 회사의 경우 해당 포지션의 팀 리더가 이력서를 검토하고 서류전형의 합/불을 판단</strong>한다. 그리고 <strong>회사의 규모가 커질수록 인사팀이 이력서를 검토하고 서류전형의 합/불을 판단</strong>하게 된다. 지원자가 많아질수록 팀 리더가 이력서를 검토하는데에 많은 시간을 할애할 수 없기 때문이다.</p>
<p>이 때, 누가 이력서를 검토하는지를 미리 알 수 있다면 각각 전략을 맞춰 세울 수 있다.</p>
<div class="tabs" id="fields"><ul class="nav-tabs"><li class="tab active"><a href="#fields-1">팀 리더가 내 이력서를 봐주는 경우</a></li><li class="tab"><a href="#fields-2">인사팀이 내 이력서를 봐주는 경우</a></li></ul><div class="tab-content"><div class="tab-pane active" id="fields-1"><p>팀 리더가 이력서를 검토할 때 가장 먼저 보는 것은 <strong>기본적인 조건</strong>들을 충족하는지이다. </p>
<blockquote>
<p>그렇기에 우리는 우선 <strong>채용공고에 적힌 필수 조건들을 충족한다는 점을 최대한 어필</strong>해야한다. </p>
</blockquote>
<p>이러한 필수 조건들을 대부분 “XX분야에 대한 경험 보유자” 정도로 뭉뚱그려져있다. 이는 “<strong>XX분야를 직접 해봐야지만 알 수 있는 점들을 경험한 사람</strong>“을 찾는 것으로 이해할 수 있기 때문에, 진행한 프로젝트에서 특정 디테일들을 잘 적어주면 좋다.</p>
<p>팀 리더가 이력서를 검토할 때 고려하는 다른 하나의 점은, <strong>팀 내부적으로 “XX분야 경험을 가진 사람이 왔으면 좋겠다” 라는 희망</strong>을 충족시키는 점이 있는지이다. 내 이력서가 <strong>내가 지원하는 팀의 내부적 니즈를 충족할 수 있는 점</strong>들이 적혀있다면, 종종 <strong>특정 다른 부분이 부족하더라도 팀에서 선호하는 지원자가 될 가능성</strong>이 있다.</p>
<p>이러한 니즈는 그러면 어떻게 알 수 있을까? 평소에 네트워킹을 충실하게 하였다면 팀 리더와 이미 안면을 텄을 수도 있다. 또, 링크드인과 같은 SNS 등을 통해서 해당 팀 리더에게 직접 연락하여 면담을 요청할 수 있다.</p>
<blockquote>
<p>아직 학생이라면, 그냥 철면피 한번 깔고 “회사에 관심있습니다! 10분만 시간내주세요!” 또는 “커피 한잔 사주세요!”를 외쳐보자</p>
</blockquote></div><div class="tab-pane" id="fields-2"><p>인사팀의 경우 2가지 케이스로 볼 수 있다.</p>
<hr>
<p>첫째, <strong>인사팀이 키워드만 검토하는 경우</strong>이다. </p>
<p>중/대형 회사처럼 팀장이 모든 이력서를 검토하기에는 매일 너무 많은 이력서가 들어오는 곳에서 이러한 방식을 사용한다. <strong>보통 30초~1분 내로 이력서 스캔</strong>이 끝나며, 하루에 몇백개씩 이력서가 들어오는 글로벌 회사의 경우 이력서 스캔에 5초가 걸린다는 이야기도 있다. 최근에는 <strong>AI 이력서 검토 시스템</strong>을 사용해서 키워드를 뽑아내 적합도를 평가하기도 하지만, 아직 국내에서는 AI 기반 이력서 검토 시스템은 크게 활성화되지 않은 것 같다.</p>
<blockquote>
<p>이 경우, <strong>이력서에 오타 안내고</strong>, <strong>학점 적당히 잘 받고</strong>, <strong>채용공고에 들어가있는 키워드만 그대로 적어넣으면 왠만하면 서류전형 통과</strong>할 수 있다.</p>
</blockquote>
<hr>
<p>둘째, <strong>인사팀이 제대로 이력서를 검토하는 경우</strong>이다.</p>
<p>보통 해당 분야에 대한 경력/이해를 가진 인력이 (e.g. Technical recruiter) 인사팀에 있을 때 이 경우에 해당한다. </p>
<blockquote>
<p>이러한 경우에는 최대한 <strong>키워드를 유지하면서 ‘내가 어떤 좋은 효과를 팀/프로젝트에 가져왔는지’ 위주로 작성</strong>하는 것이 좋다.</p>
</blockquote>
<p>종종 <strong>이력서에 대한 역질문</strong>이 들어오기도 한다. 이럴 때는 <strong>최대한 프로답게</strong> (i.e. 신속하게 답하기 + 충분한 내용을 간결하게) 답해줘야한다. Technical recruiter는 우리가 지원하는 포지션에 대한 이해를 가지고 있지만, <strong>결국 첫인상만을 판단하는 역할</strong>이기 때문에 최대한 프로답게 보이는 것이 좋다.</p></div></div></div>

<p> </p>
<hr>
<h2 id="이력서-적는-방법"><a href="#이력서-적는-방법" class="headerlink" title="이력서 적는 방법"></a>이력서 적는 방법</h2><p>이력서를 단 1장 적는 것이라면 이번 파트를 무시해도 좋다.하지만 대부분 취직/이직을 할 때는 동시에 여러 곳에 지원하는 경우가 많다.</p>
<p>이번 섹션에서는 <strong>여러 곳에 지원할 때 효율적+효과적으로 이력서를 적는 방법</strong>을 소개한다.</p>
<p> </p>
<h3 id="0-‘이력서를-적고-뿌린다-’-스포일러-절대-안됨"><a href="#0-‘이력서를-적고-뿌린다-’-스포일러-절대-안됨" class="headerlink" title="0. ‘이력서를 적고 뿌린다?’ (스포일러: 절대 안됨)"></a>0. ‘이력서를 적고 뿌린다?’ (스포일러: 절대 안됨)</h3><p>자XX닷컴, 원XX, 링XXX 등과 같이 <strong>단 하나의 이력서를 만들고 지원 포탈을 통해서 여러 회사에 뿌리는 서비스</strong>가 많이 있다.</p>
<p>또는, <strong>재학중인 대학교에서 제공하는 ‘이력서 템플릿’에 모든 이력을 적어서 학교가 여러곳에 뿌려주는 경우</strong>도 있다.</p>
<p>이력서를 낼 때 이런 방식은 정말 <strong>최악</strong>이다. <strong>진정성이 보이지 않고, 전혀 효과적이지 않다</strong>.</p>
<p>하나의 경력만으로 모든 직무에 지원할 수 없다. 포지션마다 요구하는 기술/경험의 특성이 다르고, 각각의 회사/필드는 그것을 잘 이해하는 사람을 뽑아 문제를 해결하려고 하기 때문이다.</p>
<p>어떤 포지션에 지원할 때는 <strong>해당 포지션에 걸맞게 이력서를 큐레이션해서 적은 후 제출</strong>하도록 하자.</p>
<p> </p>
<h3 id="1-포지션-탐색"><a href="#1-포지션-탐색" class="headerlink" title="1. 포지션 탐색"></a>1. 포지션 탐색</h3><p>이력서를 적기 전에 우선 가장 먼저 내가 지원하려는 포지션에는 <strong>어떤 이력을 가진 사람이 제일 뽑힐 확률이 높을지</strong>를 고민하며, <strong>내가 부족한 부분</strong>을 찾아야한다.</p>
<p>내가 부족한 부분을 찾았은 후 <strong>이러한 점들을 채우기 위해 계획</strong>을 세워야한다.</p>
<p><strong>해야하는 일들의 우선순위</strong>와 각각의 일들을 <strong>해내는데에 필요한 시간</strong>을 계산하여 중요한 순서부터 해내야한다.</p>
<p>우선순위를 정할때는 다음과 같은 내용을 기반으로 정할 수 있다.</p>
<ol>
<li><strong>Near-&gt;Far</strong> : 채용공고에서 요구하는 기술/경험과 정확히 매칭되는 것들부터 먼 순서대로</li>
<li><strong>Tangible-&gt;Intangible</strong> : 공식적으로 결과물이 남는 것들부터 그렇지 않은 순서대로</li>
</ol>
<p>CS분야 연구자로 예시를 들었을 때, 준비할 수 있는 것들에는 다음과 같은 것들이 있겠다.</p>
<ul>
<li>학력 / 경력</li>
<li>연구 프로젝트 경력</li>
<li>논문/학술지 게재</li>
<li>개인 사이드 프로젝트 경력 + Github 코드</li>
<li>기술 블로그 + 스터디 + 세미나 발표 + 논문 리뷰</li>
<li>대회 + 온라인 코딩사이트 레벨 올리기 + 캐글 레벨 올리기</li>
</ul>
<p><strong>JD에서 요구하는 경험/경력을 충족하지 못했는데 지원하는 것은 무모한 것</strong>이다. 종종 JD에서 요구되지는 않았지만 남들을 상회하는 특별한 경험/경력을 가진 경우 인정되는 경우도 있지만 (e.g. 3년차지만 5년+ 포지션에 지원하는 경우), 이러한 경우는 대부분 면접과정에서 설명을 거쳐서 설득에 성공해야지만 가능한 것이다. 서류단계에서 엄청난 경쟁률이 있는 곳 (e.g. 글로벌 투자은행 신입 지원) 같은 곳에서는 이런 방법은 절대 통하지 않으니, 요행을 바라지 말고 JD의 요구사항을 맞추는 데에 준비를 철저히 해야한다.</p>
<p> </p>
<h3 id="2-Long-CV-작성"><a href="#2-Long-CV-작성" class="headerlink" title="2. Long CV 작성"></a>2. Long CV 작성</h3><p>JD에서 요구하는 내용들을 충족하였다면 <strong>Long CV</strong>를 적는다.</p>
<p>Long CV는 지금까지의 <strong>모든 이력/경력을 페이지 제한 없이 전부 그냥 다 적어내는 것</strong>이다.</p>
<p>보통 다음과 같은 내용을 적는다.</p>
<ul>
<li>이름</li>
<li>인적사항 &amp; 연락처 (이메일, 전화번호, 주소)</li>
<li>링크 (Github, Slideshare, Medium, Twitter)</li>
<li>Summary</li>
<li>Employment</li>
<li>Education</li>
<li>Research experience (연구직 한정)</li>
<li>Projects</li>
<li>Skills</li>
<li>Conference talks</li>
<li>Teaching</li>
<li>Certificates &amp; Awards (또는 Scholarships &amp; Awards)</li>
<li>Community experience</li>
<li>Extra-curricular activities</li>
</ul>
<blockquote>
<p>사진은 넣지 않는다. 국내는 아직 넣는게 관행이지만, 해외 지원의 경우 특정 나라에서는 사진을 넣는 것 자체가 불법이다.</p>
</blockquote>
<p>Long CV는 <strong>제출용이 아니며</strong>, 이후 지원하는 포지션마다 큐레이션해서 적는 Short CV를 적기위한 데이터 풀이라고 할 수 있다.</p>
<blockquote>
<p>Long CV를 잘 적어두면, 이후 지원하고 싶은 포지션이 생겼을 때 <strong>필요한 내용들만 복사+붙혀넣기해서 빠르게 Short CV를 만들 수 있다</strong>.</p>
</blockquote>
<p>Long CV를 적을 때 <strong>기본적인 디자인 요소</strong>까지 고려해서 적어야한다. 폰트는 어떤 것을 쓰고, 마진은 어느정도가 되고, 헤딩1/2/3의 위치, 좌측/우측 정렬 등등에 대한 것들을 미리 만들어놓아야한다.</p>
<p>CV에는 두가지 디자인이 많이 사용된다 (+ 노션 CV…).</p>
<div class="tabs" id="디자인"><ul class="nav-tabs"><li class="tab active"><a href="#디자인-1">그래픽 CV 템플릿</a></li><li class="tab"><a href="#디자인-2">클래식 템플릿</a></li><li class="tab"><a href="#디자인-3">노션</a></li></ul><div class="tab-content"><div class="tab-pane active" id="디자인-1"><p>그래픽 CV 템플릿은 <strong>온라인 CV 빌더 서비스</strong>를 이용해서 빠르고 이쁘게 CV를 적는 방법이다.</p>
<p>색깔이 알록달록한 편이기 때문에 눈에 확 띄는 CV를 적을 수 있고, 또 왠만한 폰트/마진 등과 같이 귀찮은 설정들이 이미 다 잘 되어있기 때문에 빠르게 CV를 적을 수 있다.</p>
<p>필자는 대학생 때 <span class="exturl" data-url="aHR0cHM6Ly9ub3ZvcmVzdW1lLmNvbS8=">novoresume<i class="fa fa-external-link-alt"></i></span>와 같은 사이트를 통해 CV를 만들고 사용했었다.</p>
<p>당시에 이 방식을 사용했던 이유는 ‘MS Word로 줄 간격도 잘 못맞춰서 삐뚤빼뚤 적은 수많은 사회초년생 CV들 중 눈에 띄고싶어서’였다 (i.e. <strong>‘날좀보소’ 전략</strong>). 실제로 잘 먹힌 전략이였다.</p>
<p>하지만 대학을 졸업하면서 프로들의 세계로 오면서 클래식 템플릿으로 바꾸게 되었다.</p></div><div class="tab-pane" id="디자인-2"><p>개인적으로, <strong>프로답게 보이고 싶다면</strong> 무조건 이 템플릿을 써야한다고 생각한다.</p>
<p>색깔과 그래픽적 요소를 다 빼고 글만 있는 형태이다.</p>
<p>디자인적 요소로 어필하는 것이 아닌, <strong>순전히 내용만으로 어필</strong>하기 때문에 내용에 대한 기억이 훨씬 강렬하게 남는다.</p>
<p>이러한 디자인은 MS Word로도 적을 수 있지만, 워드의 경우 줄맞춤이 굉장히 귀찮기 때문에 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L2xhdGV4X3Jlc3VtZV90ZW1wbGF0ZV9rb3I=">LaTeX 템플릿<i class="fa fa-external-link-alt"></i></span>을 쓰는 것을 추천한다. </p>
<p>아래 이미지가 LaTeX로 작성했을 때 보이는 형태이다.</p>
<p><img src="https://github.com/changh95/latex_resume_template_kor/raw/main/resume_img.png?raw=true"></p></div><div class="tab-pane" id="디자인-3"><p>개인적으로 <strong>비추</strong>이다.</p>
<p>이쁘게 적을 수 있고, 길게 적을 수도 있다.</p>
<p>하지만 왠만한 회사들에서는 pdf 형태의 CV를 요구한다.</p>
<p>pdf CV는 대충 적어놓고 ‘노션에 제대로 잘 적어놨어요~’하는 경우를 몇번 본 적이 있는데, 이 경우는 전략이 잘못되었다고 본다.</p>
<p>CV를 잘 적고 노션링크를 첨부한 경우에는 좋지만… 대부분 CV에서 적은 내용이 노션 링크에 반복되는 경우가 많다.</p>
<p>CV는 pdf형태로 잘 적고 프로젝트 디테일 등을 노션으로 이쁘게 표현하면 효과적일 수 있겠지만… 잘 적은 CV만으로도 5초 안에 마음을 사로 잡을 수 있다.</p>
<p>이에 비해 노션은 렌더링에만 최소 3초는 걸린다.</p></div></div></div>

<p>Long CV를 적는 것을 시간낭비라고 생각하는 사람들도 있다. 하지만 CV를 적어야할 때 마다 ‘내가 현재 어떤 상태인지’를 파악하는데에는 생각보다 시간이 많이 든다. 이전에 적어둔 Long CV가 있다면, 당시에 적어놨던 내용 + 그때부터 새롭게 했던 일들을 추가하기만 하면 새로운 Long CV가 빠르게 만들어진다.</p>
<p> </p>
<h3 id="3-Short-CV-작성"><a href="#3-Short-CV-작성" class="headerlink" title="3. Short CV 작성"></a>3. Short CV 작성</h3><p>드디어! 지원하려는 <strong>채용공고에 맞춰서 Long CV -&gt; Short CV로 정제</strong>한다.</p>
<p>우선 Short CV는 <strong>신입이라면 무조건 1장</strong> 안에 작성하는 것을 적극 추천한다. <strong>주니어라면 가능하면 1장 / 불가능하다면 최대 2장 안에 작성</strong>하는 것을 적극 추천한다 (e.g. 낸 논문이 많다던지… 받은 장학금이 너무 많다던지…).</p>
<p>Short CV에는 <strong>검토자로써 나를 뽑고싶게 만드는 내용’만’ 적는다</strong>. 가장 쉬운 방법은 채용공고에 적힌 키워드를 그대로 매칭하는 방법이다.</p>
<p><strong>가장 중요한 내용부터 순서대로 적는다</strong>. 채용공고에서 어떠한 전문성을 요구하는지 잘 이해하고 적어야한다. </p>
<div class="tabs" id="중요한-내용"><ul class="nav-tabs"><li class="tab active"><a href="#중요한-내용-1">신입</a></li><li class="tab"><a href="#중요한-내용-2">주니어</a></li></ul><div class="tab-content"><div class="tab-pane active" id="중요한-내용-1"><p>갓 대학을 졸업한 신입의 경우 전문성은 학위에서 나타나기 때문에 다음과 같은 순서를 가진다.</p>
<ul>
<li>학위/학점</li>
<li>기업/랩실 인턴 경험</li>
<li>논문</li>
<li>사이드프로젝트</li>
<li>스킬</li>
<li>관심사를 보여줄 수 있는 아무거나</li>
</ul>
<p>어디 대학을 나왔는지는 요즘 tech 쪽에서는 크게 중요하지 않다. 하지만 학점은 성실함의 지표처럼 보기도 한다.</p>
<p>컨설팅 분야와 같이 특정 업계에서는 아직도 ‘XX대학출신만 받음’ 같은게 있긴 하다.</p></div><div class="tab-pane" id="중요한-내용-2"><p>주니어의 전문성은 경력과 프로젝트 경험에서 나오기 때문에 다음과 같은 순서를 가진다.</p>
<ul>
<li>경력</li>
<li>프로젝트</li>
<li>스킬</li>
<li>연구경험(연구직한정)</li>
<li>학위</li>
<li>관심사를 보여줄 수 있는 아무거나</li>
</ul></div></div></div>

<p>종종 빈 공간이 남기도 한다. 우선 <strong>CV에 빈 공간은 있으면 안된다</strong>. 하지만 그렇다고해서 관련이 없는 내용을 넣어서 빈 공간을 채우는 것은 더더욱 안된다. <strong>관련이 없는 내용을 넣기보다는 폰트와 마진을 조정해서 한눈에 들어오는 Short CV를 만들자</strong>.</p>
<p><strong>영문 CV의 경우 모든 문장은 무조건 동사로 시작</strong>한다 (e.g. I made a program that does … -&gt; Developed a program for …). 알맞은 동사를 적어주는 것으로 임팩트를 제대로 줄 수 있다. <strong>자주 사용되는 동사</strong>로는 Analyzed, researched into, Dev toolseloped, implemented, maintained, improved, built가 있다. 예외의 경우도 있는데, 첫째로는 Summary 섹션을 작성할 때는 제대로 문장을 작성해야하고, 두번째로는 포지션을 이야기할 때는 (e.g. Tech Lead for team) 동사로 시작하지 않아도 되는 점 이다 (하지만 그닥 추천하지는 않는다).</p>
<p>Short CV의 작성이 끝나면 주변 사람들에게 보여주자. <strong>동기, 교수님, 선배, 후배 다 보여주고 피드백을 받자</strong>. 내가 적은 CV가 좋은지 아닌지는 절대 본인이 알 수 없다. 피드백을 받으면 ‘XXX한 이유로 그렇게 적었는데~’ 하면서 설득하려고 하지 말고 겸허히 받아드리고 고치자.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>일상</tag>
        <tag>이력서</tag>
        <tag>후기</tag>
      </tags>
  </entry>
  <entry>
    <title>Harris Corner (Harris &amp; Stephens 1988)</title>
    <url>/20201215-Harris-corner/</url>
    <content><![CDATA[<p><a href="https://drive.google.com/file/d/1FQoRwpzDBjhaz63mldfn71ydHF_dztM4/view?usp=sharing"><strong><em>Harris corner 논문 + 노트필기</em></strong></a>를 첨부합니다.</p>
<br>

<h1 id="논문-소감"><a href="#논문-소감" class="headerlink" title="논문 소감"></a>논문 소감</h1><hr>
<p>Harris corner는 굉장히 오래된 기술이고, 현재 시점에서는 거의 사장된 기술이다.</p>
<p>ORB와 같이 더 빠르고 강인한 feature detector가 선호되기 때문이다.</p>
<p>종종 ORB에서 FAST score 대신 Harris corner score를 계산하지만, 이 또한 하나의 휴리스틱일 뿐이다.</p>
<br>

<p>Harris corner를 통해 우리는 distinctive point로써 corner의 장점과 기하학적 특성을 배운다.</p>
<p>그리고 보통 곧바로 더 성능이 좋은 SIFT, FAST 를 공부한다.</p>
<br>

<p>하지만 이번에 논문을 직접 읽어보면서 몇가지 새롭게 알게 된 사실이 있다.</p>
<ol>
<li>Harris detector는 corner뿐만이 아닌 edge도 검출할 수 있다 (그것도 꽤 강인하게).</li>
<li>논문은 사실 edge tracking에 초점을 두고, corner detection은 사실 부산물이다.</li>
<li>논문이 추구했던 Edge tracking 방법론들은 현재 시점에서 모두 사장되었다.</li>
</ol>
<p>우리가 현재 휴리스틱으로 사용하고 있는 방법들도, 언젠가는 사장되어 다른 기발한 기술이 대체하겠지? 라는 생각을 해본다.</p>
<br>

<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><hr>
<ul>
<li>이미지 시퀀스에서 3D 공간이 어떻게 변화하는지 이해하기 위해서는, edge filtering이 굉장히 중요하다. <br><br></li>
<li>이 논문에서는 auto-correlation을 이용해 corner와 edge를 둘 다 검출 할 수 있는 detector 알고리즘을 제안한다.</li>
</ul>
<br>

<p>Abstract를 통해 다음과 같은 생각이 든다.</p>
<p>Edge filtering이 왜 3D 공간 변화를 이해하는데에 도움이 될까?</p>
<p>Auto-correlation이란?</p>
<br>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><hr>
<ul>
<li>Image feature를 이용해서 3D 공간 또는 3D 공간 속 움직임을 이해할 수 있다. <br><br></li>
<li>Image feature에는 corner와 edge가 있다.<ul>
<li>Corner에는 connectivity가 없기 때문에 higher level description을 얻기 어렵다.<ul>
<li>여기서 higher level description이란, 면 (surface)이나 물체 (object) 정보를 뜻한다.</li>
</ul>
</li>
<li>Edge는 connectivity 정보가 있다.</li>
</ul>
</li>
</ul>
<br>

<p>Feature matching을 통해 3D 공간 속 움직임 정보를 추출해낼 수 있다는 것을 유추할 수 있다.</p>
<p>여기서 point feature (i.e. corner)로 부터 higher level description을 얻을 수 없다고 하였지만, 요즘에는 descriptor 정보를 추출하고 clustering함으로써 물체를 구분할 수 있다 (e.g. Bag of Visual Words).</p>
<p>전체적으로 저자는 edge에 좀 더 관심이 있어보인다.</p>
<br>

<h1 id="The-Edge-Tracking-Algorithm"><a href="#The-Edge-Tracking-Algorithm" class="headerlink" title="The Edge Tracking Algorithm"></a>The Edge Tracking Algorithm</h1><hr>
<ul>
<li>Edge matching은 epipolar geometry를 통해 할 수 있다.<ul>
<li>하지만, camera motion을 모르는 경우에는 aperture problem이 있기 때문에 매칭을 할 수 없다. </li>
<li>이 경우, motion을 다른 방법으로 풀어내면 된다. <br><br></li>
</ul>
</li>
<li>Edge matching을 통해 edge tracking을 풀 수 있다.<ul>
<li>Edge tracking을 통해 edge의 3D 공간 속 위치를 알아낼 수 있다. <br><br></li>
</ul>
</li>
<li>Edge connectivity를 이용해서 공간이나 물체의 wireframe 모델을 재구축하거나, 3D surface 정보를 추출할 수 있다. <br><br></li>
<li>Canny edge operator를 통해 edge를 추출할 수 있다.<ul>
<li>Detection threshold를 잘못 설정하면 edge의 생김새가 달라지기 때문에, 일정한 edge를 지속적으로 추출하기 어렵고 매칭 또한 어렵다.<ul>
<li>이 문제를 해결하기 위해서, 이미지에서 corner를 추출하고, 이 코너점에 edge끼리 겹칠 수 있도록 threshold를 조정해야했다. </li>
<li>Corner detector에 대해 설명하기 위해 (다음 목차에서) Moravec 코너 추출법에 대해 소개한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<p>이 기술이 edge를 추출하기 위한 목적으로 설계된 것이 여기서 보인다.</p>
<p><strong>1988년도 당시에는 object나 3D scene을 wireframe model 등으로 이해하려는 시도</strong>가 많았다.</p>
<p>본문에서 Canny와 corner detector를 함께 사용해서 안정적인 edge 추출 방식에 대해 이야기하였다.</p>
<p>하지만 이 방법은 굉장히 오래걸리는 계산이였을 것 이라고 추출한다.</p>
<p>이유는… <strong>Canny 자체가 굉장히 무거운 알고리즘</strong>인데, edge 정보와 corner정보가 일치할 때 까지 threshold를 바꿔가며 Canny 계산을 해야하기 때문이다.</p>
<br>

<h1 id="Moravec-Revisited"><a href="#Moravec-Revisited" class="headerlink" title="Moravec Revisited"></a>Moravec Revisited</h1><hr>
<ul>
<li>Moravec 코너 추출법은, 이미지 상 로컬 윈도우를 하나 만들고, 한픽셀씩 우, 우/상, 상, 좌/상 으로 움직인다.<ul>
<li>이 때, 윈도우 내부의 평균 밝기 값을 기록한다. 수식으로는 다음과 같이 표현된다.<br>$E(x, y) = \sum_{u,v} (w_{u,v} | I_{x+u, y+v} - I_{u,v})^2$.<ul>
<li>$E$ - 평균 밝기 값의 변화량</li>
<li>$w$ - 윈도우 내부의 평균 픽셀 값을 계산하는 행동</li>
<li>$u,v$ - 픽셀 위치 변화</li>
<li>$x,y$ - 이미지 속 픽셀 위치 (window 중앙 픽셀의 위치)</li>
<li>$I$ - 이미지 밝기 값  <br><br></li>
</ul>
</li>
</ul>
</li>
<li>이 평균 밝기 값을 통해 corner인지, edge인지, 아무것도 아닌지를 평가한다.<ul>
<li>Edge도 corner도 아닌 경우: 평균 밝기 값에 아주 작은 변화만 있을 것이다.</li>
<li>Edge의 경우:  평균 밝기 값이 한쪽 방향으로 이동할 때만 큰 변화가 생긴다. 예를 들어, 좌/우로 움직일 때는 변화가 크기만, 상/하로 움직일 때는 변화가 작을 것이다.</li>
<li>Corner의 경우: 어떤 방향으로 움직여도 평균 밝기 값이 크게 변화한다.</li>
<li>평균 밝기 값이 작게, 또는 크게 변했다는 것을 결정하기 위해 임의의 Threshold 값을 사용했다.</li>
</ul>
</li>
</ul>
<img src="/20201215-Harris-corner/corner.png" class="" title="corner">

<br>

<p>왜 우, 우/상, 상, 좌/상 일까?</p>
<p>굉장히 이상한 움직임인데, 이 부분은 아직까지 이해가 가지 않는다.</p>
<p><a href="https://www.semanticscholar.org/paper/Obstacle-avoidance-and-navigation-in-the-real-world-Moravec/93b376bd451db8ed94a18c556da16f25a3e7961b?p2df"><strong>Moravec의 박사논문</strong></a>은 1980년도에 나왔으며, 직접 개발한 corner detector 알고리즘과 feature matching을 통한 motion 추정 방식을 소개한다. </p>
<p><strong>가장 초기의 corner detector 알고리즘</strong>인데, <strong>Harris corner가 실질적으로 그 아이디어를 계승</strong>한다고 볼 수 있다 (수식도 굉장히 비슷하다).</p>
<p>이 논문은 박사논문이다보니 굉장히 길어서 읽기 힘든데, Harris corner 논문에서 쉽게 정리해주는게 너무 감사하다.</p>
<p>또 다른 신기한 점은, 왠만한 Harris corner 블로그 글들을 보면 실제 논문에서 사용하는 $u,v$와 $x,y$ 노테이션을 반대로 사용한다는 것이다.</p>
<br>

<h1 id="Auto-Correlation-Detector"><a href="#Auto-Correlation-Detector" class="headerlink" title="Auto-Correlation Detector"></a>Auto-Correlation Detector</h1><hr>
<ul>
<li>Moravec 알고리즘에는 몇가지 단점이 있다. <br><br></li>
<li>Sliding window가 실제로 좌/우/상 등으로만 움직이기 때문에, 코너 response가 anisotropic하다 (즉, 특정 방향만의 값으로 계산되었다)<ul>
<li>이 문제를 해결하기 위해, 논문에서 제안하는 알고리즘은 <strong>image gradient를 계산함으로써 정확하게 intensity shift 방향을 알아내었다</strong>.</li>
<li><strong>Image gradient는 Taylor expansion을 통해 1차 미분</strong>으로 근사되었고, 이 1차 미분 값은 <strong>sobel operator</strong>로 구할 수 있다.<ul>
<li>Taylor Expansion:<br>$E_{x,y} = \sum_{u,v}(w_{u,v}[I_{x+u,y+v} - I_{u,v}])^2 \simeq \sum_{u,v}(w_{u,v}[xX + yY + O(x^2,y^2)])$</li>
<li>Sobel operator:<br>$ X = I \circledast (-1, 0, 1) \simeq \delta I / \delta x$,<br>$ Y = I \circledast (-1, 0, 1)^T \simeq \delta I / \delta y$</li>
<li>Small shift approximation:<br>$ E(x,y) = Ax^2 + 2Cxy + By^2 $<br>, where<br>$ A = X^2 \circledast w$<br>$ B = Y^2 \circledast w$<br>$ C = (XY) \circledast w$</li>
</ul>
</li>
<li>X방향, Y방향, XY방향에 대한 정보를 <strong>2x2 매트릭스에 담아서 auto-correlation matrix를 구성</strong>하였다.<ul>
<li>논문 버전으로는…<br>$\begin{bmatrix}<br>  A &amp; C \<br>  C &amp; B<br>\end{bmatrix}$</li>
<li>모던 버전으로는…<br>$E(x,y)=\left[\begin{array}{ll}x &amp; y\end{array}\right]\left[\begin{array}{ll}\sum\left(\frac{\partial I}{\partial x}\right)^{2} &amp; \sum \frac{\partial I}{\partial x} \frac{\partial I}{\partial y} \ \sum \frac{\partial I}{\partial x} \frac{\partial I}{\partial y} &amp; \sum\left(\frac{\partial I}{\partial y}\right)^{2}\end{array}\right]\left[\begin{array}{l}x \ y\end{array}\right]$ <br><br></li>
</ul>
</li>
</ul>
</li>
<li>Sliding window가 노이즈의 영향이 있었다.<ul>
<li>기존의 sliding window는 그냥 평균 값을 사용했지만, <strong>Harris corner의 window는 Gaussian blur를 적용</strong>하였다. <strong>또, sliding window의 모양을 원형으로 바꿨다</strong>.<br>$ w_{u,v} = exp -(u^2 + v^2) / 2 \sigma ^2 $ <br><br></li>
</ul>
</li>
<li>단순히 Threshold 값보다 높으면 됬기 때문에, edge에도 잘 반응하였다.<ul>
<li>Auto-correlation 매트릭스는 해당 sliding window 속 이미지의 shape을 표현하기도 하였다. 즉, 이 매트릭스의 Eigenvalue 들을 구할 수 있으면, 해당 shape이 corner인지, edge인지 구분할 수 있었다.</li>
<li>두개의 Eigenvalue 중 둘 다 작으면 flat, 하나만 크면 edge, 둘 다 크면 corner로 판단할 수 있다. <ul>
<li>여기에 적용된 논리는 위의 Moravec 알고리즘에 적용된 논리와 비슷하다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>

<p>Harris corner의 알고리즘을 설명해준다.</p>
<p>사실 Moravec과 크게 다른 바가 없다.</p>
<p>Image gradient를 사용했다는 점, 그리고 노이즈를 줄여줬다는 점을 제외하고는 실질적으로 Moravec의 기존 아이디어를 그대로 사용한다고 볼 수 있다.</p>
<br>

<h1 id="Corner-Edge-response-function"><a href="#Corner-Edge-response-function" class="headerlink" title="Corner/Edge response function"></a>Corner/Edge response function</h1><hr>
<ul>
<li><p>Eigenvalue를 계산해내어 shape을 추정하는 것이 정확하겠지만, 계산량이 너무 많다.</p>
<ul>
<li>Tr(M)과 Det(M)을 사용하여 대신 계산한다. k 는 사용자가 정할 수 있다.<br>$Tr(M) = \alpha + \beta = A + B $<br>$Det(M) = \alpha\beta = AB - C^2 $<br>Corner response $R = Det(M) - kTr(M)^2$  <br><br></li>
</ul>
</li>
<li><p>Sliding window가 상하좌우대각 8 방향 모두 response 값이 local maximum이 나타난다면 corner로 인식한다. </p>
</li>
<li><p>Sliding window가 x나 y방향으로 local minima이고 음수라면 edge로 인식한다.</p>
</li>
<li><p>Edge hysteresis를 적용해서 edge continuity를 보강할 수 있다.</p>
</li>
</ul>
<br>

<p>논문에서 corner response는 alpha와 beta를 구해내서 얻어낸 Determinant와 Trace 값을 사용한다.</p>
<p>이는 그때 <strong>당시 컴퓨터로 eigenvalue decomposition을 하기엔 너무 오래걸렸기 때문</strong>이다.</p>
<p>하지만 <strong>요즘 컴퓨터는 빠르고, 2x2 매트릭스의 eigenvalue decomposition은 금방할 수 있다</strong>.</p>
<p>그렇기에 <strong>더욱 정확한 값을 얻어내기 위해, OpenCV 구현 코드 등에서는 eigenvalue를 구하고</strong>, </p>
<p>$Det(M) = \lambda_{1}\lambda_{2}$<br>$Tr(M) = \lambda_{1} + \lambda_{2}$</p>
<p>…를 사용한다.</p>
<p>아래는 corner/edge/plane을 구분하는 criteria 를 시각화한 그래프이다 (<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLm9wZW5jdi5vcmcvbWFzdGVyL2RjL2QwZC90dXRvcmlhbF9weV9mZWF0dXJlc19oYXJyaXMuaHRtbA==">출처<i class="fa fa-external-link-alt"></i></span>).</p>
<img src="/20201215-Harris-corner/response.jpeg" class="" title="response">



<br>

<h1 id="코드-구현"><a href="#코드-구현" class="headerlink" title="코드 구현"></a>코드 구현</h1><p>OpenCV에서는 Harris corner와 연관된 기능이 총 3개가 있다.</p>
<ol>
<li><code>cv::cornerHarris</code><ul>
<li>Harris corner detection을 수행하고, Harris response map을 리턴한다.</li>
<li>Response map을 리턴하는 방식은, 비교적 최신 feature들에 비해 굉장히 특이한 방식이다. 최신 feature들은 $x,y$ 픽셀 위치를 리턴한다. <br><br></li>
</ul>
</li>
<li><code>cv::cornerMinEigenVals</code><ul>
<li>Image block 내부의 minimum eigenvalue ($min(\lambda_1,\lambda_2)$)를 리턴한다.</li>
<li>굳이 모든 계산을 다 거치지 않고, 해당 위치가 corner인지 아닌지 구분할 수 있다.<br>왜냐하면 corner는 두개의 eigenvalue가 threshold 값보다 높아야하기 때문이다. <br><br></li>
</ul>
</li>
<li><code>cv::cornerMinEigenValsVecs</code><ul>
<li>Image block 내부의 eigenvalue와 eigenvector를 계산해 리턴한다.</li>
<li>이 방법을 쓰면 eigenvector를 이용해서 corner orientation을 구할 수 있다. <br><br></li>
</ul>
</li>
</ol>
<br>

<p>이 세 함수 중 어떤걸 실행하던, 모든 구현체가 <code>cv::cornerMinEigenValsVecs</code> 함수로 모이게 된다.</p>
<p>대신 결과 값을 계산할 때 ENUM값으로 분기가 결정된다.</p>
<p>이렇게 만든 이유는 세 함수 모두 공통된 코드를 가지고 있고, 이러한 방법으로 코드를 하나로 모아 전체 코드의 양을 줄이기 위한 방법이지 않았을까 생각한다.</p>
<p>아래 코드는 AVX 및 SIMD 코드를 제거한 Native 코드이다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Pre-defined enums</span></span><br><span class="line"><span class="class"><span class="keyword">enum</span> &#123;</span> MINEIGENVAL=<span class="number">0</span>, HARRIS=<span class="number">1</span>, EIGENVALSVECS=<span class="number">2</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span></span><br><span class="line">cornerEigenValsVecs( <span class="keyword">const</span> Mat&amp; src, Mat&amp; eigenv, <span class="keyword">int</span> block_size,</span><br><span class="line">                     <span class="keyword">int</span> aperture_size, <span class="keyword">int</span> op_type, <span class="keyword">double</span> k=<span class="number">0.</span>,</span><br><span class="line">                     <span class="keyword">int</span> borderType=BORDER_DEFAULT )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// block_size는 local window의 크기</span></span><br><span class="line">    <span class="comment">// aperture_size는 sobel operator 윈도우 크기. CV_SCHARR를 쓰면 -1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> depth = src.depth();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 여기 코드를 보면 opencv는 normalization을 위한 scale을 1/2^(r-1)을 사용한다.</span></span><br><span class="line">    <span class="comment">// 근데 여기 링크를 보니, 원래 스케일은 1/2^(2r-3)이여야한다고 한다.</span></span><br><span class="line">    <span class="comment">// 왜 이런 값을 쓰는걸까...? Sobel filtering을 위한 normalization이라고 하는데, sobel 함수도 봐야하는건지... </span></span><br><span class="line">    <span class="comment">// 출처 https://stackoverflow.com/questions/54021876/significance-of-sobels-scale-when-searching-harris-corners</span></span><br><span class="line">    <span class="keyword">double</span> scale = (<span class="keyword">double</span>)(<span class="number">1</span> &lt;&lt; ((aperture_size &gt; <span class="number">0</span> ? aperture_size : <span class="number">3</span>) - <span class="number">1</span>)) * block_size;</span><br><span class="line">    <span class="keyword">if</span>( aperture_size &lt; <span class="number">0</span> ) <span class="comment">// aperture_size가 0보다 작을 때는 CV_SCHARR인 경우...</span></span><br><span class="line">        scale *= <span class="number">2.0</span>;</span><br><span class="line">    <span class="keyword">if</span>( depth == CV_8U )</span><br><span class="line">        scale *= <span class="number">255.0</span>;</span><br><span class="line">    scale = <span class="number">1.0</span>/scale;</span><br><span class="line"></span><br><span class="line">    CV_Assert( src.type() == CV_8UC1 || src.type() == CV_32FC1 );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sobel / scharr filter 적용</span></span><br><span class="line">    Mat Dx, Dy;</span><br><span class="line">    <span class="keyword">if</span>( aperture_size &gt; <span class="number">0</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        Sobel( src, Dx, CV_32F, <span class="number">1</span>, <span class="number">0</span>, aperture_size, scale, <span class="number">0</span>, borderType );</span><br><span class="line">        Sobel( src, Dy, CV_32F, <span class="number">0</span>, <span class="number">1</span>, aperture_size, scale, <span class="number">0</span>, borderType );</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        Scharr( src, Dx, CV_32F, <span class="number">1</span>, <span class="number">0</span>, scale, <span class="number">0</span>, borderType );</span><br><span class="line">        Scharr( src, Dy, CV_32F, <span class="number">0</span>, <span class="number">1</span>, scale, <span class="number">0</span>, borderType );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Size size = src.size();</span><br><span class="line">    <span class="function">Mat <span class="title">cov</span><span class="params">( size, CV_32FC3 )</span></span>;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; size.height; i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">float</span>* cov_data = cov.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* dxdata = Dx.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* dydata = Dy.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line"></span><br><span class="line">        j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>( ; j &lt; size.width; j++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">float</span> dx = dxdata[j];</span><br><span class="line">            <span class="keyword">float</span> dy = dydata[j];</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 매 픽셀마다 3개씩 (dx^2, dxdy, dy^2)를 저장</span></span><br><span class="line">            cov_data[j*<span class="number">3</span>] = dx*dx;</span><br><span class="line">            cov_data[j*<span class="number">3</span>+<span class="number">1</span>] = dx*dy;</span><br><span class="line">            cov_data[j*<span class="number">3</span>+<span class="number">2</span>] = dy*dy;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Box filter는 Local window 내부의 평균 intensity 값을 구해준다.</span></span><br><span class="line">    boxFilter(cov, cov, cov.depth(), Size(block_size, block_size),</span><br><span class="line">        Point(<span class="number">-1</span>,<span class="number">-1</span>), <span class="literal">false</span>, borderType );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 여기서 ENUM에 따라 분기점이 갈림</span></span><br><span class="line">    <span class="keyword">if</span>( op_type == MINEIGENVAL )</span><br><span class="line">        calcMinEigenVal( cov, eigenv );</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>( op_type == HARRIS )</span><br><span class="line">        calcHarris( cov, eigenv, k );</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>( op_type == EIGENVALSVECS )</span><br><span class="line">        calcEigenValsVecs( cov, eigenv );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h2 id="cv-calcHarris"><a href="#cv-calcHarris" class="headerlink" title="cv::calcHarris"></a>cv::calcHarris</h2><p>아래 코드 역시 AVX와 SIMD를 제거한 Native 코드이다.</p>
<p>Harris Corner의 주된 부분이 되는 $Det(M) - Tr(M)^2$ 계산이 끝에 들어간다.</p>
<p>근데 x,y로 돌려주는 부분은 어디에 있을까?</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">calcHarris</span><span class="params">( <span class="keyword">const</span> Mat&amp; _cov, Mat&amp; _dst, <span class="keyword">double</span> k )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    Size size = _cov.size();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( _cov.isContinuous() &amp;&amp; _dst.isContinuous() )</span><br><span class="line">    &#123;</span><br><span class="line">        size.width *= size.height;</span><br><span class="line">        size.height = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; size.height; i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* cov = _cov.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        <span class="keyword">float</span>* dst = _dst.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        j = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>( ; j &lt; size.width; j++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 이전에 저장해둔 dx^2, dxdy, dy^2를 읽어와서 </span></span><br><span class="line">            <span class="comment">// |a c|</span></span><br><span class="line">            <span class="comment">// |c b|</span></span><br><span class="line">            <span class="comment">// 로 계산.</span></span><br><span class="line">            <span class="comment">// det(M) = ab-c^2</span></span><br><span class="line">            <span class="comment">// tr(M) = a+c</span></span><br><span class="line">            <span class="keyword">float</span> a = cov[j*<span class="number">3</span>];</span><br><span class="line">            <span class="keyword">float</span> b = cov[j*<span class="number">3</span>+<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">float</span> c = cov[j*<span class="number">3</span>+<span class="number">2</span>];</span><br><span class="line">            dst[j] = (<span class="keyword">float</span>)(a*c - b*b - k*(a + c)*(a + c)); <span class="comment">// Harris corner의 특징: Det(M) - Tr(M)^2</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h2 id="cv-calcEigenValsVecs"><a href="#cv-calcEigenValsVecs" class="headerlink" title="cv::calcEigenValsVecs"></a>cv::calcEigenValsVecs</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">calcEigenValsVecs</span><span class="params">( <span class="keyword">const</span> Mat&amp; _cov, Mat&amp; _dst )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Size size = _cov.size();</span><br><span class="line">    <span class="keyword">if</span>( _cov.isContinuous() &amp;&amp; _dst.isContinuous() )</span><br><span class="line">    &#123;</span><br><span class="line">        size.width *= size.height;</span><br><span class="line">        size.height = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size.height; i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* cov = _cov.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        <span class="keyword">float</span>* dst = _dst.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line"></span><br><span class="line">        eigen2x2(cov, dst, size.width);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">eigen2x2</span><span class="params">( <span class="keyword">const</span> <span class="keyword">float</span>* cov, <span class="keyword">float</span>* dst, <span class="keyword">int</span> n )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>( <span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">double</span> a = cov[j*<span class="number">3</span>];</span><br><span class="line">        <span class="keyword">double</span> b = cov[j*<span class="number">3</span>+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">double</span> c = cov[j*<span class="number">3</span>+<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> u = (a + c)*<span class="number">0.5</span>;</span><br><span class="line">        <span class="keyword">double</span> v = <span class="built_in">std</span>::<span class="built_in">sqrt</span>((a - c)*(a - c)*<span class="number">0.25</span> + b*b);</span><br><span class="line">        <span class="keyword">double</span> l1 = u + v;</span><br><span class="line">        <span class="keyword">double</span> l2 = u - v;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> x = b;</span><br><span class="line">        <span class="keyword">double</span> y = l1 - a;</span><br><span class="line">        <span class="keyword">double</span> e = <span class="built_in">fabs</span>(x);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( e + <span class="built_in">fabs</span>(y) &lt; <span class="number">1e-4</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            y = b;</span><br><span class="line">            x = l1 - c;</span><br><span class="line">            e = <span class="built_in">fabs</span>(x);</span><br><span class="line">            <span class="keyword">if</span>( e + <span class="built_in">fabs</span>(y) &lt; <span class="number">1e-4</span> )</span><br><span class="line">            &#123;</span><br><span class="line">                e = <span class="number">1.</span>/(e + <span class="built_in">fabs</span>(y) + FLT_EPSILON);</span><br><span class="line">                x *= e, y *= e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> d = <span class="number">1.</span>/<span class="built_in">std</span>::<span class="built_in">sqrt</span>(x*x + y*y + DBL_EPSILON);</span><br><span class="line">        dst[<span class="number">6</span>*j] = (<span class="keyword">float</span>)l1;</span><br><span class="line">        dst[<span class="number">6</span>*j + <span class="number">2</span>] = (<span class="keyword">float</span>)(x*d);</span><br><span class="line">        dst[<span class="number">6</span>*j + <span class="number">3</span>] = (<span class="keyword">float</span>)(y*d);</span><br><span class="line"></span><br><span class="line">        x = b;</span><br><span class="line">        y = l2 - a;</span><br><span class="line">        e = <span class="built_in">fabs</span>(x);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>( e + <span class="built_in">fabs</span>(y) &lt; <span class="number">1e-4</span> )</span><br><span class="line">        &#123;</span><br><span class="line">            y = b;</span><br><span class="line">            x = l2 - c;</span><br><span class="line">            e = <span class="built_in">fabs</span>(x);</span><br><span class="line">            <span class="keyword">if</span>( e + <span class="built_in">fabs</span>(y) &lt; <span class="number">1e-4</span> )</span><br><span class="line">            &#123;</span><br><span class="line">                e = <span class="number">1.</span>/(e + <span class="built_in">fabs</span>(y) + FLT_EPSILON);</span><br><span class="line">                x *= e, y *= e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        d = <span class="number">1.</span>/<span class="built_in">std</span>::<span class="built_in">sqrt</span>(x*x + y*y + DBL_EPSILON);</span><br><span class="line">        dst[<span class="number">6</span>*j + <span class="number">1</span>] = (<span class="keyword">float</span>)l2;</span><br><span class="line">        dst[<span class="number">6</span>*j + <span class="number">4</span>] = (<span class="keyword">float</span>)(x*d);</span><br><span class="line">        dst[<span class="number">6</span>*j + <span class="number">5</span>] = (<span class="keyword">float</span>)(y*d);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<br>

<h2 id="cv-calcMinEigenVal"><a href="#cv-calcMinEigenVal" class="headerlink" title="cv::calcMinEigenVal"></a>cv::calcMinEigenVal</h2><p>$ (A+C) - \sqrt{(A-C)^2 + B^2} $ 계산이 들어간다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">calcMinEigenVal</span><span class="params">( <span class="keyword">const</span> Mat&amp; _cov, Mat&amp; _dst )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    Size size = _cov.size();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>( _cov.isContinuous() &amp;&amp; _dst.isContinuous() )</span><br><span class="line">    &#123;</span><br><span class="line">        size.width *= size.height;</span><br><span class="line">        size.height = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>( i = <span class="number">0</span>; i &lt; size.height; i++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* cov = _cov.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        <span class="keyword">float</span>* dst = _dst.ptr&lt;<span class="keyword">float</span>&gt;(i);</span><br><span class="line">        j = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>( ; j &lt; size.width; j++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">float</span> a = cov[j*<span class="number">3</span>]*<span class="number">0.5f</span>;</span><br><span class="line">            <span class="keyword">float</span> b = cov[j*<span class="number">3</span>+<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">float</span> c = cov[j*<span class="number">3</span>+<span class="number">2</span>]*<span class="number">0.5f</span>;</span><br><span class="line">            dst[j] = (<span class="keyword">float</span>)((a + c) - <span class="built_in">std</span>::<span class="built_in">sqrt</span>((a - c)*(a - c) + b*b));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Feature Detector</tag>
        <tag>Visual Feature</tag>
      </tags>
  </entry>
  <entry>
    <title>IROS 2020 - Past, Present, and Future of SLAM (Prof. Luca Carlone 발표)</title>
    <url>/20201223-IROS2020-Luca-Carlone-SLAM-workshop/</url>
    <content><![CDATA[<h1 id="이번-토크의-주제는"><a href="#이번-토크의-주제는" class="headerlink" title="이번 토크의 주제는?"></a>이번 토크의 주제는?</h1><p>이번 토크에서는 3개의 주제를 훑는다.</p>
<ul>
<li>Spatial Perception and SLAM</li>
<li>Solving SLAM via nonlinear optimization</li>
<li>Open Problems</li>
</ul>
<p>Luca Carlone 교수님의 랩실이 주로 연구하는 분야로 발표를 이끌어가신다.<br>이 랩실에서는 Spatial AI로는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8tNVh4WFJBQlhKcw==">Kimera<i class="fa fa-external-link-alt"></i></span>, nonliear optimization으로는 <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzc1NTcwNzU=">IMU-preintegration<i class="fa fa-external-link-alt"></i></span> 등 큰 획을 긋는 연구를 진행하였다.</p>
<hr>
<h1 id="SLAM이-쓰이는-곳은"><a href="#SLAM이-쓰이는-곳은" class="headerlink" title="SLAM이 쓰이는 곳은?"></a>SLAM이 쓰이는 곳은?</h1><p>Robotics 기술은 정말 다양한 필드에 많이 쓰인다.<br>Robotics 기술자로써 굉장히 좋은 시기이다.</p>
<p>Ground : Transportation, domestics, goods/manufacturing, Mining<br>Air : Monitoring, disaster response, precision agriculture<br>Space : Exploration, Science<br>plus… medical applications, environmental monitoring (underwater?)</p>
<p>로봇은 자신이 돌아다니는 환경을 이해할 줄 아는게 중요하다.<br>이러한 능력을 Spatial Perception (공간 인지)이라고 한다.</p>
<hr>
<h1 id="Spatial-Perception이란"><a href="#Spatial-Perception이란" class="headerlink" title="Spatial Perception이란?"></a>Spatial Perception이란?</h1><img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/spatial_perception.png" class="" title="Spatial perception">

<p>Spatial perception: 센서를 통해 얻은 실제 세상의 정보를 프로세싱을 통해 우리가 사용할 수 있는 representation으로 바꾸는 능력.</p>
<p>Perception을 통해 우리는 map, robot pose, poses of objects/people/other robots 등을 얻을 수 있다.<br>Map을 얻는 작업은 Mapping, Pose를 얻는 작업은 localization이라고 한다.</p>
<hr>
<h1 id="SLAM이란"><a href="#SLAM이란" class="headerlink" title="SLAM이란?"></a>SLAM이란?</h1><p>SLAM 기술을 통해 sensor measurement로부터 Map의 정보와 Robot trajectory를 얻을 수 있다. </p>
<p>Chicken-and-egg 문제:</p>
<ul>
<li>Map이 있을 때 Localization은 쉽고</li>
<li>Robot pose가 있을 때 Mapping은 쉽지만</li>
<li>둘 다 없을때는 어렵다.<ul>
<li>SLAM은 둘 다 없을 때 하는 것.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="3D-Map을-표현하는-방법은"><a href="#3D-Map을-표현하는-방법은" class="headerlink" title="3D Map을 표현하는 방법은?"></a>3D Map을 표현하는 방법은?</h1><ul>
<li>Point cloud</li>
<li>Voxel<ul>
<li>3D version of occupancy grid map</li>
</ul>
</li>
<li>Geometries (segments, lines, planes)</li>
<li>triangular/polygonal mesh.</li>
</ul>
<p>Point cloud나 Geometry 기반의 map을 먼저 만들경우, 이 정보로부터 Voxel이나 Mesh 기반을 만들기 어렵지 않다. </p>
<hr>
<h1 id="센서-데이터-타입-1-Proprioceptive-measurement-Odometry"><a href="#센서-데이터-타입-1-Proprioceptive-measurement-Odometry" class="headerlink" title="센서 데이터 타입 #1 - Proprioceptive measurement - Odometry"></a>센서 데이터 타입 #1 - Proprioceptive measurement - Odometry</h1><p><strong>자기 자신에 대한 정보를 읽을 수 있는 센서</strong>를 <strong>proprioceptive</strong> 센서라고 함.</p>
<p>그 중 로보틱스에서 많이 사용되는 정보는 odometry 정보임.<br><strong>Odometry</strong>: <strong>Relative motion between consecutive time</strong><br>센서 값을 읽을 때 마다 그 사이의 상대적인 움직임 (위치 값, 방향 값).<br>이 위치, 방향 값들을 state로 다룰 수 있음.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/odometry.png" class="" title="odometry">

<p>Wheel odometry로 얻을 수도 있지만,<br>Visual, LiDAR 등으로 얻을 수도 있음.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/motion_model.png" class="" title="motion model">

<ul>
<li>센서로 추정한 이번 프레임의 state 값이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.731ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="2.26ex" role="img" focusable="false" viewBox="0 -675.5 990.4 998.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(572, -315.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container> (아는 정보)</li>
<li>센서로 추정한 지난 프레임의 state 값이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.845ex" xmlns="http://www.w3.org/2000/svg" width="4.285ex" height="2.373ex" role="img" focusable="false" viewBox="0 -675.5 1894.1 1049"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(572, -315.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>이고 (아는 정보)</li>
<li>로봇의 실제 이동량이 (<strong>odometry</strong>) <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container> 고 (모르는 정보)</li>
<li>노이즈가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.947ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 860.5 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(406, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container> 일 때… (대충 아는 정보)</li>
</ul>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container>는 어떤 함수여야 정확한 odometry <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container> 값을 알 수 있는가?<br>Motion model은 뭐 여러 실험을 통해 얻어낼 수 있다.<br>Motion model을 한번 얻어내고 나면, 동일한 motion model을 사용해서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.845ex" xmlns="http://www.w3.org/2000/svg" width="4.285ex" height="2.373ex" role="img" focusable="false" viewBox="0 -675.5 1894.1 1049"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(572, -315.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>값과 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.731ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="2.26ex" role="img" focusable="false" viewBox="0 -675.5 990.4 998.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(572, -315.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>을 가지고 (+<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.947ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 860.5 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(406, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container> 값도 있지만… 얘는 안변한다는 전제하에…) 로봇의 실제 이동량인 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>를 추정할 수 있는 것이다.</p>
<p>위와 같은 방식으로 이동량을 추정하면 결과에 소량의 에러가 포함된다.<br>에러에는 다음과 같은 이유가 있다. </p>
<ul>
<li>system model error (== motion model 함수를 정확하게 추론하지 못해 나오는 에러)</li>
<li>wheel slip, gear backlash (예상치 못한 물리적인 현상)</li>
<li>measurement noise (센서 노이즈)</li>
<li>sensor, processor quantization error (아날로그 -&gt; 디지털 신호 변환을 위해 생기는 어쩔 수 없는 에러)</li>
</ul>
<hr>
<h1 id="Odometry-drift-Solve-by-extroceptive-measurements"><a href="#Odometry-drift-Solve-by-extroceptive-measurements" class="headerlink" title="Odometry drift - Solve by extroceptive measurements"></a>Odometry drift - Solve by extroceptive measurements</h1><p>Motion model을 통해 두개의 프레임간의 이동량 (odometry)를 추정을 할 수 있다.</p>
<p>그러면<br>프레임 -&gt; 프레임 -&gt; 프레임 -&gt; 프레임 -&gt; 프레임 -&gt; 프레임 -&gt; 프레임 -&gt; 프레임 …<br>이 매 프레임간의 이동량을 계속 누적해나가면, trajectory를 구할 수 있을 것이다.</p>
<p>하지만 위의 이론은 실제에서는 작동하지 않는데, 이는 에러도 함께 누적이 되기 때문이다.<br>즉, 시간이 지날수록 에러가 점점 커지고 쓸 수 없게 된다.</p>
<p>이렇게 위치추정 값을 누적해나가는 방법을 <strong>Dead-reckoning</strong>이라고 한다.<br>실제로 항공/해양 쪽에서 쓴다고 하는데, 이 경우에는 엄청나게 비싸고 좋은 IMU센서와 GPS를 결합한 INS 시스템을 사용하는 경우가 많다.</p>
<p>로봇에서는 값싼 MEMS IMU나 휠 인코더 등을 사용하면 당연히 금방 오차가 쌓여버린다.</p>
<p>사람의 눈을 가리고 걷게하면 odometry라고 볼 수 있다.<br>사막에서 사람의 눈을 가리고 걷게하면 일자로 걷지 못할 것이다.<br>눈을 뜨게 해준다면 주위 환경을 보고 자신의 위치를 보정하며 (<strong>relocalize</strong>) 일자로 걸을 수 있다.<br>로봇도 똑같이 주변 환경에 대한 정보를 이용할 수 있게 해주면, 제대로 자신의 위치를 추정할 수 있을 것이다.</p>
<p>즉, 주위 환경에 위치한 움직이지 않는 landmark를 이용하면 된다. </p>
<hr>
<h1 id="Extroceptive-measurements"><a href="#Extroceptive-measurements" class="headerlink" title="Extroceptive measurements"></a>Extroceptive measurements</h1><img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/extroceptive_measurement.png" class="" title="extroceptive measurement">

<p><strong>외부 환경에 대한 정보를 읽는 센서</strong>를 <strong>extroceptive sensor</strong>라고 함.<br>자주 보이는 센서로는 카메라, LiDAR / 레이저 스캐너, 레이더 등이 있다.</p>
<p>레이저 스캐너는 센서와 주위 환경 (e.g. 벽)과의 거리를 재는 센서이다.<br>위의 사진에서는 벽과의 거리를 재는 수식, 즉 measurement model의 (observation model이라고도 부른다) 정보를 가지고 있다.<br>센서로 재는 것이다 보니 어쩔 수 없디 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.776ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 784.8 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mi" transform="translate(406, -150) scale(0.707)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></g></svg></mjx-container>가 들어가긴 한다. </p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/measurement_model.png" class="" title="measurement_model">

<hr>
<h1 id="SLAM-as-state-estimation"><a href="#SLAM-as-state-estimation" class="headerlink" title="SLAM as state estimation"></a>SLAM as state estimation</h1><img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/slam_state_estimation.png" class="" title="slam as state estimation">

<ul>
<li>Map에 있는 landmark들의 정보</li>
<li>Robot의 pose (Map 안에서의 위치+방향 정보)</li>
</ul>
<p>이 두개의 정보를 <strong>동시에</strong> 추정해야한다.<br>이 정보들은 모두 각각의 state로 표현할 수 있다.<br>Landmark의 state (x,y,z)… Robot pose의 state (tx,ty,tz, rx,ry,rz)…<br>이 모든 state를 한번에 계산하는 것이 SLAM이 하는 것이라고 교수님은 말씀하신다.<br>State의 정보는 어떤 센서에 따라 달라질 수 있다.</p>
<p>여기서 State의 값을 어떻게 추정하는게 좋을까?<br>하나의 값을 계산하는 방법이 있고, 어떠한 위치에 있을 수 있는 확률을 추정하는 방법이 있겠다.<br>우리는 노이즈가 존재하는 센서 데이터를 통해 계산을 하기 때문에, 단일 state 값을 계산하는 것은 위험할 수 있다. 그러므로, 가장 높은 확률을 가진 값을 추정하고, 그 값이 정확한 state일 확률을 함께 표현해주는 것이 좋다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/belief.png" class="" title="belief">

<p>Proprioceptive measurement <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.485ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1540.5 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(778, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>와 Extroceptive measurement <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.243ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1433.5 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="TeXAtom" transform="translate(465, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(778, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>가 있을 때, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>는 어떤 확률 분포를 가질까?<br>이 질문을 쉽게 표현하면 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.241ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 990.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>의 사후확률 (Posterior probability)은 무엇인지 묻는 것이다.</p>
<hr>
<h1 id="Brief-history-of-SLAM"><a href="#Brief-history-of-SLAM" class="headerlink" title="Brief history of SLAM"></a>Brief history of SLAM</h1><p>기본적인 것은 이제 알았으니, 지금까지의 SLAM 방법론의 변화에 대해 알아보자.</p>
<p>1990~2000 - <span class="exturl" data-url="aHR0cHM6Ly9qb3VybmFscy5zYWdlcHViLmNvbS9kb2kvYWJzLzEwLjExNzcvMDI3ODM2NDk4NjAwNTAwNDA0">EKF SLAM<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>Extended Kalman filter 기반.</li>
</ul>
<p>2000-2007 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEyNDE4ODU=">Particle Filter SLAM<i class="fa fa-external-link-alt"></i></span> 쓰룬 형…</p>
<ul>
<li>Particle filter 기반</li>
</ul>
<p>2007-PRESENT - <span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMjMvQToxMDA4ODU0MzA1NzMz">MAP Estimation<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>EKF and PF에 있는 단점을 보완하기위해 나타난 방법.</li>
<li>2005년 이전에는 실시간으로 작동할 수 없었다.</li>
</ul>
<hr>
<h1 id="MAP-Estimation-or-full-smoothing"><a href="#MAP-Estimation-or-full-smoothing" class="headerlink" title="MAP Estimation (or full smoothing)"></a>MAP Estimation (or full smoothing)</h1><p><strong>MAP</strong> Estimation = <strong>Maximum A Posteriori</strong> Estimation</p>
<p>다른 이름으로는</p>
<ul>
<li>Factor graph optimization</li>
<li>Full smoothing</li>
<li>Graph-SLAM</li>
<li>Smoothing and Mapping</li>
<li>Bundle adjustment (Structure from Motion)</li>
</ul>
<p>MAP Estimation은 사실 다른 분야에서도 많이 사용되는 estimation 기법이다.</p>
<hr>
<h1 id="MAP-Estimation-이해하기"><a href="#MAP-Estimation-이해하기" class="headerlink" title="MAP Estimation 이해하기"></a>MAP Estimation 이해하기</h1><img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/measurements.png" class="" title="measurements">

<p>Proprioceptive measurement가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.485ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1540.5 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(778, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>, 즉 1번 프레임부터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container> 번째 프레임까지의 데이터를 가지고 있다고 해보자.<br>이 measurement를 측정하는 에러 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.947ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 860.5 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(406, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>가 작다면, 사실상 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.947ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 860.5 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="TeXAtom" transform="translate(406, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>를 무시해도 될 것이다.<br>그렇다면 그림에 나온대로 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.691ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8703.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(1212.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2212.8, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2762.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3151.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5045.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5490.6, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(6481, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7147.8, 0)"><path data-c="2243" d="M55 283Q55 356 103 409T217 463Q262 463 297 447T395 382Q431 355 446 344T493 320T554 307H558Q613 307 652 344T694 433Q694 464 708 464T722 432Q722 356 673 304T564 251H554Q510 251 465 275T387 329T310 382T223 407H219Q164 407 122 367Q91 333 85 295T76 253T69 250Q55 250 55 283ZM56 56Q56 71 72 76H706Q722 70 722 56Q722 44 707 36H70Q56 43 56 56Z"></path></g><g data-mml-node="mn" transform="translate(8203.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container> 이 된다.</p>
<p>Exteroceptive measurement도 비슷한 방식으로 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="17.773ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 7855.7 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="TeXAtom" transform="translate(465, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(799, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1593.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2593.8, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(3169.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3558.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(4549.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4993.8, 0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(5633.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6299.9, 0)"><path data-c="2243" d="M55 283Q55 356 103 409T217 463Q262 463 297 447T395 382Q431 355 446 344T493 320T554 307H558Q613 307 652 344T694 433Q694 464 708 464T722 432Q722 356 673 304T564 251H554Q510 251 465 275T387 329T310 382T223 407H219Q164 407 122 367Q91 333 85 295T76 253T69 250Q55 250 55 283ZM56 56Q56 71 72 76H706Q722 70 722 56Q722 44 707 36H70Q56 43 56 56Z"></path></g><g data-mml-node="mn" transform="translate(7355.7, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>이 될 수 있다.</p>
<p>여기서부터는 굉장히 쉽다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/eq1.png" class="" title="eq1">

<p>Proprioceptive measurement와 Exteroceptive measurement의 에러 값들이 최소값이 되게 만들려면 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>가 각각 어떤 값이 되야하는가? 를 묻는 것이 MAP Estimation의 기본이라고 할 수 있다.<br>즉, 비선형 최적화 프로그래밍인 것이다.</p>
<p>제곱이 들어가는 이유는, 최적화 과정에서 에러값이 큰 경우 더 크게 페널티를 주기 위해서이다.<br>예를 들어, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container> 값이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.973ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4408.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(939, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2708.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3153.3, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4019.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>에 비해 큰 오차를 가지고 있다면, 이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container> 값은 정답에서 많이 먼 값이다. 반대로, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container> 값이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.973ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4408.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(939, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2708.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3153.3, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4019.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>에 비해 작은 오차를 가지고 있다면, 이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container> 값은 정답에서 가까운 값이다.<br>우리는 최적화 프로그래밍을 통해, 정답에서 먼 값에서 가까운 값으로 점진적으로 구하고 싶기 때문에, 오차카 큰 값들은 크게 페널티를 주고, 오차가 작은 값들은 작게 페널티를 줘야한다.<br>이 때, 이 오차 값에 제곱을 시키면, 큰 값들은 크게 페널티를 주고, 작은 값들은 작게 페널티 줄 수 있다.<br>이런 방식을 보통 least squares 최적화 방법이라고 한다.</p>
<p>위와 같이 ‘가장 작은 에러를 가지는 state의 값들’을 추정하는 최적화 프로그래밍은, 사실 반대로 생각하면 ‘가장 높은 정확도/확률을 가지는 state의 값들’이 될 수 있다.</p>
<p>이러한 방식을 수식으로 표현하면, 아래와 같은 Maximum-a-posteriori (MAP) estimation 문제로 표현할 수 있다.<br>1번 프레임부터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.179ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 521 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></svg></mjx-container>번째 프레임들(즉, 지금까지 쌓여온 모든 프레임들)의 센서 값들로 (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>) 추정했을 때, 가장 확률적으로 말이 되는 state는 무엇인가?를 찾는 것이다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/eq2.png" class="" title="eq2">

<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/map_estimation.png" class="" title="MAP estimation">

<hr>
<h1 id="MAP-Estimation-예시"><a href="#MAP-Estimation-예시" class="headerlink" title="MAP Estimation - 예시"></a>MAP Estimation - 예시</h1><p>1개의 landmark를 가진 환경에서, 자동차가 landmark의 위치를 추정하면서 정사각형 경로를 그리며 이동하는 시나리오를 생각해보자.<br>자동차는 proprioceptive 데이터로 odometry 값을 추정할 수 있다. (휠 인코더 등으로…)</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/example_1.png" class="" title="Example - before optimisation">

<p>우선 Odometry (control 값)만 가지고 추정을 했을 때, 이동 경로는 이렇게 추정 될 수 있다.<br>Odometry 값에는 노이즈가 어느정도 껴있기 때문에, 이 odometry 값을 쌓았을 때 정사각형 모양이 나오지 않는다.</p>
<p>여기서 extreroceptive 결과를 보았을 때, 말이 되지 않는 결과가 나온다.<br>즉, proprioceptive 센서 값과 exteroceptive 센서 값이 서로 동의하지 않는 상황이 온다는 것이다.</p>
<p>Exteroceptive 센서가 정확하게 읽었다는 전제를 깔면, proprioceptive 센서 값으로 이동량을 추정하는 식, 즉 motion model에 에러가 있다는 것을 알 수 있다.</p>
<p>이 에러를 최소화하기 위해 gradient descent 방식을 사용해서 최적화를 한다. (딥러닝에서 사용하는 adam, SGD 등이 아닌, Gauss-Newton이나 Levenberg-Marquardt 방식이 사용된다.)</p>
<br>

<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/example_2.png" class="" title="Example - after optimisation">

<p>이렇게 Motion model과 Measurement model의 에러를 최소화 할 수 있는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>와 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>의 state를 찾아낸다면, proprioceptive 센서와 exteroceptive 센서가 모두 동의할 수 있는 결과를 얻게 된다.</p>
<p>이 결과가 ‘가장 확률적으로 말이 되는 state’가 되겠으며, 이 값을 유도하는 과정이 Maximum-a-posteriori (MAP) Estimation이 되겠다.</p>
<hr>
<h1 id="Factor-graph"><a href="#Factor-graph" class="headerlink" title="Factor graph"></a>Factor graph</h1><img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/factor_graph.png" class="" title="Factor graph">

<p>방금 전 예제에서는 1개의 landmark와 robot pose 트랙킹에 대한 시나리오를 보았지만, 실제 SLAM에서는 다수의 landmark를 사용한다.<br>하지만 그렇다고 MAP 과정이 달라지지는 않는다.<br>단순히 고려해야할 변수의 수가 많아지는 것 뿐이다.</p>
<p>변수가 많아지면, 각각의 robot pose와 landmark관의 관계를 서술하기가 복잡해진다.<br>예를 들어, pose1 에서는 landmark1,2,3이 보일 수 있고, pose2 에서는 landmark 2,4,1042 등등이 보일 수도 있는 것이다.<br>최적화 프로그래밍을 할 때는 이 관계가 모두 정확하게 정의되어야하는데, 이 때 graph를 사용해서 각각 변수의 관계를 표현해주면 쉽다.</p>
<p>각각의 node는 pose나 landmark position을 의미하고,<br>edge (또는 factor)는 센서값을 포함한다.</p>
<hr>
<h1 id="오픈소스-라이브러리"><a href="#오픈소스-라이브러리" class="headerlink" title="오픈소스 라이브러리"></a>오픈소스 라이브러리</h1><p>이러한 최적화 프로그래밍 문제를 푸는 solver 라이브러리로는</p>
<ul>
<li>GTSAM (조지아 공대)</li>
<li>g2o (프라이버그 대학)</li>
<li>ceres-solver (구글)<br>등이 있다.</li>
</ul>
<p>실제로 이 라이브러리들을 쓴거를 보려면 오픈소스 구현체들을 보면 좋다.<br>ORB-SLAM, Cartographer, ROS 구현체들이 추천되었다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/large_scale_optimisation.png" class="" title="Large Scale Optimisation">

<p>사진에 보이는 것 처럼 몇천~몇만개의 factor가 있는 그래프도 solver 라이브러리를 통해 몇초만에 계산할 수 있다.</p>
<hr>
<h1 id="Sub-field-of-SLAM"><a href="#Sub-field-of-SLAM" class="headerlink" title="Sub-field of SLAM"></a>Sub-field of SLAM</h1><ol>
<li>Visual-SLAM<ul>
<li>카메라를 써서 하는 SLAM.<ul>
<li>ORB-SLAM</li>
<li>DSO 등등</li>
</ul>
</li>
</ul>
</li>
<li>Visual-Inertial SLAM<ul>
<li>카메라 + IMU 데이터를 써서 하는 SLAM<ul>
<li>Kimera</li>
<li>OpenVINS</li>
<li>VINS-mono 등등</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>… 라이다 슬램은 따로 얘기 안해주시는 것 같다.</p>
<hr>
<h1 id="Open-problems"><a href="#Open-problems" class="headerlink" title="Open problems"></a>Open problems</h1><p>현재 SLAM이 가지는 약점들을 (i.e. 잘 안되는 부분) 단점이라고 생각할 수도 있고, 어떻게보면 연구를 할 수 있는 기회라고 볼 수도 있다.<br>연구자의 입장으로는 긍정적으로 생각했으면 좋겠다.</p>
<hr>
<h1 id="Problem-1-Robustness-tuning"><a href="#Problem-1-Robustness-tuning" class="headerlink" title="Problem 1 - Robustness, tuning"></a>Problem 1 - Robustness, tuning</h1><p>Robustness for outliers, robustness for tuning 이 필요하다.</p>
<p>온라인에 공개된 오픈소스 라이브러리들을 다운받아서 쓰면, Small-scale의 잘 컨트롤 된 환경에서는 잘 된다.</p>
<p>근데 그 이상으로 넘어가려고 하면, 엄청나게 세심하게 파라미터 튜닝을 해야하고, 새로운 환경으로 갈 경우 또 다시 파라미터 튜닝을 해야한다.</p>
<p>연구 단계에서는 데이터셋을 찍고 튜닝을 하고, 몇번이고 테스트하면서 튜닝을 해주면 된다.</p>
<p>근데 안전이 고려되어야하는 제품에서는 어떨까?<br>사람이 계속해서 감독해주고 튜닝을 해줘야하는 이러한 과정은 자율주행차 등등에서 안전 측면에서 보았을 때 굉장히 위험하다.<br>갈길이 멀다!</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/robust_perception.png" class="" title="Robust perception">

<p>테슬라에서 만든 드라이빙 어시스턴스 시스템이 인명사고를 내는 경우도 있고,</p>
<p>이미 잘 되는 것 처럼 보이는 Google street view 등에서도 사람을 고용해서 잘못된 부분을 직접 수정하게 하는 작업을 하기도 한다.</p>


<p>이러한 튜닝이 필요한 이유는, 각각의 환경에서 나타나는 특정 현상들에 대해 대비하기 위해 파라미터를 맞추는 것이다.<br>보통 우리가 예상하지 못한 현상들을 outlier 데이터라고 하는데, 이 데이터들을 잘 걸러내서 우리에게 유용한 데이터만 사용해서 SLAM 계산을 하는 것이 중요하다.</p>
<p>여기서 Luca Carlone 교수님의 랩실에서 연구하는 주제인 Certifable algorithms 연구에 대해 소개가 된다.<br>이 기술로 풀려고 하는 문제는 ‘수많은 Outlier에 강인한 알고리즘을 만들 수 있는가?’ 이다.<br>이번 RAL &amp; ICRA 2020에서 best paper finalist에 올라갔다고 한다. </p>
<hr>
<h1 id="Problem-2-High-level-understanding"><a href="#Problem-2-High-level-understanding" class="headerlink" title="Problem 2 - High-level understanding"></a>Problem 2 - High-level understanding</h1><p>SLAM 기술의 발전은 대부분 수학/알고리즘의 발전에 집중해왔으며, 상당수의 연구가 1. 실시간 환경에서 작동하게 만들거나, 2. 정확하게 geometry를 딸 수 있게 만드는데에 집중해왔다.</p>
<p>하지만 사람이 공간을 인지하는데에는 geometry만 있는 것이 아니다.<br>사람은 point cloud나 geometric primitives로 세상을 보지 않는다.<br>사람은 물체의 semantic 정보를 읽을 수 있고, context도 읽을 수 있다.<br>이 부분에 있어서 SLAM과 인간의 spatial perception에는 큰 차이가 있다.<br>이 갭이 좁혀져야한다.</p>
<p>High-level understanding을 처음 적용한 논문은 ICL의 Andrew Davison 교수님께서 밀고 계시는 ‘Spatial AI’의 개념이다.<br>이 랩실에서 2013년에 나온 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS90bXJBaDFDcUNSbw==">SLAM++<i class="fa fa-external-link-alt"></i></span>이라는 논문에서, depth image로부터 object-level reconstruction을 수행하는 것을 보여준다.<br>Map은 그냥 덩그러니 포인트 클라우드가 아닌, Map을 만들 때 어떤 오브젝트가 들어가있는지 이해하면서 하나씩 오브젝트를 맵에 추가해내간다는 것이다.<br>영상을 보면 의자와 책상을 하나씩 map에 추가해내가는 것을 볼 수 있다.</p>
<p>Luca Carlone 교수님은 본인의 랩실에서 발표한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8tNVh4WFJBQlhKcw==">Kimera<i class="fa fa-external-link-alt"></i></span>를 소개한다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/kimera.png" class="" title="Kimera">

<ol>
<li>CPU에서 돌 수 있는 </li>
<li>Metric scale을 가졌고</li>
<li>Semantic understanding을 수행할 수 있는</li>
<li>SLAM<br>이라고 하신다.</li>
</ol>
<p>스테레오 카메라 + IMU를 사용해서 3D semantic metric mesh model이 나온다고 한다.</p>
<hr>
<h1 id="Problem-3-소형화"><a href="#Problem-3-소형화" class="headerlink" title="Problem 3 - 소형화"></a>Problem 3 - 소형화</h1><p>SLAM 알고리즘들이 많이 ‘최적화’ 되고 하드웨어가 발전하면서, 최근에는 정말로 가벼운 모바일 하드웨어에서도 SLAM을 돌릴 수 있게 되었다.</p>
<p>여기서 이야기하는 ‘정말로 가벼운 모바일 하드웨어’는 사실 &gt;10W 의 전자기기… 보통 SBC 등을 이야기 하는 것이다.<br>과거에 비해서는 엄청나게 나아진 것이지만, 미래를 보았을 때 갈길이 멀다는 것을 알 수 있다.<br>왜냐면 결국 우리는 더 가벼운 플랫폼, &lt;5W나 &lt;200mW 등의 초소형 기기에서도 돌리고 싶을 것이기 때문이다.</p>
<p>이는 사람의 시각 능력보다도 훨씬 좋은 성능이여야하는 것인데…<br>아래 사진을 보면 아직 우리의 알고리즘은 사람의 시각 능력보다 훨씬 비효율적이고, 미래에 갈 길이 멀다는 것을 알 수 있다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/human_vs_machine.png" class="" title="Humans vs Machines">

<hr>
<h1 id="공부해야할-것"><a href="#공부해야할-것" class="headerlink" title="공부해야할 것"></a>공부해야할 것</h1><p>이론:</p>
<ul>
<li>Geometry</li>
<li>Estimation</li>
<li>Probability</li>
<li>Optimization</li>
</ul>
<p>구현:</p>
<ul>
<li>Real-time</li>
<li>Robustness</li>
<li>Impact</li>
</ul>
<hr>
<h1 id="추천-공부-방식"><a href="#추천-공부-방식" class="headerlink" title="추천 공부 방식"></a>추천 공부 방식</h1><p>이걸로 시작하면 좋다고 하신다.</p>
<img src="/20201223-IROS2020-Luca-Carlone-SLAM-workshop/recommendations.png" class="" title="Recommendation">














]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Kimera</tag>
        <tag>Certifiable algorithm</tag>
        <tag>MAP estimation</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2020 - Deep Direct Visual SLAM (Prof. Daniel Cremers 발표)</title>
    <url>/20201225-CVPR2020-Cremers-SLAM-workshop/</url>
    <content><![CDATA[<hr>
<h1 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h1><img src="/20201225-CVPR2020-Cremers-SLAM-workshop/avengers.png" class="" title="어벤저스 ㄷㄷ">

<p>소개되는 내용에 참여신 분들은 TUM의 Cremers 교수님 랩실의 어벤저스 이다.</p>
<p>Nan Yang - DVSO와 D3VO를 성공시킨 떠오르는 딥러닝 슬램 스타,<br>Lukas von Stumberg - GN-Net, LM-Reloc 등 기존 다이렉트 방식의 한계를 깨부수는 딥러닝 기법을 만든 연구자,<br>Rui Wang - 수많은 DSO의 베리에이션을 만들었고, 최신 Cremers 교수님 랩실의 굵직한 연구에는 다 참여하는 연구자,<br>Jörg Stückler - 막스 플랑크…,<br>Jakob Engel - Direct 방식을 띄워준 장본인인 LSD-SLAM과 DSO의 1저자,<br>그리고 Cremers 대장님</p>
<p>숨만 쉬어도 새 논문이 나올 것 같은 조합 ㄷㄷ</p>
<p>기대 만땅으로 공부를 시작합니다</p>
<hr>
<h1 id="Feature-based-SLAM-vs-Direct-SLAM"><a href="#Feature-based-SLAM-vs-Direct-SLAM" class="headerlink" title="Feature-based SLAM vs Direct SLAM"></a>Feature-based SLAM vs Direct SLAM</h1><p>SLAM의 목적은 Camera motion + 3D Structure를 구해내는 방법이다.</p>
<p>Erwin Kruppa라는 오스트리아 수학자/과학자가 SLAM의 기초 아이디어를 처음 제안하였다.<br>1913년에 (무려 100년도 전에… ㄷㄷ) “두개의 view로부터 5개의 correspondence가 있으면, 그 view들 사이의 motion과 3D 구조를 유한한 해로 추측할 수 있다’라는 것을 증명했다고 한다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/keypoint_vs_direct.png" class="" title="Keypoint vs Direct">

<br>

<h2 id="Feature-based-SLAM"><a href="#Feature-based-SLAM" class="headerlink" title="Feature-based SLAM"></a>Feature-based SLAM</h2><p>Kruppa의 방식을 그대로 따른 방법이 Feature-based (Keypoint-based) SLAM이다.<br>Kruppa가 이야기한대로, 2개의 이미지로부터 correspondence match를 만들어준다.<br>이 때, feature detector / descriptor를 이용한다.<br>또, 매치가 잘 안될 경우를 고려해서 RANSAC 등의 테크닉을 함께 사용한다.<br>Bundle adjustment를 수행하면 camera motion과 3D structure를 얻을 수 있다.</p>
<br>

<h2 id="Direct-SLAM"><a href="#Direct-SLAM" class="headerlink" title="Direct SLAM"></a>Direct SLAM</h2><p>Cremers 교수님은 다른 방법 (Direct method)를 제안한다.<br>카메라 이미지로부터는 엄청난 양의 데이터가 들어온다.<br>이 데이터로는 색, 밝기 등등에 대한 정보가 많이 있는데 (dense data), Point feature를 뽑는 과정에서 우리는 이걸 모두 버리게 된다 (sparse data).<br>심지어 Point feature를 뽑아도 이 point가 motion / structure 추정에 도움이 되는 point인지 알기 어렵다.<br>그렇기에 좋지 않은 point가 계산에 들어가게 될 수도 있고, 이 경우에는 motion / structure 추정에도 좋지 않은 결과를 내게 된다. (RANSAC이 어느정도 처리를 해주긴 하지만, RANSAC 파라미터 튜닝이 굉장히 귀찮은 작업이다.)<br>이렇게 point feature를 뽑지 않고, 카메라 이미지 자체를 사용하는 방식이 Direct 방식이다.<br>Direct 방식의 장점은, Raw data를 그대로 이용해서 기존의 feature detector / descriptor 방식의 에러들을 제거할 수 있다는 것이다.</p>
<p>두 방식의 다른 점을 뽑으라면,<br>Feature-based SLAM은 reprojection error를 최소화하는 reconstruction을 목적으로 최적화 프로그래밍 (즉, motion과 structure가 서로 동의할 수 있는 추정치를 가질 때 까지)을 수행하는 것이고,<br>Direct SLAM은 이미지 밝기 값에 대한 에러인 photometric error / brightness consistency 값이 최소화 되는 최적화 프로그래밍을 한다 (이 과정은 optical flow등을 생각하면 굉장히 유사하다).</p>
<hr>
<h1 id="초기-Direct-SLAM-LSD-SLAM"><a href="#초기-Direct-SLAM-LSD-SLAM" class="headerlink" title="초기 Direct SLAM - LSD-SLAM"></a>초기 Direct SLAM - LSD-SLAM</h1><p><span class="exturl" data-url="aHR0cHM6Ly92aXNpb24uaW4udHVtLmRlL3Jlc2VhcmNoL3ZzbGFtL2xzZHNsYW0=">LSD-SLAM<i class="fa fa-external-link-alt"></i></span>은 Jakob Engel이 ECCV 2014 학회에서 발표한 최초의 Large scale 환경을 커버할 수 있는 direct 기반 SLAM 이다.</p>
<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/lsd_slam.png" class="" title="LSD-SLAM">

<p>LSD-SLAM은 다음과 같은 방식으로 작동한다.<br>영상이 들어오면 Tracking과 Depth map estimation 두개의 작업이 동시에 이뤄진다.<br>그리고 keyframe에서 추출된 Depth map들은 Point cloud 형태의 Global map으로 추가된다.<br>Large-scale에서 작동할 수 있는 이유는 오른쪽 상단의 Similarity optimisation 방식이 정확하게 계산을 해주기 때문이다.<br>이 방식은 원래 LiDAR SLAM에서 사용하던 것인데, trajectory를 효과적으로 계산하는 것을 보고 채용해왔다.</p>
<p>Cremers 교수님 말씀으로는, 대부분의 연구/작업은 Jakob Engel이 진행하였고, 본인은 LSD-SLAM이라는 이름을 만들어서 인기있게 만든 것 뿐이라고 하신다 (LSD는 마약 이름이기도 하니까, 노이즈마케팅이라고 하신다 ㅋㅋㅋ…)</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/lsd_tracking.png" class="" title="LSD-SLAM Tracking">

<p>LSD-SLAM이 Direct SLAM인 이유에 대해서 잠시 설명하자면…<br>Keyframe 이미지와 카메라에서 방금 들어온 최신 이미지가 있을 때,<br>이 이미지를 어떤 rigid body motion (i.e. 3d rotation + 3d translation)으로 warp해야, keyframe 이미지와 똑같이 생기는가? 라는 문제를 푸는 것이다.<br>여기서 keyframe 이미지와 현재 이미지에서의 밝기를 (intensity) 비교하기 때문에, Direct 방식이라고 할 수 있다.<br>그리고 이 과정은 단순히 6개의 파라미터 (i.e. xyz rotation, xyz translation)만 추정하기 때문에 계산량이 적고, 그렇기에 CPU 속 1개의 코어에서 실시간으로 돌아간다고 한다.<br>나머지 코어에서는 다른 부분인 Depth map estimation과 Global realignment / pose graph optimisation을 수행한다고 한다.</p>
<p>이러한 방식으로 LSD-SLAM은 Large-scale에서도 굉장히 작은 drift을 가지게 된다고 한다.<br><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9HbnVRelAzZ3R5NA==">영상<i class="fa fa-external-link-alt"></i></span>으로 볼 수 있다.</p>
<p>LSD-SLAM에는 단점이 하나 있다.<br>LSD-SLAM 은 camera trajectory를 계산하고나서 map에 대한 계산을 이어간다.<br>하지만 사실 이러한 계산은 단일 과정으로 jointly하게 풀어내는게 훨씬 더 좋다.<br>trajectory에 에러가 생긴만큼, map 계산에 그 에러가 전파되기 때문이다.<br>물론 jointly하게 계산하는 방식은 방식은 계산해야할 파라미터의 수가 훨씬 많아져서 iterative하게 풀어낼 수 밖에 없다.<br>그리고 iterative하게 풀기 위해서는 어쩔 수 없이 photometric bundle adjustment라는 방식을 만들어내야한다.<br>이 당시에는 photometric bundle adjustment라는 방식이 만들어지지 않았었다.</p>
<p>이 단점을 해소하기 위해 Jakob Engel은 그 다음 방법인 DSO를 제안한다.</p>
<hr>
<h1 id="Direct-SLAM-DSO-Direct-Sparse-Odometry"><a href="#Direct-SLAM-DSO-Direct-Sparse-Odometry" class="headerlink" title="Direct SLAM - DSO (Direct Sparse Odometry)"></a>Direct SLAM - DSO (Direct Sparse Odometry)</h1><p>DSO는 굉장히 robust하다.<br><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9DNi14d1NPT2RxUQ==">데모 영상<i class="fa fa-external-link-alt"></i></span>을 보면 (약 10~15초 쯤), 주변에 사람들이 걸어다니는데도 그들을 무시하면서 잘 트랙킹을 한다.<br>SLAM은 원래 static environment에서만 잘된다.<br>하지만 DSO는 움직이는 객체가 있어도 잘 되는 것 처럼 보인다.</p>
<p>DSO는 7개의 연속된 keyframe 이미지로부터 camera motion과 structure를 동시에 계산하는 (i.e. joint optimisation) photometric bundle adjustment를 수행하면서, semi-dense reconstruction을 수행한다. </p>
<p>DSO는 pose graph optimisation이라던지 loop closure와 같은 global optimisation을 수행하지 않는다.<br>그렇기 때문에 오랜기간동안 트랙킹을 진행할 경우 어쩔 수 없이 drift가 쌓일 수 밖에 없다.<br>그래서 DSO도 ‘SLAM’ 이라는 이름 대신 ‘Odometry’라는 이름을 채택하였다.<br>하지만 그럼에도 drift가 굉장히 작은 것을 볼 수 있다 - 몇백 m를 움직였는데도 drift는 2-3m 밖에 나타나지 않는데, 그만큼 DSO가 정확하다는 것을 볼 수 있다.</p>
<p>여기서 우리는 SLAM에서 ‘정확도’라는 개념에 대해서 조금 생각을 해보게 된다.<br>Large-scale에서 정확도는 어떻게 재야할까?<br>Small-scale에서 정밀하게 트랙킹하며 GT 데이터로 사용되는 모션 캡처 카메라는 large-scale에서 사용할 수 없다.<br>GPS, IMU 등은 오차가 +- x m단위로 부정확해서 논외이다.<br>시뮬레이션으로 만든 방식으로는 정확한 GT를 얻어낼 수 있다.<br>하지만 시뮬레이션에서 잘 작동하는 SLAM 방식을 만들어도 종종 실제 환경에서 잘 작동하지 않는 경우가 있다.<br>그러므로 시뮬레이션 GT도 사용하기 애매하다.</p>
<p>Jakob Engel이 택한 방법은, real-life scene에 대한 큰 데이터셋을 구축하는 것이다.<br>Indoor, outdoor 환경에서 perspective, fisheye 등의 다양한 카메라 셋팅을 사용해서 약 50개의 시퀀스를 구비해놨다.<br>모든 시퀀스는 끝나갈 때 쯤에 시퀀스가 처음 시작한 위치로 돌아오는데, 이 때 최종 위치와 초기 위치를 align시켜 정확한 위치를 찾고, DSO가 추정한 위치 값을 비교하여 translation, rotation, scale 등에 대한 ‘정확도’를 추정하였다.</p>
<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/dso.png" class="" title="DSO performance">

<p>DSO의 정확도 벤치마크 결과이다.<br>DSO가 ORB-SLAM (잘되는 feature-based SLAM 모듈)보다 훨씬 정확하다는 결과가 나온다.<br>DSO에는 pose graph optimisation이나 loop closure가 없는데도 말이다.<br>Cremers 교수님은 ‘이미지를 직접 다루는 Direct 방식이기 때문에’ 이런 정확도가 나올 수 있다고 이야기한다. </p>
<br>

<hr>
<h1 id="Deep-Neural-Networks"><a href="#Deep-Neural-Networks" class="headerlink" title="Deep Neural Networks"></a>Deep Neural Networks</h1><p>딥러닝 방식은 현재 Classical한 방식이 만드는 reconstruction 정확도를 이기지 못한다고 알려져있다.<br>물론 여기에는 정말 다양한 이유가 있지만, 이번 발표의 주제가 아니기 때문에 깊게 들어가지 않는다.<br>그럼에도 딥러닝은 지속적으로 연구할만한 가치가 있다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/semantic.png" class="" title="Semantic map">

<p>SLAM에 딥러닝을 섞었을 때, 현재 단계에서는 Semantic understanding을 통해 가장 효과적으로 가치를 창출할 수 있다. </p>
<p>단순히 3D point cloud map을 만드는게 아니라, 실제로 어떤 object가 어디에 있는지도 알려주는 것이다.<br>위의 사진은 Cremers 교수님의 스타트업인 Artisense에서 만든 3D semantic map이다.<br>하늘색은 건물, 초록색은 식물, 주황색은 도보, 파란색은 자동차 등등이 라벨링이 되어있는 것을 볼 수 있다.<br>이는 여러대의 자동차에 각각 센서 시스템을 구축하고, 각각의 차에서 얻어진 맵을 하나로 합친 결과이다.<br>Artisense의 목적은 저렴한 센서 시스템을 구축해서 자율주행을 위한 정확하고 쓸모있는 3D semantic map을 구축하는 것이 목표라고 한다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/deep.png" class="" title="Deep learning slam">

<p>실제로 딥러닝을 SLAM 파이프라인에 집어넣는 연구는 2017년부터 시작되었다고 볼 수 있다.</p>
<p>딥러닝을 이용해서 직접적으로 카메라 포즈를 구해내고 3D reconstruction을 수행하는 것이다.<br>이러한 연구들은 이런 계산이 ‘가능하다는 것’을 보여주었다.<br>하지만 아직 State-of-the-Art 성능을 보여주지는 못했다.</p>
<p>이러한 이유 중 하나를 이야기하자면, 많은 연구가 SLAM에서 필요로 하는 과정을 정확하게 이해하지 못하고 overly-ambitious + blind하게 end-to-end로 풀어내려는 것이라고 생각한다.<br>현재의 기술로는, 뛰어난 성능의 추론능력을 보여주는 딥러닝 네트워크를 기존의 SLAM 프레임워크에 부분적으로 사용하는 방식이 가장 효과적이라고 본다.</p>
<hr>
<h1 id="딥러닝-SLAM-DVSO-Deep-Virtual-Stereo-Odometry"><a href="#딥러닝-SLAM-DVSO-Deep-Virtual-Stereo-Odometry" class="headerlink" title="딥러닝 SLAM - DVSO (Deep Virtual Stereo Odometry)"></a>딥러닝 SLAM - DVSO (Deep Virtual Stereo Odometry)</h1><img src="/20201225-CVPR2020-Cremers-SLAM-workshop/dvso.png" class="" title="Deep Virtual Stereo Odometry">

<p>Cremers 교수님 랩실에서 제일 먼저 성공적으로 만든 딥러닝 기반 SLAM은  ECCV 2018 학회에서 공개한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9zTFpPZUM5el90dw==">Deep Virtual Stereo Odometry<i class="fa fa-external-link-alt"></i></span>이다. </p>
<p>CVPR 2017 학회에서 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDIuMDI3MDY=">Kuznietsov 2017<i class="fa fa-external-link-alt"></i></span> 논문에서 발표된 Single image depth estimation 네트워크의 개선하여 DVSO에서는 새로운 StackNet이라는 네트워크를 제안하였다.<br>위에서 언급하였던 Nan Yang, Rui Wang, Jörg Stückler가 연구를 리드하였다.<br>Depth estimation의 성능만 보았을 때 StackNet은 기존의 Kuznietsov의 연구보다 더 좋은 성능을 보였다.<br>하지만 DVSO 논문의 포인트는 그게 아니다.<br>DVSO는 이 StackNet을 기존의 SLAM 프레임워크에 포함시켜, Classical 방식의 SLAM보다 더 정확한 SLAM을 만들어냈다.<br>StackNet을 이용해서 Keyframe에 대한 초기 depth map을 만들었다는 점과, Direct SLAM 파이프라인을 고려해서 reconstruction되는 3D 구조와 StackNet이 추론하는 depth map이 서로 동의할 수 있게 + brightness consistency를 고려한 새로운 loss function을 사용했다는 점이 이 논문의 포인트이다.<br>또, 이 방식은 Self-supervised learning으로 트레이닝 할 수 있다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/dvso_comp.png" class="" title="DVSO Performance">

<p>DVSO는 기존의 Classical 방식들과 비교했을 때, 월등히 좋은 성능을 보여준다.<br>특히, State-of-the-Art인 ORB-SLAM 2와 Stereo-DSO와 비교했을 때도 더 좋은 성능을 보여준다. 놀라운 점은 이 두가지 방식은 정확한 맵핑을 위해 Stereovision을 사용했음에도 불구하고, Mono camera를 사용한 DVSO가 더 좋은 성능을 보여줬다는 것이다.<br>또 다른 놀라운 점은 scale drift에 있다.<br>보통 Monocular SLAM을 사용할 때는 scale drift가 나타난다.<br>하지만 DVSO는 scale drift가 거의 없다.</p>
<p>딥러닝을 잘 사용하기만 하면 하드웨어의 한계도 뛰어넘을 수 있다는 결과가 나타난 것이기 때문에, Artisense가 추구하는 ‘저렴한 센서 시스템으로 정확한…’ 부분과 잘 맞아떨어진다고 볼 수 있다.</p>
<hr>
<h1 id="딥러닝-SLAM-D3VO-3-Depth-Pose-Uncertainty"><a href="#딥러닝-SLAM-D3VO-3-Depth-Pose-Uncertainty" class="headerlink" title="딥러닝 SLAM - D3VO (3 - Depth, Pose, Uncertainty)"></a>딥러닝 SLAM - D3VO (3 - Depth, Pose, Uncertainty)</h1><p>DVSO가 진화한 버전이 D3VO라고 볼 수 있다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/d3vo.png" class="" title="What is D3VO">

<p>CVPR 2020 학회에 발표된 이 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDEwNjA=">논문<i class="fa fa-external-link-alt"></i></span>의 주 저자는 DVSO의 주 저자인 Nan Yang이다.<br>DVSO가 딥러닝 기술로 depth를 추정하고 direct slam 파이프라인에 포함시킨거였다면, D3VO는 depth, camera pose, uncertainty를 모두 딥러닝으로 추정하고 SLAM 파이프라인에 포함시키는 것이다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/affine_brightness.png" class="" title="How D3VO works">

<p>D3VO가 작동하는 방법은 다음과 같다.</p>
<p>우선, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.686ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 745.3 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(440, -150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> (시간 t에 얻는 이미지 I)를 single image depth estimation 네트워크에 넣어서 depth map을 추정한다.</p>
<p>동시에, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.686ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 745.3 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(440, -150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> 와 그 다음 프레임인 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.077ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 918.1 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="TeXAtom" transform="translate(440, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 289) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g></g></g></g></svg></mjx-container>를 pose estimation 네트워크에 넣어서 relative motion을 (i.e. 3D rotation + 3D translation) 구한다.<br>이 3D transformation 값으로 이미지를 align시켜서, 전체 네트워크를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex" xmlns="http://www.w3.org/2000/svg" width="18.14ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 8017.9 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(681, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(935, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1233, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2269.5, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3325.3, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3776.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4165.3, 0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="TeXAtom" transform="translate(440, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4910.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5355.3, 0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="TeXAtom" transform="translate(440, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 289) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(605.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mo" transform="translate(1383.5, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mi" transform="translate(2161.5, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7628.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> loss로 self-supervised training을 할 수 있다.<br>이 값이 brightness consistency가 되겠다. </p>
<p>D3VO에서 새롭게 내보이는 방식인 딥러닝을 통한 affine brightness correction도 이 단계에서 계산한다.<br>실제 환경에서 카메라를 가지고 이미지를 얻을 때 aperture가 (i.e. 카메라 조리개 값 == 빛을 받는 양) 변한다면 이미지의 밝기가 변한다.<br>즉, 동일 위치를 바라보고 있는 상태에서도 두 이미지의 밝기가 다르다는건데, 이는 기존의 Direct 방식이 풀 수 없는 조건이였다.<br>이 부분을 딥러닝 네트워크를 통해 affine brightness correction 값을 학습하여, 동일 위치를 바라볼 때 direct 방식으로 이미지를 비교할 수 있도록 조명의 밝기를 맞춰준다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/uncertainty.png" class="" title="Aliatoric uncertainty">


<p>이 affine brightness correction은 사실 빛의 반사 모델 중 하나인 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTGFtYmVydGlhbl9yZWZsZWN0YW5jZQ==">lambertian model<i class="fa fa-external-link-alt"></i></span>에 기반한 방법이다.<br>하지만 실제 세상에서는 사실 금속이나 유리처럼 특별히 반사가 잘 되거나 안되는 재질이 있다.<br>이러한 재질에서는 어떤 값이 나올지 모르기 때문에, 이에 해당하는 uncertainty를 할당해줘야하는데, 이를 위해서 <span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnMubmlwcy5jYy9wYXBlci8yMDE3L2ZpbGUvMjY1MGQ2MDg5YTZkNjQwYzVlODViMmI4ODI2NWRjMmItUGFwZXIucGRm">Kendall 2017 논문<i class="fa fa-external-link-alt"></i></span>과 <span class="exturl" data-url="aHR0cHM6Ly93d3cucm9ib3RzLm94LmFjLnVrL352Z2cvcHVibGljYXRpb25zLzIwMTgvS2xvZHQxOC9rbG9kdDE4LnBkZg==">Klodt 2018 논문<i class="fa fa-external-link-alt"></i></span>에서 제안된 aliatoric uncertainty 기법을 사용하였다.<br>이 uncertainty가 높은 부분에서는 아무래도 brightness consistency가 낮을 것이라고 예상을 할 수 있기 때문에, 이 부분에 uncertainty 값을 이용해서 최적화 과정에 weight를 줄 수 있다.<br>사진에 보이는 것 처럼, 금속이나 유리 재질 뿐만이 아니라 나무 꼭대기처럼 motion이 생겨서 높은 brightness consistency를 가지는 부분은 높은 uncertainty를 가지는 것 처럼 보이게 된다.</p>
<p>그 후, 모든 Depth, Uncertainty, Pose, Affine brightness correction을 Frontend tracking과 Backend optimisation에 넣는 것이다.<br>Frontend에서는 nonlinear factor graph를 사용해서 brightness consistency를 계산하고, Backend에서는 reconstruction된 모델이 pose와 depth에 대해 추론한다 (추론 모델은 이 둘에 대해 사전 트레이닝 되어있다).</p>
<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/d3vo_depth.png" class="" title="Depth estimation">

<p>결과를 보았을 때, 우선 depth estimation의 성능을 보면 sota 모델 중 하나인 MonoDepth2보다 더 좋은 성능을 냈으며, 굉장히 좋은 generalization을 보여줬다.<br>반사가 잘 되는 재질에서 aliatoric uncertainty가 굉장히 잘 나타나는 것을 볼 수 있다.</p>
<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/d3vo_vo.png" class="" title="Visual odometry">

<p>SLAM에 대한 결과를 보면, classical method나 deep learning 기반 기술 양쪽 모두에서 State-of-the-art를 달성한 것을 볼 수 있다.<br>특히, Stereo Visual-inertial odometry인 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDQuMDY1MDQ=">Basalt<i class="fa fa-external-link-alt"></i></span>와 동일한 성능을 내는 것을 볼 수 있는데, 여기서 다시 한번 D3VO는 monocular visual odometry인 것을 생각하면 엄청나게 성능이 좋다는 것을 알 수 있다.</p>
<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/d3vo_recon.png" class="" title="Reconstruction">

<p>Mono DSO와 D3VO의 reconstruction 결과를 비교했을 때도, D3VO의 결과가 훨씬 깨끗한 점을 알 수 있다.</p>
<hr>
<h1 id="딥러닝-SLAM-GN-Net-Multi-weather-relocalization"><a href="#딥러닝-SLAM-GN-Net-Multi-weather-relocalization" class="headerlink" title="딥러닝 SLAM - GN-Net (Multi weather relocalization)"></a>딥러닝 SLAM - GN-Net (Multi weather relocalization)</h1><img src="/20201225-CVPR2020-Cremers-SLAM-workshop/vis_loc.png" class="" title="Visual localization under lighting and weather">

<p>이쯤되면 Direct 방식의 가장 큰 문제점을 알 수 있다.<br>조명이 바뀌어버리면, photometric 최적화를 할 수 없다는 점이다.<br>그렇다면, 동일한 장소라도 다른 시간대에 온 경우에는 (낮/밤, 여름/겨울) 조명이 다르니 동일 장소로 인식할 수 없을 것이다.<br>위에 있는 사진을 예시로 들면, 여름에는 비가와서 물이 고였고 잔디가 파릇파릇하게 자랐고, 겨울에는 눈이 덮혔다.<br>이 때문에 이미지 속 밝기 값이 많이 다르게 보이고, 이전에 봤던 위치랑은 다르게 인식이 될 것이다.</p>
<p>하지만 이러한 high-level representation은 딥러닝 기반 기술이 제일 잘 푸는 문제 중 하나이다.<br>이 문제를 풀어보기위해 데이터셋을 제작하고, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDQuMTE5MzI=">GN-Net<i class="fa fa-external-link-alt"></i></span>이라는 기술을 von Sturmberg가 발표하였다.<br>GN-Net은 high-dimensional feature space에서 intensity transformation을 하는데, 사실 이러한 방법은 이전에 다른 네트워크에서도 많이 제안되었던 방법이다.<br>이 논문이 다른 점은, Gauss-Netwon 기반 최적화를 하는 SLAM에서 좋은 결과를 낼 수 있도록 학습했다는 점이다.</p>
<br>

<img src="/20201225-CVPR2020-Cremers-SLAM-workshop/gn-net.png" class="" title="GN-Net">

<p>여기 이미지에서 왼쪽 아래 빨간 박스에 쳐진 이미지는 현재 보고있는 이미지, 오른쪽 아래 박스에 쳐진 이미지는 트레이닝 데이터셋에서 보았던 가장 비슷한 이미지이다.<br>그리고 회색 포인트 클라우드는 이전에 만들었던 포인트 클라우드이고, 파란색 포인트 클라우드는 새로 만들어지는 포인트 클라우드이다.<br>그래서 사진을 보았을 때, 회색에는 없지만 파란색에는 있는 차가 있지만, 바닥에 그려진 차선은 정확하게 일치하는 것을 볼 수 있다.</p>
<p>GN-Net은 GPS 센서 없이 direct 방식만으로도 relocalization을 할 수 있다는 것을 증명하였다.<br>또, 지금까지의 direct 방식에서는 odometry 방식으로 SLAM을 했왔는데, 이제는 large-scale에서 필수적인 relocalization을 할 수 있다는 것을 증명하였다.</p>
<hr>
<h1 id="발표에-대한-질문"><a href="#발표에-대한-질문" class="headerlink" title="발표에 대한 질문"></a>발표에 대한 질문</h1><h2 id="Photometric-error를-사용할-때의-단점이-있다면-무엇인가요"><a href="#Photometric-error를-사용할-때의-단점이-있다면-무엇인가요" class="headerlink" title="Photometric error를 사용할 때의 단점이 있다면 무엇인가요?"></a>Photometric error를 사용할 때의 단점이 있다면 무엇인가요?</h2><p>Classical한 방식으로 Brightness consistency는 특정 상황에서 깨지게 됩니다.<br>그래서 이러한 단점을 해결하기 위해 딥러닝 기법을 적용하였습니다.<br>이번 토크에서는 3가지 방법을 언급하였습니다.</p>
<ul>
<li>Affine brightness change</li>
<li>Aliatoric uncertainty</li>
<li>Brightness variation (multi-weather)</li>
</ul>
<p>네트워크가 brightness variation을 이해하는것이 중요합니다.</p>
<h2 id="다른-센서와-퓨전한다면-어떤-것과-해야할까요"><a href="#다른-센서와-퓨전한다면-어떤-것과-해야할까요" class="headerlink" title="다른 센서와 퓨전한다면 어떤 것과 해야할까요?"></a>다른 센서와 퓨전한다면 어떤 것과 해야할까요?</h2><p>이 질문은 연구보다는 실무에 계신 분들이 하실 것 같습니다.<br>이번 토크에서는 Visual-SLAM만을 특정하여 이야기 하였고, 카메라만 가지고 어느정도까지 뽑아낼 수 있는지에 대해 이야기하였습니다.</p>
<p>하지만 실제 환경에서 자율주행차나 로봇에서 사용할 것이라면, 가능한 센서 조합을 많이 테스트해보고 제일 정확한 방법을 사용하는 것이 맞습니다.<br>저희가 테스트 했을 때는 fast rotation 등이 있었을 때 IMU가 굉장히 도움이 됬었습니다.<br>GPS (RTK GPS)가 있다면 global reference도 얻을 수 있구요.<br>LiDAR나 RADAR도 굉장히 좋구요.</p>
<p>하지만 제가 드리고 싶은 조언은, 센서 퓨전을 할것이라면 정확히 ‘어떤 목적을 위해서 센서 퓨전을 하는지’ 아는 것이 중요하다고 생각합니다.<br>왜 이 센서를 퓨전하는 것이고, 어떤 방식으로 퓨전을 할 것이고를 충분히 이해해야 센서들의 장점들을 섞을 수 있는 것이고, 제대로 이해하지 못한다면 센서들의 단점만 섞게 될 수도 있습니다.</p>
<p>Artisense에서 사용하는 Stereo Visual-inertial 시스템은 3km 정도를 주행하고 왔을 때 약 1 m 정도 에러가 나타나는데, 이는 센서 퓨전을 잘 수행했기 때문에 에러를 줄일 수 있었던 것으로 봅니다. </p>
<h2 id="Robustness와-Accuracy를-위해서-End-to-End를-공부하는게-좋을까요-아니면-Hybrid를-공부하는게-좋을까요"><a href="#Robustness와-Accuracy를-위해서-End-to-End를-공부하는게-좋을까요-아니면-Hybrid를-공부하는게-좋을까요" class="headerlink" title="Robustness와 Accuracy를 위해서 End-to-End를 공부하는게 좋을까요? 아니면 Hybrid를 공부하는게 좋을까요?"></a>Robustness와 Accuracy를 위해서 End-to-End를 공부하는게 좋을까요? 아니면 Hybrid를 공부하는게 좋을까요?</h2><p>지금 상황에서는 답변하기 어려운 질문입니다.<br>Artisense에서는 둘 다 연구를 하고 있습니다.<br>딥러닝은 correspondence estimation, single image depth estimation 등의 기술에 굉장히 효과적입니다만, classical 방식에서 optimisation 기법 역시 굉장히 효과적인 방법입니다.<br>우리가 알고 있는 것도 잘 쓸줄 알아야하죠 - 카메라의 작동 원리라던가, 이미지 투영 방식, rigid body motion과 perspective projection을 통해서 이미지가 만들어지는 과정이라던지, epipolar geometry라던지… 우리는 정말 잘 알고 있습니다.<br>딥러닝은 효과적인 방법이긴 하지만, 잘 될거라는 확신을 가지고 깊게 이해하지 못한 채 막 쓰는 것은 좋지 않고, 기존의 classical 방식을 통한 인사이트를 기반으로 어떤 목적으로 사용하고 어떻게 적용 될지 정확히 이해했을 때 쓰는 것이 좋습니다. </p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Direct SLAM</tag>
        <tag>LSD-SLAM</tag>
        <tag>DSO</tag>
        <tag>DVSO</tag>
        <tag>D3VO</tag>
        <tag>GN-Net</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2020 - From SLAM to Spatial AI (Prof. Andrew Davison 발표)</title>
    <url>/20201226-CVPR-2020-SLAM-workshop-Davison/</url>
    <content><![CDATA[<h1 id="Visual-SLAM"><a href="#Visual-SLAM" class="headerlink" title="Visual-SLAM"></a>Visual-SLAM</h1><p>로봇, 드론, 모바일/스마트폰 AR 소프트웨어, VR/AR 헤드셋 등에서 Visual-SLAM을 사용한다.<br>이는 카메라가 작고 싸기 떄문에 제품화하기 좋기 때문이다.<br>또, 이미지를 통해 정말 다양한 기능을 이용할 수 있기 때문이다.</p>
<p>Visual-SLAM을 사용하는 이유는 대부분 positioning 또는 어느정도의 sparse/semi-dense mapping을 수행하기 위해서이다.<br>sparse / semi-dense mapping을 넘어, dense / semantic mapping을 사용하는 제품들도 점차 나타나기 시작했다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/level_of_performance.png" class="" title="Levels of performance in SLAM">

<p>Davison 교수님이 멤버로 계신 SLAMCORE라는 스타트업에서 정의한 SLAM 기술의 단계는 위와 같다.</p>
<p>SLAM은 앞으로 Spatial AI의 일부로 들어갈 것이다.<br>Spatial AI는 컴퓨터 비전을 사용하여 실시간으로 디바이스가 주변 환경과 상호작용을 할 수 있게 해주는 AI 시스템이다. </p>
<p>SLAM의 목적은 localization 한번 하는 것, mapping 한번 하는 것으로 끝나는게 아니다.<br>SLAM의 성능은 제품에 모듈로써 포함될 수 있는 정도가 되어야한다.<br>실시간으로 작동하고 인터랙션을 할 수 있는 데모를 볼 수 있어야한다.<br>직접 데모로 보여주던가, 제품을 내던가, 아니면 오픈소스 프로젝트로 오픈할 수 있어야한다.<br>논문에서 데이터셋을 이용한 벤치마크 성능을 보여주는 것 보다 이것이 더 중요하다.</p>
<hr>
<h1 id="Spatial-AI-Spatial-IA가-풀어야-할-숙제"><a href="#Spatial-AI-Spatial-IA가-풀어야-할-숙제" class="headerlink" title="Spatial AI, Spatial IA가 풀어야 할 숙제"></a>Spatial AI, Spatial IA가 풀어야 할 숙제</h1><h2 id="Spatial-AI가-풀어야-할-숙제"><a href="#Spatial-AI가-풀어야-할-숙제" class="headerlink" title="Spatial AI가 풀어야 할 숙제"></a>Spatial AI가 풀어야 할 숙제</h2><p>대량생산될 청소 로봇을 만든다고 생각해보자.<br>이 로봇은 복잡한 구조의 방과 물건들 사이에서 닦고 쓸줄 알아야한다.<br>이 로봇을 설계할 때, 우리는 이 로봇의 가격, 디자인, 크기, 안전, 소비전력 등을 고려해야한다.<br>그리고, 이 모든게 보통의 사용자가 쓸 수 있는 범위 안에 있어야한다.</p>
<p>지금 SLAM의 단계로는 복잡한 구조의 방과 물건에서 정확하게 localization / mapping에서 어려울 수 있다.</p>
<br>

<h2 id="Spatial-IA가-풀어야-할-숙제"><a href="#Spatial-IA가-풀어야-할-숙제" class="headerlink" title="Spatial IA가 풀어야 할 숙제"></a>Spatial IA가 풀어야 할 숙제</h2><p>우선 Spatial IA는 Intelligence Augmentation을 줄인 말이다.<br>간단하게 생각하면 증강현실을 이용해서 주변 환경과 사물에 대해 인터랙션을 추가한 것이라고 볼 수 있다.</p>
<p>증강현실 안경을 만든다고 생각해보자.<br>우선 이 안경의 크기는 보통의 안경과 같은 사이즈여야한다.<br>약 65g 정도의 무게를 가지고, 배터리는 하루종일 돌아갈 수 있어야한다.<br>동시에, 실시간으로 정확하게 정보를 증강시킬 수 있어야하며, 증강되는 정보는 주변 사물, 환경, 사람 등에 대한 정보를 모두 포함한다.</p>
<p>지금 <span class="exturl" data-url="aHR0cHM6Ly93d3cubWljcm9zb2Z0LmNvbS9lbi11cy9ob2xvbGVucw==">MS 홀로렌즈<i class="fa fa-external-link-alt"></i></span> 정도면 사전에 등록해놓은 정보들을 실시간으로 증강시키고, 사람의 손과 눈 정보는 읽을 수 있다.<br>하지만 이 경우, 무게는 약 600g에 배터리는 몇시간 남짓하지 않고, 가격은 5백만원 대이다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/gap.png" class="" title="Product gap">

<p>여기까지 생각하면 몇가지 질문이 생긴다.<br>어떻게 만들어야할까?<br>만들 수 있기는 한걸까?<br>만든다면 누가 만들것인가?</p>
<p>물론 아직 아무도 만들어본 적이 없기 때문에, 어떻게 만드는지는 아무도 모른다.</p>
<p>대학과 기업 연구실에서 많은 연구를 하고 있지만, Spatial AI까지의 길은 아직 멀고도 멀어보인다.<br>특히, Robustness와 Efficiency에서 많이 발전해야할 것 같다.</p>
<br>

<p>Davison 교수님께서 </p>
<ol>
<li>Spatial AI가 풀어야할 문제들과 </li>
<li>이 문제를 다 풀어낸 Spatial AI의 모습은 어떨지<br>구상을 하시고 적으신 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDMuMTEyODgucGRm">논문<i class="fa fa-external-link-alt"></i></span>이 있다.</li>
</ol>
<p>교수님이 구상하시는 Spatial AI의 특징을 두개만 꼽으면 아래와 같다.</p>
<ol>
<li>3D geometry의 정보를 내포하고 있는 어떠한 map에 대한 정보가 있어야한다.</li>
<li>튜닝 파라미터의 수가 적어야한다 + 몇개의 파라미터를 변경하는 것으로 여러 도메인의 spatial AI를 넘나들 수 있어야한다 (e.g. 로봇 -&gt; AR).</li>
</ol>
<hr>
<h1 id="SLAM의-역사와-발전-방향"><a href="#SLAM의-역사와-발전-방향" class="headerlink" title="SLAM의 역사와 발전 방향"></a>SLAM의 역사와 발전 방향</h1><p>SLAM이 미래에 어떻게 될지 보기 위해, 과거로부터 거슬러 올라가보자.</p>
<h2 id="1-MonoSLAM"><a href="#1-MonoSLAM" class="headerlink" title="1. MonoSLAM"></a>1. MonoSLAM</h2><img src="/20201226-CVPR-2020-SLAM-workshop-Davison/monoslam.png" class="" title="MonoSLAM">

<p>가장 초기의 SLAM으로 2003년에 만들어진 <span class="exturl" data-url="aHR0cHM6Ly93d3cucm9ib3RzLm94LmFjLnVrL0FjdGl2ZVZpc2lvbi9QdWJsaWNhdGlvbnMvZGF2aXNvbl9pY2N2MjAwMy9kYXZpc29uX2ljY3YyMDAzLnBkZg==">MonoSLAM<i class="fa fa-external-link-alt"></i></span>을 소개하셨다.<br>Hand-held 카메라를 사용해서 feature detection + tracking을 수행하고, Extended Kalman filter를 통해 camera pose와 map point의 위치를 jointly하게 계산해냈다.<br>2003년의 노트북으로 30FPS의 성능이 나왔고, 성공적으로 실시간 데모를 할 수 있었다.</p>
<p>물론 단점도 있었다.</p>
<ol>
<li>feature의 수가 작았고, </li>
<li>robust하지 않았다.</li>
</ol>
<p>그렇기에 feature가 많은 공간에서 천천히 움직여야만 작동했다.</p>
<br>

<hr>
<h2 id="Dense-Direct-Visual-SLAM의-시작"><a href="#Dense-Direct-Visual-SLAM의-시작" class="headerlink" title="Dense / Direct Visual SLAM의 시작"></a>Dense / Direct Visual SLAM의 시작</h2><h2 id="DTAM"><a href="#DTAM" class="headerlink" title="DTAM"></a>DTAM</h2><img src="/20201226-CVPR-2020-SLAM-workshop-Davison/denseslam.png" class="" title="Dense SLAM">

<p>2009년에 GPGPU를 사용해서 실시간 dense reconstruction을 성공시켰다.<br>당시 Davison 교수님 랩실에서 박사과정을 하고 있던 Richard Newcombe이 연구를 주도하였다.<br>이 연구를 발전시켜 2011년에 PTAM의 dense버전인 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9EZjlXaGdpYkNRQQ==">DTAM<i class="fa fa-external-link-alt"></i></span>이 발표되었다.<br>PTAM은 sparse map밖에 쓸 수 없었지만, DTAM은 이를 dense map으로 확장을 했다고 볼 수 있다.<br>이 연구를 통해 sparse map에 비해 dense map이 가지는 장점을 확실하게 알 수 있었다.</p>
<h2 id="KinectFusion"><a href="#KinectFusion" class="headerlink" title="KinectFusion"></a>KinectFusion</h2><p>조금 다른 방향의 연구가 진행된 것도 있다.<br>Microsoft Kinect와 같은 RGB-D 카메라를 사용한 연구이다.<br>2010년에는 <span class="exturl" data-url="aHR0cHM6Ly93d3cubWljcm9zb2Z0LmNvbS9lbi11cy9yZXNlYXJjaC93cC1jb250ZW50L3VwbG9hZHMvMjAxNi8wMi9pc21hcjIwMTEucGRm">KinectFusion<i class="fa fa-external-link-alt"></i></span>이 발표되었다.<br>이 연구 역시 Richard Newcombe의 작품이다.</p>
<p>DTAM의 경우는 monocular 카메라를 사용해서 dense map을 만드는 계산 방법을 만들어내는데에 많은 고민을 했다고 한다.<br>하지만 KinectFusion의 경우 하드웨어의 힘을 받아 dense map을 만드는 작업을 손쉽게 성공시켰다.<br>그리고 next step이라고 생각했던 ‘좀 더 큰 맵’을 만드는 과정과 volumetric representation 기술을 연구할 수 있게 되었다고 한다.</p>
<br>

<h2 id="ElasticFusion-SemanticFusion"><a href="#ElasticFusion-SemanticFusion" class="headerlink" title="ElasticFusion, SemanticFusion"></a>ElasticFusion, SemanticFusion</h2><img src="/20201226-CVPR-2020-SLAM-workshop-Davison/semanticfusion.png" class="" title="Semantic Fusion">

<p>2016년에는 loop-closure가 가능한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9YeVNyaFpwT0RZcw==">ElasticFusion<i class="fa fa-external-link-alt"></i></span>을 발표하였다.<br>이 방식은 기존의 KinectFusion에서 진화한 버전이라고 볼 수 있다.<br>그리고 2017년도에는 ElasticFusion으로 만들어내는 Map 정보에 딥러닝 CNN 기술로 semantic 정보를 추가한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9fYXFQOXJ1bWtnUQ==">SemanticFusion<i class="fa fa-external-link-alt"></i></span>이 발표된다.<br>이 연구를 통해 dense semantic SLAM, deep-SLAM 연구가 시작되었다.</p>
<hr>
<h2 id="Deep-SLAM에-필요한-계산-설계도"><a href="#Deep-SLAM에-필요한-계산-설계도" class="headerlink" title="Deep SLAM에 필요한 계산 설계도"></a>Deep SLAM에 필요한 계산 설계도</h2><p>SLAM 기술이 발전되면서 몇가지 트렌드가 극명하게 드러났다.</p>
<ol>
<li>정확도의 향상을 위해 점점 더 많은 양의 계산이 요구되었다.</li>
<li>실시간성 유지를 위해 병렬처리를 요구하게 되었으며, 이에 따른 하드웨어의 수도 늘어났다.</li>
<li>새로운 기능들이 추가하면서 점점 더 많은 데이터의 타입을 저장하게 되었다.</li>
</ol>
<p>SLAM 시스템은 점점 더 복잡하게 바뀌어갔다.<br>과거와 비교했을 때, SemanticFusion과 같은 deep SLAM의 설계 구조를 보면 굉장히 복잡했다.<br>그럼에도 SLAM은 계속 발전해나가야한다.<br>하지만 이 트렌드가 유지된다면, Spatial AI 단계에 다다랐을 때 시스템이 요구하는 설계는 겉잡을 수 없을정도로 복잡할 것이다.</p>
<p>아래는 간단한 Spatial AI가 수행해야하는 연산의 관계와 데이터의 타입들을 그래프 형태로 정리한 것이다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/computation_graph.png" class="" title="Spatial AI computation graph">

<p>수많은 다양한 종류의 계산과 데이터가 오고가는 것을 볼 수 있다.<br>이 뜻은, 어떠한 계산에 최적화가 들어가도 해당 계산만 최적화가 될 것이라는거다.<br>즉, 최적화를 해도 전체 시스템의 성능에 큰 영향을 주지 못할 수 있다.<br>하드웨어 디펜던시도 걸려있다.<br>딥러닝 추론과 같은 계산은 병렬처리에 특화되어 GPU와 같은 하드웨어에서 잘 돌지만 CPU에서 잘 돌지 못할것이다.<br>반대로 최적화 프로그래밍 계산은 고성능 클럭을 가진 CPU에서만 잘된다.<br>하드웨어 간의 통신도 고려해야하며, 종종 큰 데이터가 이동해야할 때 효율성이 급격하게 떨어질 수도 있다 (e.g. CPU-&gt;GPU 정보이동).<br>정리하자면, 전체 시스템의 최적화을 하기에는 수많은 제약이 걸려있으며, 최적화는 굉장히 어려울 것이다.</p>
<p>고성능 고효율의 spatial AI를 만들기 위해서는 이 그래프 전체를 최적화해야한다.<br>Davison 교수님은 두가지 방법을 제안하셨다.</p>
<ol>
<li>Representation을 개선하기</li>
<li>하드웨어를 개선하기</li>
</ol>
<p>–</p>
<h1 id="CodeSLAM-SceneCode-DeepFactors-딥러닝-keyframe-정보-압축"><a href="#CodeSLAM-SceneCode-DeepFactors-딥러닝-keyframe-정보-압축" class="headerlink" title="CodeSLAM, SceneCode, DeepFactors - 딥러닝 keyframe 정보 압축"></a>CodeSLAM, SceneCode, DeepFactors - 딥러닝 keyframe 정보 압축</h1><h2 id="CodeSLAM"><a href="#CodeSLAM" class="headerlink" title="CodeSLAM"></a>CodeSLAM</h2><img src="/20201226-CVPR-2020-SLAM-workshop-Davison/codeslam.png" class="" title="Code SLAM">

<p>Visual-SLAM에서 keyframe 정보는 굉장히 많이 사용되는 정보 중 하나이다.<br>Visual-SLAM에서는 추출된 keyframe들로부터 3D reconstruction을 수행한다.<br>최근에는 Keyframe들로부터 depth map이나 semantic label map 등을 만들기도 한다.</p>
<p>Davison 교수님 랩실에서 최근 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDQuMDA4NzQ=">CodeSLAM<i class="fa fa-external-link-alt"></i></span>과 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDMuMDY0ODI=">SceneCode<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDEuMDUwNDk=">DeepFactors<i class="fa fa-external-link-alt"></i></span>라는 연구를 발표되었다.<br>위의 사진은 SceneCode에서 가져왔다.</p>
<p>가장 초기 연구였던 CodeSLAM에서는 딥러닝 기법인 variational autoencoder를 사용해서 keyframe 이미지로부터 추출하는 depth map을 압축할 수 있다는 것을 보여주었다.<br>Variational autoencoder는 encoder 부분에서 차원을 축소하면서 컴팩트한 데이터 형태로 바꿀 수 있다.<br>Decoder를 사용해서 다시 복원시킬 수도 있지만, 이 연구에서는 ‘압축’에 집중하기에 decoder는 고려하지 않는다.<br>Variational autoencoder를 SLAM에 적용했을 때, keyframe에서 얻을 수 있는 depth map을 컴팩트한 데이터 형태인 ‘code’로 변환할 수 있게 된다.<br>이 code는 약 32개의 파라미터를 가지며, 이는 기존의 방식에 비해 굉장히 작다는 것을 알 수 있다.<br>또, 이 code의 파라미터를 조정함으로써 값을 변화시킬 수도 있다.</p>
<h2 id="SceneCode"><a href="#SceneCode" class="headerlink" title="SceneCode"></a>SceneCode</h2><p>후속 연구인 SceneCode에서는 depth map과 semantic map의 code를 이용해 joint optimisation을 수행하는 방법을 제안한다.<br>Depth map의 code화는 CodeSLAM에서 보여준 방식과 동일하다.<br>Semantic map의 code화는 새롭게 제안하는 방식이지만, CodeSLAM과 크게 차이가 나지는 않는 것 같다.<br>여러 keyframe들로터 depth map code와 semantic map code가 있을 때, 이 code 들의 값을 조정하면서 joint optmisation을 수행할 수 있다.<br>Code가 포함하는 파라미터의 수가 굉장히 작기 때문에, joint optimisation의 계산량도 많지 않다고 한다.</p>
<h2 id="DeepFactors"><a href="#DeepFactors" class="headerlink" title="DeepFactors"></a>DeepFactors</h2><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/deepfactors.png" class="" title="DeepFactors">

<p>DeepFactors에서는 위 방식을 실제 Factor graph optmisation에 포함시켜 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9odG5SdUdLWm1adw==">실시간 데모<i class="fa fa-external-link-alt"></i></span>를 만들었고, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2pjemFybm93c2tpL0RlZXBGYWN0b3Jz">오픈소스 프로젝트<i class="fa fa-external-link-alt"></i></span>로 공개하였다.<br>CodeSLAM에서는 하나의 방법론을 탐색하였다면, DeepFactors에서는 전체 SLAM 시스템에서 작동하는 수준으로 끌어올린 것이다.<br>DeepFactors에서는 camera motion부터 dense scene geometry까지 joint optmisation을 수행한다.</p>
<p>…여기까지 만들었지만, 정작 Davison 교수님은 이 방법에 대해 아직 의구심이 남아있으신 것 같다.<br>Depth map이나 3D scene에서 바뀔 수 있는 정보들을 고작 32개의 파라미터에 모두 담을 수 있을까?<br>한계가 있을 것이라고 보시는 것 같다.</p>
<hr>
<h1 id="SLAM-Hierarchical-map-Live-Maps-Dynamic-scene-graph-MoreFusion-Fusion-NodeSLAM"><a href="#SLAM-Hierarchical-map-Live-Maps-Dynamic-scene-graph-MoreFusion-Fusion-NodeSLAM" class="headerlink" title="SLAM++, Hierarchical map (Live Maps, Dynamic scene graph), MoreFusion, Fusion++, NodeSLAM"></a>SLAM++, Hierarchical map (Live Maps, Dynamic scene graph), MoreFusion, Fusion++, NodeSLAM</h1><p>SLAM에서 가장 효과적인 Map의 representation은 무엇일까?<br>Robotics, AR과 같은 분야에서는 objects를 다루는 경우가 많고, 각각의 objects에 정보를 저장해야한다.<br>Spatial AI에서 결국 우리가 가장 집중하게 되는 것은 ‘objects’이지 않을까 생각한다.<br>하지만 현재의 파이프라인에서는 reconstructed map에서부터 objects를 추출해야하는데, 굉장히 비효율적이다.<br>“SLAM에서 사용하는 map을 object단위로 만들 수 있을까?”라는 질문이 생긴다.</p>
<h2 id="SLAM"><a href="#SLAM" class="headerlink" title="SLAM++"></a>SLAM++</h2><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/slamplusplus.png" class="" title="SLAM++">

<p>2013년에 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS90bXJBaDFDcUNSbw==">SLAM++<i class="fa fa-external-link-alt"></i></span>이라는 연구를 발표했다.<br>SLAM++은 뎁스카메라를 사용하면서 프론트엔드 단계에서 object recognition 기능을 추가하였다.<br>생성된 map은 object instance끼리 연결된 graph형태로 포현된다.<br>각각의 object instance는 3D 공간상의 위치+방향 정보와 해당하는 object class의 3D 모델 정보를 가지고 있었다.<br>이 방법의 단점이라면, 3D 공간 안에 어떤 object가 있어야하는지 미리 알고있어야한다는 것이다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/map_hierarchy.png" class="" title="Hierarchical Map">

<p>현재 활발하게 연구되고 있는 분야는 Hierarchical map 구조이다.<br>이 구조는 SLAM++에서 영감을 받았다고 할 수 있다.<br>Facebook Reality Labs의 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9KVGE4em4wUk5WTQ==">Live Maps<i class="fa fa-external-link-alt"></i></span>나 MIT의 Luca Carlone 교수님 랩실에서 나온 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9KVGE4em4wUk5WTQ==">3D Dynamic Scene Graph<i class="fa fa-external-link-alt"></i></span><br>SLAM++은 object level만 다룬다고 하면, 이 방식들은 여러 level로 그래프를 구성한다는 것이다.<br>SLAM++에서 수행하는 작업이 Hierarchical map 속 하나의 level이 될 수 있다는 것이다.</p>
<h2 id="MoreFusion"><a href="#MoreFusion" class="headerlink" title="MoreFusion"></a>MoreFusion</h2><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/morefusion.png" class="" title="MoreFusion">

<p><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS82b0xVaHVaTDRrbw==">MoreFusion<i class="fa fa-external-link-alt"></i></span>은 CVPR 2020 학회에서 선보인 object-level SLAM과 6D pose estimation 기법이다.<br>MoreFusion은 뎁스카메라 이미지와 사전에 3D CAD 데이터가 있는 물체들을 기반으로 map을 만들려고 한다.<br>이 점이 SLAM++와 굉장히 유사하다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/morefusion2.png" class="" title="MoreFusion2">

<p>MoreFusion은 object가 겹쳐올려진 상태에서도 정확한 맵을 복원하여 로봇팔을 통해 물체들과 상호작용을 할 수 있다.<br>보통 Object가 겹쳐 올려진 상태에서는 서로가 서로를 가리기 때문에 6D pose를 정확하게 구할 수 없다.<br>MoreFusion은 volumetric reasoning를 (object끼리 서로 통과할 수 없는 constraint) 사용하여 정확하게 6D pose를 구할 수 있다.<br>하지만 SLAM++이 방 크기를 스캔할 수 있던데에 비해 MoreFusion은 훨씬 작은 테이블탑 크기에서만 사용할 수 있다. </p>
<h2 id="Fusion"><a href="#Fusion" class="headerlink" title="Fusion++"></a>Fusion++</h2><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/fusionplusplus.png" class="" title="Fusion++">

<p>SLAM++이나 MoreFusion처럼 object에 대한 사전정보가 없으면 어떻게 할까?<br><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8ybHVLTkMwM3g0aw==">Fusion++<i class="fa fa-external-link-alt"></i></span>은 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDMuMDY4NzA=">Mask-RCNN<i class="fa fa-external-link-alt"></i></span>이라는 딥러닝 object intance segmentation 기술을 이용해, 그 자리에서 object를 검출하고 volumetric reconstruction을 수행하여 object-level SLAM을 수행한다.</p>
<h2 id="NodeSLAM"><a href="#NodeSLAM" class="headerlink" title="NodeSLAM"></a>NodeSLAM</h2><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/nodeslam.png" class="" title="NodeSLAM">

<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDQ0ODU=">NodeSLAM<i class="fa fa-external-link-alt"></i></span>은 SLAM++와 Fusion++의 중간점이라고 볼 수 있다.<br>SLAM++은 검출하려는 object들의 모양을 알고 있어야하고, Fusion++은 아예 이러한 정보가 아예 없는 상황에서 만들어내는 과정이다.<br>NodeSLAM은 검출될 object들의 neural shape descriptor (i.e. differentiable shape feature)를 사전에 딥러닝 autoencoder로 학습해두고, 검출 단계에서 이 정보를 사용한다.<br>보통 shape descriptor들은 정확한 shape을 읽어내려는 것이 아니라 object의 class를 구분하는데에 쓰인다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/nodeslam_2.png" class="" title="NodeSLAM_2">

<p>여기서 neural shape descriptor가 학습되는 과정은, 아까 설명되었던 CodeSLAM에서 했던 것 처럼 autoencoder의 encoder를 통해 압축된 code가 추출된다.<br>그리고 DeepFactors에서 이 code를 조정함으로써 camera pose와 map정보를 jointly optimise할 수 있던 것 처럼, NodeSLAM에서도 object shape와 camera pose를 jointly optimise할 수 있다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/nodeslam_3.png" class="" title="NodeSLAM_3">

<p>Object shape와 camera pose이 joint optimisation을 거쳐 얼마나 정확해지냐면…<br>MoreFusion이 3D CAD 모델을 가지고 할 수 있었던 똑같은 작업인 Robot manipulation을 할 수 있을 정도로 정확해진다.</p>
<hr>
<h1 id="하드웨어-개선-Graphcore-IPU"><a href="#하드웨어-개선-Graphcore-IPU" class="headerlink" title="하드웨어 개선 - Graphcore IPU"></a>하드웨어 개선 - Graphcore IPU</h1><br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/processors.png" class="" title="Processors">

<p>SLAM에서 쓸 수 있는 하드웨어는 여러가지 종류가 있다.<br>지난 PAMELA 프로젝트를 통해서 여러 하드웨어에 대해 SLAM 성능 벤치마크를 진행했었다.<br>다양한 SLAM 알고리즘을 단 하나의 칩으로 정확도, 속도, 효율성 모두 잡을 수는 없었다.<br>SLAM을 위한 하드웨어가 있어야한다는 생각도 들었다.<br>하지만 Sparse mapping, Dense mapping, CNN, Semantic mapping 등등 기술 트렌드가 너무나도 빨리 발전하기 때문에, 단 하나의 플랫폼에 엮이면 안된다는 생각도 들었다.<br>SLAM의 하드웨어 연구는 아직 계속 진행되어야한다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/ipu.png" class="" title="Graphcore IPU">

<p>요즘 눈여겨보고 있는 프로세서는 Graph processor이다.<br>영국의 Graphcore라는 업체에서 IPU라는 이름으로 판매하고있기도 하다.</p>
<p>IPU는 그래프 연산에 최적화된 설계를 가지고 있으며, AI 계산을 빠르게 수행하기 위해 개발되었다.<br>CPU는 보통의 연산에 효과적이지만, AI 계산에서는 좋지 않다.<br>GPU는 기존에는 그래픽스 렌더링을 위해 제작되어있지만, 현재는 AI 계산을 할 수 있도록 많이 기능이 추가되었다.<br>하지만 GPU 역시 비효율적인 부분이 있다.<br>IPU는 이 중 가장 효율적이게 작동할 수 있을 것이다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/ipu_memory.png" class="" title="IPU Memory">

<p>IPU 역시 GPU와 비슷하게 병렬처리에 특화되어있다.<br>GPU는 모든 코어가 동일한 작업을 할 때 효과적인 연산을 할 수 있다.<br>그에 비해 IPU는 각각의 코어가 다른 작업을 하면서도 효과적으로 작업할 수 있다.</p>
<p>IPU는 GPU에 비해서 메모리 공간이 GPU에 비해 훨씬 크다.<br>이 덕분에 프로세서 코어 당 더 많은 메모리를 사용할 수 있다.<br>또, 각 코어끼리 메모리에 저장된 내용을 공유할 수도 있다.<br>이를 통해, IPU는 각각의 코어에서 계산을 한 후 주변 코어에 결과 값을 넘겨주는 연산에 특화된 것을 알 수 있다.<br>딥러닝에서 사용하는 뉴럴넷 계산이 이러한 구조 덕에 훨씬 빠르게 계산될 수 있다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/ipu_vis.png" class="" title="Visualization of IPU processes">

<p>위 그림은 AlexNet이라는 CNN 아키텍처를 가진 네트워크를 사용할 때 필요한 연산을 IPU에 탑재했을 때, 각각의 코어에서 담당하는 작업을 시각화 한 것이다.<br>IPU의 특성 상 가까이 있는 코어들끼리 비슷한 연산을 함께 할 수 있는 것을 볼 수 있다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/ipu_spatial_ai.png" class="" title="IPU for Spatial AI">

<p>오늘 토크의 주제였던 Spatial AI를 달성하기 위해 IPU를 사용한다면, 프로세서의 구조가 어떻게 나눠질지 예상해본 그림이다.<br>위의 AlexNet 이미지와 어느정도 유사함을 보인다.</p>
<p>IPU가 Spatial AI에 특화될 수 있는 또 하나의 이유는 map을 표현하는 그래프 구조와 큰 연관이 있다.<br>Map 위에 있는 위치들이 가까울 수록 실제 IPU에서도 가깝게 위치시킬 수 있다.<br>멀리 있는 위치들끼리는 코어끼리의 거리 상 멀리 떨어트려 놓을 수 있다.</p>
<br>

<img src="/20201226-CVPR-2020-SLAM-workshop-Davison/ipu_graph.png" class="" title="Gaussian belief propagation within graph in IPU">

<p>2019년 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMTQxMzk=">FutureMapping2<i class="fa fa-external-link-alt"></i></span>라는 논문에서는 State estimation에서 쓰이는 Gaussian belief propagation을 Graph processor에서 수행하는 것을 제안하였다.<br>Gaussian belief propagation은 옛날에 사용되었던 방식이고, 현재 CPU에서 돌린다면 Gaussian belief propagation보다 많은 solver 라이브러리에 탑재된 Bundle adjustment가 훨씬 빠르게 작동한다.<br>하지만 CVPR 2020 학회에서 발표한 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDMxMzQ=">Bundle adjustment on a Graph Processor<i class="fa fa-external-link-alt"></i></span> 논문에 따르면, Gaussian belief propagation 기반으로 bundle adjustment를 구현하여 IPU에 올렸을 때 CPU에서 돌린 bundle adjustment보다 20~40배는 더 빠르게 계산을 수행한 것을 확인할 수 있었다.</p>
<hr>
<h1 id="발표-질문들"><a href="#발표-질문들" class="headerlink" title="발표 질문들"></a>발표 질문들</h1><h2 id="Spatial-AI의-설계-그래프에는-여러-모듈이-있는데-그중-특볋히-발전시키기-어려운-모듈이-있을까요"><a href="#Spatial-AI의-설계-그래프에는-여러-모듈이-있는데-그중-특볋히-발전시키기-어려운-모듈이-있을까요" class="headerlink" title="Spatial AI의 설계 그래프에는 여러 모듈이 있는데, 그중 특볋히 발전시키기 어려운 모듈이 있을까요?"></a>Spatial AI의 설계 그래프에는 여러 모듈이 있는데, 그중 특볋히 발전시키기 어려운 모듈이 있을까요?</h2><p>딱 하나만 꼽기는 어렵고…<br>Robustness 문제가 생기는 모듈들은 전부 어려운 것 같다.</p>
<p>예를 들어서, Semantic label을 만들어주는 모듈의 경우에는 하나의 데이터셋에서는 잘되지만 처음 보는 데이터셋에서는 굉장히 안될 수 있다.<br>이런 문제는 딥러닝 관점에서 보았을 때 unsupervised learning으로 풀어야하지 않을까 생각한다.<br>또는, 로봇이 처음 보는 환경에 있을 때, 그 자리에서 학습을 할 수 있어야한다고 생각한다.</p>
<h2 id="IPU-칩이-Long-term-SLAM을-할-수-있을까요"><a href="#IPU-칩이-Long-term-SLAM을-할-수-있을까요" class="headerlink" title="IPU 칩이 Long-term SLAM을 할 수 있을까요?"></a>IPU 칩이 Long-term SLAM을 할 수 있을까요?</h2><p>IPU 칩에서는 factor graph를 통해서 in-place computation을 하면 좋다.<br>하지만 물론 당연히 map이 커지고, 더 많은 level의 hierarchical map을 만든다면, 칩이 감당할 수 없을 수 있다.<br>이런 경우에는 incremental abstraction을 써야한다.<br>예를 들어, 현재 보고 있는 이미지에서 포인트가 100개가 뽑혔다고 해보자.<br>그런데, 약 5초정도 후 보았을 때, 그 포인트가 알고보니까 어떤 하나의 plane에서 나왔다고 해보자.<br>이런 경우에는 포인트 100개가 아니라 plane을 저장하면 훨씬 메모리를 적게 사용할 수 있을 것이다.<br>이런식으로 세밀한 데이터 node들이 모여서 super-node를 만들어가고… 이 과정이 반복될 수 있지 않을까 생각한다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Graphcore</tag>
        <tag>IPU</tag>
        <tag>Spatial AI</tag>
        <tag>MonoSLAM</tag>
        <tag>DTAM</tag>
        <tag>KinectFusion</tag>
        <tag>ElasticFusion</tag>
        <tag>SemanticFusion</tag>
        <tag>CodeSLAM</tag>
        <tag>SceneCode</tag>
        <tag>DeepFactors</tag>
        <tag>SLAM++</tag>
        <tag>MoreFusion</tag>
        <tag>Fusion++</tag>
        <tag>NodeSLAM</tag>
        <tag>FutureMapping</tag>
        <tag>Graph processor</tag>
        <tag>Andrew Davison</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2020 - Deep Visual SLAM Frontends - SuperPoint, SuperGlue and SuperMaps (Tomasz Malisiewicz 발표)</title>
    <url>/20201227-cvpr2020-slam-malisiewicz/</url>
    <content><![CDATA[<h1 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h1><p>Tomasz Malisiewicz는 토마스 말리세비치라고 읽는다.</p>
<p>이번 토크는 3가지 기술을 커버한다.</p>
<ol>
<li>SuperPoint<ul>
<li>Magic Leap에서 딥러닝을 SLAM에 적용하기 위한 시도라고 소개한다.</li>
</ul>
</li>
<li>SuperGlue<ul>
<li>Graph neural network와 Attention을 이용하여 feature matching의 성능을 높인 기술이다.</li>
</ul>
</li>
<li>SuperMaps<ul>
<li>아직 완성되지는 않았지만, SLAM에서 pairwise matching을 제거하고 완전한 end-to-end 방식으로 넘어가기 위한 로드맵을 소개한다.</li>
</ul>
</li>
</ol>
<hr>
<h1 id="SuperPoint-SIFT를-딥러닝으로-대체하기"><a href="#SuperPoint-SIFT를-딥러닝으로-대체하기" class="headerlink" title="SuperPoint - SIFT를 딥러닝으로 대체하기"></a>SuperPoint - SIFT를 딥러닝으로 대체하기</h1><p>(SIFT는 SLAM에서 잘 안쓰지 않나…? ORB가 더 말이 될 것 같지만, Tomasz 본인이 SIFT를 대체하는 기술이라고 적었다.)</p>
<h2 id="SLAM-Frontend란"><a href="#SLAM-Frontend란" class="headerlink" title="SLAM Frontend란?"></a>SLAM Frontend란?</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/slam.png" class="" title="SLAM in 2 parts">

<p>Visual SLAM은 두가지 단계로 나눠진다.<br>Frontend는 keypoint extraction과 keypoint matching 작업을 수행한다.<br>Backend는 bundle adjustment와 같은 nonlinear least squares optimisation 작업을 수행하며 카메라 위치와 주변 환경 구조를 최적화한다.</p>
<p>Frontend는 이미지만을 다루기 때문에, 최근 classification, detection, segmentation 등 2D 이미지를 다루는 딥러닝 CNN 기술을 frontend에도 사용해볼 수 있다.</p>
<h2 id="SuperPoint-특징"><a href="#SuperPoint-특징" class="headerlink" title="SuperPoint 특징"></a>SuperPoint 특징</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpoint_simple.png" class="" title="SuperPoint in a nutshell">

<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MTIuMDc2Mjk=">SuperPoint<i class="fa fa-external-link-alt"></i></span>는 CVPR 2018 학회에서 발표된 연구이다.<br>SuperPoint를 간단하게 설명하면, 이미지 속 2D keypoint의 위치와 해당 feature의 descriptor 정보를 내뱉는 CNN 네트워크이다.</p>
<p>SuperPoint는 Keypoint의 위치와 descriptor 정보를 jointly 하게 계산할 수 있다.<br>이는 기존의 keypoint descriptor 알고리즘들이 keypoint 주변의 patch 정보를 사용했던 점과 확연히 다른 점이라고 볼 수 있다.</p>
<p>논문에서는 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDkuMTU1Ni5wZGY=">VGG<i class="fa fa-external-link-alt"></i></span> 네트워크를 backbone으로 사용하였다.<br>하지만 다른 backbone을 쓰지 못할 이유는 없다.</p>
<p>SuperPoint는 GPU에서 실시간으로 돌 수 있게 설계되었다고 한다.<br>이는 Keypoint location과 descriptor 계산이 ~90%의 weight 정보를 공유할 수 있게 만듬으로써 가능했다고 한다.<br>또, backbone 네트워크가 크지 않아서 가능했다고 한다.</p>
<p>이렇게 나온 SuperPoint를 가지고 Backend에 그대로 넣어 SLAM을 할 수 있다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/decoder.png" class="" title="Keypoint decoder">

<p>SuperPoint는 어떻게 Keypoint를 추출하는걸까?<br>SuperPoint의 코어 컴포넌트 중 하나인 keypoint decoder는, 어떠한 픽셀이 keypoint인지 아닌지 classification 문제를 푼다고 한다.<br>SuperPoint는 우선 8x8 크기의 패치 안에는 무조건 하나의 keypoint만 있을 수 있다고 가정한다.<br>8x8 크기의 패치 안에는 keypoint가 될 수 있는 64개의 후보 픽셀들이 있다.<br>여기에 1개의 ‘dustbin’이라는 후보를 추가한다.<br>SuperPoint의 목적은, 이 65개의 후보들에 대한 distribution을 생성하는 것이다.<br>Distribution을 쓰면 이 모든 후보들에 대한 heatmap을 얻을 수 있다.</p>
<h2 id="SuperPoint-학습-과정"><a href="#SuperPoint-학습-과정" class="headerlink" title="SuperPoint 학습 과정"></a>SuperPoint 학습 과정</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/training.png" class="" title="Training">

<p>SuperPoint는 다수의 이미지 페어들에서 좋은 keypoint matching이 되는 것을 목적으로 만들었다.<br>즉, matching이 잘 되는 keypoint만을 추출하는 것이 중요하다.<br>우선 Keypoint matching을 하기 위해서는 2D location을 추출해주는 ‘keypoint detector’와 keypoint의 매칭을 위한 정보를 추출해주는 ‘keypoint descriptor’가 필요하다.<br>여기서, 같은 keypoint를 다른 각도에서 바라보아도 비슷한 keypoint descriptor가 뽑히며, 다른 keypoint의 descriptor와는 차이가 나타나야한다.<br>이러한 점을 학습하기 위해, SuperPoint의 학습과정에서는 ground truth 데이터로 keypoint matching이 잘 된 이미지 페어 (i.e. 2장)이 필요했다.<br>또, 2장의 이미지로부터 효과적으로 학습하기 위해 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS82amZ3OE11S3dwSQ==">Siamese training<i class="fa fa-external-link-alt"></i></span> 기법으로 학습되었다.</p>
<p>2장의 이미지는 다음과 같이 생성되었다.<br>원본 이미지를 <span class="exturl" data-url="aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2hvbW9ncmFwaHktZXhhbXBsZXMtdXNpbmctb3BlbmN2LXB5dGhvbi1jLw==">homography warp<i class="fa fa-external-link-alt"></i></span>를 통해 또 다른 이미지를 생성한다.<br>Homography warp를 하면, 원본 이미지에서의 픽셀 위치를 새롭게 생성된 warp 이미지에서도 찾을 수 있다.<br>즉, 원본 이미지에서의 keypoint 위치를 다른 이미지에서도 똑같이 찾을 수 있다는 것이다.</p>
<p>Keypoint detector는 keypoint label을 supervised learning으로 학습하였다.</p>
<p>그리고, keypoint detector로 찾은 keypoint location에서 descriptor를 추출하여 비교한 후 descriptor 학습을 수행하였다.<br>Keypoint descriptor 학습은 최근 많이 사용되는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS95SWR0eDNwUWtkZw==">contrastive loss<i class="fa fa-external-link-alt"></i></span>를 이용한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9NMEVqckZRSDQ5bw==">metric learning<i class="fa fa-external-link-alt"></i></span> 기법을 사용하였다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/natural_image.png" class="" title="Natural images">

<p>아까 위에서 Keypoint detector는 keypoint label을 기반으로 학습을 진행했다고 하였다.<br>그러면 이 label은 어디서 오는 것일까?<br>가장 생각하기 쉬운 방법은 SIFT, SURF, ORB 등의 방식에서 뽑아서 label 데이터를 만드는 것이다.<br>하지만 이 방식으로 트레이닝을 하면, SuperPoint의 최대성능이 SIFT, SURF 정도밖에 되지 않기 때문에, 사용할 수 없다.<br>그러면 다른 방법으로 keypoint label을 만들어야한다.<br>일단, 사람이 직접 label을 추가할 수는 없다.</p>
<p>이러한 문제를 해결하기 위해, <span class="exturl" data-url="aHR0cHM6Ly9ob3lhMDEyLmdpdGh1Yi5pby9ibG9nL1NlbGYtU3VwZXJ2aXNlZC1MZWFybmluZy1PdmVydmlldy8=">Self-supervised learning<i class="fa fa-external-link-alt"></i></span> 방식을 도입하였다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/self_supervised_training.png" class="" title="Render">

<p>존경하는 hoya012 블로그에서는 Self-supervised learning을 다음과 같이 설명하였다.<br>“Network로 하여금 만든 pretext task를 학습하게 하여 데이터 자체에 대한 이해를 높일 수 있게 하고, 이렇게 network를 pretraining 시킨 뒤 downstream task로 transfer learning을 하는 접근 방법”</p>
<p>SuperPoint에서는 pretext task로써 네트워크가 간단한 환경에서 keypoint를 찾는 기능을 학습하게 하였다.<br>이 과정은 2017년 발표된 SuperPoint 이전의 논문인 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDcuMDc0MTA=">Towards Geometric Deep SLAM<i class="fa fa-external-link-alt"></i></span> 논문에서 소개하는 MagicPoint에 해당한다.<br>가상 렌더링을 사용하여 이미지 데이터와 렌더링된 모델의 vertex 위치 데이터를 기반으로 네트워크를 학습하였다.<br>Robustness를 높이기 위해  가상 이미지 데이터는 다양한 조명과 노이즈도 첨가되어있다.<br>이를 통해 네트워크는 keypoint의 특성을 학습하게 되었다.</p>
<p>가상 이미지로 학습한 결과를 Real-world 데이터셋으로 옮기기 위해 (transfer), <span class="exturl" data-url="aHR0cHM6Ly9jb2NvZGF0YXNldC5vcmcvI2hvbWU=">MS-COCO 데이터셋<i class="fa fa-external-link-alt"></i></span>에 다시 학습을 하였다.<br>MS-COCO 데이터셋은 keypoint에 대한 label이 없다.<br>이 데이터셋에서 keypoint label을 만들기 위해 homographic adaptation이라는 기법을 사용했다.<br>Homographic adaptation 기법에는 추후에 설명한다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/magicpoint.png" class="" title="MagicPoint Performance">

<p>지금까지 소개한 부분인 MagicPoint는 기존의 FAST, Harris, Shi-Tomasi keypoint detector에 비해 월등히 좋은 성능을 가진다.<br>특히, 노이즈가 있는 상황에서 더 잘 작동하는 것을 볼 수 있다.<br>이는 MagicPoint가 noise가 있는 상황에서도 잘 작동할 수 있게 학습되었기 때문이다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/homographic_adaptation.png" class="" title="Homographic adaptation">

<p>아까 부족했던 Homographic adaptation에 대해 설명하겠다.<br>Homographic adaptation의 목적은 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLm9wZW5jdi5vcmcvbWFzdGVyL2Q5L2RhYi90dXRvcmlhbF9ob21vZ3JhcGh5Lmh0bWw=">homography 기법<i class="fa fa-external-link-alt"></i></span>을 이용해 planar camera motion을 만들어내는 것이다.<br>두 이미지 사이의 camera motion을 알 수 있다면, 하나의 이미지 위의 keypoint 위치들을 다른 이미지 위에 그대로 옮길 수 있다.<br>즉, self-labelling 기술이 된다는 것이다.<br>Homograpic adaptation을 통한 self-labelling을 통해 우리는 두가지 효과를 기대할 수 있다.</p>
<ol>
<li>잘못된 keypoint detection을 피할 수 있다</li>
<li>Keypoint detection의 repeatability (i.e. 다른 각도에서 보아도 동일한 위치에서 keypoint가 검출되는 능력)을 향상시킬 수 있다.</li>
</ol>
<p>Label이 없는 원본 이미지에 homography를 적용하여 여러 각도에서 바라보는 이미지들을 생성한다.<br>그 후, 각각의 이미지에 MagicPoint를 적용하여 keypoint를 추출한다.<br>이 keypoint들을 homography 변환으로 원본 이미지에 다시 옮겨주면, keypoint들의 ‘superset’이 생기는 것이다.<br>(SuperPoint의 ‘super’가 여기서 왔다고 한다).</p>
<h2 id="SuperPoint-성능-비교"><a href="#SuperPoint-성능-비교" class="headerlink" title="SuperPoint 성능 비교"></a>SuperPoint 성능 비교</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpoint_eval.png" class="" title="Evaluation of SuperPoint">

<p>SuperPoint, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDMuMDkxMTQ=">LIFT<i class="fa fa-external-link-alt"></i></span>, SIFT, ORB를 비교한 결과이다.<br>LIFT는 또 다른 딥러닝 기반 keypoint detector + descriptor 이다.<br>SuperPoint가 매칭된 keypoint의 수도 가장 높고, keypoint들의 위치도 이미지에 고루고루 퍼져있다.<br>그에 비해, Visual-SLAM에서 많이 사용되는 ORB feature는 좋지 않은 성능을 보여준다.<br>특히, 대부분의 keypoint가 한곳에 밀집되어있고, 그 keypoint끼리 생김새가 비슷할 때 매칭의 성능이 많이 떨어진다.<br>이런 경우에는 camera motion이 정확하게 계산될 수 없다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpoint_eval2.png" class="" title="Evaluation of SuperPoint 2">

<p>두번째 비교이다.<br>위에서 설명한 ORB의 단점이 잘 보이는 또 다른 예시이다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpoint_eval3.png" class="" title="Evaluation of SuperPoint 3">

<p>세번째 비교이다.<br>LIFT에 비해 회전에 좀 더 강인한 모습을 보인다.<br>하지만 SuperPoint도 완전하게 rotation-invariant하지 않다는 점을 인지해야한다.<br>Backbone으로 사용한 VGG 네트워크는 rotation-invariant하지 않기 때문이다.<br>그렇기 때문에 SuperPoint도 최대 15~30도 정도의 회전만을 고려했다고 한다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/3d.png" class="" title="3D Performance">

<p>SuperPoint의 학습과정는 전부 2D 이미지에서 학습한 것을 볼 수 있다.<br>그러면 3D에서 잘 작동할지 궁금해진다.<br>결과만 이야기하면, 잘 작동한다.</p>
<p>SuperPoint를 직접 사용해보고 싶으면 이 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hZ2ljbGVhcC9TdXBlclBvaW50UHJldHJhaW5lZE5ldHdvcms=">Github 링크<i class="fa fa-external-link-alt"></i></span>를 참조하면 좋다.<br>PyTorch 기반으로 작동한다고 한다.</p>
<hr>
<h1 id="DeepChArUco-SuperPoint의-성능을-보여주는-다른-예시"><a href="#DeepChArUco-SuperPoint의-성능을-보여주는-다른-예시" class="headerlink" title="DeepChArUco - SuperPoint의 성능을 보여주는 다른 예시"></a>DeepChArUco - SuperPoint의 성능을 보여주는 다른 예시</h1><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/deepcharuco.png" class="" title="DeepChArUco">

<p>SuperPoint의 robustness를 보여주는 프로젝트를 하나 소개하려고 한다.</p>
<p>ChArUco라는 패턴이 있다.<br>이 패턴은 체스보드 패턴에 <span class="exturl" data-url="aHR0cHM6Ly93d3cudWNvLmVzL2ludmVzdGlnYS9ncnVwb3MvYXZhL25vZGUvMjY=">ArUco<i class="fa fa-external-link-alt"></i></span> 마커를 임베딩한 패턴이다.<br>ArUco 마커들은 각각의 ID를 가지고 있으며, 이 ID를 기반으로 fiducial marker detection을 할 수 있다.<br>또, 체스보드와 ArUco 마커 코너 점들을 이용하여 2D-3D correspondence matching을 통해 pose estimation도 수행할 수 있다.</p>
<p>DeepChArUco 연구에서는 기존의 Corner detector와 ArUco 마커 ID detection 알고리즘을 새롭게 변형하였다.<br>Corner detector는 SuperPoint로 대체되었다.<br>그리고 ArUco 마커 ID detection도 딥러닝 기반 classifier로 변형되었다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/deepcharuco_dark.png" class="" title="DeepChArUco in dark environment">

<p>DeepChArUco 방식은 사람도 구분할 수 없는 어두운 상황에서도 정확하게 마커를 찾을 수 있다.<br><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9TbW9yZzlkZmZjMA==">데모 영상<i class="fa fa-external-link-alt"></i></span>을 보면, 어두워도, 그림자가 져도 찾을 수 있다.</p>
<p>딥러닝 기반 Local feature detection의 성능이 뛰어나다는 것을 보여주는 좋은 예시라고 볼 수 있다.</p>
<br>

<hr>
<h1 id="SuperPointVO-SuperPoint로-Visual-Odometry를-할-수-있을까"><a href="#SuperPointVO-SuperPoint로-Visual-Odometry를-할-수-있을까" class="headerlink" title="SuperPointVO - SuperPoint로 Visual Odometry를 할 수 있을까?"></a>SuperPointVO - SuperPoint로 Visual Odometry를 할 수 있을까?</h1><p>이 연구는 2018년 발표된 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MTIuMDMyNDU=">Self-improving visual odometry<i class="fa fa-external-link-alt"></i></span> 연구를 소개한다.</p>
<p>위에서 보여준 것 처럼, SuperPoint로 얻은 keypoint는 3D 환경에서도 트랙킹이 잘 되는 것을 볼 수 있다.<br>그리고 실제로 optmisation 프로그래밍을 추가하여 visual odometry 시스템을 만들었을 때도 성공적으로 3D reconstruction을 수행할 수 있었다.<br>SuperPoint가 visual odometry에서도 쓰일 수 있다는 점이 입증되었으나, visual odometry를 위해 최적화되어있진 않다.</p>
<p>이번 연구는 visual odometry를 수행하여 나오는 데이터를 기반으로 네트워크를 학습한다.<br>이로써 오랜 시간이 지나도 keypoint correspondence를 만들 수 있게 네트워크를 학습할 수 있다.<br>또, 어떤 keypoint가 오랜 시간이 지나도 stable하게 매칭이 되는지 학습할 수 있다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpointvo.png" class="" title="SuperPointVO">

<p>학습 과정은 다음과 같다.<br>우선 Monocular 이미지 시퀀스들로부터 SuperPoint를 추출하고, correspondence를 만든다.<br>그 후, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9pN2llclZrWFlhOA==">SfM (Structure-from-Motion)<i class="fa fa-external-link-alt"></i></span>을 이용하여 3D reconstruction을 수행한다.<br>(어차피 학습은 비실시간에서 수행하기 때문에 SfM이라는 단어를 사용하였다. 이론상 VO backend나 SfM이나 크게 다르지 않다)</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/reproj_error.png" class="" title="Stable / Unstable points">

<p>이렇게 생긴 3D map을 각각의 이미지에 재투영하고 (reprojection), <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvMjkwOTUzNDkvaG93LWlzLXRoZS1yZXByb2plY3Rpb24tZXJyb3ItY2FsY3VsYXRlZC1pbi1tYXRsYWJzLXRyaWFuZ3VsYXRlLWZ1bmN0aW9uLXNhZGx5">reprojection error<i class="fa fa-external-link-alt"></i></span>를 계산한다.<br>이 때 3d map point의 평균 reprojection error가 1 픽셀보다 낮다면, 어떤 방향에서 보던지 3D map을 정확하게 만들어내는 keypoint 가 되겠다.<br>반대로, 평균 reprojection error가 5 픽셀보다 높으면, 3D map을 정확하게 만들지 못하는 keypoint가 되겠다.<br>중간에 어중간한 keypoint들은 무시한다.</p>
<p>이 정보들을 기반으로 CNN 네트워크를 학습한다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpointvo_training.png" class="" title="Training SuperPointVO">

<p>SuperPointVO의 학습과정은 SuperPoint를 학습하는 과정과 유사하다.<br>Siamese training 구조로 keypoint loss와 descriptor loss를 구한다.</p>
<br>

<h2 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h2><img src="/20201227-cvpr2020-slam-malisiewicz/superpointvo_result1.png" class="" title="SuperPointVO result 1">

<p>1초 정도의 프레임 간격에는, SuperPointVO의 성능이 크게 부각되지 않는다.<br>이는, 바라보고 있는 방향이 비슷할 때는 왠만한 keypoint detector는 좋은 성능을 보인다는 것이다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superpointvo_result2.png" class="" title="SuperPointVO result 2">

<p>하지만 3초정도 프레임 간격에는 카메라 위치와 방향에 큰 차이가 나게 된다 (i.e. wide baseline이 생긴다).<br>이런 경우에 SuperPointVO의 성능이 부각된다.</p>
<br>

<hr>
<h1 id="SuperGlue-Graph-neural-network-Self-attention-기반-keypoint-매칭"><a href="#SuperGlue-Graph-neural-network-Self-attention-기반-keypoint-매칭" class="headerlink" title="SuperGlue - Graph neural network + Self-attention 기반 keypoint 매칭"></a>SuperGlue - Graph neural network + Self-attention 기반 keypoint 매칭</h1><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue.png" class="" title="SuperGlue">

<p>SuperGlue는 Graph Neural Network에 Optimal transport procedure를 추가한 알고리즘이다.</p>
<p>SuperGlue의 목적은 wide baseline에서 SuperPoint를 매칭하는 것이며, motion model을 사용하지 않고도 기존의 motion-guided matching 기법보다 더 좋은 성능을 내는 것이다.<br>이는, SLAM을 진행하면서 종종 잘못된 motion model을 사용했을 때 feature matching의 성능이 급격하게 감소하는 것을 방지하기 위함이다.</p>
<p>SuperGlue는 GPU에서 실시간으로 돌아가면서 State-of-the-Art 성능을 보여준다.<br>SuperGlue는 SuperPoint와 쓸 수도 있고, SIFT와 쓸 수도 있다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_architecture.png" class="" title="SuperGlue Architecture">

<p>(원래 영상에서는 깊게 설명하지 않지만, 다른 SuperGlue 영상에서 내용을 가져와서 첨부하였습니다)</p>
<p>SuperGlue는 2가지 단계로 나눠져있다.<br>첫번째는 Attention을 이용한 graph neural network이다.<br>이 부분은 Keypoint들의 location 정보와 descriptor 정보를 attention 메커니즘을 사용해서 contextual cue에 대한 정보를 학습한다.<br>두번째는 기존의 (i.e. 딥러닝을 쓰지 않는) 최적화 프로그래밍을 수행하며, 사용한 알고리즘은 sinkhorn 알고리즘이다.<br>Sinkhorn 알고리즘을 통해 differentaible solver를 만듬으로써, domain knowledge에 대한 constraint를 만든다.</p>
<h2 id="Keypoint-encoder"><a href="#Keypoint-encoder" class="headerlink" title="Keypoint encoder"></a>Keypoint encoder</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_front.png" class="" title="SuperGlue Architecture - creating representation">

<p>SuperGlue의 가장 앞단에서는, 매칭을 수행할 이미지 2장으로부터 추출된 keypoint들을 모두 attention 메커니즘에 input 데이터로 사용한다.<br>이런 방법은 기존의 매칭 방법과 큰 차이를 보인다.<br>기존의 매칭 방법은, 각각의 이미지에서 따로 descriptor 등의 계산을 거친 후 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hcml1c211amEvZmxhbm4=">nearest neighbour 방식<i class="fa fa-external-link-alt"></i></span> 등으로 매칭을 수행하였다.<br>하지만 비슷한 local feature descriptor가 많이 있을 때, 매칭 candidate가 너무 많기 때문에 이런 경우에는 매칭에 실패하기도 한다.</p>
<p>SuperPoint를 사용했을 경우를 가정해보고 설명한다.<br>빨간색이 1번 이미지에서 나온 정보, 파란색이 2번 이미지에서 나온 정보이다.<br>두 이미지에 각각 SuperPoint를 검출한 후 keypoint location 정보를 keypoint encoder 네트워크로 보내 섞는다.<br>이 encoded keypoint location 정보는 local visual appearance 정보와 (i.e. visual descriptor) multi-layer perceptron 레이어을 거치며 합쳐진다.<br>이 결과 keypoint representation이 만들어진다.</p>
<h2 id="Attentional-aggregation-Attentional-Graph-Neural-Network-update"><a href="#Attentional-aggregation-Attentional-Graph-Neural-Network-update" class="headerlink" title="Attentional aggregation (Attentional Graph Neural Network update)"></a>Attentional aggregation (Attentional Graph Neural Network update)</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_graph.png" class="" title="SuperGlue Architecture - Attentional aggregation">

<p>Keypoint representation은 Attentional graph neural network 구조 속 다른 keypoint 정보들로부터 iterative하게 업데이트 된다.</p>
<p>업데이트가 되는 방식이 두개가 있는데, self-edge 업데이트와 cross-edge 업데이트가 있다.<br>하나의 keypoint 정보가 동일한 이미지의 다른 keypoint 정보로부터 업데이트가 되는 것이 self-edge update이다.<br>그리고 keypoint 정보가 다른 이미지의 다른 keypoint 정보로부터 업데이트가 되는 것이 cross-edge udpate이다.<br>이 keypoint 정보들을 node로 표현하고 update 관계를 edge로 표현하면, self-edge와 cross-edge로 이뤄진 하나의 graph 형태가 나타난다.<br>이 업데이트 과정은 graph neural network의 한 종류인 Message passing neural network를 사용하며, 각각의 message는 self-attention 또는 cross-attention을 이용하여 계산된다.<br>이 Self-attention은 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDYuMDM3NjI=">Transformer<i class="fa fa-external-link-alt"></i></span>에서 영감을 받았다.</p>
<p>Self/cross-attention을 통한 message 계산은 database retrieval과 비슷하다고 한다.<br>Query 데이터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.674ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 740 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(446, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>와 비슷한 Key 데이터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.951ex" height="2.236ex" role="img" focusable="false" viewBox="0 -694 862.3 988.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(521, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>를 찾고, 해당하는 value 데이터 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>를 찾는다고 한다.<br>이 때, 평균 value 값에 query~key 데이터의 similarity 값을 weight로 곱해준 값을 message 값으로 사용한다고 한다.<br>이 때문에, 이 업데이트 과정을 attentional aggregation이라고도 한다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/attentional_aggregation.png" class="" title="Query to Key and Values">

<p>위 과정을 조금 더 쉽게 설명하겠다.<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> keypoint 정보로 Query를 했을 때, Key 값으로 여러 keypoint candidate가 나온다.<br>이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 값은 keypoint location과 visual appearance 정보를 encode 하고 있기 때문에, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.951ex" height="2.236ex" role="img" focusable="false" viewBox="0 -694 862.3 988.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(521, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>들로 spatial neighbour, self-similarity, salient keypoint를 매칭할 수 있다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/self_cross.png" class="" title="Self/Cross attention">

<p>Self-attention으로는 동일한 이미지에서 다른 keypoint들로부터 self-similarity 정보를 효과적으로 얻어낼 수 있다.<br>Cross-attention은 이미지 간 candidate match로 찾을 수 있다.</p>
<h2 id="Optimal-matching"><a href="#Optimal-matching" class="headerlink" title="Optimal matching"></a>Optimal matching</h2><br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_optimal_matching.png" class="" title="Optimal matching layer">

<p>Attentional aggregation이 끝나면 최종 matching descriptor 정보가 각각의 이미지로부터 나온다.<br>Matching descriptor 정보를 dot product하면, 두 keypoint descriptor 정보간의 밀접도를 (i.e. affinity) 표현하는 score matrix가 생긴다.<br>종종 occlusion의 이유로 매칭이 될 수 없는 keypoint가 있는 것을 대비하여, score matrix에 dustbin score를 추가시켰다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9taWNoaWVsc3RvY2suZ2l0aHViLmlvL09wdGltYWxUcmFuc3BvcnQv">Sinkhorn 알고리즘<i class="fa fa-external-link-alt"></i></span>을 사용하여 partial assignment 최적화 문제를 GPU에서 잘 돌 수 있도록 바꿔주었다.</p>
<p>SuperGlue의 네트워크 최앞단부터 partial assignment matrix까지 전부 ground truth correspondence 데이터를 통해 end-to-end 트레이닝이 가능하다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_eval.png" class="" title="Evaluation of SuperGlue">

<p>기존의 Nearest neighbour 매칭 방식에 비교했을 때, SuperGlue가 훨씬 더 많은 매칭 결과를 보여준다.<br>특히나, 바닥에서 반복되는 패턴이 있음에도 굉장히 매칭이 잘된다.</p>
<br>

<img src="/20201227-cvpr2020-slam-malisiewicz/superglue_eval2.png" class="" title="Evaluation of SuperGlue 2">

<p>MagicLeap 팀이 지금까지 테스트 해본 결과 SuperPoint + SuperGlue 조합이 제일 잘 되는 것을 확인하였다.</p>
<p>SuperGlue는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hZ2ljbGVhcC9TdXBlckdsdWVQcmV0cmFpbmVkTmV0d29yaw==">오픈소스<i class="fa fa-external-link-alt"></i></span>로 공개되어있다.<br>GPU가 달린 노트북/데스크탑으로 VGA 이미지로부터 약 512개의 keypoint를 추출했을 때, 15FPS 정도로 작동한다고 한다.</p>
<p>Tomasz의 조언에 따르면, 데모가 굉장히 잘된다고 한다.<br>논문을 읽고나서 이 기술을 쓸지 말지 결정하는데는 몇시간이 걸릴 수도 있지만, 그냥 웹캠 하나 꽂고 돌려보면 바로 써봐야겠다는 결정을 할거라고 한다.<br>본인도 이 데모의 failure case를 만들어보기 위해 카메라 포커스도 바꿔보고, occlusion도 만들어봤지만 그냥 너무 잘된다고 한다.</p>
<p>실제로 CVPR 2020 학회에서 진행된 3개의 대회 (Image matching challenge, Local features for visual localization, Visual localization for handheld devices) 대회에서 모두 우승했다고 한다.</p>
<br>

<hr>
<h1 id="SuperMaps-SuperPoint-SuperGlue의-미래…"><a href="#SuperMaps-SuperPoint-SuperGlue의-미래…" class="headerlink" title="SuperMaps - SuperPoint + SuperGlue의 미래…?"></a>SuperMaps - SuperPoint + SuperGlue의 미래…?</h1><p>SuperPoint + SuperGlue의 다음은 뭘까?</p>
<p>SuperPoint + SuperGlue 콤보는 이미지 pair를 사용할 수 있다.<br>SuperMaps는 여러장의 이미지를 쓸 수 있지 않을까?<br>여러장의 이미지는 보통 3D Map을 의미하기도 한다.<br>3D Map 끼리 섞는것도 가능하지 않을까?</p>
<p>SuperPoint + SuperGlue 콤보는 keypoint matching만 해주고 정작 제일 중요한 camera pose estimation은 하지 않는다.<br>SuerperMaps는 camera pose estimation도 할 수 있지 않을까?</p>
<p>SuperPoint + SuperGlue 콤보는 loop closure 기능을 내포하지 않는다.<br>그리고 SuperGlue의 계산량은 꽤 높은 편이다.<br>SuperGlue를 사용하기 어렵기 때문에, loop closure를 구현하기도 쉽지는 않다.<br>SuperMaps는 loop closure를 위한 keyframe embedding 기능이 있어야한다.<br>SuperPoint들을 (i.e. local keypoints) 모아서 하나의 global descriptor를 만드는 것도 하나의 방법이 될 것 같다.</p>
<p>SuperPoint와 SuperGlue는 각각 트레이닝되야한다.<br>SuperMaps는 가능하면 한번에 end-to-end 트레이닝이 되면 좋을 것 같다.<br>필수는 아니다.</p>
<p>SuperPoint는 CNN 기반이고, SuperGlue는 GNN 기반이다.<br>그렇기 때문에 두 네트워크의 receptive field 개념이 굉장히 다르다.<br>SuperMaps는 공통된 receptive field가 있으면 좋을 것 같다.</p>
<hr>
<h1 id="딥러닝-SLAM에서-풀어야-할-숙제"><a href="#딥러닝-SLAM에서-풀어야-할-숙제" class="headerlink" title="딥러닝 SLAM에서 풀어야 할 숙제"></a>딥러닝 SLAM에서 풀어야 할 숙제</h1><ul>
<li>Multi-user SLAM<ul>
<li>다수의 agent를 통해서 representation/map을 만드는 방법이 있어야한다.</li>
</ul>
</li>
<li>Object recognition이 SLAM frontend로 들어가야한다<ul>
<li>Semantics 정보를 다룰 수 있어야한다</li>
</ul>
</li>
<li>Life-long SLAM<ul>
<li>시간이 지날수록 map도 함께 바뀌어가며 진화해야한다</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Deep SLAM</tag>
        <tag>AR</tag>
        <tag>Tomasz Malisiewicz</tag>
        <tag>Magic Leap</tag>
        <tag>SuperPoint</tag>
        <tag>DeepChArUco</tag>
        <tag>SuperPointVO</tag>
        <tag>SuperGlue</tag>
        <tag>SuperMaps</tag>
      </tags>
  </entry>
  <entry>
    <title>2020년 10~12월 SLAM 논문 소식</title>
    <url>/20201229-dec-2020-slam-news/</url>
    <content><![CDATA[<h1 id="10월"><a href="#10월" class="headerlink" title="10월"></a>10월</h1><br>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMTAuMDc4MjA=">DynaSLAM II : Tightly-Coupled Multi-Object Tracking and SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>DynaSLAM 1편의 저자인 Berta Bescos, ORB-SLAM 3의 제 1저자인 Campos, ORB-SLAM 시리즈를 만든 랩실의 교수님인 Tardos 교수님의 새로운 작품.</li>
<li>Object pose estimation + tracking과 SLAM을 joint optmisation으로 풀어냄.</li>
<li>CubeSLAM과 비슷하지만 좀 더 발전된 approach인듯 <br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMDYzMjMucGRm">LM:Reloc<i class="fa fa-external-link-alt"></i></span><ul>
<li>Direct SLAM의 최강자인 Cremers 교수님, 그리고 최근 Deep Direct SLAM 연구를 이끌고 있는 Nan Yang과 Lukas von Stumberg의 연구</li>
<li>Direct 방식으로도 visual localisation이 된다는 것을 증명.<ul>
<li>곧 Direct 방식에서도 feature를 사용하지 않는 loop closure 방식이 나올 수도 있음.</li>
</ul>
</li>
<li>이전의 GN-Net의 발전된 버전인듯<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMDc5MjkucGRm">Multi-Resolution 3D Mapping with Explicit Free Space Representation for Fast and Accurate Mobile Robot Motion Planning<i class="fa fa-external-link-alt"></i></span><ul>
<li>Imperial College London 소속의 Stefan Leutenegger와 Pablo Alcantarilla의 작품.<ul>
<li>Leutenegger는 BRISK, OKVIS, ElasticFusion, SemanticFusion의 저자, Alcantrilla는 KAZE / AKAZE feature의 저자임.</li>
</ul>
</li>
<li>SLAM을 이용한 Mapping에서 Free space를 정확하고 빠르게 모델링 할 수 있는 기법으로 보임. Map에서 정확한 경계를 표현하기 위해 공간을 Tree 구조로 쪼개서 정밀한 경계는 깊은 tree level로 들어간다. 이 부분을 multi-resolution occupancy mapping이라는 표현을 사용함.<ul>
<li>이 외로 novelty가 더 있어보이지만 아직 깊게 읽어보지 못함.</li>
</ul>
</li>
<li>ICL의 연구처럼 보이지만, 사실 SLAMCore의 연구라고 볼 수 있음.<ul>
<li>Robot mapping + path planning을 효과적으로 할 수 있는 제품이 곧 SLAMCore의 제품에 실릴 것이라는 이야기.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tL3kyMDIwLzAzMDI2MjguaHRtbA==">Method and System for Performing Simultaneous Localization and Mapping using Convolutional Image Transformation<i class="fa fa-external-link-alt"></i></span><ul>
<li>MagicLeap의 새로운 특허. 당연히 DeTone과 Malisiewicz, Rabinovich의 이름이 들어갔다. </li>
<li>MagicLeap에서 생각하고있는 SuperMaps의 아이디어를 담고있다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3JwZy5pZmkudXpoLmNoL2RvY3MvM0RWMjBfSGlkYWxnby5wZGY=">Learning Monocular Dense Depth from Events<i class="fa fa-external-link-alt"></i></span><ul>
<li>Event하면 역시 Scaramuzza 교수님 작품이다.</li>
<li>매번 새로운 기능이 추가되는 모습이 보기 좋다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMDk0MDkucGRm">SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal Scenes<i class="fa fa-external-link-alt"></i></span><br>ORB-SLAM의 Tardos 교수님 + Montiel 교수님 랩실 작품<ul>
<li>Deformable scene에서 SLAM + Dynamic object는 거를 수 있음</li>
<li>수술 환경에서 SLAM을 하려는 연구는 관심을 가지는 사람의 수도 적고 어려운 환경이라 굉장히 희귀하다. Peter Mountney 이후로 오랜만에 보는 듯. <br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMDkzNzgucGRm">Freetures: Localization in Signed Distance Function Maps<i class="fa fa-external-link-alt"></i></span><ul>
<li>Microsoft와 ETH Zurich의 합작.<ul>
<li>Delmerico, Nieto 등 참가했고, Roland Seigwart, Marc Pollefeys, Cesar Cadena 교수님도 참가하셨음.</li>
</ul>
</li>
<li>3D dense map으로부터 localisation을 수행하는 방법을 제안함.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMDkyMzIucGRm">Elastic and Efficient LiDAR Reconstruction for Large-Scale Exploration Tasks<i class="fa fa-external-link-alt"></i></span><ul>
<li>ICL의 Stefan Leutenegger 교수님 랩실의 작품.</li>
<li>3D LiDAR를 이용해서 Large scale volumetric reconstruction을 수행함.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMTAwNTEucGRm">Tracking from Patterns: Learning Corresponding Patterns in Point Clouds for 3D Object Tracking<i class="fa fa-external-link-alt"></i></span><ul>
<li>VINS-mono를 만든 HKUST의 Shaojie Shen과 Peiliang Li의 작품.</li>
<li>SLAM은 아니지만… LiDAR 기반 3D object detection 파이프라인에 temporal tracking 기능을 추가함으로써 안정적인 트랙킹을 만듬.<br></li>
</ul>
</li>
</ul>
<br>

<hr>
<h1 id="11월"><a href="#11월" class="headerlink" title="11월"></a>11월</h1><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tL3kyMDIwLzAzMzQ4NDkuaHRtbA==">Relocalization systems and methods<i class="fa fa-external-link-alt"></i></span><ul>
<li>MagicLeap에서 내놓은 Relocalization 알고리즘 특허. <ul>
<li>10월에 내놓았던 특허에서 빈 부분을 채워주는 듯</li>
</ul>
</li>
<li>인풋 이미지로부터 global feature를 CNN으로 embedding을 한 후, nearest neighbour로 가장 가까운 이미지를 찾아서 Relocalization하는 듯<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMTI0NTUucGRm">Primal-Dual Mesh Convolutional Neural Networks<i class="fa fa-external-link-alt"></i></span><ul>
<li>MIT의 Luca Carlone 교수님과 Zurich의 Scamaruzza 교수님이 함께 발표.</li>
<li>Mesh 데이터를 처리하는 새로운 CNN 알고리즘을 개발.<ul>
<li>기존의 방식은 GNN 방식을 따라서 mesh를 그래프로 보며 처리했지만, mesh의 특성을 잘 반영하지 못했음.</li>
<li>새로운 방식은 이전 방식이 잘 반영하지 못하던 mesh의 edge나 face도 사용 가능.  <br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTAuMTUyNjEucGRm">Deep Shells: Unsupervised Shape Correspondence with Optimal Transport<i class="fa fa-external-link-alt"></i></span><ul>
<li>Daniel Cremers 교수님이 참여… 근데 솔직히 왜 하신건지는 아직 잘 이해는 안감.</li>
<li>3D representation 중에 shell이라는 방식이 있음.<ul>
<li> 3D model이 deformable하게 변할 때, optimal transport 기법을 사용하여 shape correspondence를 맞추는 방식을 제안함.</li>
</ul>
</li>
<li> 이 논문과 직접적인 관계는 없지만, SLAM 쪽에서도 3D representation 연구를 많이 하는 것 같음. 특히 딥러닝 3D representation은 하나의 class에서 다른 instance들로 differentiable하게 변환이 가능하기 때문에, SLAM의 입장에서는 joint optimisation이 가능해지는 것. <br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3Jhcy5wYXBlcmNlcHQubmV0L2ltYWdlcy90ZW1wL0lST1MvZmlsZXMvMDk4OC5wZGY=">Accurate Mapping and Planning for Autonomous Racing<i class="fa fa-external-link-alt"></i></span><ul>
<li>Roland Siegwart 교수님 및 다수의 연구자가 참가한 연구임.</li>
<li>유명한 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9GYktMRTd1YXI5WQ==">영상<i class="fa fa-external-link-alt"></i></span>.</li>
<li>SLAM과 optimal path planning, control이 모두 들어가있음.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3Jhcy5wYXBlcmNlcHQubmV0L2ltYWdlcy90ZW1wL0lST1MvZmlsZXMvMDM4Ni5wZGY=">Robot Navigation in Crowded Environments Using Deep Reinforcement Learning<i class="fa fa-external-link-alt"></i></span><ul>
<li>Roland Siegwart 교수님이 또…</li>
<li>SLAM이 주가 되는 논문은 아니다. LiDAR odometry + scan 정보를 사용하지만, 논문의 메인 주제는 강화학습임.</li>
<li>그래도 SLAM과 다른 모듈이 함께 사용될 수 있는걸 보여주니 여기 넣어봤음.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3Jhcy5wYXBlcmNlcHQubmV0L2ltYWdlcy90ZW1wL0lST1MvZmlsZXMvMDM1OS5wZGY=">From Points to Planes - Adding Planar Constraints to Monocular SLAM Factor Graphs<i class="fa fa-external-link-alt"></i></span><ul>
<li>자라고자 대학의 Javier Civera 교수님 랩실 연구</li>
<li>Plane enforcement로 만들어낸 plane landmark를 Factor Graph optimisation에 사용함.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL3Jhcy5wYXBlcmNlcHQubmV0L2ltYWdlcy90ZW1wL0lST1MvZmlsZXMvMTg5MS5wZGY=">A Robust Multi-Stereo Visual-Inertial Odometry Pipeline<i class="fa fa-external-link-alt"></i></span><ul>
<li>Michael Kaess 교수님 랩실 작품.</li>
<li>Stereo pair에서 검출되는 모든 landmark 정보와 camera pose를 jointly optimise한다고 함.</li>
<li>1-point RANSAC 라는 기술을 쓰는데… 이게 뭘까…<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDI2NTgucGRm">Compositional Scalable Object SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>Kaess 교수님 랩실 작품.</li>
<li>Object-level SLAM이 새로 나옴. <ul>
<li>RGB-D 기반, semantic data association을 포함함. </li>
<li>Large-scale indoor reconstruction에 사용 가능.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDE5NzUucGRm">Rearrangement: A Challenge for Embodied AI<i class="fa fa-external-link-alt"></i></span><ul>
<li>어벤저스 교수님 논문.<ul>
<li>Dhruv Batra, Angel Chang, Sonia Chernova, Andrew Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su.</li>
</ul>
</li>
<li>‘물체를 제자리에 돌려놓는 방법’에 대한 여러가지 방법을 함께 탐색하였다.<ul>
<li>Geometric, language, Image, Experience가 중심 토픽이다.<ul>
<li>Geometric - SLAM, 3D, Reconstruction, Planning</li>
<li>Language - NLP, Image-language, Communication</li>
<li>Image - Context undertanding, semantic understanding</li>
<li>Experience - Reinforcement learning, Memory<br></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tLzIwMjAwMzM0OTAyLnBkZg==">Providing Semantic-Augmented Artificial-Reality Experience<i class="fa fa-external-link-alt"></i></span><ul>
<li>Richard Newcombe가 있는 Facebook에서 내놓은 연구.</li>
<li>Semantic AR를 수행하는 인프라 구조.<ul>
<li>모바일 -&gt; 서버 -&gt; 제 3의 컴퓨팅 리소스를 거친다.</li>
<li>Mesh를 만들어내는 과정이 필수적으로 들어간다. 아무래도 surface 정보를 이용한 interaction을 하려는 의도인 것 같다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDI1NzQucGRm">Learning Trajectories for Visual-Inertial System Calibration via Model-based Heuristic Deep Reinforcement Learning<i class="fa fa-external-link-alt"></i></span><ul>
<li>Roland Siegwart 교수님과 Cadena 교수님 및 다른 연구원들의 작품</li>
<li>Visual-Inertial 시스템의 캘리브레이션은 굉장히 정확한 캘리브레이션을 요구함. 이 때, 캘리브레이션은 보통 캘리브레이션 타겟을 바라보면서 특정 움직임이 요구됨.<ul>
<li>Model-based 강화학습 방식으로 캘리브레이션이 잘 되는 움직임을 학습함<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tL3kyMDIwLzAzNDk3NjMuaHRtbA==">Semantic Fusion (특허)<i class="fa fa-external-link-alt"></i></span><ul>
<li>Facebook의 Richard Newcombe</li>
<li>Semantic Fusion이라는 이름으로 특허를 냈다.<ul>
<li>Imperial College의 Davison 교수님 랩실에서 한 그 SemanticFusion과는 다른 것 같다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDYuMDQyNTAucGRm">AdaLAM: Revisiting Handcrafted Outlier Detection<i class="fa fa-external-link-alt"></i></span><ul>
<li>ETH Zurich의 Marc Pollefeys, Torsten Sattler, Viktor Larsson의 연구이다.</li>
<li>Descriptor끼리 매칭할 때 nearest neighbour로 매칭하는 기존의 방식은 outlier가 너무 많이 생긴다.</li>
<li>이 연구는 새로운 local affine motion verification과 sample-adaptive threshold를 사용한 hierarchical pipeline을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDYxOTQucGRm">A Factor-Graph Approach for Optimization Problems with Dynamics Constraints<i class="fa fa-external-link-alt"></i></span><ul>
<li>Factor graph optmisation 연구의 대가인 Frank Dellaert 교수님 랩실의 연구이다.</li>
<li>Dynamic factor graph라는 방법론을 제시한다. <ul>
<li>SLAM에서 쓰일 것은 아니지만, 가능성이 아예 없지는 않다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDclMkY5NzgtMy0wMzAtNTg1NDItNl8zNg==">SceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans<i class="fa fa-external-link-alt"></i></span><ul>
<li>Facebook의 Angela Dai와 Matthias Nießner의 연구이다.</li>
<li>RGB-D로 스캔한 환경에서 1. object에 CAD model prior를 fit하고, 2. 동시에 layout + object pose를 jointly optmise한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDcwNDQucGRm">Tactile SLAM: Real-time inference of shape and pose from planar pushing<i class="fa fa-external-link-alt"></i></span><ul>
<li>Michael Kaess 교수님… 왜 비전 안 하세요…</li>
<li>비전 방식이 아닌, Force/torque 센서를 통해 얻는 센서 값으로 Shape guessing을 한다는 논문이다. SLAM에서도 전체 map은 모르지만 센서로 취득한 local map을 쌓아가면서 전체 map을 만드는 것 처럼, 똑같은 방식으로 force/torque 방식에 적용한 논문이다.</li>
<li>SLAM…이라고 하긴 좀 그렇지만, 다른 방향으로도 새로운 방법론이 존재한다는걸 보기에는 좋은 예시이다.<br></li>
</ul>
</li>
</ul>
<br>

<hr>
<h1 id="12월"><a href="#12월" class="headerlink" title="12월"></a>12월</h1><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTE4MTQucGRm">MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera<i class="fa fa-external-link-alt"></i></span><ul>
<li>Nan Yang, Lukas von Stumberg와 Cremers 교수님의 작품.</li>
<li>Dynamic object는 photometric inconsistency를 이용한 mask module을 사용하여 움직이는 물체임을 인지함.<ul>
<li>이를 이용해 Static / moving object에 대한 depth 를 정확하게 추정함.</li>
<li>이 점이 기존의 MVS와 다른 점임.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL2ltcGFjdC5jaWlyYy5jdnV0LmN6L3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIwLzExL1N1bmVnYXJkM0RWMjAyMC5wZGY=">Deep LiDAR localization using optical flow sensor-map correspondences<i class="fa fa-external-link-alt"></i></span><ul>
<li>Torsten Sattler의 연구이다.</li>
<li>딥러닝 기반 optical flow를 사용해서 LiDAR 기반 localization 방법을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW1pbmVyLmNuL3B1Yi81NzM2OTVlYzZlM2IxMjAyM2U1MDFkZGUvYS1yZXZpZXctb2YtcG9pbnQtY2xvdWQtcmVnaXN0cmF0aW9uLWFsZ29yaXRobXMtZm9yLW1vYmlsZS1yb2JvdGljcz9saW5rdG9jb21tZW50PXRydWU=">A Review of Point Cloud Registration Algorithms for Mobile Robotics<i class="fa fa-external-link-alt"></i></span><ul>
<li>Siegwart 교수님이 참여하신 리뷰 논문이다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vbmxpbmVsaWJyYXJ5LndpbGV5LmNvbS9kb2kvZnVsbC8xMC4xMDAyL3JvYi4yMjAwNw==">Towards automating construction tasks: Large‐scale object mapping, segmentation, and manipulation<i class="fa fa-external-link-alt"></i></span><ul>
<li>Margarita Chli라는 ETH Zurich의 조교수로 계신 분의 연구이다. 이전에 Roland Siegwart 교수님 랩실에서 Stefan Leutenegger랑 연구도 같이 한듯… 이번 연구로 처음 알게 되었다.</li>
<li>Application 논문이다. 공사 현장에서 드론 스캔으로 map을 만든 후, 포크레인에 LiDAR를 달아서 map merge 및 localisation을 수행. 그 후 object들에 대한 semantic 정보를 추출한 다음에, path planning 및 제어를 사용하여 물체를 집어올리고 이동한다.</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTE3MjQucGRm">Rotation-only bundle adjustment<i class="fa fa-external-link-alt"></i></span><ul>
<li>Seong Hun Lee와 Civera 교수님의 작품.</li>
<li>Rotation만 최적화 하는 방법.<ul>
<li>Position과 3d structure의 값이 부정확해도 잘 된다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTIzNjAucGRm">A Structure-Aware Method for Direct Pose Estimation<i class="fa fa-external-link-alt"></i></span><ul>
<li>Nathan Jacobs?라는 잘 모르는 분의 연구이다.</li>
<li>CNN 기반 camera pose estimation 문제를 푸는데, direct방식과 indirect방식의 장점만을 가져와서 섞었다고 한다. Sota를 찍은건 아닌 것 같다.<ul>
<li>Scene coordinate regression과 monocular depth estimation을 수행한다. 그 후 depth map을 camera intrinsic을 사용하여 3D scene coordinates로 만든 후, 3D-3D correspondence를 계산하여 pose regression을 한다고 한다.</li>
<li>솔직히 좋은 연구 방향은 아닌 것 같다. Monocular depth estimation은 현재 단계에서는 sharp &amp; crisp하게 나오는 것이 아니라, general하게 나타나기 때문이다. Edge 쪽에서도 많이 뭉뜽그려진다. 하지만 논문 초반에 나오는 intro + related works가 읽기 좋다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDA1OTUucGRm">DeFMO: Deblurring and Shape Recovery of Fast Moving Objects<i class="fa fa-external-link-alt"></i></span><ul>
<li>Marc Pollefeys 교수님이 연구에 참여하셨다.</li>
<li>보통 빠르게 움직이는 물체에는 이미지에서 blur가 진다. 이 연구에서는 이미지로부터 배경과 물체를 분리해낸 후, Temporal super-resolution을 물체가 움직이는 모습을 초고속 카메라가 깔끔하게 찍은 것 처럼 복원해준다.<ul>
<li>이 때, 깔끔한 이미지의 representation도 얻어낼 수 있다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDgyMTYucGRm">FMODetect: Robust Detection and Trajectory Estimation of Fast Moving Objects<i class="fa fa-external-link-alt"></i></span><ul>
<li>Pollefeys 교수님이 연구에 참여하셨다.</li>
<li>DeFMO와 연관점이 많은 연구이다. 빠르게 움직이는 물체를 deblurring하고 trajectory를 딥러닝으로 복원해내는 연구이다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDg3MzAucGRm">Event-based Motion Segmentation with Spatio-Temporal Graph Cuts<i class="fa fa-external-link-alt"></i></span><ul>
<li>VINS-mono의 Shaojie Shen의 연구이다.</li>
<li>Event 카메라로 motion segmentation을 수행하는 방법을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cDovL2ltcGFjdC5jaWlyYy5jdnV0LmN6L3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIwLzExL1N0ZW5ib3JnM0RWMjAyMC5wZGY=">Using Image Sequences for Long-Term Visual Localization<i class="fa fa-external-link-alt"></i></span><ul>
<li>Torsten Sattler의 연구이다.</li>
<li>기존의 Visual localization은 1개의 이미지를 맵 데이터와 비교해서 위치를 복원해냈다. 하지만 보통 실제 시나리오에서는 사진 1장이 아니라 이미지 시퀀스를 받는다.</li>
<li>이번 연구는 사진 1장 대신 이미지 시퀀스를 사용함으로써 더 안정적이게 위치를 복원하는 방법을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTE5NDYucGRm">Benchmarking Image Retrieval for Visual Localization<i class="fa fa-external-link-alt"></i></span><ul>
<li>NAVER LABS의 Martin Humenberger, Gabriela Csurka, Yohann Cabon과 Torsten Sattler의 연구이다.</li>
<li>다양한 Visual localization 방법들의 벤치마크 결과를 냈다.</li>
<li>그리고 벤치마킹 툴인 Kapture를 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTMxOTEucGRm">Appearance-Invariant 6-DoF Visual Localization using Generative Adversarial Networks<i class="fa fa-external-link-alt"></i></span><ul>
<li>저자들이 누구일까…? 찾아도 안나온다. Shiguo Lian이라는 사람은 cryptography 관련 사람이 나오는데…</li>
<li>Visual localization에서 사용되는 feature들은 invariant하지 않다는 점을 파고든다. 이를 위해 CycleGAN을 사용하여 만들어낸 다른 appearance를 가진 이미지들로 feature extraction 네트워크를 학습한다.</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDkxNjQucGRm">Point Transformer<i class="fa fa-external-link-alt"></i></span><ul>
<li>옥스포드의 Philip Torr 교수님과 Intel Labs의 Vladlen Koltun의 연구.</li>
<li>Transformer에서 사용되는 Self-attention을 포인트 클라우드 프로세싱에 쓸 수 있을까? 에 대한 답<ul>
<li>Classification, Part segmentation, semantic segmentation 모두 잘 된다는 결과.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDMwMjMucGRm">Efficient Volumetric Mapping Using Depth Completion With Uncertainty for Robotic Navigation<i class="fa fa-external-link-alt"></i></span><ul>
<li>Stafan Leutenegger의 연구이다.</li>
<li>RGB-D 맵핑을 할 때 free-space에 대한 depth는 항상 정확하게 추정하기 어려웠다. 이 연구에서는 딥러닝으로 추정한 uncertainty 값을 이용해 depth completion을 수행하고, free-space에 대한 맵핑의 속도와 퀄리티를 향상시켰다. <br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDMwODIucGRm">Quantifying Aleatoric and Epistemic Uncertainty Using Density Estimation in Latent Space<i class="fa fa-external-link-alt"></i></span><ul>
<li>Cadena, Siegwart, Van Gool, Tombari 교수님의 연구이다.</li>
<li>최근 Deep-SLAM쪽에서 종종 uncertainty를 표현하기 위해 사용되는 aleatoric / epistemic uncertainty를 추정하는 새로운 방식을 제안한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTA5ODgucGRm">Post-hoc Uncertainty Calibration for Domain Drift Scenarios<i class="fa fa-external-link-alt"></i></span><ul>
<li>Cremers 교수님이 연구에 참여하셨다.</li>
<li>Deep learning classifer는 보통 training domain에서는 높은 정확도와 confidence score를 가지지만, 다른 domain으로 옮기면 confidence score가 급격하게 떨어진다.</li>
<li>이 confidence score를 다시 올려주는 post-hoc uncertainty calibration 방법을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDE5MDkucGRm">Patch2Pix: Epipolar-Guided Pixel-Level Correspondences<i class="fa fa-external-link-alt"></i></span><ul>
<li>Torsten Sattler가 참여한 연구이다.</li>
<li>최근 성공적이였던 딥러닝 기반 keypoint matching 방식을 계승하는 연구이다.<ul>
<li>Patch-level match proposal을 추출한 후, epipolar geometry를 weakly-supervised 방식으로 학습한 네트워크를 통해서 refine하여 pixel-wise match 정보를 regress한다.</li>
<li>이 과정에서 outlier는 자동으로 제거된다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDclMkY5NzgtMy0wMzAtNTg2MDEtMF8zNg==">CAD-Deform: Deformable Fitting of CAD Models to 3D Scans<i class="fa fa-external-link-alt"></i></span><ul>
<li>Matthias Nießner가 연구에 참여하였다.</li>
<li>CAD-model 데이터를 3D scan 데이터에 deformable fitting을 통해 정확하게 alignment하는 방식을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTQ3NDQucGRm">RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction<i class="fa fa-external-link-alt"></i></span><ul>
<li>이번 연구 역시 Matthias Nießner가 연구에 참여하였다.</li>
<li>Raw point cloud로부터 jointly하게 object detection + dense object surface reconstruction을 수행하는 법을 제안한다. <br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDgxOTcucGRm">Seeing Behind Objects for 3D Multi-Object Tracking in RGB-D Sequences<i class="fa fa-external-link-alt"></i></span><ul>
<li>Matthias Nießner와 Facebook의 Angela Dai의 연구이다.</li>
<li>RGB-D로 3d Detection을 한 후 지속적으로 tracking을 수행한다. 이 때, occlusion 등을 가려진 부분을 ‘hallucination’을 통해 채워넣어서 지속적으로 안정적이게 트랙킹한다.<ul>
<li>이를 통해 object간의 occlusion이 있을때도 안정적이게 multi-object tracking을 수행하였다.<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkyNzE4NzU=">Real-Time Temporal and Rotational Calibration of Heterogeneous Sensors Using Motion Correlation Analysis<i class="fa fa-external-link-alt"></i></span><ul>
<li>VINS-mono의 저자인 Tong Qin과 Shaojie Shen의 연구이다.</li>
<li>새로운 이종 센서간의 Temporal calibration 방식을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTI0MzAucGRm">SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition<i class="fa fa-external-link-alt"></i></span><ul>
<li>Rui Wang과 Cremer 교수님의 작품이다.</li>
<li>SOE-Net은 Point cloud 기반으로 place recognition을 할 때 필요한 local descriptor를 뽑는 방법과, large-scale point cloud based retrieval (i.e. long range)에 유용한 데이터를 추출하는 방법을 새롭게 제안한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTMzODgucGRm">3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer<i class="fa fa-external-link-alt"></i></span><ul>
<li>Roland Siegwart 교수님께서 연구에 참여하셨다.</li>
<li>Object class 정보를 가지고 있는 point cloud와, 특정 style을 표현하는 point cloud가 있을 때, 기존의 style-transfer와 비슷하게 적용할 수 있다.</li>
<li>이를 통해 동일 class지만 세부 모양이 다른 point cloud를 생성해내거나, 특정 이미징 디바이스의 point cloud 캡처 방식을 가져올 수 있다.<ul>
<li>Class 내부에서 모양이 다른 point cloud들 간의 differentiable representation도 가지게 되는것이 아닌가…하고 생각이 든다. Shape + camera pose joint optimisation도 가능해지지 않을까?<br></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMDcyMzMucGRm">Stable View Synthesis<i class="fa fa-external-link-alt"></i></span><ul>
<li>Intel의 Vladlen Koltun의 작품이다.</li>
<li>SLAM이 이미지들로부터 camera pose와 structure를 구해내는 작업이라면, view synthesis는 camera pose와 structure를 기반으로 photorealistic image를 만들어내는 작업이다.</li>
<li>최근 SLAM쪽에서도 view synthesis에 대한 연구를 많이 진행하고 있는데, 이는 structure에 대한 representation을 구하기 아주 유용하기 때문이다.</li>
<li>(또는, 부족한 정보를 view synthesis로 만들어서 채워넣을수도 있지 않을까?)<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDMuMDg5MzQucGRm">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis<i class="fa fa-external-link-alt"></i></span><ul>
<li>Ren Ng? 이라는 교수님이 지도하신 것 같다.</li>
<li>내용은 잘 이해가 되지 않았지만 View Synthesis에서 제일 좋은 성능을 뽑아낸다고 들었다. 굉장히 핫하다고… ㄷㄷ<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTEuMTQ3OTEucGRm">NeuralFusion: Online Depth Fusion in Latent Space<i class="fa fa-external-link-alt"></i></span><ul>
<li>Pollefeys 교수님이 연구에 참여하셨다.</li>
<li>Depth map aggregation을 latent feature space에서 학습한다고 한다. 사실 깊게 이해는 잘 안된다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMjYzLTAyMC0wMTM5OS04">Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis<i class="fa fa-external-link-alt"></i></span><ul>
<li>Torsten Sattler와 Davide Scaramuzza 교수님의 작품이다.</li>
<li>Visual Localization을 위해 reference pose를 만들 때는 SfM을 사용한다. 하지만 조명 변화가 생겼을 때는 local feature 매칭이 안되기 때문에 쓰기 어려워진다.</li>
<li>이번 연구는 View synthesis를 통해 새로운 learned feature를 포함하는 reference pose 데이터를 만드는 semi-automated 방법을 소개한다.<br></li>
</ul>
</li>
</ul>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDk3OTMucGRm">SceneFormer: Indoor Scene Generation with Transformers<i class="fa fa-external-link-alt"></i></span><ul>
<li>Matthias Nießner의 연구이다</li>
<li>Indoor scene generation이라고 하는데… 아직 목적을 잘 모르겠다. 좀 더 읽어봐야곘다.<br></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Deep SLAM</tag>
        <tag>3D</tag>
      </tags>
  </entry>
  <entry>
    <title>2020년 회고</title>
    <url>/20201231-retro/</url>
    <content><![CDATA[<h1 id="Overall…"><a href="#Overall…" class="headerlink" title="Overall…"></a>Overall…</h1><p>2020년은 <strong>행복한 한 해</strong>였다.</p>
<p>생각했던 것들을 행동으로 옮겼다.<br>행동했던 것들은 좋은 결과를 가지고 왔다.<br>좋은 결과는 나를 더 많은 좋은 기회들과 좋은 인연들로 이끌었다.</p>
<p>그러면 완벽한 한 해였을까?<br>그건 또 아니였다.<br>생각했던 것 처럼 흘러가지 않았던 일들도 있었다.</p>
<p>그래도 후회되는건 없다.<br>선택의 기로에서 항상 최선의 선택을 했다.<br>편법을 쓰지 않고 나 자신에게 당당할 수 있는 행동을 했다.<br>힘든 길을 마다하지 않았다.<br>내 쇼트텀 목표와 롱텀 목표에 도움이 되는 길을 골랐다.</p>
<p>만약 혼자서 모든 것을 헤쳐내려고 했었으면, 지금 이렇게 만족하지 못했을 것이다.<br>이번 해에는 정말 다양한 방면으로 새로운 사람들을 만났다.<br>또 여럿 알고만 지내던 분들과 깊은 인연이 새롭게 생겼다.</p>
<p><strong>멋진 분들!<br>항상 제가 힘들 때 바로잡아주시고 흔들릴 때 방향을 보여주셔서 감사합니다.<br>이번 한 해 함께 멋진 순간을 보낼 수 있어서 영광이였습니다.<br>앞으로도 잘 부탁드립니다</strong> <span class="github-emoji" alias="relieved" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60c.png?v8">😌</span></p>
<img src="/20201231-retro/gom.gif" width="300" height="300" alt="[alternative_text]" title="[이예에~~]">

<hr>
<h1 id="감사함"><a href="#감사함" class="headerlink" title="감사함"></a>감사함</h1><p>2020년에는 <strong>혼자였다면 절대 하지 못했을 일들</strong>이 많았다.</p>
<p>혼자서는 시간과 힘이 없어서 생각도 못했을 일들,<br>혼자서는 견뎌내지 못하고 포기했을 일들…</p>
<p>서로에게 힘이 되주면서<br>함께 힘든 일들을 함께 헤쳐나가고 멋진일을 해냈다.</p>
<p>이러한 과정에 배움도 많았고 <strong>너무나도 귀한 인연</strong>들을 만들게 되었다.</p>
<p>신기했던 건, 많은 분들과 원래는 SNS로만 알던 사이였던 것이다.<br>페이스북으로 친구추가만 되어있고 얘기는 한번도 못 해본 사이라던지…</p>
<p>이렇게 인연이 이어지는 게 너무 신기하고…<br>그리고 <strong>함께 해주신 모든 분들께 너무나도 감사하다</strong> <span class="github-emoji" alias="heart_eyes" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60d.png?v8">😍</span><br><strong>내년도 함께 멋진 일해요!!</strong></p>
<img src="/20201231-retro/thanks.png" class="" title="Thank you!">

<hr>
<h1 id="Visual-SLAM-전액-기부-라이브-세미나"><a href="#Visual-SLAM-전액-기부-라이브-세미나" class="headerlink" title="Visual SLAM 전액 기부 라이브 세미나"></a>Visual SLAM 전액 기부 라이브 세미나</h1><p>2020년 상반기는 코로나 바이러스로 시끌벅적했다.<br>2019년 말 부터 진행했던 ORB-SLAM 스터디도 중단되어야 했었다.<br><span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3I=">SLAM KR 커뮤니티<i class="fa fa-external-link-alt"></i></span>로부터 모셨던 연사님들과 50명의 참여 인원분들께 너무 죄송했다 ㅠㅠ…</p>
<p>혼자 SLAM 공부를 하다보면 이런 생각이 항상 든다.<br>이 재밌는걸 혼자만 한다고??<br>ORB-SLAM 스터디도 함께 SLAM 할 사람을 더 끌어들이기 위해 시작했는데… 아쉬웠다.</p>
<p>그러던 와중 <strong>게임 방송에서 기부 미션</strong>을 하는 것을 보았다.<br>마침 나라에서도 <strong>코로나로 인한 사회적 취약계층 지원을 위한 모금활동</strong>을 하고 있었다.<br>이거면 사람들을 SLAM에 끌어들일 수 있겠다! 하고 생각이 들었다.</p>
<img src="/20201231-retro/seminar.png" class="" title="Visual-SLAM Seminar">

<p>그래서 <a href="https://youtu.be/XN1Ehh-YwlY"><strong>SLAM에 관련된 YouTube 라이브 세미나</strong></a>를 2번 진행해봤다.<br>처음하는거라… 도네이션 시스템, 컨텐츠 배정, 방송환경 셋팅 등등 준비할게 많았다.<br>시간을 많이 넘긴 것만 빼면 생각보다는 잘 되었다. (논스탑 5시간, 4시간 라이브는 쉽지 않았다)</p>
<p>기대했던 것 보다 더 많은 분이 좋아해주셔서 굉장히 뿌듯했다.<br>동시 접속자 100명의 힘을 받아 <strong>총 643,000원을 전국재난구호협회에 기부</strong>했다.<br>그리고, <strong>기술 세미나로 사회에 기여한다는 아이디어가 실제로 가능하다고 확신</strong>이 들었다.</p>
<hr>
<h1 id="전액-기부-라이브-세미나-with-어나인-커뮤니티"><a href="#전액-기부-라이브-세미나-with-어나인-커뮤니티" class="headerlink" title="전액 기부 라이브 세미나 (with 어나인 커뮤니티)"></a>전액 기부 라이브 세미나 (with 어나인 커뮤니티)</h1><p>ANAIN 커뮤니티 주최의 2019년 연말 파티에서 ‘200명이 넘는 전액기부 행사를 해보고싶다’라고 한 적이 있다.<br>그리고 이런 행사가 실제로 가능한지는 Visual-SLAM 라이브 세미나에서 이미 검증이 되었다.<br>좀 더 큰 스케일로, 기술 커뮤니티가 정말로 사회적인 도움을 줄 수 있다는 것을 보여주고 싶었다.</p>
<p>내가 익숙한 <strong>딥러닝 컴퓨터 비전 커뮤니티에서 연사님들을 모집</strong>했다.<br>지식을 공유하는 것을 즐기시는 분들, 비전공자의 이목도 끄는 프로젝트를 진행하신 분들…<br>전액기부라는 부담스러운 자리임에도 많은 분들이 연락드렸을 때 흔쾌히 수락해주셨다.</p>
<p>컴퓨터 비전 커뮤니티만이 할 수 있는 무언가는 무엇일까?<br>결국 우리는 어떤 시각적인 시스템을 만드는데, 그것이 누군가에게는 눈이 될 수 있다고 생각이 들었다.<br>어나인 쪽에서 추천해주신 ‘<a href="http://artblind.or.kr/"><strong>우리들의 눈</strong></a>‘<strong>이라는 시각장애인을 위한 예술 교육 단체를 전액 기부</strong> 대상으로 정했다.</p>
<img src="/20201231-retro/anain.png" class="" title="ANAIN seminar">

<p>‘<a href="https://festa.io/events/1159"><strong>비전을 나누는 시간</strong></a>‘이라는 이름으로 다양한 컴퓨터 비전 분야를 커버하였다.<br>딥러닝 이론, 딥러닝 앱 개발, GAN, 모델 압축/양자화, LiDAR SLAM, Visual-SLAM이 주제였다.<br>나는 ‘<a href="https://youtu.be/QKtyoNAdoqc"><strong>딥러닝 없는 컴퓨터 비전: 증강현실과 SLAM</strong></a>‘이라는 주제로 발표를 했다.<br>이 세미나를 통해서 참가하신 분들께서 다양한 분야에 대해 새로운 지식을 얻어가셨길 바란다.<br><strong>총 875,000원이 기부</strong>되었고, 멋진 분들과 함께 의미있는 일을 했다는 점이 굉장히 뿌듯하다.</p>
<p>기존의 기술 세미나들은 장소 대관 및 케이터링 등을 위해 수익을 창출해야하는 경우가 많았다.<br>하지만 언택트 세미나의 경우에는 장소 대관 및 케이터링이 필요하지 않은데, 이러한 차이점이 어떤 새로운 가치를 만들 수 있을지 고민하면 좋을 것 같다.<br>개인적으로, 이번과 같은 <strong>기부를 유도하는 기술 세미나가 좀 더 많아지기를 소망한다</strong>.</p>
<p>사실 이 행사를 기획하는 과정에서 코로나 상태가 너무 시시각각 변했다.<br>몇달의 시간동안 행사는 오프라인에서 온라인으로 바뀌었고, 연사진도 바뀌었고, 방식도, 기부대상도 바뀌었다.<br><strong>긴 기간동안 많은 변화에 지칠만도 한데, 끝까지 믿어주시고 함께 해주신 어나인 커뮤니티 운영진 분들과 연사진 분들께 무한한 감사를 표합니다…!</strong> <span class="github-emoji" alias="relieved" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60c.png?v8">😌</span> <span class="github-emoji" alias="relieved" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60c.png?v8">😌</span></p>
<hr>
<h1 id="SLAM-오픈카톡방"><a href="#SLAM-오픈카톡방" class="headerlink" title="SLAM 오픈카톡방"></a>SLAM 오픈카톡방</h1><p>작년 11월에 “<span class="exturl" data-url="aHR0cHM6Ly9vcGVuLmtha2FvLmNvbS9vL2c4VDVreExi">저희는 SLAM 마스터가 될겁니다<i class="fa fa-external-link-alt"></i></span>“ 라는 오픈카톡방을 개설했다.<br>본래 목적은 ORB-SLAM 스터디를 위한 소통 창구였지만… 코로나로 인해 스터디가 중단되었다.<br>그럼에도 인원들은 점차 늘어났고, 현재는 SLAM 공부 관련 질답과 SLAM을 공부하는 사람들의 친목의 장으로 바뀌었다.</p>
<img src="/20201231-retro/kakao.png" class="" title="slam masters">

<p>현재 인원은 <strong>약 480명</strong> 정도인데, 다행이도 내가 생각했던 카톡방의 분위기가 잘 유지되고 있다.<br>카톡방의 성격에 대해 논쟁을 벌인적도 아직 없다.<br>평소에 잡담을 많이 하는 편은 아니지만 열띈 토론을 할 때는 몇백개씩 메세지가 쌓이기도 하는데, 굉장히 좋은 현상이라고 본다.</p>
<p>카톡방에는 국내 로봇이나 드론 기업의 대표님과 여러 종사자 분들이 계신다.<br>어느정도 SLAM과 비전에 경험이 있으신 분들은 Visual, LiDAR, 자동차, 드론, 딥러닝 등등 전문 분야가 다양한 편이다.<br>또, 특정 마이너한 기술에 조예가 깊으신 분들이 있어서 (e.g. CUDA, AHRS 항법), 이야기 물꼬가 터질 때는 굉장히 유용한 정보가 많이 나온다.<br><strong>학생일 때 ‘이런 커뮤니티가 있었으면 좋겠다’하고 생각했던 대로 만들어진 것 같아서 뿌듯하다</strong>. </p>
<p>나는 이 톡방이 SLAM을 처음 공부하는 분들에게 도움이 되었으면 한다.<br>SLAM에 관심을 가지고 공부하시려는 분들은 많지만, 실제로 제대로 하시는 분들은 정말로 찾기 어렵다.<br>이러한 이유는 SLAM 공부를 처음 시작할 때 어디서부터 공부해야할지 감을 잡기조차 어려운데, 국문으로 된 자료는 거의 없기때문이라고 생각한다.<br>나는 우리 카톡방과 같은 커뮤니티가 이 부분을 어느정도 채워줄 수 있을 것이라고 보고, 미래에는 <strong>국내에 SLAM 하시는 분들이 좀 더 많아졌으면 좋겠다</strong>고 생각한다.</p>
<p>나 자신도 이 카톡방을 통해 많은 이득을 얻고 있다.<br>톡방의 멤버분들로부터 다양한 분야의 기술에 인사이트를 얻었고, 진로를 결정하는 데에도 여러 조언을 받고 있다.<br>새롭게 생긴 인연들도 있었는데, 함께 해주심에 정말로 감사하게 생각한다.<br>앞으로도 계속 함께 성장하고 서로로부터 배웠으면 좋겠다.</p>
<hr>
<h1 id="SLAM-풀잎스쿨"><a href="#SLAM-풀잎스쿨" class="headerlink" title="SLAM 풀잎스쿨"></a>SLAM 풀잎스쿨</h1><p>겨울에 3달동안 모두의 연구소에서 ‘<a href="https://home.modulabs.co.kr/product/13th-wise-slam-study/"><strong>슬기로운 SLAM 스터디 - Visual odometry &amp; 3D Vision 구현!</strong></a>‘ 이라는 스터디를 운영했었다.<br>꽤나 빡빡한 스터디 스케줄에도 불구하고 많은 분들께서 참여를 희망하셨고, 총 16명을 선정하여 진행하였다.<br>최후의 순간에는 10명이 남아주셨는데, 2019년 말 <a href="https://home.modulabs.co.kr/product/%EC%9D%98%EB%A3%8C%EC%98%81%EC%83%81-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B9%8C%EC%A7%80-image-guided-intervention/"><strong>의료영상 스터디</strong></a>에서 50%가 남은 것 보다는 더 나은 결과라고 본다.</p>
<p>스터디를 통해 정말 다양한 부분으로 발전한 것 같다.</p>
<ul>
<li>회사에서 하던 Windows 환경 개발이 익숙해지다보니 Linux 환경에서의 개발이 부담감이 있었다.<br>그리고 스터디원들로부터 ‘SLAM 제대로 할꺼면 리눅스 밖에 답이 없습니다’를 듣고 시작하게 되었다.<br>Bash 및 CLI, gcc, CMakeLists, 에디터, ROS 등등 <strong>‘언젠가는 해봐야지’ 하고 생각했던 것들에 도전</strong>하게 되었다.<br>Vim + Tmux 조합은 너무 멋있어보였다 ㅋㅋ<br>실제로 Windows + MSVC로 안될건 없다고 본다.<br>하지만 내가 부담감을 느끼고 어려워하던 분야로 발을 내딛었다는 것에 의미를 둔다. </li>
<li>위와 비슷한 이유로, 오픈소스 SLAM 알고리즘들도 직접 뜯어보게 되었다.<br>1년 전에 봤을때는 분명 너무 어려웠고 이해가 안가던 코드였다.<br>하지만 지금처럼 이론 베이스가 좀 쌓이고 나서 보니, 생각보다 잘 읽히게 되었다.<br><strong>더 이상 오픈소스 SLAM 알고리즘을 뜯어보는데에 두려움은 없다</strong>.</li>
<li>VITAMIN-E, hloc, Visual-LiDAR fusion과 같이 아직 내가 접해보지 못한 분야의 데모를 실제로 보게 되었다.<br><strong>이론을 이해하고 구현에 옮길 수 있는 능력은 굉장히 멋진 것이라고 느꼈다</strong>.</li>
</ul>
<img src="/20201231-retro/study.png" class="" title="Study">

<p><strong>하지만 스터디가 내가 생각했던 대로 흘러가지만은 않았다</strong>.<br>원래 목적은 10주 안에 AR 마커 트랙킹 + Sliding window optmisation 기반 visual odometry를 섞은 시스템을 구상했었다.<br>하지만 개발 도중에 다음과 같은 오류를 범했다.</p>
<ul>
<li><p>MSVC 환경에서 모든 라이브러리 셋팅을 맞추고 시작했음에도, 자동으로 OpenCV, Eigen, GTSAM 등 필수 라이브러리를 git repo에서 땡겨와 프로젝트를 설정하는 CMakeLists.txt를 만들고 싶어졌다.<br>여기에 1주일 반을 날렸다.</p>
</li>
<li><p>MSVC 환경에서 모든 라이브러리 셋팅을 맞추고 시작했음에도, ‘SLAM 하는 사람이니까’ Ubuntu + gcc 환경에서 만들어보고 싶다는 생각이 들어서 ssd를 주문하고 듀얼부팅 셋팅을 맞췄다.<br>이런 결정을 내리고, 주문하고, 부품을 기다리고, 삽질을 하고 익숙해지는데에 시간을 낭비했다.</p>
</li>
<li><p>스터디에서 실험 데이터셋을 하나 만들어서 사용하면 좋겠다고 생각해서, AirSim을 이용해서 가상 데이터셋을 찍어내는 인터페이스를 만들었다.<br>하지만 데이터셋은 사실 필요한 사람이 직접 구하면 되는거였다.</p>
</li>
<li><p>마지막 4주간 회사에서 빡빡한 마감 일정 + 야근을 요구하였다.<br>때문에 마지막 4주 동안 거의 개발을 진행하지 못했다.</p>
</li>
</ul>
<p>결국 <strong>나는 이번 스터디에서 처음에 목표로 삼았던 시스템 개발에 실패했다</strong>.<br>그리고 나를 포함한 다른 참여원의 다수가 처음에 계획했던 목표를 달성하지 못하였다.<br>스터디 리더인 내가 영 속도를 내지 못해서 나타난 현상이라고 생각하고 책임을 느낀다 ㅠㅠ…</p>
<p>원래는 이 스터디가 끝나고나서 <strong>공간 컴퓨팅 LAB</strong>을 열려고 했었다.<br>근데 이런 형태의 스터디는 아직 미완성이라고 판단된다.<br>같은 방식으로 랩을 연다면 똑같은 이유로 흐지부지하게 끝날 수도 있다고 생각된다.</p>
<p><strong>미래에 100% 성공할 수 있는 목표는 없다</strong>고 생각한다.<br>하지만 그렇다고 새로운 장애물이 생길 때 마다 대처하는 것도 준비성이 부족하다고 생각이 든다.<br>뭔가 모든 목표를 100% 완성하지는 못해도… 마일스톤을 확실하게 정해둬서 그 <strong>마일스톤을 넘겼다는거에 의미를 두면 조금 낫지 않을까</strong> 생각이 든다.</p>
<hr>
<h1 id="SLAM-DUNK-이론-스터디"><a href="#SLAM-DUNK-이론-스터디" class="headerlink" title="SLAM DUNK 이론 스터디"></a>SLAM DUNK 이론 스터디</h1><p><a href="https://www.facebook.com/groups/slamkr"><strong>SLAM KR 커뮤니티</strong></a>에서 진행하는 “<a href="https://www.youtube.com/c/SLAMKR/videos"><strong>SLAM DUNK 이론 스터디</strong></a>“에 현재 진행형으로 참여하고 있다.<br>매주 주말 Zoom으로 만나 돌아가면서 발표를 하는데, 굉장히 많은 내용을 배우고 있다.<br>여기서 나는 SLAM 기술의 개념에 대해 설명하는 부분과, RANSAC 테크닉에 대한 발표를 맡았다.<br>상대적으로 부담이 덜 가는 스터디인 점이 마음에 든다.</p>
<img src="/20201231-retro/slam_dunk.png" class="" title="SLAM dunk study">

<p>이번 스터디로부터 <strong>기초적인 부분에서 많이 배워가고 있다</strong>.<br>다르게 생각하면, 그만큼 내 기초적인 부분은 비어있었다.<br>이번에 스터디에서 ‘bundle adjustment 코드를 보면 딱히 probability, distribution 등을 다루는 것 처럼 보이지 않는데, 이게 어떻게 단순 최적화가 아닌 MAP estimation인지 이해가 안된다’ 라는 질문이 있었는데, 말문이 막혔던 기억이 난다.</p>
<p>돌이켜보면, SLAM 이전부터 <strong>나는 무언가를 공부할 때 항상 말로 풀어내는 cause &amp; effect 방식으로 먼저 익혔다</strong>.<br>전체적인 흐름을 읽는데는 효과적이였으나, 실제 구현이나 깊은 이론을 이해할 때는 항상 빈 부분들이 나타났다.<br>그리고 그때마다 다시 공부하여 수학적인 이해를 채웠다.</p>
<p>확실히 이 방법은 빠르게 많은 내용을 익히는데는 도움이 되지만, 결국에는 다시 깊게 공부해야한다는 단점이 있다.<br>현재의 내 포지션에서 <strong>이 방법이 가장 효과적이고 효율적일까?</strong><br>배울건 아직 너무나도 많기 때문에 속도를 완전히 포기할 수는 없다.</p>
<hr>
<h1 id="cv-learn-블로그"><a href="#cv-learn-블로그" class="headerlink" title="cv-learn 블로그"></a>cv-learn 블로그</h1><p>내 이전 블로그인 <a href="http://cv-learn.com/"><strong>cv-learn 블로그</strong></a>가 <strong>Notion Awards에서 거론되었다!</strong> (우승하지는 못했지만 ㅎㅎ) </p>
<img src="/20201231-retro/cv-learn.jpg" class="" title="cv-learn">

<p>cv-learn 블로그에는 올해 약 9~10월까지 글을 작성했다.<br>이 중 <a href="https://cv-learn.com/Outlier-Robust-estimation-0-Outlier-fa0c9cb92fdc4de79900118766c55cbf"><strong>Robust estimation 글</strong></a>, <a href="https://cv-learn.com/50-CV-SLAM-a6c06c0fbd824bc98572169a5a5e6793"><strong>50가지 SLAM 기술 면접 질문 글</strong></a>, <a href="https://cv-learn.com/C-10ca8877f435428bbf38f71585bbf9c1"><strong>C++ 공부 글</strong></a>이 가장 기억에 남는다.</p>
<p><strong>내가 공부한 것을 글로써 적어내는 것은 정말로 효과적인 방법</strong>인 것 같다.<br>인앤아웃으로 이해해야 나만의 방식으로 온전히 표현할 수 있기 때문이다.<br>하지만 생각보다 시간이 많이 들어가기에… 절대 시간대비 효율적이라고 보지는 않는다.</p>
<img src="/20201231-retro/cv-learn2.png" class="" title="cv-learn">


<p>내가 알고 있다고 생각했던 것들도 글을 적다보면 <strong>‘이게 맞는건가?’ 라는 질문</strong>이 생긴다.<br><strong>이런 의심이 들 때 포기하지 않고 꼬리에 꼬리를 물고 가는 것이 중요하다</strong>.<br>그러다보면 어느순간 누구에게도 설명해줄 수 있을 것 같은 자신감이 들게 된다.</p>
<p>하지만 이와 별개로, <strong>글을 적을 때는 모멘텀이 중요하다</strong>고 생각한다.<br>적기 시작한 순간부터 3일을 넘긴다면 그 글은 절대 끝나지 않을 것이다.<br>실제로 이번 해에 적다가 포기한 글만 20개는 넘어가는 것 같다.</p>
<p>글을 적을 때도 <strong>목적과 마일스톤</strong>을 확실히 잡고, <strong>이슈가 생길때마다 하나씩 빠르게 잡아가는 것</strong>이 중요하다고 생각한다.</p>
<hr>
<h1 id="새-Github-blog"><a href="#새-Github-blog" class="headerlink" title="새 Github blog"></a>새 Github blog</h1><p>cv-learn은 올해 약 9~10월정도까지 사용하였고, 무료 호스팅의 daily request 리밋 문제로 인해 현재는 <a href="https://changh95.github.io/"><strong>changh95.github.io</strong></a>로 옮겼다.<br>아직 새 블로그에는 제대로 SEO 셋팅이 충분히 되지 않았다. (얼른 해야지…)</p>
<p>새 블로그는 <strong>hexo</strong>를 사용해서 만들었다.<br>심플한 레이아웃에 github 계정으로 답글을 다는 시스템을 도입했는데, 아주 마음에든다 <span class="github-emoji" alias="smiley_cat" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63a.png?v8">😺</span></p>
<img src="/20201231-retro/gh.png" class="" title="changh95.github.io">

<p>새 블로그에서는 11월 24일부터 시작해서 <strong>가능한 빠르게 매일 글을 적어보는 실험</strong>을 하고 있다.<br><a href="https://changh95.github.io/categories/C/"><strong>실시간 컴퓨터 비전을 위한 C++ 최적화</strong></a> 시리즈를 번역하면서 약 2주동안 매일 글을 적는 버릇을 만들었다.<br>그리고 지금은 <a href="https://changh95.github.io/categories/SLAM/"><strong>CVPR, IROS와 같은 국제 학회의 SLAM 워크샵 내용을 정리한 글</strong></a>을 올리고 있다.</p>
<p>2021년 상반기까지는 아마 학회영상 위주로 적을 것 같다.<br>그 후에는 기존의 cv-learn을 github 블로그로 전부 올려야한다 (꽤 많은 양이다!)</p>
<hr>
<h1 id="지름-지름-지름"><a href="#지름-지름-지름" class="headerlink" title="지름, 지름, 지름"></a>지름, 지름, 지름</h1><img src="/20201231-retro/buy.png" class="" title="Jireum">

<p><strong>가전제품 최고야…</strong></p>
<p>델 모니터 2대, 커스텀 기계식 키보드, 노이즈 캔슬링 헤드폰, 갤럭시 S20 울트라, 델 노트북…</p>
<p>오래 쓸 수 있는 제품들 + 만족감 위주로 질렀던 것 같다.<br>하지만 아직 알뜰한 구매 프로세스를 모르다보니 ‘흑우’소리를 많이 들었다 ㅠㅠ<br>업무에 도움이 될 수 있는 용품, 업무에 자존감을 갖게해주는 용품, 롱런에 도움이 되는 용품이 주가 된다.</p>
<p>하지만 그래도 슬슬 <strong>소비보다는 저축과 투자</strong>를 해야하지 않을까 싶다.</p>
<hr>
<h1 id="공부-욕심"><a href="#공부-욕심" class="headerlink" title="공부 욕심"></a>공부 욕심</h1><p><strong>Getting out of my comfort zone</strong>이 이번 해의 업무를 정확하게 표현하는 것 같다.</p>
<img src="/20201231-retro/never.png" class="" title="but i" alt="m not walking alone">

<p>작년에는 연구소였던 우리 팀이 개발팀으로 바뀌면서, <strong>이번 해에는 연구보다는 엔지니어링에 치중</strong>했다.<br>그렇기에 이번 해 내 업무중에는 논문을 읽거나 SLAM과 관련있던 업무는 하나도 없었다.<br>처음에는 논문을 읽고 구현하는 작업을 하고 싶었지만, 어차피 <strong>senior가 되기 위해서는 엔지니어링 스킬도 받쳐줘야 한다</strong>고 생각했다.</p>
<p>그렇기에 회사에서는 엔지니어링, 퇴근 후에는 SLAM 논문을 읽는 <strong>주경야독</strong>을 했다 ㅋㅋ.<br><strong>SLAM과 연구/논문은 그냥 취미</strong>였고, <strong>회사에서는 순전히 엔지니어링</strong>만 했다.<br>그리고 언젠가 엔지니어링 기반이 잡히면, 내가 읽어온 논문 중에서 기발한 아이디어를 만들어낼 수 있지 않을까! 하는 생각을 했다 ㅋㅋ</p>
<br>

<img src="/20201231-retro/lol.png" class="" title="lol">

<p>하지만 <strong>쉽지 않았다</strong>… ㅠㅠ<br>처음 해보는 것들이 정말 많았다.</p>
<ul>
<li>시뮬레이터</li>
<li>모던 OpenGL</li>
<li>모던 C++</li>
<li>파이썬 + 주피터</li>
<li>실험 자동화 인프라 구축<br>등등…</li>
</ul>
<p>무언가를 처음 배울 때는 ‘<strong>너무 시간을 뺏기지 말고, 빠르게 무언가를 만들자</strong>‘ 라는 생각을 했다.<br><strong>그럼에도 숙련된 팀원의 눈에는 내 속도가 생각보다 느리다는 평</strong>을 받기도 했다.<br>숙련자의 눈에서 봤을 때 개발 속도가 느린건지, 아니면 처음하는 것을 고려해서 늦은건지는 아직 잘 모르겠다.</p>
<p>배우는 방식에 대해 다시 한번 생각을 해봐야할 것 같다.<br><strong>빠르게 배우려면 어떻게 해야하까?</strong><br>현재 내린 결론으로는, 우선 <strong>많이 하다보면 나만의 방식을 익힐 것</strong>이라고 생각한다.<br>그러니 2021년에도 <strong>포기하지 말고, 떨지말고 꾸준히 배워야겠다</strong>. </p>
<img src="/20201231-retro/paper.png" class="" title="Read papers">

<p>2020년 <strong>연차의 최소 절반은 배우기 위해 쓴 것 같다</strong>.<br>재밌는 논문이 나오면 반차를 써서 근처 카페로 가서 논문을 읽었다.<br>ORB-SLAM3 가 나왔을 때 진짜 부리나케 카페로 달려간게 기억난다 ㅋㅋ </p>
<p>물론 그 당시에는 신나서 그렇게 한 것도 있지만…<br>연말에 돌이켜보면, 오히려 내가 엔지니어링에 집중해서 빨리 끝냈으면 연차 없이 논문 볼 시간이 났지 않았을까… 하는 생각도 난다 ㅋㅋ</p>
<img src="/20201231-retro/conference.jpg" class="" title="conferences">

<p>이번 해 <strong>버추얼 학회들은 주경야독에 굉장히 효과적</strong>이였다.<br>학회 참가에 비용이 많이 들지 않았을 뿐 더러, 다들 밤 시간에 진행되었다.<br>집에 있는 노트북과 컴퓨터들 다 써서 관심있는 워크샵/튜토리얼들을 다 녹화하고, 오전에 출근하면서 지하철에서 봤다.<br>그 때 진작에 노트를 만들어 둘 것이 많이 후회된다 <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span></p>
<p>CVPR, ECCV, IROS, RAL, SIGGRAPH Asia에서 <strong>SLAM과 자율주행, 드론 관련 워크샵과 튜토리얼</strong>은 다 녹화해두고 상당수를 봤다.<br>최신 연구를 보면서 느끼는 것은, Geometric vision 분야도 이제 딥러닝이 거의 필수적으로 공부해야하는 것이다.<br>최신 연구에서는 Self-supervised learning, contrastive loss, self-attention 등등 <strong>최신 딥러닝 개념들도 충실하게 이해하는 것을 요구</strong>했다.<br>그리고 종종 새로운 아이디어를 NeurIPS나 ICLR 등 진짜 딥러닝 학회에 나온 연구에서 가져오는 경우가 있었다.<br>이 연구를 쫒아가기 위해서는 나도 <strong>딥러닝 공부 및 경험</strong>이 필요할 것 같다.</p>
<p>2021년에는 최신 SLAM 연구의 동향 파악이 끝나면 우선 NetVLAD와 딥러닝 피쳐 기술을 사용한 hierarchical localisation 기술을 먼저 확인해보고싶다.</p>
<img src="/20201231-retro/books.png" class="" title="books">

<p>2020년 내내 공부해야할 것은 점점 더 많이 보였고, 월급은 차곡차곡 들어오고, 욕심도 스멀스멀 올라왔다.<br>그러다보니 월급을 술술 책으로 다 빠졌다.</p>
<p>CPU가 아닌 <strong>GPU</strong>나 <strong>FPGA</strong>도 공부해보고 싶었고…<br><strong>수학</strong>도 다시 확실하게 잡고 싶었고…<br><strong>딥러닝</strong>도 공부하고 싶었고…<br>최근에는 <strong>C++</strong> 쪽을 깊게 보고 있다 ㅋㅋ</p>
<p>2021년에는 공부하기에 부족함은 없을 것 같다 ㅋㅋ…<br>대신 2020년에는 이곳저곳 방황했으니… 2021년에는 <strong>한 뱡향을 확고하게 잡은 후 어느정도 익힐 때 까지 방향을 돌리지 않으려고 한다</strong>.</p>
<hr>
<h1 id="논산-훈련소"><a href="#논산-훈련소" class="headerlink" title="논산 훈련소"></a>논산 훈련소</h1><img src="/20201231-retro/NOnsan.png" class="" title="no...">

<p>2021년의 하이라이트 중 하나는 나라의 부름을 받아 <strong>논산 훈련소</strong>를 다녀온 것이다.<br>사진은 영장을 건네주시는 우리 팀장님… ㅋㅋㅋ</p>
<p>연구원들을 많이 배려해주신 덕분에, <strong>읽고싶던 논문과 책을 많이 읽고 왔다</strong>.<br>Bundle adjustment, ORB-SLAM, SVO, DSO 논문을 읽고 왔는데, 그 때 SLAM 분야에 대한 이해도가 전체적으로 많이 좋아진 것 같다.</p>
<hr>
<h1 id="2021년에-되고-싶은-모습"><a href="#2021년에-되고-싶은-모습" class="headerlink" title="2021년에 되고 싶은 모습"></a>2021년에 되고 싶은 모습</h1><p>2020년에는 너무 감사한 일이 많았다.<br>2021년에는 나도 <strong>주변인들에게 선한 영향력을 만드는 사람</strong>이 되고 싶다.<br>인간적으로나, 업무적으로나, 기술적으로나…</p>
<p>그러기 위해서는 다음 3가지가 필요하다고 생각한다.<br>이 3개를 한번에 얻기는 어렵지만… 노력을 게을리 하진 않을것이다 <span class="github-emoji" alias="smiley_cat" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63a.png?v8">😺</span></p>
<ul>
<li><strong>여유</strong></li>
<li><strong>구현력</strong></li>
<li><strong>학습력</strong></li>
</ul>
<p>사람이 너그러워지고, 누군가에게 힘이 되주기 위해서는 <strong>내면에 여유</strong>가 있어야 한다고 생각한다.</p>
<p>내면에 여유를 가지기 위해서는 업무적으로 데드라인 등에 쫒기지 않아야한다.<br>그런 의미로 <strong>업무적으로 요구되는 것을 빠르게 구현</strong>할 수 있어야한다.<br>실력만 없고 성격이 좋은 사람이 더 독이 된다는 말도 있다.</p>
<p>마지막으로, 모든게 빠르게 바뀌는 기술분야인만큼 <strong>현재의 상태에서 벗어나 새로운 모습으로 바뀌는 것을 두려워하면 안된다</strong>.<br>어제는 딥러닝을 안 쓰다가, 오늘은 텐서플로우/파이토치를 써야하고, 내일은 IPU나 FGPA를 하고 있을수도 있다.<br>현재 상태에서 best를 찾되, 새로운 패러다임이 나타나면 바뀌어야할 줄 알아야한다.</p>
<p>이런 모습으로, 내년에 소중한 인연들과 함께 <strong>또 멋진 일</strong>을 하고싶다 <span class="github-emoji" alias="heart_eyes" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60d.png?v8">😍</span></p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>Retrospective</tag>
      </tags>
  </entry>
  <entry>
    <title>카메라 센서 사이즈의 영향</title>
    <url>/20210116-sensor-size/</url>
    <content><![CDATA[<h1 id="카메라-센서란"><a href="#카메라-센서란" class="headerlink" title="카메라 센서란?"></a>카메라 센서란?</h1><p>통상적으로 카메라 센서는 카메라 내부 이미징 센서를 뜻한다.<br>이미징 센서는 빛을 받아드려 전자신호로 바꾸는 역할을 하는데, CMOS 와 CCD 타입이 있다.</p>
<hr>
<h1 id="카메라-센서-사이즈-종류"><a href="#카메라-센서-사이즈-종류" class="headerlink" title="카메라 센서 사이즈 종류"></a>카메라 센서 사이즈 종류</h1><img src="/20210116-sensor-size/Untitled.png" class="" title="Sensor sizes">

<p>카메라 센서 사이즈에는 굉장히 많은 종류가 있다</p>
<p>위 이미지는 <strong>센서 사이즈 가이드라인</strong>이다. 많은 센서 제품들이 저 가이드라인을 따라 디자인 되지만, 사실 꼭 저 사이즈를 따르라는 법은 없다. 실제로 가이드라인을 따른 줄 알았던 센서를 직접 재봤을 때 사이즈가 가이드라인과 다를 때도 있다. 이런 경우에는 대부분 inch 단위의 사이즈 가이드라인을 정확히 따른게 아닌, mm로 개발을 마치고 나서 가장 가까운 inch 사이즈에 맞춰서 1/xx inch 센서로 표기한 경우이다. </p>
<ul>
<li>아이폰 XS, 갤럭시 10에 들어간 센서 사이즈는 1/2.5” 이다.</li>
<li>FLIR사의 BlackFly 시리즈에서 아무거나 골라본 카메라에는 1/2.9” 사이즈를 쓴다고 한다.</li>
<li>들고다니기 좋은 여행용 카메라에는 1” 센서가 자주 쓰인다.<ul>
<li>카메라 커뮤니티에서는 1” 센서가 가장 무게/시간/가격/성능에 대한 최적화가 잘 되어있는 센서라고 알려져있다.</li>
</ul>
</li>
<li>1”부터 APS-C 급 까지 진지한 취미용 카메라 취급을 받는다.</li>
<li>프로페셔널 이미징에는 Full Frame 카메라가 필요하다고 한다.</li>
</ul>
<p>이와 같이 여러 용도로 카메라 센서 사이즈가 나뉘고, 오른쪽 이미지를 보면 이 카메라 센서들마다 실제로 사이즈가 많이 차이나는 것을 볼 수 있다.</p>
<hr>
<h1 id="해상도와의-관계"><a href="#해상도와의-관계" class="headerlink" title="해상도와의 관계"></a>해상도와의 관계</h1><img src="/20210116-sensor-size/Untitled%201.png" class="" title="Resolutions">

<p>해상도는 카메라 센서를 얼마나 잘게 쪼개서 많은 픽셀을 만들 수 있는가에 대한 척도이다.<br>FullHD (1920 x 1080) 해상도의 경우, 이미징 센서 위에 가로로 1920개의 픽셀을 얹고, 세로로 1080개의 픽셀을 얹은것이다.<br>이 픽셀 하나하나가 0에서 255의 밝기 값을 가지고 픽셀들을 전부 다 같이 붙혀놓았을 때 이미지가 완성이 된다.</p>
<p>이 뜻은 큰 카메라 센서든 작은 카메라 센서든 똑같은 grid로 쪼개면 모두 같은 해상도를 가진다는 것이다.<br>예를 들어 Full Frame도 1920 x 1080으로 쪼갤 수 있고, 1/3.2”도 1920 x 1080으로 쪼갤 수 있다.<br>이 때문에 <strong>해상도와 카메라 센서 사이즈는 설계 상 관계가 없다</strong>.</p>
<p>하지만 그렇다고해서 해상도와 센서 사이즈의 성능 상 관계가 없다는 것은 아니다.<br>좋은 이미지를 찍기 위해서는 이미징 센서의 해상도와 사이즈의 <strong>성능/가격 상의 관계</strong>를 잘 이해하고, 이에 맞는 센서 구성을 선택해야한다.</p>
<p>해상도는 사진 속 공간에 대한 정보가 얼마나 오밀조밀한지 표현하는 척도가 된다.<br>해상도가 낮다면, 예를 들어 하나의 사진을 10개의 픽셀로만 표현하려고 하면, 단순히 10개의 블록 색상으로 표현해야하기 때문에 사진이 무엇을 의미하는지 알기 어렵다.<br>반대로 해상도가 높다면, 예를 들어 FullHD 라면 1920 x 1080개의 블록 색상으로 사진을 표현할 수 있고, 이는 꽤 정확한 이미지를 얻어낼 수 있다.<br><strong>이미지 해상도가 높으면, 더 미세하게 공간을 표현할 수 있다.</strong> </p>
<p>하지만 높은 해상도는 더 잘게 쪼갠 이미지 센서를 뜻한다.<br>이미지 센서를 작은 픽셀로 쪼갰다면 각각의 픽셀은 작은 면적을 가지게 되는데, 빛을 받을 수 있는 면적이 작기 때문에 받을 수 있는 빛의 신호가 작게 된다.<br>작은 신호를 읽기 위하여 우리는 이 신호를 증폭시키는데, 이 때 고정적으로 들어오는 전자기적 신호 역시 함께 증폭이 되면서 <strong>높은 해상도의 센서에는 대체적으로 높은 노이즈가 측정된다</strong>.</p>
<p>그러면 카메라 센서 사이즈는 어떠한가?<br>카메라 센서가 크면 보통 빛을 받을 수 있는 면적이 더 커진다.<br>이 때문에 카메라에는 이미지 노이즈가 적고, 더 큰 dynamic range를 가지게 된다 (Dynamic range에 대해서는 추후 글에서 더 소개한다).<br>이 때문에 <strong>카메라 센서 사이즈가 더 크면 어두운 곳에서 촬영을 할 수가 있고, 또 이미지 노이즈가 적어지기 때문에 더 좋은 이미지를 얻어낼 수 있다고 볼 수 있다</strong>.</p>
<p>그러면 큰 이미지 센서에 높은 해상도를 쓰면 장땡 아닌가?<br>큰 센서에서는 픽셀을 잘게 쪼개도, 작은 센서에서 쪼갠거보다는 픽셀이 몇배나 클 것이다.<br>그만큼 노이즈도 적게 나올 것이니, 큰 이미지 센서 + 높은 해상도 조합을 사용하면 깔끔한 이미지가 나오게 된다.<br>하지만, 카메라 제조 공정을 생각했을 때… 큰 이미지 센서는 더 많은 반도체 wafer의 면적을 가져가는거고, 높은 해상도를 제조하는 공정 역시 비싸기 때문에…<br>*<em>큰 이미지 센서 + 높은 해상도의 조합은 상당히 비싸다**</em></p>
<p>작은 센서에 높은 해상도를 만드는게 더욱 정밀 작업처럼 보여서 비싸보일 수 있으나, 실제로는 큰 이미지 센서가 더욱 비싸다.<br>그 이유는, wafer에서 작은 이미지 센서를 100개 제작하고 그 중 5개가 실패했을때의 나오는 손실비용과, wafer에서 큰 이미지 센서 10개를 제작하고 그 중 5개가 실패했을때의 나오는 손실비용은 wafer의 고정 가격으로부터 나온다.<br>공정의 가격보다 재료 + risk의 가격이 더 크다.</p>
<hr>
<h1 id="사진-크기와의-관계"><a href="#사진-크기와의-관계" class="headerlink" title="사진 크기와의 관계"></a>사진 크기와의 관계</h1><img src="/20210116-sensor-size/Untitled%202.png" class="" title="Image size">

<p>같은 구성의 렌즈를 사용했다는 전제 하에, <strong>카메라 센서가 커진다면 담을 수 있는 사진의 크기도 커진다</strong>.<br>여기서 이야기하는 ‘사진의 크기’는 디지털 이미지 사이즈가 아니라 센서가 담을 수 있는 3D 공간의 크기이다.<br>디지털 이미지 사이즈는 작은 센서나 큰 센서나 같은 해상도를 가지고 있다면 똑같다.<br>다만 카메라 센서 사이즈에 맞춰서 3D 공간에서 온 빛을 받기 때문에, 큰 카메라 센서를 사용할 때는 더 큰 3D 공간의 형상을 사진에 담을 수 있다.</p>
<p>“어, 근데 내 DSLR로 찍으나 스마트폰으로 찍으나 사진 안에 담기는 3D 공간은 비슷한 것 같은데??” 라고 물어보실 수 있다.<br>스마트폰 카메라 센서의 경우 1/2.5” 정도로 작기 때문에 실제로 빛을 받을 수 있는 공간은 매우 적은게 맞다.<br>(어떻게 연결할지는 나도 모르지만), 스마트폰 카메라와 DSLR 카메라가 같은 렌즈를 사용했다면, 당연히 DSLR 카메라가 더 넓은 공간을 사진에 담을 수 있다.<br>하지만, 실제로는 센서 사이즈에 맞춰서 알맞은 focal length를 가진 렌즈를 이용하기 때문에, 빛이 같은 각도로 들어올 수 있게 만듬으로써 같은 시야각을 유지할 수 있다.</p>
<p>여기서 우리는 focal length를 이용함으로써 시야각을 다르게 바꿀 수 있다는 것을 알 수 있다.</p>
<br>

<img src="/20210116-sensor-size/Untitled%203.png" class="" title="lens">

<p>그러면 이제 “다른 렌즈를 사용해서 시야각의 문제가 해결된다면, 굳이 큰 카메라 센서를 쓸 필요가 없는게 아니야?” 라고 물어보실 수 있다.<br>맞는 말이지만, 시야각이 넓은 렌즈를 사용할 때 보통 더 큰 distortion이 생긴다.<br>렌즈 distortion은 렌즈의 굴절에 따라 이미지 역시 구겨지는 현상을 뜻하는데, 보통 카메라 calibration을 통해서 보정처리를 함으로써 깔끔한 이미지로 펴낼 수 있으나, 렌즈의 외곽으로 갈 수록 (특히나 저렴한 렌즈는 더더욱) 이미지의 질이 급격하게 떨어진다.  </p>
<p>아래 이미지를 보면, 이미지 중앙 부분은 깔끔하나, 외곽의 이미지의 질은 좋지 않은 것을 볼 수 있다.<br>작은 카메라 센서들은 큰 공간을 보기 위해 굴절률이 높은 렌즈를 사용해야하고, 이는 더 큰 이미지 질의 저하로 이어진다.<br>그에 비해 큰 카메라 센서는 굴절률이 그렇게까지 높은 렌즈를 사용하지 않아도 되기 때문에, 코너 부분에서 질이 떨어지는 정도이다.<br>큰 카메라 센서를 사용할때는 보통 이 코너 부분을 잘라내버려서 이쁜 이미지를 얻어낼 수 있는데, 작은 카메라 센서에서는 이 방법이 불가능하고 대신 이미지 전체에 보정처리를 해야한다.</p>
<p>정리하면, <strong>카메라 센서가 크면 더 넓은 장면을 이미지로 담을 수 있지만, 렌즈로써 극복이 가능하다.</strong><br><strong>다만, 큰 카메라 센서의 경우에는 렌즈에 의한 이미지의 질 저하가 작은 카메라 센서보다 낮다</strong>.</p>
<img src="/20210116-sensor-size/Untitled%204.png" class="" title="lens-2">
<hr>
<h1 id="이미지-노이즈와-Dynamic-Range에-대한-관계"><a href="#이미지-노이즈와-Dynamic-Range에-대한-관계" class="headerlink" title="이미지 노이즈와 Dynamic Range에 대한 관계"></a>이미지 노이즈와 Dynamic Range에 대한 관계</h1><img src="/20210116-sensor-size/Untitled%205.png" class="" title="Dynamic range">

<p>카메라 센서가 크다면 빛을 받을 수 있는 면적이 커지기 때문에, 같은 시간동안 더 많은 양의 빛을 받을 수 있다.<br>위의 그래프를 본다면, 그래프의 오른쪽으로 가면 갈수록 더 많은 양의 signal, 즉 빛을 받을 수 있다는 것을 볼 수 있다.<br>또, 밑에 노이즈의 양은 항상 일정하게 있다는 것을 볼 수 있는데, 이는 빛→전자로 변환할 때 생기는 오류, 렌즈를 통하면서 생기는 빛의 변화, 센서 내부의 노이즈 등등 여러가지 노이즈 인자들을 간단하게 모델링 한 것이고, 그래프에서 처럼 무조건 일정하지는 않으나 꽤 작은 값이라는 것을 표현한 것이다.<br>이 그래프가 의미하는 것은, 카메라 센서가 커질 수록 Signal-to-Noise Ratio 가 커진다는 것이다.<br>SNR이 커진다는 것은 노이즈가 있음에도 불구하고 선명한 화질의 사진을 만들 수 있다는 것이고, <strong>큰 카메라 센서일수록 Signal은 많이 받지만 Noise는 일정하기 때문에 높은 SNR, 즉 선명한 사진을 만들 수 있다는 점을 말한다.</strong></p>
<p>또 그래프에서 볼 수 있는 것은, 센서 사이즈마다 최대로 받을 수 있는 빛의 양이 정해진다는 것이다.<br>셔터 시간이 정해져있다면 센서의 면적이 넓은 편이 더 많은 빛을 받을 수 있고, 최대-최소 빛의 양을 정규화 한다고하면 <strong>큰 카메라 센서는 작은 카메라 센서에 비해 더 큰 빛의 양의 range를 가지게 된다.</strong><br>이 range를 보통 Dynamic range라고 칭하며, 더 큰 dynamic range를 가진 이미징 장비의 경우 high dynamic range imaging (HDR)이라고 한다.<br>Dynamic range가 작다면, 0-30의 intensity range에서 어두운 부분은 0, 1, 1, 1, 3, 2 … 이렇게 나오며, 0,1,2,3의 차이를 알기가 쉽지 않은데, HDR의 경우 0-3000의 intensity range의 경우는 어두운 부분이 0, 24, 46, 64, 38 등으로 차이가 극명하게 나기 때문에, 같은 어두운 부분이라도 더 디테일하게 표현이 가능하고, 밝기보정에 더 강건하다 (0-30과 0-3000은 임의로 잡은 숫자이다).<br>이 때문에, <strong>카메라 센서가 커질 수록, dynamic range는 더 커지며, 이를 통해 어두운 환경에서도 좋은 이미지를 얻을 수 있다.</strong></p>
<img src="/20210116-sensor-size/Untitled%206.png" class="" title="Dynamic range - 2">

<p>우측이 훨씬 잘보인다. 우측은 HDR!</p>
<img src="/20210116-sensor-size/Untitled%207.png" class="" title="Dynamic range -3">

<p>Dynamic range가 낮다면 찍기 어려울 사진이다.</p>
<hr>
<h1 id="사진-찍는-속도와-ISO에-대한-관계"><a href="#사진-찍는-속도와-ISO에-대한-관계" class="headerlink" title="사진 찍는 속도와 ISO에 대한 관계"></a>사진 찍는 속도와 ISO에 대한 관계</h1><p>카메라가 사진을 찍을 때, 우선적으로 내부 회로에서 센서에 빛을 담으라는 신호를 주고 셔터를 닫으면서 빛의 유입을 멈춘다.<br>더 이상 빛의 유입이 없을 때, 그때까지 모아온 신호를 전자화 시켜 사진을 reconstruct 한다.<br>큰 카메라 센서와 작은 카메라 센서가 있을 때, 셔터가 똑같은 속도로 내려온다면 큰 카메라 센서는 셔터가 이동해야할 거리가 작은 카메라 센서보다 길기 때문에, 셔터가 내려오는 속도가 같더라도 작은 카메라 센서의 셔터가 먼저 닫힐 것이다.<br>이 점은, 작은 카메라 센서가 더 빨리 사진을 찍을 수 있다 (셔터 시간이 적다)라고 생각할 수 있고, 이는 비디오 촬영 또는 초고속 비디오 촬영에 유용할 것이라고 생각할 수 있다.<br>하지만, <strong>실제로 초고속 비디오 촬영에는 큰 센서를 이용한다. 이유는 아래에서 확인한다.</strong></p>
<p>사진을 더 빠르게 촬영하려고 셔터 스피드를 늘렸다고 해보자.<br>셔터 스피드가 빨라지면 그만큼 센서가 빛에 노출되는 시간이 적어지기 때문에 signal 약해지고, 이는 사진 전체를 어둡게 한다.<br>빠른 셔터 스피드로 밝은 이미지를 촬영하려면 signal을 늘려야 하는데, 물리적으로는 불가능하니까 전자 신호를 증폭시키면 된다.<br>이때 <strong>빛을 전자신호로 변환시킬 때 신호 증폭에 대한 계수를 ISO라고 한다</strong>. </p>
<p>엄청나게 빠르게 사진을 찍으려고 하면 셔터 스피드를 엄청나게 빠르게 하여 부족해진 빛 만큼 ISO를 높혀 신호를 증폭시킬 수 있다.<br>또, aperture를 엄청 줄여서 depth 조정을 할 때 빛의 input이 많이 줄어드는데, 이때도 똑같은 방법으로 ISO를 높혀서 밝은 이미지를 얻을 수 있다.<br>다만 ISO를 높일 경우, signal에 기본적으로 내재되어있는 noise 값도 함께 증폭되며, 동시에 의미있는 signal의 range가 줄어들기 때문에 dynamic range가 줄어든다.<br>작은 카메라 센서는 SNR이 낮기 때문에 (전체 signal양 속 noise의 비율이 높기 때문에), ISO를 높히면 noise값도 빠르게 커진다.<br>그에 비해 큰 카메라 센서는 SNR이 높기 때문에, ISO를 높혀도 noise값이 차지하는 비율이 커지는 속도가 작은 카메라 센서보다 훨씬 느리다.<br>그러기에 <strong>큰 카메라 센서는 작은 카메라 센서보다 더 높은 ISO 한계를 가지고 있다</strong>는것을 알 수 있다. </p>
<img src="/20210116-sensor-size/Untitled%208.png" class="" title="ISO">

<p>(말 나온 김에 ISO와는 관계 없지만) 초고속 카메라에서 큰 센서를 이용하는 이유는 하나 더 있다.<br>초고속 촬영에는 높은 셔터 스피드와 높은 ISO 값을 가져야 하는데, 생각보다 이 ‘높은 셔터 스피드’를 구현하는게 쉽지 않다.<br>셔터는 소프트웨어가 아니라 기계적으로 움직이는 것이기 때문에, 정밀하고 빠르게 움직이기 위해서는 이를 다루는 장비 역시 복잡한 형태를 가지게 되고, 자연스럽게 크기가 커지게 된다.<br>또, 빠르게 이미지를 찍어내다보면 그 이미지 용량 역시 엄청나게 되는데, 이를 저장할 수 있는 메모리가 필요해지고, 또 내부 프로세서도 좋아져야 하며, 이걸 모두 감당하기 위한 파워도 좋아져야한다.<br>이 때문에 전체적으로 auxiliary circuit이 비대해지며, 카메라의 사이즈가 커진다.<br>그러기에 초고속 카메라는 어차피 소형화를 목적으로 한 것이 아니기에 큰 센서를 사용해도 무방하며, 또 큰 센서를 사용하면 ISO와 노이즈 문제를 해결할 수 있다.<br>또, <strong>초고속 카메라의 원래 목적에 대해서 생각을 하면 ‘빠른 것을 촬영해서 느리게 봄으로써 분석을 할 수 있다’라는 것인데, 분석을 위해서는 깨끗한 이미지가 필요하고 깨끗한 이미지를 위해서는 초고속 촬영 중 노이즈가 적은 큰 카메라 센서가 요구된다.</strong></p>
<img src="/20210116-sensor-size/Untitled%209.png" class="" title="ISO-2">

<p>낮은 ISO와 높은 ISO. 낮은 ISO는 빛을 받아드리는 동안 물체가 움직여서 blur가 나타날 수 있지만, 높은 ISO는 말 그대로 snapshot을 얻어낼 수 있다.</p>
<p>정리하자면, <strong>큰 카메라 센서는 작은 카메라 센서보다 더 높은 ISO 값을 가지고 사진을 빠르게 촬영할 수 있으나, 기본적으로 같은 ISO 값에서는 작은 카메라 센서의 촬영속도가 더 빠르다</strong>.<br>특수 촬영효과를 노리고 ISO를 높힌게 아니라 촬영속도를 높히려고 ISO를 높힌것이라면, 큰 카메라 센서 vs 작은 카메라 센서에 대한 실험을 해보는 것도 좋을 것 같다. </p>
<hr>
<h1 id="휴대용성에-대한-관계"><a href="#휴대용성에-대한-관계" class="headerlink" title="휴대용성에 대한 관계"></a>휴대용성에 대한 관계</h1><p><strong>카메라 센서 사이즈가 커지면 당연히 그에 맞춰 센서의 무게도 늘어난다.</strong><br>또, 카메라 센서 사이즈에 맞춰서 렌즈도 같이 커지기 때문에 무게가 급격하게 더 많이 늘어난다.<br>Full-Frame 카메라의 경우 다행히도 가방에 넣어서 들고다닐 수 있는 정보이고, 1/3” 센서의 경우 들고다니는 카메라라기 보다는 정말 작은 센서라고 생각하고 들고다녀야한다.</p>
<img src="/20210116-sensor-size/Untitled%2010.png" class="" title="Size">

<p>1/3” 센서 크기의 HTC One 카메라와, Full Frame의 Canon 5D Mark II 카메라</p>
<hr>
<h1 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h1><p><strong>목적에 따라 카메라를 잘 선정하는 것이 중요하다.</strong> </p>
<p>무게가 중요하다면 작은 카메라 센서, 좋은 이미지가 필요하다면 되도록이면 큰 이미지 센서를 찾겠지만, <strong>현재 해결하려는 연구의 목적과 end-consumer가 어떤 형태의 비전 시스템을 사용하는게 자연스러울지 생각하는 것도 중요하다.</strong> </p>
<p>모바일 환경에서 비전 카메라가 필요하다면 당연히 모바일 카메라를 쓰는게 자연스러울 것이고, 별을 관측하는 카메라가 필요하다면 당연히 큰 이미지 센서를 찾는게 자연스러울 것이다. </p>
<p>Trade-off가 필요한 카메라 세팅도 있는데, 예를 들어 SLAM 연구를 한다고 할 때는 좋은 이미지도 필요하지만 빠른 속도와 가벼운 카메라가 선호되기 때문에, 원하는 specification을 잘 추려내서 카메라를 선정하는 것이 필요하다. </p>
<p>특수 카메라 시스템 (e.g. fibre imaging, spectroscopy)의 경우에는 HDR, 속도, 무게 등등이 고려대상이 아닐 수도 있으며, resolution만이 criteria가 된다거나, 또는 이미징 센서가 받을 수 있는 spectral band가 고려 대상일 수 있다. 이런 경우에는 원하는 specification을 만족하는 한, 작은 카메라를 쓰는게 아무래도 편할 것이다.</p>
<hr>
<h3 id="사진-출처-및-정보-출처-링크"><a href="#사진-출처-및-정보-출처-링크" class="headerlink" title="사진 출처 및 정보 출처 링크"></a>사진 출처 및 정보 출처 링크</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY2FtYnJpZGdlaW5jb2xvdXIuY29tL3R1dG9yaWFscy9kaWdpdGFsLWNhbWVyYS1zZW5zb3Itc2l6ZS5odG0=">Digital Camera Sensor Sizes: How it Influences Your Photography<i class="fa fa-external-link-alt"></i></span></p>
<p><a href="https://newatlas.com/camera-sensor-size-guide/26684/"></a></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGF2ZW1vcnJvd3Bob3RvZ3JhcGh5LmNvbS9jYW1lcmEtc2Vuc29yLXNpemUtZ3VpZGUjRHluYW1pY19SYW5nZV9JU09fU2Vuc29yX1NpemU=">Camera Sensor Size Photography Guide [Updated 2019]<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93YXRjaGd1YXJkdmlkZW8uY29tL3VsdHJhLXdpZGUtZHluYW1pYy1yYW5nZQ==">Ultra-Wide Dynamic Range - WatchGuard Video<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuZ2VhcmJlc3QuY29tL2Jsb2cvaG93LXRvLzE3LXR5cGVzLW9mLWNvbW1vbi1zY3JlZW4tcmVzb2x1dGlvbnMtMjg4Mw==">17 types of common screen resolutions<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9zbmFwc2hvdC5jYW5vbi1hc2lhLmNvbS9hcnRpY2xlL2VuL2xlc3Nvbi01LXdoYXQtaXMtaXNvLXNwZWVk">[Lesson 5] What is ISO Speed?<i class="fa fa-external-link-alt"></i></span> </p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
        <category>카메라 기초 시리즈</category>
      </categories>
      <tags>
        <tag>Camera</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 1월 SLAM 뉴스</title>
    <url>/20210128-2021-january-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h1 id="이번-달-내가-관심가지는-논문들"><a href="#이번-달-내가-관심가지는-논문들" class="headerlink" title="이번 달 내가 관심가지는 논문들"></a>이번 달 내가 관심가지는 논문들</h1><h2 id="시스템"><a href="#시스템" class="headerlink" title="시스템"></a>시스템</h2><details>
  <summary> Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDEuMDY4OTQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>MIT의 Luca Carlone 교수님 연구</li>
<li>Dynamic Scene Graph 라는 개념을 실제로 구현한 논문이다.</li>
<li>‘Kimera includes … visual-inertial SLAM, metric-semantic 3D reconstruction, object localization, human pose and shape estimation, and scene parsing’… OMG… 이전에 개발되던 Kimera의 끝판왕을 완성한 것 같다.</li>
<li>처음에 나는 Dynamic Scene Graph는 자율주행에서 만드는 HD-Map의 indoor 버전으로 보았다. <ul>
<li>HD-Map과 DSG 모두 Localization을 위한 point cloud layer를 가지고 있다. 또 semantic layer를 HD-Map은 차선과 road sign으로 구현하고, DSG는 object-level map을 구현했다는 비스무리한 공통점이 있다고 생각했는데…</li>
<li>하지만 이번 논문으로 생각이 조금 달라졌다. 오히려 DSG가 더 큰 개념인 느낌? Hierarchical graph를 통해서 구조를 정의하고 search를 빠르게 할 뿐만이 아니라, semantic 한 정보를 저장하는 것도 훨씬 많을 수 있다는 생각이 든다. <ul>
<li>HD-Map에서 road sign 정보를 저장하는 것은 정말로 유턴, 좌회전 정보만을 저장하겠지만, DSG에서 object에 대한 정보를 적는거는 object에 대한 모든 정보를 저장하고 수많은 인터페이스를 연결할 수 있다. </li>
<li>예를 들어, 노트북에 대한 정보에 유저 auth 정보 등을 함께 저장한다면 정말로 ‘노트북켜줘’ 라는 말로 버츄얼 디스플레이를 띄워서 컴퓨터에 로그인 하는 방법도 가능할거라는 생각이 든다. </li>
<li>물론 HD-Map에도 장소에 대한 정보 + 인터페이스를 추가할 수 있겠지만, 인도어 환경인 DSG가 조금 더 가깝게 semantic + interface 정보를 저장할 수 있지 않을까 싶다. </li>
</ul>
</li>
</ul>
</li>
<li>근데 논문 왤케 길어~~~~  </li>
</ul>
</details>

<details>
  <summary> NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjAzMTc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>ICL의 Andrew Davison 교수님의 연구</li>
<li>이미 알려질대로 알려진 NodeSLAM 연구이지만, 제대로 publication으로 나온 것 같다. Neural shape descriptor를 code (오토인코더를 통해 벡터로 압축시켜 optimise를 가능하게 만든 형태)로 만들고 RGB-D 정보와 Uncertainty를 사용해서 camera pose, object shape, object pose, map shape를 한번에 최적화 하는 논문이다.</li>
<li>이전의 연구인 MoreFusion은 Shape prior를 알고있는 상황, Fusion++은 shape prior를 전혀 모르는 상황에서 사용했었다. MoreFusion의 경우 받았던 critic은 ‘정확하게 shape prior를 모르는 경우에는 어쩔꺼냐?’ 라는 것이였고, Fusion++에 대한 critic은 ‘instance segmentation으로 뽑은거는 너무 generalise된 것이 아닌가… shape 추정이 너무 안된다, 결과물이 완전 체리픽킹이다’ 라는 이야기를 들었다. <ul>
<li>이 두개의 critic을 한번에 깨주는 논문이 NodeSLAM이지 않을까 싶다. ‘정확한 shape prior는 없지만 대충 어떤 class가 있을 것인지 알고 있고, 해당 class에 대한 Shape은 최적화를 통해 정확히 얻어낸다. 그렇기 때문에 수많은 shape들에도 사용할 수 있고, 정확도도 높다!’ 라는것을 보여주는데… </li>
<li>역시 감탄만 나오는 Davison 교수님 (물개박수)</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> LOCUS: A Multi-Sensor Lidar-Centric Solution for High-Precision Odometry and 3D Mapping in Real-Time </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkyOTMzNTkv">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>MIT의 Luca Carlone 교수님 랩실 논문은 못참지</li>
<li>아카이브에 안뜨려나…</li>
</ul>
</details>

<details>
  <summary> Local to Global: Efficient Visual Localization for a Monocular Camera </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvTGVlX0xvY2FsX3RvX0dsb2JhbF9FZmZpY2llbnRfVmlzdWFsX0xvY2FsaXphdGlvbl9mb3JfYV9Nb25vY3VsYXJfQ2FtZXJhX1dBQ1ZfMjAyMV9wYXBlci5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>네이버 랩스 + ICCV 2019년도 Point &amp; Line SLAM의 이상준님의 논문</li>
<li>Visual localization 연구는 대부분 Hierarchical localization 방식을 따라 많이들 NetVLAD + SuperPoint 방식을 사용한다. <ul>
<li>이 방식은 잘 되지만 (?), SuperPoint가 아직 모바일 디바이스에서 실시간으로 슈슉 돌아가기 어렵다는 단점이 있다. </li>
<li>아무래도 연구자들은 ‘하드웨어 개발이 해결해주겠지’ 라고 생각을 하는 것 같다. </li>
</ul>
</li>
<li>이번 연구를 통해서 실시간 VO에 많이 사용되는 ORB를 SuperPoint을 조합하여 실시간성 + 정확도를 둘 다 잡아보는 SuperORB + SuperKeyframe 이라는 연구를 보여준다. </li>
<li>논문을 보면 저자 분이 VO 시스템에 대해 깊은 이해를 가지고 있다는 것을 볼 수 있다 (ㄷㄷ) 소스 코드 한번만 볼 수 있다면 소원이 없을듯</li>
</ul>
</details>

<details>
  <summary> ALVIO: Adaptive Line and Point Feature-based Visual Inertial Odometry for Robust Localization in Indoor Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTUwMDgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>카이스트에서 나온 VIO</li>
</ul>
</details>

<details>
  <summary> VINS-PL-Vehicle: Points and Lines-based Monocular VINS Combined with Vehicle Kinematics for Indoor Garage </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ2Mzk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>VIO에 visual keypoint와 line을 함께 쓰는 연구</li>
</ul>
</details>

<details>
  <summary> High Precision Vehicle Localization based on Tightly-coupled Visual Odometry and Vector HD Map </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ2NTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>VIO에 Vector HD-Map을 하나의 센서처럼 estimation에 포함시킨 연구</li>
</ul>
</details>

<details>
  <summary> A Versatile Keyframe-Based Structureless Filter for Visual Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTUxNzAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Object-Level Semantic Map Construction for Dynamic Scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMjA3Ni0zNDE3LzExLzIvNjQ1L3BkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>3D 모델 정보를 사용해서 detection을 하고 object tracking을 하고 SLAM을 하는 연구.<ul>
<li>MoreFusion과 비슷한 연구?</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Using Detection, Tracking and Prediction in Visual SLAM to Achieve Real-time Semantic Mapping of Dynamic Scenarios </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ2OTM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>ORB-SLAM2에 Object detection을 추가하고 estimation을 돌린 연구.</li>
</ul>
</details>

<details>
  <summary> DeepSurfels: Learning Online Appearance Fusion </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMTQyNDAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>ETH Zurich와 Microsoft Mixed Reality Labs의 Marc Pollefeys 교수님 </li>
<li>조금 더 읽어봐야할듯…</li>
</ul>
</details>


<h2 id="서베이"><a href="#서베이" class="headerlink" title="서베이"></a>서베이</h2><details>
  <summary> Survey and Evaluation of RGB-D SLAM </summary>

<ul>
<li><a href="(https://ieeexplore.ieee.org/abstract/document/9330596/)">논문 링크</a></li>
<li>Survey 논문은 언제나 환영!</li>
<li>최근 딥러닝 기반 Single image depth estimation 기능을 통해 monocular camera로도 depth map을 얻어내어 visual odometry에 포함시키는 연구가 많이 진행되고 있다. <ul>
<li>물론 딥러닝 기반 depth estimation은 센서로 얻어낸 정보보다는 많이 부정확하기에 기존의 RGB-D SLAM 파이프라인을 쓰는 것 처럼 사용하기는 어렵다. </li>
<li>Depth 값을 error cost로 사용해 정확한 모델을 얻어내기보다는 search space를 줄여주는 하나의 가이드 정도로 사용해준다면, 아니면 depth 값을 사용하여 surfel 값을 얻어낸다던지 free space를 찾는다던지로 monocular에서는 불가능했던 새로운 기능을 추가할 수 있지 않을까 생각을 하게 된다.</li>
</ul>
</li>
<li>이런 방식들을 실제로 구현해내려고 한다면 기존의 RGB-D SLAM에 대한 이해가 필요하다고 보는데, 이 survey 논문이 굉장히 깊게 설명해주는 것 같다. 아이러브 서베이! ::inlove::</li>
</ul>
</details>

<details>
  <summary> A Survey on 3D LiDAR Localization for Autonomous Vehicles </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ4MTI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>LiDAR 데이터 localization 방식을 정리해준 survey 논문이라고??</li>
<li>Survey 논문은 사랑입니다</li>
</ul>
</details>

<details>
  <summary> Single-View 3D reconstruction: A Survey of deep learning methods </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwOTc4NDkzMjAzMDE4NDk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Single-view 3D reconstruction은 사실 잘 모르는 분야이지만, 어떻게보면 SLAM의 목적이 어느정도 이 분야에 들어가있다고 볼 수 있다.</li>
<li>Multi-view geometry로 3D reconstruction을 하는것이 Structure-from-Motion과 SLAM인데…<ul>
<li>그걸 Single-view로 하는 연구는 딥러닝만 가능하다고 보고 있다.</li>
<li>물론 아직 multi-view가 정확도가 더 뛰어난 편이 많기 때문에 (그리고 그 정확도는 localization에 중요하기 때문에) 아직 이 분야를 깊게 보지는 않았다.</li>
</ul>
</li>
<li>근데 보기 쉽게 survey로 만들어주다니!<ul>
<li>Survey 논문은 사랑입니다(3)</li>
</ul>
</li>
<li>SLAM쪽에서는 대부분 point cloud나 voxel, octree 데이터를 다루는데, 이 논문에서는 추가로 mesh, implicit surfaces, primitive 정보까지 다룬다.</li>
</ul>
</details>

<details>
  <summary> Deep learning, Inertial Measurements Units, and Odometry: Some Modern Prototyping Techniques for Navigation Based on Multi-Sensor Fusion </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL01hcnRpbl9Ccm9zc2FyZC9wdWJsaWNhdGlvbi8zNDU5ODg4MjVfRGVlcF9sZWFybmluZ19JbmVydGlhbF9NZWFzdXJlbWVudHNfVW5pdHNfYW5kX09kb21ldHJ5X1NvbWVfTW9kZXJuX1Byb3RvdHlwaW5nX1RlY2huaXF1ZXNfZm9yX05hdmlnYXRpb25fQmFzZWRfb25fTXVsdGktU2Vuc29yX0Z1c2lvbi9saW5rcy81ZmMxMDA3MDQ1ODUxNWI3OTc3OGEzOTUvRGVlcC1sZWFybmluZy1JbmVydGlhbC1NZWFzdXJlbWVudHMtVW5pdHMtYW5kLU9kb21ldHJ5LVNvbWUtTW9kZXJuLVByb3RvdHlwaW5nLVRlY2huaXF1ZXMtZm9yLU5hdmlnYXRpb24tQmFzZWQtb24tTXVsdGktU2Vuc29yLUZ1c2lvbi5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>이거 왜 EKF 설명 잘되어있지…?</li>
<li>근데 박사 논문이라서 211페이지 허우…</li>
</ul>
</details>


<h2 id="Relative-Absolute-pose-estimation"><a href="#Relative-Absolute-pose-estimation" class="headerlink" title="Relative / Absolute pose estimation"></a>Relative / Absolute pose estimation</h2><details>
  <summary> A Complete, Accurate and Efficient Solution for the Perspective-N-Line Problem </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMTAyNzg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>CMU의 Michael Kaess 교수님 논문 연구</li>
<li>Perspective-N-Line이라면, line 들 정보를 통해서 absolute camera pose를 찾을 수 있다는걸까? <ul>
<li>Line 기반 트랙킹 기술을 한동안 보았는데, 이 기술과 함께 써서 visual keypoint가 부족한 실내 corridor 환경 등등에서 SLAM을 할 수 있지 않을까 라는 상상을 해본다. </li>
<li>Point &amp; line SLAM 꼭 한번 제대로 만들어보고싶다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Fast and Robust Certifiable Estimation of the Relative Pose Between Two Calibrated Cameras </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDg1MjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Global optimum이 되는 relative pose estimation을 구하는 방법<ul>
<li>RANSAC보다 (당연히) 정확함</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Hybrid Rotation Averaging: A Fast and Robust Rotation Averaging Approach </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDkxMTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Laurent Kneip 교수님 연구실</li>
<li>Rotation averaging 에서 Local method와 Global method가 있다고 한다.<ul>
<li>보통 Local method를 사용하고, global method는 느려서 잘 안쓴다고 한다.</li>
</ul>
</li>
<li>이 논문에서는 Global의 방식과 Local 방식의 장점을 혼합한 hybrid 방식을 제안한다고 한다.</li>
</ul>
</details>

<details>
  <summary> Provably Approximated ICP </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDM1ODgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Point cloud간의 transformation을 구할 때 global optimum을 찾는 방식을 증명하고 구하는 방식을 제안한 연구.</li>
</ul>
</details>

<details>
  <summary> On the Tightness of Semidefinite Relaxations for Rotation Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDIwOTkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Viktor Larsson 교수님 연구</li>
<li>Semi-definite relaxation…?</li>
</ul>
</details>

<details>
  <summary> A Linear Approach to Absolute Pose Estimation for Light Fields </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3VzZXJzLmljcy5mb3J0aC5nci9+bG91cmFraXMvcHVibC8yMDIwXzNkdi5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Lightfield camera에 특화된 absolute camera pose estimation 기법을 제안.</li>
</ul>
</details>

<details>
  <summary> Factor Graphs: Exploiting Structure in Robotics </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5udWFscmV2aWV3cy5vcmcvZG9pL2Ficy8xMC4xMTQ2L2FubnVyZXYtY29udHJvbC0wNjE1MjAtMDEwNTA0">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>논문 아직 안나옴 (…)</li>
</ul>
</details>

<details>
  <summary> Comparison of camera-based and 3D LiDAR-based place recognition across weather conditions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDU0Mjk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>카메라 vs LiDAR vs 카메라+LiDAR 퓨전 방식의 place recognition을 비교하는 연구.</li>
</ul>
</details>

<h2 id="딥러닝"><a href="#딥러닝" class="headerlink" title="딥러닝"></a>딥러닝</h2><details>
  <summary> QuadroNet: Multi-Task Learning for Real-Time Semantic Depth Aware Instance Segmentation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvR29lbF9RdWFkcm9OZXRfTXVsdGktVGFza19MZWFybmluZ19mb3JfUmVhbC1UaW1lX1NlbWFudGljX0RlcHRoX0F3YXJlX0luc3RhbmNlX1NlZ21lbnRhdGlvbl9XQUNWXzIwMjFfcGFwZXIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Zoox의 연구</li>
</ul>
</details>

<details>
  <summary> Boosting Monocular Depth with Panoptic Segmentation Maps </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvU2FlZWRhbl9Cb29zdGluZ19Nb25vY3VsYXJfRGVwdGhfV2l0aF9QYW5vcHRpY19TZWdtZW50YXRpb25fTWFwc19XQUNWXzIwMjFfcGFwZXIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Real-Time Uncertainty Estimation in Computer Vision via Uncertainty-Aware Distribution Distillation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvU2hlbl9SZWFsLVRpbWVfVW5jZXJ0YWludHlfRXN0aW1hdGlvbl9pbl9Db21wdXRlcl9WaXNpb25fdmlhX1VuY2VydGFpbnR5LUF3YXJlX0Rpc3RyaWJ1dGlvbl9EaXN0aWxsYXRpb25fV0FDVl8yMDIxX3BhcGVyLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Self-Supervised Pretraining of 3D Features on any Point-Cloud </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDI2OTEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>3D 포인트 클라우드를 사용하기 위해 pretraining하는 기법에 대해 소개함.</li>
</ul>
</details>


<h1 id="시간이-되면-읽어볼-논문들"><a href="#시간이-되면-읽어볼-논문들" class="headerlink" title="시간이 되면 읽어볼 논문들"></a>시간이 되면 읽어볼 논문들</h1><ul>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDk2NTcucGRm">VIO-Aided Structure from Motion Under Challenging Environments<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMjIxOC02NTgxLzEwLzEvMjMvaHRt">Monocular Visual Inertial Direct SLAM with Robust Scale Estimation for Ground Robots/Vehicles<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9hYnMvMTAuMTE0NS8zNDMwOTg0LjM0MzEwMzg=">On Camera Pose Estimation for 3D Scene Reconstruction<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvS3V6bmlldHNvdl9Db01vREFfQ29udGludW91c19Nb25vY3VsYXJfRGVwdGhfQWRhcHRhdGlvbl9Vc2luZ19QYXN0X0V4cGVyaWVuY2VzX1dBQ1ZfMjAyMV9wYXBlci5wZGY=">CoMoDA: Continuous Monocular Depth Adaptation Using Past Experiences<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMS9wYXBlcnMvTGlfU2VsZi1TdXBlcnZpc2VkX1Zpc3VhbC1MaURBUl9PZG9tZXRyeV9XaXRoX0ZsaXBfQ29uc2lzdGVuY3lfV0FDVl8yMDIxX3BhcGVyLnBkZg==">Self-supervised Visual-LiDAR Odometry with Flip Consistency<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9XQUNWMjAyMVcvQVZWL3BhcGVycy9QYXBhZG9wb3Vsb3NfTmV1cmFsX1Zpc2lvbi1CYXNlZF9TZW1hbnRpY18zRF9Xb3JsZF9Nb2RlbGluZ19XQUNWV18yMDIxX3BhcGVyLnBkZg==">Neural vision-based semantic 3D world modeling<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwODQ2LTAyMC0wMTI5Ny04">A Semi-Direct Monocular Visual SLAM Algorithm in Complex Environments<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDk0MDE=">An Efficient Iterated EKF-Based Direct Visual-Inertial Odometry for MAVs Using a Single Plane Primitive<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkyOTYyNTE=">Global visual and Semantic Observations for Outdoor Robot Localization<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL01ldGluX1R1cmFuMy9wdWJsaWNhdGlvbi8zNDc5MzczMzJfUmVzZWFyY2hfb25fdGhlX0F2YWlsYWJpbGl0eV9vZl9WSU5TLU1vbm9fYW5kX09SQi1TTEFNM19BbGdvcml0aG1zX2Zvcl9BdmlhdGlvbi9saW5rcy81ZmU4ZDU5ZTI5OWJmMTQwODg1MDMxZjcvUmVzZWFyY2gtb24tdGhlLUF2YWlsYWJpbGl0eS1vZi1WSU5TLU1vbm8tYW5kLU9SQi1TTEFNMy1BbGdvcml0aG1zLWZvci1BdmlhdGlvbi5wZGY=">Research on the Availability of VINS-Mono and ORB-SLAM3 Algorithms for Aviation<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDA1ODUucGRm">UPSLAM: Union of Panoramas SLAM<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDUzODY=">Visual-IMU State Estimation with GPS and OpenStreetMap for Vehicles on a Smartphone<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDE4NDEucGRm">CNN-based Visual Ego-Motion Estimation for Fast MAV Maneuvers<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cDovL3NjaXMuc2NpY2hpbmEuY29tL2VuLzIwMjEvMTEyMjA0LnBkZg==">Homography-based camera pose estimation with known gravity direction for UAV navigation<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ1MzA=">On-the-fly Extrinsic Calibration of Non-Overlapping in-Vehicle Cameras based on Visual SLAM under 90-degree Backing-up Parking<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ4NDc=">Single-Shot 3D Detection of Vehicles from Monocular RGB Images via Geometrically Constrained Keypoints in Real-Time<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMTQyNC04MjIwLzIxLzIvNDA5L3BkZg==">TIMA SLAM: Tracking Independently and Mapping Altogether for an Uncalibrated Multi-Camera System<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjQyNzE2MjAzMDM1MTg=">Deep regression for LiDAR-based localization in dense urban areas<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cudGFuZGZvbmxpbmUuY29tL2RvaS9hYnMvMTAuMTA4MC8wMTY5MTg2NC4yMDIwLjE4Njk1ODY=">Stereo camera visual SLAM with hierarchical masking and motion-state classification at outdoor construction sites containing large dynamic objects<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDI4NzQucGRm">A general framework for modeling and dynamic simulation of multibody systems using factor graphs<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05MzE2MjI0">A SLAM System Based on RGBD Image and Point-Line Feature<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDY1OTQucGRm">PLUME: Efficient 3D Object Detection from Stereo Images<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDY1NjIucGRm">Asynchronous Multi-View SLAM<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMjYzLTAyMC0wMTQyNy03">Incremental Rotation Averaging<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05MzE2NzEw">PL-GM:RGB-D SLAM With a Novel 2D and 3D Geometric Constraint Model of Point and Line Features<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL1pvbmdoYWlfQ2hlbi9wdWJsaWNhdGlvbi8zNDgyNTYwNDlfRFAtU0xBTV9BX3Zpc3VhbF9TTEFNX3dpdGhfbW92aW5nX3Byb2JhYmlsaXR5X3Rvd2FyZHNfZHluYW1pY19lbnZpcm9ubWVudHMvbGlua3MvNWZmZjljMzg0NTg1MTU1M2EwNDE4MTA2L0RQLVNMQU0tQS12aXN1YWwtU0xBTS13aXRoLW1vdmluZy1wcm9iYWJpbGl0eS10b3dhcmRzLWR5bmFtaWMtZW52aXJvbm1lbnRzLnBkZg==">DP-SLAM: A visual SLAM with moving probability towards dynamic environments<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS9TMDAzMTMyMDMyMTAwMDA5MQ==">Visual SLAM for robot navigation in healthcare facility<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMTQyNC04MjIwLzIxLzMvNzA1L3BkZg==">Panoramic Visual SLAM Technology for Spherical Images<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05MzE2Njk4">LSTM and Filter Based Comparison Analysis for Indoor Global Localization in UAVs<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 2월 SLAM 뉴스</title>
    <url>/20210213-february-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h1 id="이번-달-내가-관심가지는-논문들"><a href="#이번-달-내가-관심가지는-논문들" class="headerlink" title="이번 달 내가 관심가지는 논문들"></a>이번 달 내가 관심가지는 논문들</h1><details>
  <summary> Tight Integration of Feature-Based Relocalization in Monocular Direct Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDExOTEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Daniel Cremers 교수님과 Rui Wang의 연구<ul>
<li>DSO에 relocalization 기능이…?</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> PHASER: A Robust and Correspondence-Free Global Pointcloud Registration </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjc0NTg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Cadena 교수님, Roland Siegwart 교수님, Juan Nieto 교수님 연구 </li>
</ul>
</details>

<details>
  <summary> Freetures: Localization in Signed Distance Function Maps </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjc0OTM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Jeffrey Delmerico, Juan Nieto, Roland Siegwart, Marc Pollefeys, Cesar Cadena Lerma (슬벤저스 ㄷㄷ)</li>
</ul>
</details>

<details>
  <summary> Embedding Manifold Structures into Kalman Filters </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM4MDQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>엄마… 수학 논문…</li>
</ul>
</details>

<details>
  <summary> Escaping Poor Local Minima in Large Scale Robust Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMTA5MjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Landmark Based Visual Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5yc2Mud2NlLmFjLmluL2Fzc2V0L1JTQyUyMDIwMTklMjBQcm9jZWVkaW5nLnBkZiNwYWdlPTU2">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Corner Cases for Visual Perception in Automated Driving: Some Guidance on Detection Approaches </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDU4OTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Switching-Coupled Backend for Simultaneous Localization and Dynamic Object Tracking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDM2ODM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Unified Hybrid Formulation for Visual SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly91d3NwYWNlLnV3YXRlcmxvby5jYS9iaXRzdHJlYW0vaGFuZGxlLzEwMDEyLzE2ODA3L1lvdW5lc19HZW9yZ2VzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Younes의 PhD 졸업 논문</li>
</ul>
</details>

<details>
  <summary> Asynchronous Multi-View SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDY1NjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Improved Visual Localization via Graph Filtering </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMjMxMy00MzNYLzcvMi8yMC9wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Ian Reid 교수님 랩실</li>
</ul>
</details>

<details>
  <summary> RigidFusion: RGB-D Scene Reconstruction with Rigidly-moving Objects </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL2dlb21ldHJ5LmNzLnVjbC5hYy51ay9wcm9qZWN0cy8yMDIxL3JpZ2lkZnVzaW9uL3BhcGVyX2RvY3MvcmlnaWRmdXNpb25fZWcyMS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Niessner 교수님 랩실</li>
</ul>
</details>

<details>
  <summary> Review of Place Recognition Approaches: Traditional and Deep Learning Methods </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL01lbGlrZS1TYWgvcHVibGljYXRpb24vMzQ4MjA5MjkwX1Jldmlld19vZl9QbGFjZV9SZWNvZ25pdGlvbl9BcHByb2FjaGVzX1RyYWRpdGlvbmFsX2FuZF9EZWVwX0xlYXJuaW5nX01ldGhvZHMvbGlua3MvNWZmODU3OWVhNmZkY2NkY2I4M2JlMmE5L1Jldmlldy1vZi1QbGFjZS1SZWNvZ25pdGlvbi1BcHByb2FjaGVzLVRyYWRpdGlvbmFsLWFuZC1EZWVwLUxlYXJuaW5nLU1ldGhvZHMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Optical flow and scene flow estimation: A survey </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwMzEzMjAzMjEwMDA0ODA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Attention Models for Point Clouds in Deep Learning: A Survey </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMTA3ODgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<h1 id="시간나면-읽어볼-논문들"><a href="#시간나면-읽어볼-논문들" class="headerlink" title="시간나면 읽어볼 논문들"></a>시간나면 읽어볼 논문들</h1><details>
  <summary> Automatic object annotation in streamed and remotely explored large 3D reconstructions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3NjaG9sYXIuZ29vZ2xlLmNvbS9zY2hvbGFyX3VybD91cmw9aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jb250ZW50L3BkZi8xMC4xMDA3L3M0MTA5NS0wMjAtMDE5NC00LnBkZiZobD1lbiZzYT1YJmQ9MTYyNjExMDAzMjUyMjUyOTUwNiZlaT1jaEVjWVBmMUhlYUt5d1NyMnFMUUJBJnNjaXNpZz1BQUdCZm0wQ05UdGdwVHdCXzNabFJzdjU1TU43MlpQZnpnJm5vc3NsPTEmb2k9c2Nob2xhcmFscnQmaGlzdD1nNHNZeUdJQUFBQUo6OTM0ODA5MTgxNDk5OTcwMDAxMjpBQUdCZm0xSDdYYzFHaUNVQjZ5clFETlRVc0x2VGNET1NBJmh0bWw9">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Characterizing Marginalization and Incremental Operations on the Bayes Tree </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ib29rcy5nb29nbGUuY28ua3IvYm9va3M/aGw9ZW4mbHI9bGFuZ19lbiZpZD11WVliRUFBQVFCQUomb2k9Zm5kJnBnPVBBMjI3Jm90cz1kdGR4bW96VGIyJnNpZz1YRnpOQ1BvckFLXzhacVMxLU9qVnZfamtMNjAmcmVkaXJfZXNjPXkjdj1vbmVwYWdlJnEmZj1mYWxzZQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Kaess, Leonard 교수님</li>
</ul>
</details>

<details>
  <summary> Camera Calibration with Pose Guidance </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMTAyMDIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SOSD-Net: Joint Semantic Object Segmentation and Depth Estimation from Monocular images </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDc0MjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Feature-based Initialization for Monocular Direct Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly92aXNpb24uaW4udHVtLmRlL19tZWRpYS9tZW1iZXJzL2RlbW1lbG4vZ2xhZGtvdmEyMDIwaWRwLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>오… Daniel Cremers 교수님 랩실의 석사 연구?</li>
</ul>
</details>

<details>
  <summary> RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMTg5OTA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Mask-RCNN을 사용해서 움직이는 물체를 검출하고 제거함으로써, 움직이는 물체가 있는 환경에서도 잘 되는 SLAM을 구현.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMDQ2OTM=">지난달에도 RDS-SLAM이라고 나왔는데…<i class="fa fa-external-link-alt"></i></span> ㅋㅋㅋ 이름이 겹친다</li>
</ul>
</details>

<details>
  <summary> A Collaborative Visual SLAM Framework for Service Robots </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDMyMjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Visual SLAM for robot navigation in healthcare facility </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubmNiaS5ubG0ubmloLmdvdi9wbWMvYXJ0aWNsZXMvUE1DNzgxNjk2Ny8=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> The Devils in the Point Clouds: Studying the Robustness of Point Cloud Convolutions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDc4MzIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Practical Globally Optimal Consensus Maximization by Branch-and-Bound based on Interval Arithmetic </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwMzEzMjAzMjEwMDA4NDQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Automated building change detection with amodal completion of point clouds </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjY1ODA1MjEwMDAxOTQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<hr>
<h1 id="아직-확인-안된-논문들"><a href="#아직-확인-안된-논문들" class="headerlink" title="아직 확인 안된 논문들"></a>아직 확인 안된 논문들</h1><details>
  <summary> Online Extrinsic Calibration based on Per-Sensor Ego-Motion Using Dual Quaternions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMTE0NDAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Ground-Aware Monocular 3D Object Detection for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjc0Nzg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Ego-Motion Estimation Using Recurrent Convolutional Neural Networks through Optical Flow Learning </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9zZWFyY2gucHJvcXVlc3QuY29tL29wZW52aWV3L2Y2MmZjMWNmZGYxYjI3NjI5ZDM2Y2NmNjczNmFiNWQ4LzE/cHEtb3JpZ3NpdGU9Z3NjaG9sYXImY2JsPTIwMzI0MDQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> An adaptive visual Dynamic-SLAM method based on fusing the semantic information </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjMwNjA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>위에 RDS-SLAM과 비슷한 맥락인 것 같다.</li>
</ul>
</details>

<details>
  <summary> PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDA0NjMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Unsupervised Learning of Depth and Camera Pose with Feature Map Warping </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3NjaG9sYXIuZ29vZ2xlLmNvbS9zY2hvbGFyX3VybD91cmw9aHR0cHM6Ly93d3cubWRwaS5jb20vMTQyNC04MjIwLzIxLzMvOTIzL3BkZiZobD1lbiZzYT1YJmQ9ODE2NzE1OTcwMzUwODQ2NDM1NiZlaT1XOU1lWUtuaE5vaTZtd0dOdFkyQUJBJnNjaXNpZz1BQUdCZm0xVlZ4VzUzYXF1aHQwbFRWdGxJbWlWeTVPMmFRJm5vc3NsPTEmb2k9c2Nob2xhcmFscnQmaGlzdD1nNHNZeUdJQUFBQUo6MTIwOTA5ODQzNzUxNzcwMjg5Nzg6QUFHQmZtMnFLbGxTaUt3UGJfVzAwMUNBdDQzeWRYNXFfUSZodG1sPQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Monocular Depth Estimation Using Laplacian Pyramid-Based Depth Residuals </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzMTY3Nzgv">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Gated3D: Monocular 3D Object Detection From Temporal Illumination Cues </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM2MDIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Learned Camera Gain and Exposure Control for Improved Visual Feature Detection and Matching </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDQzNDEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>좋은 visual keypoint를 찾는 적절한 camera gain과 exposure를 딥러닝으로 컨트롤! 좋은 연구인것 같다 (하지만 inference 시간이 어떻게 될까…)</li>
</ul>
</details>

<details>
  <summary> OV2SLAM : A Fully Online and Versatile Visual SLAM for Real-Time
Applications </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDQwNjAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="4.513ex" height="1.937ex" role="img" focusable="false" viewBox="0 -833.9 1994.9 855.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="msup" transform="translate(763, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mn" transform="translate(828.3, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> SLAM</li>
</ul>
</details>

<details>
  <summary> FisheyeDistanceNet++: Self-Supervised Fisheye Distance Estimation with Self-Attention, Robust Loss Function and Camera View Generalization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL1NlbnRoaWxfWW9nYW1hbmkyL3B1YmxpY2F0aW9uLzM0ODkwMDA1N19GaXNoZXllRGlzdGFuY2VOZXRfU2VsZi1TdXBlcnZpc2VkX0Zpc2hleWVfRGlzdGFuY2VfRXN0aW1hdGlvbl93aXRoX1NlbGYtQXR0ZW50aW9uX1JvYnVzdF9Mb3NzX0Z1bmN0aW9uX2FuZF9DYW1lcmFfVmlld19HZW5lcmFsaXphdGlvbi9saW5rcy82MDE1MjRkN2E2ZmRjYzA3MWJhMTU1ODUvRmlzaGV5ZURpc3RhbmNlTmV0LVNlbGYtU3VwZXJ2aXNlZC1GaXNoZXllLURpc3RhbmNlLUVzdGltYXRpb24td2l0aC1TZWxmLUF0dGVudGlvbi1Sb2J1c3QtTG9zcy1GdW5jdGlvbi1hbmQtQ2FtZXJhLVZpZXctR2VuZXJhbGl6YXRpb24ucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SuperVO: A Monocular Visual Odometry based on Learned Feature Matching with GNN </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDIxMzY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>본격 SuperPoint + SuperGlue의 후계자인척 하기</li>
</ul>
</details>

<details>
  <summary> Adversarial Imaging Pipelines </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM3MjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Points2Vec: Unsupervised Object-level Feature Learning from Point Clouds </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDQxMzYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Roland Siegwart</li>
</ul>
</details>

<details>
  <summary> IDOL: Inertial Deep Orientation-Estimation and Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDQwMjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> LION: Lidar-Inertial Observability-Aware Navigator for Vision-Denied Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM0NDMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Luca Carlone 교수님 참여!</li>
</ul>
</details>

<details>
  <summary> On the Robustness of Multi-View Rotation Averaging </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDU0NTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> DARE-SLAM: Degeneracy-Aware and Resilient Loop Closing in Perceptually-Degraded Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDUxMTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> The VCU-RVI Benchmark: Evaluating Visual Inertial Odometry for Indoor Navigation Applications with an RGB-D Camera </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDE3MTM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Speed and Memory Efficient Dense RGB-D SLAM in Dynamic Scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDE1NDI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Nonlinear Observers Design for Vision-Aided Inertial Navigation Systems </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDUxMTEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Lightweight 3-D Localization and Mapping for Solid-State LiDAR </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM4MDAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDQ5NjAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Fast and Accurate 3D Object Detection for Lidar-Camera-Based Autonomous Vehicles Using One Shared Voxel-Based Backbone </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05MzQwMTg3">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Polarimetric Monocular Dense Mapping Using Relative Deep Depth Prior* </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDUyMTIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Scale Normalized Image Pyramids with AutoFocus for Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDU2NDYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Multi-camera fast bundle adjustment algorithm based on normalized matrix dimensionality reduction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5pcmxhLmNuL2VuL2FydGljbGUvZG9pLzEwLjM3ODgvSVJMQTIwMjAwMTU2">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDY5MzEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> VIODE: A Simulated Dataset to Address the Challenges of Visual-Inertial Odometry in Dynamic Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDU5NjUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Geometric Nonlinear Stochastic Filter for Simultaneous Localization and Mapping </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDYxNTAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5jcy5veC5hYy51ay9maWxlcy8xMjU4MS92bWxvYy5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> 3D-CenterNet: 3D object detection network for point clouds with center estimation priority </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwMzEzMjAzMjEwMDA3MTY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> HyperPocket: Generative Point Cloud Completion </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDU5NzMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Panoramic Visual SLAM Technology for Spherical Images </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMTQyNC04MjIwLzIxLzMvNzA1L3BkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Robust monocular pose tracking of less-distinct objects based on Contour-part model </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMzM2MzM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> 3D mesh transformation preprocessing system in the real space for augmented reality services </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS9TMjQwNTk1OTUyMTAwMDE3NQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Two-Stage Data Association Approach for 3D Multi-Object Tracking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDg2ODQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining and Consistency </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMTI3MjcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Single Image Desmoking via Attentive Generative Adversarial Network for Smoke Detection Process </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNjk0LTAyMS0wMTA5Ni16">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Self-supervised Multi-view Stereo via Effective Co-Segmentation and Data-Augmentation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWFhaS5vcmcvQUFBSTIxUGFwZXJzL0FBQUktMjU0OS5YdUgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Deep Inertial Odometry with Accurate IMU Preintegration </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDcwNjEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Keypoint detection by wave propagation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc3BpZWRpZ2l0YWxsaWJyYXJ5Lm9yZy9qb3VybmFscy9qb3VybmFsLW9mLWVsZWN0cm9uaWMtaW1hZ2luZy92b2x1bWUtMzAvaXNzdWUtMS8wMTMwMDMvS2V5cG9pbnQtZGV0ZWN0aW9uLWJ5LXdhdmUtcHJvcGFnYXRpb24vMTAuMTExNy8xLkpFSS4zMC4xLjAxMzAwMy5zaG9ydD9TU089MQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> 3D reconstruction of deformable objects from RGB-D cameras: an omnidirectional inward-facing multi-camera system </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL2NvbW1hbmRpYS51bml6YXIuZXMvd3AtY29udGVudC91cGxvYWRzL1ZJU0FQUF8yMDIxXzIyMi5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Unstructured Road Segmentation Based on Road Boundary Enhancement Point-Cylinder Network Using LiDAR Sensor </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMjA3Mi00MjkyLzEzLzMvNDk1L3BkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Decentralised Autonomous Navigation of a UAV Network for Road Traffic Monitoring </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjkxMjk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Semantic Segmentation Leveraging Simultaneous Depth Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubWRwaS5jb20vMTQyNC04MjIwLzIxLzMvNjkwL3BkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWFhaS5vcmcvQUFBSTIxUGFwZXJzL0FBQUktNDQzMy5IdWFuZ1AucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Robust RGB-D SLAM for Dynamic Environments Based on YOLOv4 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDg3Mzg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Visual Odometry/Inertial Integration for Enhanced Land Vehicle Navigation in GNSS Denied Environment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDg2OTg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> LEAD: LiDAR Extender for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDc5ODkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDc0NDgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Point-line-based RGB-D SLAM and Bundle Adjustment Uncertainty Analysis </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDcxMTAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Monocular 3D Object Detection using Dual Quadric for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjUyMzEyMjEwMDIwNzE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> FastHand: Fast Hand Pose Estimation From A Monocular Camera </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDcwNjcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Mirror Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tL3kyMDIxLzAwMzUzMjkuaHRtbA==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Richard Newcombe</li>
</ul>
</details>

<details>
  <summary> Systems and methods for providing a mobile artificial reality user with environmental awareness </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZnJlZXBhdGVudHNvbmxpbmUuY29tLzEwOTAxMjE1Lmh0bWw=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Richard Newcombe</li>
</ul>
</details>

<details>
  <summary> Hough2Map – Iterative Event-based Hough Transform for High-Speed Railway Mapping </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDgxNDUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Roland Siegwart, Juan Nieto 교수님 랩실</li>
</ul>
</details>

<details>
  <summary> MamboNet: Adversarial Semantic Segmentation for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3NpZS5udHUuZWR1LnR3L35mdWgvcGVyc29uYWwvTWFtYm9OZXRBZHZlcnNhcmlhbFNlbWFudGljU2VnbWVudGF0aW9uZm9yQXV0b25vbW91c0RyaXZpbmcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> EfficientLPS: Efficient LiDAR Panoptic Segmentation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDgwMDkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Instance Localization for Self-supervised Detection Pretraining </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDgzMTgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Linear-PoseNet: A Real-Time Camera Pose Estimation System Using Linear Regression and Principal Component Analysis </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzNDg3NjI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> End-to-End Egospheric Spatial Memory </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDc3NjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Ronald Clark, Andrew Davison 교수님 랩실 연구</li>
</ul>
</details>

<details>
  <summary> PA-MVSNet: Sparse-to-Dense Multi-View Stereo With Pyramid Attention </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05MzUyNzYz">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Improving Visual Place Recognition Performance by Maximising Complementarity </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDg0MTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Hierarchical Attention Fusion for Geo-Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDkxODYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Intensity-SLAM: Intensity Assisted Localization and Mapping for Large Scale Environment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDM3OTgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Movement Prediction based on Pose Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9uZXRsaWJyYXJ5LmFhdS5hdC9vYnZ1a2xocy9jb250ZW50L3RpdGxlaW5mby81ODA5NDk4L2Z1bGwucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Towards Semantic SLAM: 3D Position and Velocity Estimation by Fusing Image Semantic Information with Camera Motion Parameters for Traffic Scene Analysis </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL01vc3RhZmEtTWFuc291ci0yL3B1YmxpY2F0aW9uLzM0ODcxNjcxOV9yZW1vdGVfc2Vuc2luZ19Ub3dhcmRzX1NlbWFudGljX1NMQU1fM0RfUG9zaXRpb25fYW5kX1ZlbG9jaXR5X0VzdGltYXRpb25fYnlfRnVzaW5nX0ltYWdlX1NlbWFudGljX0luZm9ybWF0aW9uX3dpdGhfQ2FtZXJhX01vdGlvbl9QYXJhbWV0ZXJzX2Zvcl9UcmFmZmljX1NjZW5lX0FuYWx5c2lzL2xpbmtzLzYwMGM3NWZlOTI4NTFjMTNmZTMyMDc4ZS9yZW1vdGUtc2Vuc2luZy1Ub3dhcmRzLVNlbWFudGljLVNMQU0tM0QtUG9zaXRpb24tYW5kLVZlbG9jaXR5LUVzdGltYXRpb24tYnktRnVzaW5nLUltYWdlLVNlbWFudGljLUluZm9ybWF0aW9uLXdpdGgtQ2FtZXJhLU1vdGlvbi1QYXJhbWV0ZXJzLWZvci1UcmFmZmljLVNjZW5lLUFuYWx5c2lzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Consistent Right-Invariant Fixed-Lag Smoother with Application to Visual Inertial SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDg1OTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Weakly Supervised Learning of Rigid 3D Scene Flow </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMDg5NDUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Probability Dueling DQN active visual SLAM for autonomous navigation in indoor environmen </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZW1lcmFsZC5jb20vaW5zaWdodC9jb250ZW50L2RvaS8xMC4xMTA4L0lSLTA4LTIwMjAtMDE2MC9mdWxsL2h0bWw=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A hybrid global structure from motion method for synchronously estimating global rotations and global translations </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjQyNzE2MjEwMDAzNTY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> D3DLO: Deep 3D LiDAR Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMTIyNDIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> STEP: Segmenting and Tracking Every Pixel </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMTE4NTkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Daniel Cremers</li>
</ul>
</details>

<details>
  <summary> New SLAM Fusion Algorithm based on Lidar/IMU Sensors </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaW9uLm9yZy9wdWJsaWNhdGlvbnMvYWJzdHJhY3QuY2ZtP2FydGljbGVJRD0xNzg2OA==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Accurate 3D Localization Using RGB-TOF Camera and IMU for Industrial Mobile Robots </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY2FtYnJpZGdlLm9yZy9jb3JlL2pvdXJuYWxzL3JvYm90aWNhL2FydGljbGUvYWJzL2FjY3VyYXRlLTNkLWxvY2FsaXphdGlvbi11c2luZy1yZ2J0b2YtY2FtZXJhLWFuZC1pbXUtZm9yLWluZHVzdHJpYWwtbW9iaWxlLXJvYm90cy8yRjREODgyMUQ4QkZBQTQzQkYxNDJEMjdGNzJCQzdFMg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Improved Genetic Algorithm for Bundle Adjustment in Photogrammetry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzMjM2NDk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Semantic loop closure detection based on graph matching in multi-objects scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzEwNDczMjAzMjEwMDAzODk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Unsupervised Learning of Lidar Features for Use in a Probabilistic Trajectory Estimator </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDIuMTEyNjEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Accurate Covariance Estimation for Pose Data from Iterative Closest Point Algorithm </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaW9uLm9yZy9wdWJsaWNhdGlvbnMvYWJzdHJhY3QuY2ZtP2FydGljbGVJRD0xNzg2Ng==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual localization 기술의 방향?</title>
    <url>/20210405-visloc-my-thoughts/</url>
    <content><![CDATA[<h2 id="실외-실내-기술변화-고려할-점"><a href="#실외-실내-기술변화-고려할-점" class="headerlink" title="실외/실내 기술변화 + 고려할 점"></a>실외/실내 기술변화 + 고려할 점</h2><p>Visual localization 기술은 <strong>실내/실외에서 이미지만으로 현재 위치를 추정하는 기술</strong>이다.</p>
<p>이는 SLAM 같은 복잡한 시스템에 비해 요구사항이 비교적 적게 느껴진다. 그에 비해 Visual localization은 <strong>그냥 사진 한장 딱 찍으면 알아서 위치 찾아주는 마법같은 기술</strong>처럼 느껴진다. (물론 실제로 이 기술도 학습 과정이라던지 여러가지로 알고보면 굉장히 복잡하겠지만, 단순히 겉으로 드러나는 느낌만 이야기 하는 것이다)</p>
<p> </p>
<h3 id="실외-visual-localization"><a href="#실외-visual-localization" class="headerlink" title="실외 visual localization"></a>실외 visual localization</h3><h4 id="GPS가-더-필요할까"><a href="#GPS가-더-필요할까" class="headerlink" title="GPS가 더 필요할까?"></a>GPS가 더 필요할까?</h4><p>실외의 경우 GPS를 사용해서 위치를 추정할 수 있었다. 물론 <strong>GPS</strong>는 1. <strong>에러가 m 단위</strong>로 나타나고, 2. 고층 빌딩 사이에서는 <strong>multi-path 문제</strong>로 위치를 잘못 추정하는 경우가 있고, 3. 자율주행의 경우 <strong>터널이나 주차장</strong> 등에 들어갈 때 if-else가 아닌 방법으로 자연스러운 알고리즘 변환을 할 수 없다.</p>
<p>그렇기에 <strong>visual localization이 정말 잘 된다면 사실상 GPS를 쓸 이유가 없게 될 수도</strong> 있겠다. <strong>GPS는 search space를 줄여주는 하나의 complementary 센서등으로 전락</strong>할 수도 있겠다.</p>
<h4 id="기존의-방법은"><a href="#기존의-방법은" class="headerlink" title="기존의 방법은?"></a>기존의 방법은?</h4><p>Visual-SLAM에서의 <strong>relocalization 기능</strong>이 어떻게 보면 이미지만 가지고 6d pose를 찾는 점이 visual localization 기술과 굉장히 유사하다. Relocalization 기술은 visual-SLAM을 통해 <strong>미리 따 놓은 3D map을 기반으로 위치를 구하는 SLAM 모듈</strong>인데, 문제는 우리가 아직 실외에서의 3d map을 잘 만들지 못한다는 점이다. 보통 Structure-from-Motion (SfM)으로 3d map을 따놓고, local feature matching과 geometric verification을 통해 pose를 구한다.</p>
<p>첫째로 우리는 <strong>‘좋은 DB를 만드는 방법을 잘 모른다</strong>. 조명, 날씨, 시점변화에 robust하다고 자신있게 내놓을만한 local feature / matching방법론도 없고, DB update policy에 대한 de-facto 방법론도 아직 없다. 그렇기 때문에 현재 단계에서는 DB를 조명 단위로, 날씨 단위로, 여러 시점에서 데이터를 수집해서 촘촘하게 만들어놔야한다. 실제로 내가 3d map을 DB 삼아 만든 적이 있는데, 실외에서 COLMAP으로 맵을 만들고 DBoW + ORB 매칭으로 테스트 해봤을 때 오전에 만든 맵은 오후에 작동하지 않았다. 실외에서는 이른 새벽, 해가 조금 뜬 후, 해 뜨고난 오전, 해가 쨍쨍한 정오, 해가 조금 들어간 오후, 노을, 어두컴컴 저녁, 시껌한 밤까지 조명 형태가 엄청 다양하다. 현재 단계의 기술로는 이 모든 케이스마다 map을 만들어야하며, DB의 search space를 줄이기 위해 현재 시간까지 feature로 받아가면서 매칭을 해야했다. 근데 DB에 없는 상황이 나타난다면? 구름이라도 좀 끼거나, 비나 눈이라도 온다면? 그때는 당연히 실패할것이다. DB가 3D map이 아닌 경우에도 같은 논리가 적용될 수 있다. 이 문제를 풀기 위해서는 <strong>조명, 날씨, 시점변화에 robust한 local feature / matching이 무조건 필요</strong>하다.</p>
<p>둘째로 우리는 <strong>‘실외 DB를 효율적으로 만드는 방법을 모른다</strong>. 실외 DB는 <strong>작게는 빌딩에서부터 크게는 도시나 나라단위</strong>로 스케일이 커질 수 있다. DB는 3D map의 형태를 가질 수도 있고, 아니면 단순 이미지 + pose 페어로 만들어질 수도 있다. SLAM 방식을 사용하는 경우 부터 이야기해보자. 기존의 많은 SLAM 알고리즘들은 하나의 mobile device에서 작동하는 것으로 설계되어있지만, 당연히 mobile device 하나로는 메모리에 이 큰 공간의 map 정보를 담을 수 없다. 결국 메모리가 다 찰때 쯤 메모리 리셋을 통해 작은 맵을 이어붙혀가면서 큰 맵을 만든다던지, 또는 다수의 스캐닝 에이전트가 하나의 큰 맵을 서버에서 만들어간다던지 등등 맵을 만드는 방법론이 바뀔 수 밖에 없다. DB가 3D map 형태를 쓰지 않는 경우에는, 실제 위치에서 찍어낸 이미지로부터 global + local feature를 뽑은 후 벡터화 시킨 다음, 그래프의 형태로 엮어서 저장하는 방법이 있을 것이다. 이 방법은 메모리적인 부분에서는 조금 덜 부하가 있겠지만, graph node를 업데이트 시킬지 또는 추가할지에 대한 방법론이 아직 정해지지 않았다. Graph node를 업데이트 시킨다는건 사실상 ‘동일한 위치에서의 데이터 변화를 업데이트한다’ 라는 뜻을 가진다. 하지만 map을 사용하지 않고는 GT pose값을 m 단위의 에러를 가지는 GPS를 써야할텐데, 당연히 높은 확률로 동일한 위치에 올라갈 수 없다는 것을 뜻한다 (실제로 동일한 위치에 올라가도 센서 노이즈로 인해 다른 위치로 인식할 수 있다). 그렇다면 graph update policy가 필요한데, 이 역시 de-facto 방식이 없다. 어찌되었건 결국 <strong>맵을 만드는 방식은 대규모 처리를 수행</strong>하는 방면으로 바뀌게 될 텐데, 아직 DB를 만들어가면서 map 또는 graph reconstruction 과정 중 geometric local minima에 빠지지 않는다고 보장할 수 있는 알고리즘도 나오지 않았고, 이러한 <strong>geometric DB 생성을 위한 대규모 처리 인프라도 아직 익숙한 사람이 없다</strong>.</p>
<p>최근 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZ2VvZ3Vlc3NyLmNvbS8=">geoguessr<i class="fa fa-external-link-alt"></i></span>라는 게임을 해봤는데, 이 게임이 대규모 실외 visual localization을 구현하는데에 어려움을 정확히 잘 짚어주는 것 같았다. 게임에서는 사진을 그냥 딱 하나 주고는 우리나라 지도에서 어디인지 pin을 찍으라고 한다. 주변 정보도 이용하고, 픽셀이 다 깨져버린 이미지에서 이정표를 읽으면서 대관령 제 4터널 앞 도로 어딘가임을 알아채고 위치를 찍었지만, 실제 위치에서 약 150m 정도 떨어졌다는 결과를 받았다. 물론 사람이 이걸 푼다는게 웃기기는 하지만, 여기서 내가 했던 생각은 ‘이미지는 거의 똑같아 보이는데 150m인지 어떻게 확신할 수 있다는 것인가?’ 였다. Visual localization 알고리즘이 주행 알고리즘에도 하나의 센서 값으로 포함되어있다면, VL의 결과가 1m, 5m, 10m 오차가 있으면 어떤 결과가 나타날까? 아니면 나처럼 150m, 200m 오차가 있다면? 너무나도 똑같이 생긴 이미지에서 추리를 하고 또 해서 나온 best 결과임에도 부족하다면, 오히려 GPS가 더 믿을만한게 되는것일까? Visual localization은 distinctive feature가 가득한 도시에서만 쓸 수 있는게 되는걸까? 도시에서만 되는 자율주행은 어떤 쓸모가 있는 것인가? <strong>전국에서 VL이 가능하게 만들려면 어떤 점이 더 개선되야 하는가?</strong></p>
<h4 id="맵을-만들까-만들지-말까"><a href="#맵을-만들까-만들지-말까" class="headerlink" title="맵을 만들까? 만들지 말까?"></a>맵을 만들까? 만들지 말까?</h4><p>Visual-localization에는 <strong>3D map을 사용하는 방법</strong>과 <strong>사용하지 않는 방법</strong>이 있다고 한다. <strong>3D map이 필요한 경우에는 결국 대규모 맵핑을 위한 인프라와 인력이 필요하게 될 것</strong>임을 생각할 수 있다. 하지만 반대로 3D map을 사용하지 않는 방법이 대세를 탈 수 있다는 생각도 들게 된다. 3D map을 사용하지 않는 방법은 3D map을 사용하는 방법보다 정확도가 낮다. 그러면 정확도의 문제만 해결되면 3D map을 만들 필요가 없어지는 것인가? 이 경우 <strong>실외에서는 3D map을 만들 필요가 없어질 수도</strong> 있다. 그렇다면 정확도는 어떻게 올려야하는 것인가? 3D map을 사용하지 않는 방법은 딥러닝 기반 pose regression과 같은 방법을 사용하는 것으로 알고 있는데, 그렇다면 <strong>단순히 모델을 개발시키고 좋은 트레이닝 데이터셋</strong>만 만들면 되는 것인가? <strong>모델의 한계</strong>는 어느정도인가? 트레이닝 데이터셋이 얼마나 필요한가? <strong>충분한 성능을 끌어올릴 수 있는 트레이닝 데이터셋은 만들 수 있는 것 (feasible)인가?</strong></p>
<p> </p>
<h3 id="실내-visual-localization"><a href="#실내-visual-localization" class="headerlink" title="실내 visual localization"></a>실내 visual localization</h3><p>실내 localization GPS 대신 <strong>UWB 센서, QR코드, WiFi, AR마커</strong> 등의 localization 기술들을 사용해서 위치를 추정한다. 물론 <strong>GPS를 쓰는 것 처럼 편하지 않다</strong>. 그렇기에 몇몇 실내 로봇들은 이러한 센서들을 특정 위치에만 설치해서 대략의 위치만 추정하고, 주행등의 목적에서는 단순히 딥러닝 기반 perception / path planning을 사용하기도 한다.</p>
<p>실내 visual localization이 된다면, <strong>localization 정보를 이용해서 훨씬 정확한 주행을 할 수 있다</strong>. 또, UWB 센서, QR코드, WiFi 등등 고려할 점들이 많은 귀찮은 센서들에 의존할 필요가 없어진다. 이 센서들은 사용한다고 해도 특수 용도 또는 complementary 센서로 전락하게 될 것이다. </p>
<p>카메라를 사용하면서 단순히 6d 위치 정보를 뽑아내는 것 뿐만이 아닌, <strong>기존의 딥러닝 컴퓨터 비전 기법</strong>들도 함께 사용할 수 있다. 이 경우, object detection이나 semantic segmentation을 통해 semantic 정보를 뽑아낼 수 있다. 내 앞에 어떤 물건을 geometric한 성질로 인식하고, 그 물건의 semantic한 성질을 기반으로 더 다양한 interaction을 사용할 수 있게 된다. 이 때 <strong>여러가지 방법을 통해 semantic한 정보가 단순히 어떤 object의 class가 아니라 instance로 구분</strong>하게 해줄 수 있는데, <strong>그 중 하나가 visual localization</strong>이 될 확률이 높다고 본다. Visual localization을 통해 <strong>어떤 object instance가 static하고 dynamic한 것인지 알 수 있다면</strong>, static한 object들은 단순히 class가 아니라 instance로 구분하여 각각의 object instance와 상호작용을 할 수 있을 것이다. 사실상 Object oriented programming에서 어떠한 class instance를 가져와 method를 호출해서 기능을 사용하는 것 처럼 사용할 수 있게 되면서, 실제 세상에서의 object와의 상호작용을 프로그래밍 할 수 있게 될 것이다. 그리고 <strong>이미지에 나타나는 물체들 중 어떤 것들이 static/dynamic한것인지 알 수 있다면 이 역시 localization의 성능을 끌어올려줄 수 있을 것</strong>이다.</p>
<p> </p>
<hr>
<h2 id="업계의-최앞단"><a href="#업계의-최앞단" class="headerlink" title="업계의 최앞단"></a>업계의 최앞단</h2><p>‘국내에서 visual localization 기술을 잘 하는 곳이 어디인가?’ 라는 질문을 들었을 때 나는 가장 먼저 <strong>네이버랩스</strong>가 떠오른다. 다른 기업들도 물론 연구개발을 하고있겠지만, 아무래도 visual localization 기술로 대중에게 제일 잘 알려져있으면서 동시에 가장 자신있게 자신들의 제품/기술력을 보여주는 곳은 네이버랩스라고 생각된다.</p>
<p>해외는 어디가 있을까? 솔직히 Visual localization으로 딱 손에 꼽을 수 있는 곳은 아직 잘 모르겠다.</p>
<p>아무래도 <strong>자율주행 관련으로 데이터를 많이 수집한 곳</strong>이지 않을까? 라고 생각해서 모빌아이, 테슬라, Waymo, ZOOX, Uber 등을 우선 생각했다. 아무래도 기존의 Bag-of-visual-words를 통한 search space reduction 후 local feature matching을 하는 파이프라인은 진작에 모두가 알고 있기 때문에, 왠만한 회사들은 다 시도해보지 않았을까 생각한다. 딥러닝 방식도 2015년에 PoseNet이 나온 후로 진작에 가능성을 보여줬으니, 역시 왠만한 회사들은 다 시도해보지 않았을까 생각한다. </p>
<p>하지만 이런 <strong>큰 회사들은 누가 먼저 더 빨리 제품을 만들 수 있는지에 대해 경쟁</strong>하고 있다. 그리고 visual localization은 아직 사실 제품의 주력 기술이 되기에는 기술, 인력, 인프라 등이 충분히 갖춰지지 않았다고 생각된다. 빠르게 먼저 ‘자율주행인것 처럼 보이는 제품’을 보여주기 위해서는 딥러닝 기반 주행 알고리즘을 짜는 것이 더 쉬울 수 있을 것 같다. 그렇다면 맵핑에 집중한 회사는 어디가 있을까? 아무래도 <strong>기존의 네비게이션 서비스를 하던 회사</strong>가 아닐까 생각한다. 그래서 해외 큰 회사들은 <strong>주행 알고리즘에 집중한 회사</strong> (e.g. 테슬라, Uber)와 <strong>맵핑에 집중한 회사</strong> (e.g. TomTom, Mapbox)가 나눠질 것 같다. 맵핑에 집중한 회사들 중, 그 중 특히나 특정 공간을 실험 지구로 삼아서 테스트를 하고 있는 회사들이 visual localization을 잘 하고 있지 않을까 생각한다. 또 다른 가능성은, 딥러닝 기반 pose regression 방식의 가장 첫번째 시도인 PoseNet의 창시자인 Alex Kendall이 세운 회사인 <strong>Wayve</strong>와 같은 곳이 이런 기술에 조금 집중하지 않았을까 생각도 든다.</p>
<p> </p>
<hr>
<h2 id="기업의-인력수요-변화"><a href="#기업의-인력수요-변화" class="headerlink" title="기업의 인력수요 변화"></a>기업의 인력수요 변화</h2><h3 id="Visual-localization이-정확하고-빠르다면"><a href="#Visual-localization이-정확하고-빠르다면" class="headerlink" title="Visual localization이 정확하고 빠르다면?"></a>Visual localization이 정확하고 빠르다면?</h3><p>Visual localization이 충분히 빨리 돌 수 있다면, pose를 계산할 때 다른 알고리즘을 고려할 필요 없이 VL만 사용하게 될 것 같다. 사전에 DB만 만들어 놓는다면 위치 추정을 위해 SLAM을 더 사용하지 않게 될 수도 있다. 방식에 따라 많이 다르겠지만, 몇가지 VL 방식들은 absolute pose를 계산해내기도 한다. 이러한 방식이 정확하게 + 강인하게 + 빠르게 작동할 수 있다면 사실상 SLAM은 앞으로 mapping만 하게 되고 VL이 모든 localization 작업을 담당하게 될 것 같다.</p>
<p>자율주행이나 로보틱스/VR/AR을 위한 인도어 맵핑과 같이 맵을 따는 작업은 기업쪽에서 맵을 따서 관리하는게 맞다. 품질 유지의 이유도 있고, 무엇보다 유저가 맵핑 과정을 좋아할리가 없기 때문이다. 유저는 다 만들어진 환경에서 잘되는 localization을 통해 서비스를 즐기고 싶을 것이다. <strong>Mapping 서비스는 B2B</strong>가 되겠고, <strong>VL 서비스는 B2C</strong>가 되면서 자연스럽게 VL 분야가 SLAM 분야보다 커질것 같다. <strong>많은 기업들에서 Visual localization에 대한 수요가 늘어날거고</strong>, 서비스가 늘어나면서 대량의 리퀘스트를 떠받치는 <strong>인프라</strong>가 필요할 것이다. VL이 빠르게 돌면서 많은 사용자를 거느리려면 아무래도 <strong>분산처리와 GPU 가속 및 인프라</strong>가 중요할 것 같다는 생각이 드는데, 이는 많은 <strong>SLAM 및 spatial computing 전문가들의 기술 스택을 백엔드로</strong> 이끌 것 같다. Visual localization 기술이 SfM을 요구하게 되는 방향으로 흘러간다면, mapping을 위한 SLAM 역시 굉장히 중요한 role을 맡게 될 것 같다. 큰 기업들은 map을 유지보수하기 위해 mapping agent를 만들거나, 아니면 모든 유저를 mapping agent로 만드는 전략을 취할 것 같은데, 어찌되었건 다수의 map update가 있어야하는 것은 맞다. 이를 위해 역시 또 대규모 맵을 다루고 유지보수하는 기술, 그리고 분산처리 + 가속 시스템이 필요할 것으로 보인다. 이 역시 많은 SLAM 기술자들의 기술 스택을 백엔드로 이끌 것 같다.</p>
<p> </p>
<h3 id="Visual-localization이-정확하지만-충분히-빨리-돌지-않는다면"><a href="#Visual-localization이-정확하지만-충분히-빨리-돌지-않는다면" class="headerlink" title="Visual localization이 정확하지만 충분히 빨리 돌지 않는다면?"></a>Visual localization이 정확하지만 충분히 빨리 돌지 않는다면?</h3><p>Visual localization이 생각보다 그렇게 빨리 돌지 않는다고 해도, <strong>VL은 충분히 SLAM과 사용될 수 있다</strong>. SLAM을 하다보면 맵 생성 + pose 계산을 하면서 메모리와 프로세서를 엄청나게 잡아먹는다. 맵핑 목적으로해서 맵을 계속 이어나가고 있다면, 메모리는 한계치가 있을 것이다. 그렇다면 단순히 localization이 목적이라면, <strong>sliding window VIO</strong> 등을 사용하면서 pose만 계산하는 방법이 있을 것이다. 적당한 local map만 유지하면서 메모리를 아끼는 전략이다. 이 경우에는 대신 global consistency를 유지하기 힘들게 되는데, 이 때 visual localization을 통해 relocalization과 같은 전략을 취해주면 충분히 <strong>global consistency</strong>를 유지하는 localization agent를 만들 수 있지 않을까 싶다.</p>
<p> </p>
<hr>
<h2 id="SLAM-공부하는-사람으로써-공부해야할-방향"><a href="#SLAM-공부하는-사람으로써-공부해야할-방향" class="headerlink" title="SLAM 공부하는 사람으로써 공부해야할 방향"></a>SLAM 공부하는 사람으로써 공부해야할 방향</h2><h3 id="Fusion-Multi-modal-Visual-inertial-Visual-LiDAR-Cross-descriptor"><a href="#Fusion-Multi-modal-Visual-inertial-Visual-LiDAR-Cross-descriptor" class="headerlink" title="Fusion - Multi-modal (Visual-inertial, Visual-LiDAR), Cross-descriptor"></a>Fusion - Multi-modal (Visual-inertial, Visual-LiDAR), Cross-descriptor</h3><p>3D map을 사용하는 visual localization 방식이 주 방식이 된다면 <strong>정확한 맵핑을 할 수 있는 SLAM 알고리즘이 꼭 필요</strong>할 것이다. 그렇다면 기존의 Visual-only 방식은 정확도가 떨어질 것이고, 정확도를 끌어올려줄 수 있는 complementary 센서를 사용하는 것이 새로운 norm이 될 것이다. 가장 가능성이 있는 것은 <strong>VINS (Visual-inertial system)**이지 아닐까 싶다. 모바일 기기와 자동차 등에도 IMU가 기본적으로 탑재되어있고, 이를 위한 (아직 visual-only보다는 훨씬 적지만) 선행 연구가 많이 되었다. 특히나 IMU preintegration 연구 이후로 박차를 가한 것으로 보인다. 실외 맵핑 특정으로 GNSS까지 함께 사용할 수 있는데, Visual-inertial-GNSS 시스템을 다루는 SLAM 엔지니어라면 몸값이 굉장히 높아질 것 같다. 또 다른 가능성은 **Visual-LiDAR</strong>가 될 텐데, 이 경우 LiDAR mapping의 경우 2D image로부터 local feature를 뽑는 것이 아닌 3d structural descriptor를 뽑을 수 있다는 것이 큰 장점이다. Visual 정보를 혼합하면서 이 <strong>3d structural descriptor를 visual 센서 값으로 읽을 수 있다면</strong>, 사실상 robust한 descriptor를 얻게 되는게 아닌가 생각이 든다. 이 경우, mapping agent는 visual-lidar 구조를 가지게 되고 VL agent는 visual-only 또는 vins가 되며 하드웨어 구조가 달라진다고 볼 수 있다. 또 다른 가능성은 <strong>Cross-descriptor</strong>로써, 다양한 local feature들을 모두 사용해서 최대한 빽빽한 3d map을 만들고, 이 정보를 모두 호환할 수 있는 local feature를 사용하는 것이다. 이를 통해 ‘오래 걸리지만 정확하게 matching이 되는 feature’ 정보를 ‘부정확하지만 빠르게 돌아가는 feature’와 호환할 수 있게 되면서 새로운 가능성을 부여한다.</p>
<p> </p>
<h3 id="Semantic-SLAM"><a href="#Semantic-SLAM" class="headerlink" title="Semantic SLAM"></a>Semantic SLAM</h3><p>Semantic SLAM은 Object detection/tracking, segmentation 등을 통해서 <strong>semantic 정보를 SLAM에 포함하는 방법</strong>이다. 간단한 방법으로는 segmentation으로 움직이는 객체들에 제외해서 static 환경만 맵핑하는 것도 있지만, 학계에서 크게 주목을 받고있지는 않다. 제대로 연구를 해볼만한 분야로써는 semantic 정보를 SLAM optimization 파이프라인에 넣는 것이 있는데, Detection / Segmentation으로부터 나오는 픽셀 값들의 reprojection error나 depth 값을 optimization에 사용한다거나, 또는 segmentation 등으로 object class를 얻어내고 shape descriptor, reflectance 같은 embedding feature를 optimization하는 방식들이 주목을 받고 있다. 하지만 이런 연구들도 <strong>사실 아직 object들을 class로 다루고 있으며, 각각의 instance로 다루지 못한다</strong>. SLAM++ 논문에서 보여준 것 처럼 동일 class의 객체들을 각각 트랙킹하면서 해당 객체들에 대한 정보를 메모리로 가지고 있을 수 있다면, UX와 interaction관련으로 엄청나게 큰 가능성을 제시해준다. 예를 들어, 실외 자율주행 중 이미지에 63빌딩이 보인다면, 기존의 방식처럼 단순히 global feature + local feature로 프로세싱하는 것이 아니라 63빌딩이 보이는 위치들로 우선 search space를 제한한뒤 그 후 global + local feature 탐색을 하면서 서치속도를 가속할 수 있겠다.</p>
<p> </p>
<h3 id="Mapping-algorithm-amp-data-structure"><a href="#Mapping-algorithm-amp-data-structure" class="headerlink" title="Mapping algorithm & data structure"></a>Mapping algorithm &amp; data structure</h3><p>기존의 local feature matching 알고리즘은 많이 FLANN 또는 Multi-probe LSH 방식을 사용한다. 즉, <strong>binary tree 또는 pseudo-hashing</strong>을 사용한다는 것이다. 우리가 컴퓨터 과학에서 알고리즘을 공부하면서 배우는 것 처럼 이 둘은 N의 사이즈가 커지면서 <strong>tree balancing이라던지 hash collision 등의 문제</strong>가 나타날 수 있다. 여러 sub-tree를 관리한다던지, multi-hashing 등을 사용해서 최대한 distance를 벌려 문제를 피해보는 것도 방법이 되겠지만, 정말로 큰 대규모 맵핑을 할 때는 어쩔 수 없이 이러한 문제를 마주할 수 밖에 없다. 이를 타파하기 위해서는 여러가지 다양한 아이디어로 search space reduction이 필요한데, 위에서 이야기한 GPS나 semantic 정보를 사용하는 것이 방법이 될 수 있을 것 같다. 또 다른 문제는, semantic slam 분야 연구가 계속되면서 object class가 아닌 object intance들을 관리할 수 있게 된다면, 이 instance들을 어떤 자료구조에 넣어서 관리할지 잘 정해지지 않았다는 것이다. 이 문제를 해결하려고 했던 SLAM++에서는 graph 형태를 사용하였는데, 우리가 semantic 정보를 이용해서 graph node를 발견하고, 이 정보를 사용해서 거대한 binary tree / hash에서의 search space reduction을 하기에는 자료구조의 형태가 너무 달라서 쉽지 않다. 서로 다른 자료구조의 interlinking은 엄청나게 복잡한 문제이고, 이 부분에 있어서 <strong>대규모 mapping 및 search에 적합한 알고리즘 + 자료구조가 제시되어야할 것</strong> 같다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>NAVER LABS</tag>
        <tag>Visual localization</tag>
        <tag>Thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern C++ Coding Guideline</title>
    <url>/20210523-cpp-coding-guideline/</url>
    <content><![CDATA[<h2 id="주의점"><a href="#주의점" class="headerlink" title="주의점"></a>주의점</h2><ul>
<li>모든 edge case를 다 커버하지 않습니다.</li>
<li>특정 라이브러리에서 요구하는 특수 문법 (e.g. CUDA)로 인해 부분적으로 최적화가 되지 않는 부분이 생길 수 있습니다.<ul>
<li>개인적으로 사용할 때는, 해당 라이브러리 사용에 맞는 컨벤션을 추가하는 것이 좋습니다.</li>
<li>협업을 할 때는, 팀원들과 상의를 통해 최대한 모두가 코드를 이해하고 리뷰할 수 있는 컨벤션을 만드는 것이 좋습니다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<hr>
<h2 id="개발-환경"><a href="#개발-환경" class="headerlink" title="개발 환경"></a>개발 환경</h2><h3 id="개발-환경-IDE"><a href="#개발-환경-IDE" class="headerlink" title="개발 환경 - IDE"></a>개발 환경 - IDE</h3><ul>
<li><a href="https://visualstudio.microsoft.com/ko/"><strong>Visual Studio C++</strong></a><ul>
<li><strong>가장 효과적인 디버깅</strong>을 할 수 있습니다.</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuamV0YnJhaW5zLmNvbS9rby1rci9jbGlvbi8=">CLion<i class="fa fa-external-link-alt"></i></span><ul>
<li>크로스 플랫폼 개발에 효과적입니다</li>
<li>CMake를 사용할 수 있어야합니다.</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9jb2RlLnZpc3VhbHN0dWRpby5jb20v">Visual Studio Code<i class="fa fa-external-link-alt"></i></span><ul>
<li>소프트웨어가 가볍습니다.</li>
<li>언제까지 MS가 무료로 풀어줄지는 잘 모르겠습니다.</li>
</ul>
</li>
<li>QT</li>
<li>Eclipse</li>
<li>Android Studio</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cudmltLm9yZy8=">Vi(m)<i class="fa fa-external-link-alt"></i></span><ul>
<li>모바일 로봇과 같이 SSH를 사용해서 원격 코딩해야할 때 좋습니다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="개발-환경-OS"><a href="#개발-환경-OS" class="headerlink" title="개발 환경 - OS"></a>개발 환경 - OS</h3><ul>
<li>Windows<ul>
<li>Visual Studio C++</li>
<li>MS HoloLens 프로그램이 할 때 좋습니다. (Windows SDK + C++ + C#)</li>
</ul>
</li>
<li>Linux<ul>
<li>CLion or Vim</li>
<li>Visual Studio Code<ul>
<li>가벼운 에디터를 선호하거나</li>
<li>또는 기타 다른 언어들도 하나의 에디터에서 사용해야할 때 (e.g. Python, Rust, Go, Scala)</li>
</ul>
</li>
<li>임베디드 / 로봇 포팅할 때 좋습니다.</li>
<li>ROS 프로그래밍 할 때 좋습니다.</li>
</ul>
</li>
<li>MacOS<ul>
<li>가능하면 피하세요!</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="개발-환경-Toolchain"><a href="#개발-환경-Toolchain" class="headerlink" title="개발 환경 - Toolchain"></a>개발 환경 - Toolchain</h3><ul>
<li>Windows<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly92aXN1YWxzdHVkaW8ubWljcm9zb2Z0LmNvbS9rby8=">Visual Studio C++<i class="fa fa-external-link-alt"></i></span>를 설치하면서 기본적인 C++을 위한 툴체인 설치.</li>
<li>크로스 플랫폼 개발을 위한 <span class="exturl" data-url="aHR0cHM6Ly9jbWFrZS5vcmcv">CMake<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuamV0YnJhaW5zLmNvbS9rby1rci9yZXNoYXJwZXItY3BwLw==">Resharper C++<i class="fa fa-external-link-alt"></i></span>를 구매 및 설치해서 intellisense 성능 강화</li>
</ul>
</li>
<li>Linux<ul>
<li>(<strong>추천!</strong>) <span class="exturl" data-url="aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2NoYW5naDk1L2NwcC1jdi1wcm9qZWN0LXRlbXBsYXRlL21haW4vc2NyaXB0cy9jcHBfdG9vbF9jaGFpbnMvaW5zdGFsbF9lc3NlbnRpYWxzLnNo">cv-learn’s cpp-essential package<i class="fa fa-external-link-alt"></i></span>를 사용해서 <code>gcc</code>/<code>clang</code> 컴파일러 설치 및 <code>git</code>, <code>build-essential</code>, <code>cmake</code>, <code>cppcheck</code> 패키지 설치</li>
<li>(<strong>추천!</strong>) <span class="exturl" data-url="aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2NoYW5naDk1L2NwcC1jdi1wcm9qZWN0LXRlbXBsYXRlL21haW4vc2NyaXB0cy9jcHBfdG9vbF9jaGFpbnMvaW5zdGFsbF9sbHZtX3Rvb2xjaGFpbi5zaA==">Ignacio Vizzo’s llvm toolchain install script<i class="fa fa-external-link-alt"></i></span>를 사용해서 <code>clang-tools</code>, <code>clang-tidy</code>, <code>libclang</code>, <code>clang-format</code>, <code>python3-clang</code>, <code>clangd</code>, <code>OpenMP</code>, <code>libc++</code>, <code>lldb</code> 설치.</li>
</ul>
</li>
</ul>
<h3 id="개발-환경-Linter-amp-Formatter"><a href="#개발-환경-Linter-amp-Formatter" class="headerlink" title="개발 환경 - Linter &amp; Formatter"></a>개발 환경 - Linter &amp; Formatter</h3><ul>
<li><code>clang-format</code>을 사용해서 코드 작성 중 ‘저장’을 누를 때 마다 자동으로 줄바꿈, space 공백처리, tab 처리 등등 교정.<ul>
<li>clang-format을 직접 만들고싶다면 <span class="exturl" data-url="aHR0cHM6Ly96ZWQwLmNvLnVrL2NsYW5nLWZvcm1hdC1jb25maWd1cmF0b3Iv">clang-format configurator<i class="fa fa-external-link-alt"></i></span>을 사용해서 직접 만듬.</li>
<li>(<strong>추천!</strong>) 또는 <span class="exturl" data-url="aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2NoYW5naDk1L2NwcC1jdi1wcm9qZWN0LXRlbXBsYXRlL21haW4vLmNsYW5nLWZvcm1hdA==">cv-learn’s clang-format<i class="fa fa-external-link-alt"></i></span>을 사용.</li>
</ul>
</li>
<li>Linux 환경에서는 <code>clangd</code>를 통해 static_analysis 수행<ul>
<li>코드에서 이상한 부분을 알려주거나, 헤더파일이 링크가 안된 부분을 상시 리포트 해줌.</li>
<li>Windows 환경에서는 Visual studio가 다 해줌.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<hr>
<h2 id="코드"><a href="#코드" class="headerlink" title="코드"></a>코드</h2><h3 id="코드-파일-이름"><a href="#코드-파일-이름" class="headerlink" title="코드 - 파일 이름"></a>코드 - 파일 이름</h3><ul>
<li><code>.cpp</code>와 <code>.hpp</code> 사용을 권장.</li>
<li>Linux-only 코드인 경우 <code>.cc</code>, <code>.hh</code>도 가능.<ul>
<li>하지만 <code>.cpp</code>, <code>.hpp</code>와 혼용하지 않는 것을 적극 권장.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-include"><a href="#코드-include" class="headerlink" title="코드 - include"></a>코드 - include</h3><ul>
<li><strong>include 자동 정렬은 비활성화</strong>한다.<ul>
<li>특정 외부 라이브러리를 사용할 때 내부 생성 매크로로 인해 include 순서에 따라서 빌드 성공/실패 여부가 생길 수 있기 때문.</li>
</ul>
</li>
<li>순서는 가능하면 <strong>먼 순서에서 가까운 순서대로 선언</strong>함. (i.e. 외부 라이브러리 -&gt; 프로젝트 내부 소스 파일)</li>
<li>“” 대신 &lt;&gt;를 사용하기.</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;myHeader.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-namespace"><a href="#코드-namespace" class="headerlink" title="코드 - namespace"></a>코드 - namespace</h3><ul>
<li>double namespace는 최대한 피하기<ul>
<li>위 보다 아래가 더 좋은 방식</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> Vehicle</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">namespace</span> Dynamics</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> VehicleDynamics</span><br><span class="line">&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-literal-값-표현-방식"><a href="#코드-literal-값-표현-방식" class="headerlink" title="코드 - literal 값 표현 방식"></a>코드 - literal 값 표현 방식</h3><ul>
<li><strong>int의 경우 정확한 비트 수를 포함하여 선언</strong>하는 것이 크로스 컴파일 시 오류가 나타나지 않음.<ul>
<li>이 때문에 <code>char</code>, <code>short</code>, <code>size_t</code>, <code>int</code> 와 같은 타입은 잘 사용하지 않음.</li>
</ul>
</li>
<li><strong>float의 경우 f를 적어줘서 정확하게 값을 표현</strong>해줌<ul>
<li><code>float num = 123.456</code>은 안됨.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// int / uint.</span></span><br><span class="line"><span class="keyword">int8_t</span> a; <span class="comment">// uint8_t a;</span></span><br><span class="line"><span class="keyword">int16_t</span> b;</span><br><span class="line"><span class="keyword">int32_t</span> c;</span><br><span class="line"><span class="keyword">int64_t</span> d;</span><br><span class="line"></span><br><span class="line"><span class="comment">// float</span></span><br><span class="line"><span class="keyword">float</span> num1 = <span class="number">123.456f</span>;</span><br><span class="line"><span class="keyword">float</span> num2 = <span class="number">123.0f</span>; <span class="comment">// &#x27;f&#x27; 이전에 0을 적어준다.</span></span><br><span class="line"><span class="keyword">float</span> num3 = <span class="number">0.1234f</span>;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-Class-설계"><a href="#코드-Class-설계" class="headerlink" title="코드 - Class 설계"></a>코드 - Class 설계</h3><ul>
<li>클래스 이름은 대문자로 시작하며 CamelCase로 작성한다.</li>
<li>클래스 멤버함수와 멤버변수들에 대한 설명은 <a href="https://www.doxygen.nl/index.html"><strong>doxygen</strong></a>을 사용하면 좋다.<ul>
<li>코드 내부에서도 통일된 방식으로 설명을 볼 수 있다.</li>
<li>또, doxygen 빌드를 하면 자동으로 모든 함수/변수에 대한 설명이 적힌 웹사이트가 생성된다. (i.e. 개발자 문서 페이지를 쉽게 만들 수 있다)</li>
</ul>
</li>
<li>한줄로 작성될 수 있는 코드는 한줄로 작성한다. (아래의 <code>getParam()</code>, <code>setParam()</code> 참조)</li>
<li>멤버변수에 접근하는 방법은 2가지 방법이 있다.<ul>
<li><strong>Getter / Setter 함수를 사용</strong>하는 방법<ul>
<li><code>int getParam() const;</code> 을 통해 클래스 외부에서 멤버변수 값을 읽는다.</li>
<li><code>void setParam(int param);</code> 을 통해 클래스 외부에서 멤버변수 값을 수정한다.</li>
</ul>
</li>
<li><strong>Public struct를 통해 값을 직접 접근</strong>하여 설정한다.</li>
<li><code>int&amp; param() &#123;return mParam;&#125;</code>과 같이 <strong>Reference 직접 접근하는 방식은 절대 사용하지 않는다</strong> (엄청 위험하다)</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> ORBSLAM</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureDetector</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> Keypoints = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Keypoint&gt;;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">Data</span></span></span><br><span class="line"><span class="class">    &#123;</span></span><br><span class="line">        <span class="keyword">int</span> mIntParam = <span class="number">0</span>;       <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">        <span class="keyword">double</span> mDoubleParam = <span class="number">0</span>; <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * @brief 간단한 함수 설명</span></span><br><span class="line"><span class="comment">    * @param param 매개변수 설명</span></span><br><span class="line"><span class="comment">    * @return 리턴값 설명</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">detect</span><span class="params">(<span class="keyword">const</span> Image&amp; img)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getParam</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;<span class="keyword">return</span> mParam&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setParam</span><span class="params">(<span class="keyword">int</span> param)</span> </span>&#123; mParam = param; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> mParam = <span class="number">0</span>;  <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">    Keypoints mKpts; <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">    Data mData;      <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-함수"><a href="#코드-함수" class="headerlink" title="코드 - 함수"></a>코드 - 함수</h3><ul>
<li><strong>함수의 이름은 해당 함수의 목적과 과정을 정확하게 담아낸다</strong>.<ul>
<li><code>void removeOutliers()</code> 대신 <code>void removeOutliersbyReprojectionError()</code>가 더 좋다.</li>
<li>함수의 이름이 길어져도 괜찮다.</li>
</ul>
</li>
<li>함수 이름은 소문자로 시작하며, 이후 CamelCase로 작성한다.</li>
<li>함수가 한줄로 작성될 수 있는 경우에는, 함수 이름에서 대괄호를 이어서 작성한다.<ul>
<li><code>int getParam() const &#123; return mParam &#125;;</code></li>
</ul>
</li>
<li>함수가 여러 줄로 작성될 때는, 함수 이름을 작성 후 그 다음줄에 대괄호를 적으며 작성한다.</li>
<li><strong><code>const</code>가 필요하지 않은 곳 외의 모든 곳에 <code>const</code>를 사용</strong>한다.<ul>
<li><code>const</code>를 적음으로써 ‘const-함’을 표현하는 것이 아닌, <strong><code>const</code>를 사용하지 않음으로써 해당 함수의 특성을 표현</strong>한다고 생각하자.</li>
<li>이러한 특성이 두드러지게 표현해야 내 코드의 목적이 잘 전달된다. (코드 리뷰에서는 이러한 점이 중요하다)</li>
<li><code>double evaluateModel(const Model&amp; model, const Data&amp; data);</code></li>
<li><code>const Images&amp; getImages(const Images&amp; imgs) const;</code></li>
</ul>
</li>
<li>namespace, class, type 등과 겹치는 함수 이름은 피한다.</li>
<li><code>if</code>, <code>for</code>, <code>while</code>을 사용할 때 내용이 1줄만 사용된다면 대괄호를 사용하지 않는다.</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-함수-argument"><a href="#코드-함수-argument" class="headerlink" title="코드 - 함수 argument"></a>코드 - 함수 argument</h3><ul>
<li>Pass by reference / pointer / value의 차이를 정확하게 인지하고 사용해야한다.</li>
<li><strong>Pass by reference</strong><ul>
<li>복사 작업이 수행되지 않는다.</li>
<li>그러므로, <strong>클래스 인스턴스</strong>를 읽거나 <strong>큰 데이터를 읽을 때</strong>는 <strong>무조건 pass by reference를 사용</strong>한다.<ul>
<li>원본 데이터를 유지하고 싶을 때는 <code>void foo(const Image&amp; a)</code></li>
<li>원본 데이터를 수정하고 싶을 때는 <code>void foo(Image&amp; a)</code></li>
</ul>
</li>
</ul>
</li>
<li>Pass by pointer<ul>
<li>데이터 내부의 특정 포인터 위치를 넘길 때 사용한다.</li>
<li>또, 클래스 인스턴스를 옵셔널하게 넘길 때 사용한다. (<code>nullptr</code>이 넘겨진다면 빈 값이 넘겨지는 것)<ul>
<li><code>void foo(int a, Data* ptr = nulltpr)</code></li>
<li>옵셔널하게 클래스 인스턴스를 넘기는 상황이 아닌 경우에는 pass by reference를 사용하는 것을 권장한다.</li>
</ul>
</li>
</ul>
</li>
<li>Pass by value<ul>
<li>복사 작업을 수행해야하는 경우에 사용한다.<ul>
<li><code>std::move</code>를 사용해서 <code>void foo(Data data)</code>를 쓸 수 있다. 하지만 권장하지 않음.</li>
</ul>
</li>
<li><strong>Primitive 값들을 넘길 때 사용</strong>한다.<ul>
<li><code>void foo(int a, double b)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Pass by reference</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gaussianFilter</span><span class="params">(<span class="keyword">const</span> Image&amp; inputImage, Image&amp; outputImage, <span class="keyword">const</span> KernelMatrix&amp; kernel)</span></span>; <span class="comment">// outputImage는 이번 함수를 통해 수정될 값이라는 것을 강조. 동시에, 다른 매개변수는 수정되지 않을 것이라는 것을 강조.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Pass by pointer</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">evaluateModel</span><span class="params">(<span class="keyword">const</span> Model&amp; model, <span class="keyword">const</span> Data* data = <span class="literal">nullptr</span>)</span> <span class="comment">// data는 optional한 값이라는 것을 강조. </span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (data == <span class="literal">nullptr</span>) <span class="comment">// 데이터가 들어오지 않은 경우에는 함수 조기 종료</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::numeric_limits&lt;<span class="keyword">double</span>&gt;::max();</span><br><span class="line"></span><br><span class="line">  ... <span class="comment">// data에 어떤 값이 들어있는 거라면 제대로 된 model evaluation 수행</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pass by value</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">detectObjects</span><span class="params">(<span class="keyword">const</span> Image&amp; inputImage, <span class="keyword">int</span> numObjects)</span> <span class="comment">// Primitive type은 거의 모든 경우 pass by value 사용.</span></span></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-변수"><a href="#코드-변수" class="headerlink" title="코드 - 변수"></a>코드 - 변수</h3><ul>
<li>변수 이름은 소문자로 시작하며 CamelCase로 작성한다.</li>
<li>전역 변수 (global variable)은 global-을 뜻하는 <code>g-</code> prefix를 가진다. (e.g. gParam)<ul>
<li>전역변수를 사용하는 것은 추천하지 않는다.</li>
</ul>
</li>
<li><strong>클래스 멤버변수</strong>는 member-를 뜻하는 <strong><code>m-</code> prefix</strong>를 가진다. (e.g. mParam).</li>
<li>Struct 멤버변수는 member-를 뜻하는 <code>m-</code> prefix를 가진다. (e.g. mParam).</li>
<li><strong>Constant 변수</strong>는 constant k value-를 뜻하는 <strong><code>k-</code> prefix</strong>를 가진다. (e.g. kPi = 3.141592654;).</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Object</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">setParam</span><span class="params">(<span class="keyword">int</span> intParam, <span class="keyword">double</span> doubleParam)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    mIntParam = intParam; <span class="comment">// &#x27;m-&#x27; prefix를 사용해 매개변수와 멤버변수를 구분. 거의 동일한 이름 사용 가능.</span></span><br><span class="line">    mDoubleParam = doubleParam;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">int</span> mIntParam; <span class="comment">// 클래스 멤버 변수는 &#x27;m-&#x27; prefix를 사용</span></span><br><span class="line">  <span class="keyword">double</span> mDoubleParam;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-템플릿"><a href="#코드-템플릿" class="headerlink" title="코드 - 템플릿"></a>코드 - 템플릿</h3><ul>
<li>템플릿 코드를 작성할 때는 설명을 정확하게 작성하는 것이 중요하다.</li>
<li>템플릿 / 타입이름은 대문자로 작성한다.</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> PRECISION = <span class="keyword">double</span>, <span class="keyword">typename</span> ESTIMATOR, <span class="keyword">typename</span> CAMERAMODEL&gt;</span><br><span class="line">class Camera</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-에러-처리"><a href="#코드-에러-처리" class="headerlink" title="코드 - 에러 처리"></a>코드 - 에러 처리</h3><ul>
<li>임베디드 환경에서 성능 최적화를 위해 에러처리는 디버깅 단계에서 전부 확인해야한다.<ul>
<li>이 때문에 try &amp; catch보다 <strong><code>assert()</code>를 사용</strong>한다.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gaussianFilter</span><span class="params">(<span class="keyword">const</span> Image&amp; inputImage, Image&amp; outputImage)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 이 함수는 inputImage에 저장된 값이 실제 이미지여야만 작동함.</span></span><br><span class="line">  <span class="comment">// assert를 이용해서 빈 데이터는 작동하지 않게 코드를 작성.</span></span><br><span class="line">  assert(!inputImage.empty()); </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (outputImage.size() != inputImage.size())</span><br><span class="line">    outputImage.setSize(ingputImage.size());</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-Unit-test"><a href="#코드-Unit-test" class="headerlink" title="코드 - Unit test"></a>코드 - Unit test</h3><ul>
<li><strong>구현되는 모든 코드에 대해 unit test를 작성</strong>한다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS9nb29nbGV0ZXN0">GTest<i class="fa fa-external-link-alt"></i></span> 프레임워크를 이용한다.</li>
</ul>
</li>
<li>너무 뻔한 테스트는 생략해도 괜찮다. (int를 반환하는 함수가 정말로 int를 반환했는지 확인한다던지…)</li>
<li><strong>Edge cases와 failure case를 가능한 많이 추가</strong>한다.</li>
<li>Unit test를 작성하는데에 <strong>시간을 충분히 할애</strong>한다.<ul>
<li>Leaky unit test를 작성했다가 추후 unit test는 통과하는데 릴리즈 코드가 작동하지 않는다면… 해당 버그를 찾아서 해결하는 시간이 훨씬 더 오래 걸릴 것이다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-코드-최적화"><a href="#코드-코드-최적화" class="headerlink" title="코드 - 코드 최적화"></a>코드 - 코드 최적화</h3><ul>
<li><strong>처음부터 완벽한 코드를 작성하려고 하지 말자</strong>.</li>
<li><strong>코드를 작성하는 3단계</strong>를 지킨다. (rough-&gt;fine-&gt;perfect)<ul>
<li>1차: <strong>‘1개의 케이스에서 잘 돌아가는 코드를 만들자’</strong><ul>
<li>유닛테스트 작성 + 코드 리뷰</li>
</ul>
</li>
<li>2차: <strong>‘고객이 요구하는 모든 케이스들 + edge 케이스에서 잘 돌아가는 코드를 만들자’</strong><ul>
<li>유닛테스트 작성 + 코드 리뷰</li>
</ul>
</li>
<li>3차: <strong>‘성능이 최적화된 코드를 만들자’</strong><ul>
<li>유닛테스트 작성 + 코드 리뷰</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-STL-Containers-모던-C-문법"><a href="#코드-STL-Containers-모던-C-문법" class="headerlink" title="코드 - STL, Containers, 모던 C++ 문법"></a>코드 - STL, Containers, 모던 C++ 문법</h3><ul>
<li>직접 구현하지 말고 <strong>STL 함수가 있는지 꼭 먼저 찾아보자</strong>.<ul>
<li>컴파일러들은 STL 함수에 대해 최적화가 정말 잘 되어있다.</li>
<li>거의 모든 경우 내가 짠 코드보다 STL 함수가 더 최적화가 잘 되어있다.</li>
</ul>
</li>
<li><strong><code>using namespace std</code>는 절대 쓰지 말자</strong>. <ul>
<li>conflict 생길 수 있다.</li>
</ul>
</li>
<li>STL을 사용하면서 코드 가독성이 떨어진다면 주석으로 충분히 설명을 잘 해주자.<ul>
<li>가능하면 주석 없이도 이해할 수 있도록 코드를 쪼개주자.</li>
</ul>
</li>
<li><strong>목적에 맞는 컨테이너를 사용</strong>하자.<ul>
<li><code>std::vector</code>, <code>std::unordered_map</code>, <code>std::map</code>…</li>
<li><code>std::unordered_map</code>의 경우 컴파일러마다 구현 방법이 다르기 때문에 결과가 다를 수 있다. 가능하면 직접 hash-function도 만들어주자.</li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; data;</span><br><span class="line"></span><br><span class="line"><span class="comment">// C++98 시대의 방법. 가독성이 떨어진다.</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; data.size(); i++)</span><br><span class="line">    foo(data[i]);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 모던 C++의 range-based for loop 방법. 가독성이 좋다.</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; d: data)</span><br><span class="line">    foo(d);</span><br><span class="line"></span><br><span class="line"><span class="comment">// STL의 방법. 코드 효율성은 높아지나 가독성은 떨어진다.</span></span><br><span class="line"><span class="built_in">std</span>::for_each(data.begin(), data.end(), [<span class="keyword">this</span>](<span class="keyword">const</span> <span class="keyword">auto</span>&amp; i)&#123; foo(i); &#125;; )</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-Enum-class-strong-types"><a href="#코드-Enum-class-strong-types" class="headerlink" title="코드 - Enum class / strong types"></a>코드 - Enum class / strong types</h3><ul>
<li>Type을 만들 때는 Strong type의 사용을 적극 권장한다 (i.e. <strong>Enum class 사용을 권장</strong>한다)<ul>
<li>Enum만 사용하는 것은 권장하지 않는다.</li>
</ul>
</li>
<li>Enum class의 멤버들은 대문자와 <code>_</code>를 사용한다.</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// Doxygen을 위한 enum class 설명</span></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="keyword">class</span> <span class="title">DetectionStatus</span> :</span> <span class="built_in">std</span>::<span class="keyword">int8_t</span></span><br><span class="line">&#123;</span><br><span class="line">    NOT_DETECTED = <span class="number">0</span>; <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">    SUCCESS = <span class="number">1</span>;      <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">    FAIL = <span class="number">2</span>;         <span class="comment">///&lt; 변수 설명</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-타입-캐스팅"><a href="#코드-타입-캐스팅" class="headerlink" title="코드 - 타입 캐스팅"></a>코드 - 타입 캐스팅</h3><ul>
<li>Explicit하게 타입 변화를 해주자.<ul>
<li><code>static_cast</code>, <code>dynamic_cast</code></li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">double</span> j = (<span class="keyword">double</span>)i; <span class="comment">// 사용 금지</span></span><br><span class="line"><span class="keyword">double</span> j = <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(i); <span class="comment">// 권장</span></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h3 id="코드-Log"><a href="#코드-Log" class="headerlink" title="코드 - Log"></a>코드 - Log</h3><ul>
<li><code>std::cout</code>은 굉장히 느린 편이므로 <a href="https://github.com/fmtlib/fmt"><strong>fmt</strong></a>를 사용한 로깅 라이브러리를 사용하는 것이 좋다.<ul>
<li>e.g. fmt 기반으로 만들어진 <a href="https://github.com/gabime/spdlog"><strong>spdlog</strong></a>가 사용하기 좋다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-모던-C"><a href="#코드-모던-C" class="headerlink" title="코드 - 모던 C++"></a>코드 - 모던 C++</h3><ul>
<li>매크로<ul>
<li><strong><code>using DEFINITION = MACRO;</code>를 사용</strong>한다.<ul>
<li>C 방식의 <code>#define MACRO DEFINITION</code>은 사용하지 않는다.</li>
</ul>
</li>
</ul>
</li>
<li>출력<ul>
<li><code>std::cout</code>을 사용한다.<ul>
<li>C 방식의 <code>printf</code>는 사용하지 않는다.</li>
<li>Log 목적이라면 위에 설명한 <strong>spdlog 사용을 적극 권장</strong>한다.</li>
</ul>
</li>
</ul>
</li>
<li>포인터<ul>
<li><strong><code>std::unique_ptr</code>, <code>std::shared_ptr</code>의 사용을 적극 권장</strong>한다.<ul>
<li>C 방식의 Raw pointer의 사용은 가능한 피한다.</li>
<li>C++ 방식의 <code>new</code>, <code>delete</code>의 사용도 가능한 피한다.</li>
</ul>
</li>
</ul>
</li>
<li>String<ul>
<li><strong><code>std::string</code>의 사용을 권장</strong>한다.<ul>
<li>C 방식의 <code>const char*</code>는 사용하지 않는다.</li>
<li>외부 라이브러리 사용을 위해 C 방식의 char가 필요하다면, std::string으로부터 <code>c_str()</code> 함수를 사용해서 바꾼다.</li>
</ul>
</li>
</ul>
</li>
<li>Array (Stack)<ul>
<li><strong><code>std::array</code>의 사용을 권장</strong>한다.<ul>
<li>C 방식의 <code>int[]</code>는 사용하지 않는다.</li>
</ul>
</li>
</ul>
</li>
<li>For loop<ul>
<li><strong>Range-based for loop의 사용을 적극 권장</strong>한다.</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="코드-성능-측정-최적화"><a href="#코드-성능-측정-최적화" class="headerlink" title="코드 - 성능 측정 + 최적화"></a>코드 - 성능 측정 + 최적화</h3><ul>
<li>코드에서 생기는 성능 bottleneck은 profiler로 찾아서 제거한다.<ul>
<li>Linux에서는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0tEQUIvaG90c3BvdA==">Hotspot<i class="fa fa-external-link-alt"></i></span>을 자주 사용한다.</li>
<li>크로스 플랫폼 + 멀티코어 환경에서는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lzZS9lYXN5X3Byb2ZpbGVy">EasyProfiler<i class="fa fa-external-link-alt"></i></span>를 사용한다.</li>
<li>GPU 프로그래밍의 경우는 해당 GPU 벤더사에서 프로파일링 도구를 제공하기도 한다. (e.g. <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9udmlkaWEtdmlzdWFsLXByb2ZpbGVy">NVIDIA nSight Visual profiler<i class="fa fa-external-link-alt"></i></span>)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="협업"><a href="#협업" class="headerlink" title="협업"></a>협업</h2><h3 id="협업-형상관리"><a href="#협업-형상관리" class="headerlink" title="협업 - 형상관리"></a>협업 - 형상관리</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tLw==">Github<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hYm91dC5naXRsYWIuY29tLw==">Gitlab<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9iaXRidWNrZXQub3JnLw==">Bitbucket<i class="fa fa-external-link-alt"></i></span>과 같은 <strong>Git 기반 온라인 코드 저장소를 사용하여 형상관리</strong>를 한다.<ul>
<li>Git에 대한 이해도가 있어야한다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXQtc2NtLmNvbS9ib29rL2tv">Git 사용방법<i class="fa fa-external-link-alt"></i></span>을 보고 배우면 좋다.</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZ2l0a3Jha2VuLmNvbS8=">GitKraken<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9kZXNrdG9wLmdpdGh1Yi5jb20v">Github Desktop<i class="fa fa-external-link-alt"></i></span>과 같은 GUI 앱을 사용해도 괜찮지만, <strong>가능하면 위의 방법으로 CLI을 익힌다</strong>.<ul>
<li>CLI 기능 중 GUI에 구현되지 않은 기능들이 많기 때문이다.</li>
</ul>
</li>
</ul>
</li>
<li>Repository 관리 전략<ul>
<li><strong>main 브랜치는 릴리즈를 위한 브랜치</strong>이다.<ul>
<li>고객이 보는 브랜치이기 때문에, 절대로 버그가 있어서는 안된다.</li>
<li><strong>admin을 제외한 그 아무도 merge를 수행할 수 없다</strong>.</li>
</ul>
</li>
<li>development 브랜치는 현재 팀이 개발중인 가장 최신의 브랜치이다.</li>
<li>feature / individual 브랜치는 개개인이 작업중인 브랜치이다.<ul>
<li>하나의 feature 브랜치에는 하나의 기능만 개발한다.</li>
<li><strong>Pull request를 통해서 development 브랜치로 작업물을 통</strong>합할 수 있다.<ul>
<li>Merge into branch 방식을 써서 통합할 수 있다.</li>
<li>Rebase branch 방식을 써서도 통합할 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>&nbsp;</p>
<h3 id="협업-코드리뷰"><a href="#협업-코드리뷰" class="headerlink" title="협업 - 코드리뷰"></a>협업 - 코드리뷰</h3><ul>
<li><strong>코드 리뷰는 Pull request를 통해 진행</strong>된다.<ul>
<li>해당 코드 개발과 관련된 인원, 또는 해당 코드를 리뷰해줄 수 있는 인원들로 <strong>최소 2명에게 코드 리뷰</strong>를 받는다.</li>
<li>코드가 아직 준비되지 않은 상황이라면 draft Pull request로 만든다.</li>
<li>Pull request에는 <strong>1. 해당 작업의 배경과 목적</strong>, <strong>2. 수행한 개발 내용 정리</strong>, <strong>3. 해당 코드를 통해 기대하는 효과</strong>를 적는다.<ul>
<li>실험한 내용이 있다면 이 역시 첨부한다.</li>
</ul>
</li>
</ul>
</li>
<li>코드 리뷰의 내용은 다음과 같은 내용을 가지며 <strong>1. 코드 퀄리티의 증진</strong>, <strong>2. 개발된 내용이 실제 목표를 달성하는지를 확인</strong>하는데에 목적을 둔다.<ul>
<li>가독성 증진을 위한 함수/변수 이름 수정</li>
<li>assert 등의 조건문 추가</li>
<li>코드 효율성 증가</li>
<li>개발 내용 확인</li>
<li>등등…</li>
</ul>
</li>
<li><strong>Pull request에서 제안하는 변화는 가능한 작고 짧게 만든다</strong>.<ul>
<li><strong>업데이트되는 코드가 너무 많고 복잡하면</strong> 제대로된 리뷰를 하기가 어려워진다.<ul>
<li>이 때, 리뷰어는 <strong>리뷰를 거부할 수 있다</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>리뷰는 코드를 작성하는것 만큼 정말 중요하다.<ul>
<li><strong>업무 시간의 30~40%는 동료의 코드를 리뷰하는데에 사용</strong>해도 된다.</li>
</ul>
</li>
<li>두명 이상에게서 approve를 받는다면 merge를 진행한다.</li>
</ul>
<p>&nbsp;</p>
<h3 id="협업-CI-CD"><a href="#협업-CI-CD" class="headerlink" title="협업 - CI/CD"></a>협업 - CI/CD</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuamVua2lucy5pby8=">Jenkins<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cuamV0YnJhaW5zLmNvbS90ZWFtY2l0eS8=">TeamCity<i class="fa fa-external-link-alt"></i></span> 등과 같이 agent 할당 프레임워크를 이용해서 CI/CD 빌드 스트림을 만들어둔다.</li>
<li>Pull request가 들어오면 CI/CD는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZlYXR1cmVzL2FjdGlvbnM=">Github Actions<i class="fa fa-external-link-alt"></i></span>를 통해 변화를 감지하고 자동으로 빌드를 수행한다.<ul>
<li>크로스 플랫폼 개발의 경우 <strong>여러 환경에 대한 빌드를 모두 성공해야 merge가 가능</strong>하다.</li>
<li>단일 플랫폼 개발의 경우 <strong>다양한 컴파일러들에 대해 빌드를 수행</strong>하고, 각각의 빌드마다 성능을 측정하여 <strong>가장 좋은 성능을 보여주는 컴파일러 빌드를 사용하여 릴리즈</strong> 할 수 있다.<ul>
<li>성능 벤치마크 스크립트를 작성하면 좋다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20210523-cpp-coding-guideline/gh-action.png" class="" title="gh_actions">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Modern C++</tag>
        <tag>cpp</tag>
        <tag>Coding guideline</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 상반기 주니어 Visual-SLAM 엔지니어 이직 후기</title>
    <url>/20210525-slam-field-job-offers/</url>
    <content><![CDATA[<h2 id="3줄-요약-TL-DR"><a href="#3줄-요약-TL-DR" class="headerlink" title="3줄 요약 - TL;DR"></a>3줄 요약 - TL;DR</h2><ol>
<li>SLAM을 뽑는 곳들이 많아졌다 + 다양해졌다</li>
<li>면접의 난이도가 어려워졌다. 코딩테스트나 구현과제는 거의 필수.</li>
<li>개인 성적 7전 3승 2패 2무 - 만족하는 회사 + 조건으로 <strong>이직 성공</strong>!</li>
</ol>
<p> </p>
<img src="/20210525-slam-field-job-offers/to_the_moon.gif" class="" title="ttm">

<blockquote>
<p>이직 성공 성성공!!! 짜릿해 최고야!!</p>
</blockquote>
<p> </p>
<hr>
<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><p>작년 9월부터 이직에 관심을 가지게 되었습니다. <strong>이직을 결심하게 된 이유</strong>는 다음과 같습니다.</p>
<ol>
<li>딥러닝을 사용하는 다른 필드를 경험해보고싶어서</li>
<li>회사에 대한 불만족</li>
<li>개인적인 사유 (본가에서 출퇴근을 해야함)</li>
</ol>
<p>강남/판교 지역에서 올해 Visual-SLAM 기술 관련으로 채용이 다수 진행되고 있는데, 해당 지역권은 본가에서 출퇴근을 할 수 있기 때문에 <strong>‘강남/판교권에 취직을 하자!’</strong> 가 목표가 되었습니다.</p>
<p>올해 <strong>2월부터 전공공부와 CS관련 지식, 코딩테스트 준비</strong>를 시작했고 <strong>5월에 최종합격 소식</strong>을 받았습니다.</p>
<p>후기를 시작하기 전 간단히 제 이력에 대해 설명하겠습니다. 전공의 적합성과 경력을 고려하셔서 후기를 읽어주시면 좋을 것 같습니다.</p>
<ul>
<li>해외 산업/기계공학 학사<ul>
<li>CS지식 X</li>
</ul>
</li>
<li>해외 의료 로보틱스 연구석사</li>
<li>2년간 국내 증강현실 기업에서 object pose estimation, object tracking 기술개발<ul>
<li>제품 경험 O</li>
<li>연구과제 경험 O</li>
</ul>
</li>
<li>기술 커뮤니티 + 블로그 운영</li>
<li>SLAM 기술 세미나 경험</li>
<li>논문 실적 X</li>
</ul>
<p> </p>
<hr>
<h2 id="포지션-탐색"><a href="#포지션-탐색" class="headerlink" title="포지션 탐색"></a>포지션 탐색</h2><h3 id="SLAM-포지션-찾는-방법"><a href="#SLAM-포지션-찾는-방법" class="headerlink" title="SLAM 포지션 찾는 방법"></a>SLAM 포지션 찾는 방법</h3><p>이직의 가장 첫 단계는 <strong>‘열려있는 포지션 탐색’</strong> 입니다.</p>
<p>포지션을 찾는 방법에는 <strong>‘내가 직접 찾기’ vs ‘헤드헌터를 통해 찾기’</strong> 가 있습니다. 개인적인 의견으로는, 신입/주니어 포지션은 <strong>본인이 직접 찾는게 더 좋다</strong>고 봅니다. 국내 SLAM 포지션은 많지 않아서 원XX, 잡XXX, 링XXX 등의 잡포스팅 서비스들을 이용하거나, 국내 SLAM 기업들 웹사이트를 찾으면 1시간 내에 거의 모든 오픈 포지션을 찾을 수 있습니다. 물론 헤드헌터를 통해서 오픈 포지션을 찾을 수도 있습니다. 하지만 제 경험 상 SLAM 직무와 필요역량에 대해 잘 이애하시는 좋은 헤드헌터 분을 찾기는 어렵습니다. 물론, 좋은 분을 찾으면 고속도로가 뚫린 것 같은 느낌을 받을 수 있을겁니다. 하지만 가장 좋은 방법은 <strong>잡포스팅 서비스에서 technical recruiter (또는 리쿠르팅 담당자)에게 직접 연락</strong>하는 것, 또는 <strong>회사 careers 페이지에 직접 찾아가 공고를 찾아 지원</strong>하는 것 이라고 생각합니다.</p>
<p> </p>
<h3 id="SLAM-포지션에-대한-기대치"><a href="#SLAM-포지션에-대한-기대치" class="headerlink" title="SLAM 포지션에 대한 기대치"></a>SLAM 포지션에 대한 기대치</h3><p>작년까지의 신입/주니어 SLAM 포지션 공고들은 SLAM이라는 단어를 쓰지 않고 visual odometry라는 단어를 사용했습니다. 아무래도 SLAM 보다 visual odometry가 더 쉬워보여서였을까요? 그러다가 <strong>2021년에 들어서 SLAM이라는 단어가 공고에 자주 나타나기 시작합니다</strong>. 더불어 VO보다는 훨씬 어려운 Visual-inertial odometry 기술 경험을 찾는 포지션들도 많이 생겼습니다. 그만큼 visual odometry 정도의 기술로는 기업들이 앞서나가지 못한다고 판단을 하게 된 것이고, 더 실력있는 인재에 대한 수요가 생겼다는 것을 의미한다고 생각합니다.</p>
<p>하지만 여전히 ROS를 사용해서 오픈소스 SLAM을 돌려보기’만’한 사람들은 환영받지 않는 것 같습니다. 오히려 반대로, <strong>기본적인 코어개발 실력이 받쳐주면서, 한발 더 나아가 하이레벨에서 SLAM을 다룰 수 있는 인재</strong>를 원하는 모습을 보였습니다.</p>
<p>SLAM 포지션을 뽑아본 기업들은 이러한 인재를 구하기가 쉽지 않다는 것을 알고 있습니다. 6개월~1년을 기다려서 뽑힐 때도 있기 때문에, 기업들은 지금까지 실력있는 인재를 데려오기 위해서는 어떤 대우를 해줘야할지 많은 고민을 했을 겁니다. 이런 과정을 거쳐 현재 기업들은 <strong>파격적인 대우를 해서라도 함께 롱런할 실력있는 사람이 나타나기만 한다면 무조건 데려오겠다</strong>라는 입장을 가진 경우가 많습니다.</p>
<p>그렇다면 당연히 제가 취해야할 스탠스는…!</p>
<img src="/20210525-slam-field-job-offers/jobs.jpg" class="" title="jobs">

<p> </p>
<h3 id="SLAM이-적용되는-기술-도메인의-특성"><a href="#SLAM이-적용되는-기술-도메인의-특성" class="headerlink" title="SLAM이 적용되는 기술 도메인의 특성"></a>SLAM이 적용되는 기술 도메인의 특성</h3><p>2021년에는 <strong>로보틱스, VR/AR, 자율주행, 드론, 자율비행</strong>까지 포지션이 다양해졌습니다. 아래는 각 분야에 대해 제가 받은 느낌입니다.</p>
<p> </p>
<div class="tabs" id="fields"><ul class="nav-tabs"><li class="tab active"><a href="#fields-1">공통</a></li><li class="tab"><a href="#fields-2">로보틱스</a></li><li class="tab"><a href="#fields-3">VR/AR</a></li><li class="tab"><a href="#fields-4">드론</a></li><li class="tab"><a href="#fields-5">자율주행</a></li><li class="tab"><a href="#fields-6">새로운 분야</a></li></ul><div class="tab-content"><div class="tab-pane active" id="fields-1"><ul>
<li><strong>Visual-inertial odometry (VIO) 에 대한 수요</strong>가 많아졌습니다.</li>
<li><strong>알고리즘 내부의 작동 방식을 이해하는 코어 개발</strong>의 중요성이 더 높아졌습니다.</li>
<li><strong>SLAM 알고리즘 튜닝 경험</strong>, <strong>C++ 프로그래밍 실력</strong>, <strong>협업 경험</strong> 은 예전이나 지금이나 중요합니다.<ul>
<li>기본적인 SLAM 실력에 더불어 <strong>SIMD, CUDA, DSP 가속 기술</strong> 과 같은 추가적인 경험이 있는 분들은 어디에서든 환영받습니다.</li>
<li><strong>센서 캘리브레이션 및 실사용 경험</strong>이 있으신 분들도 환영받습니다.</li>
</ul>
</li>
<li><strong>논문 경험은 필수는 아니지만</strong>, 있으면 당연히 좋은 인상을 남깁니다.</li>
</ul></div><div class="tab-pane" id="fields-2"><ul>
<li><strong>실내/실외 모바일 로봇</strong>에 대한 수요가 주를 이룹니다.</li>
<li><strong>Visual-odometry 코어 개발 경험</strong>와 <strong>ROS 사용 경험</strong>이 주로 요구되었습니다.</li>
<li>실외 localization을 수행하는 로봇 (e.g. 배달로봇)의 경우 <strong>VIO 기술 경험</strong>이 있다면 정말 큰 강점이 될 것 같습니다.</li>
<li>실내 localization을 수행하는 로봇 (e.g. 물류로봇, AGV)에서는 <strong>RGB-D SLAM 사용 경험</strong>, 또는 <strong>visual localization 기술 경험</strong>에 대한 수요가 높았습니다.</li>
</ul>
<p>(제가 찾은 것으로는) 아쉽게도 강남/판교 부근에서 모바일 로봇을 하는 기업은 많지 않았습니다. 의외로 마곡, 가산디지털단지, 용인 등에 위치한 회사들이 많았습니다.</p></div><div class="tab-pane" id="fields-3"><ul>
<li>주로 <strong>공간맵핑</strong>에 대한 수요가 늘었으며, <strong>VIO 기술에 대한 수요</strong>가 높았습니다.<ul>
<li>하지만 VIO 경험자가 아니더라도 <strong>영상처리 경험자는 환영</strong>하는 듯 보입니다.</li>
<li><strong>안드로이드/iOS 개발 경험</strong> 도 선호합니다.</li>
</ul>
</li>
<li>VR쪽 회사들 중에서는 <strong>LiDAR 기술 경험</strong>도 종종 찾는 것을 보았습니다.</li>
<li>모바일 개발 경험이 없더라도, <strong>ARCore/ARKit 경험</strong>,<strong>Unity C#</strong>, <strong>모바일 딥러닝</strong>, <strong>OpenGL/GLES/Vulkan/Metal 그래픽스</strong>, <strong>SIMD가속 경험</strong>이 있다면 큰 강점이 될 것 같습니다.</li>
<li>한가지 특이한 점은, <strong>해외쪽에서는 클라우드 컴퓨팅 사용 경험</strong>을 종종 찾기도 하는데 국내 기업들에서는 아직 보이지 않는 모습입니다.</li>
</ul>
<p>(제가 찾은 것으로는) 강남/판교 부근에서 VR/AR을 하는 기업들은 소수 있었습니다. SLAM 관련 채용도 진행하고 있었고, 기술적으로 상당히 앞선 부분이 있었습니다. 의외로 VR/AR을 하는 회사들 중에서는 SLAM을 하지 않는 회사들이 많습니다. 이러한 회사들은 대부분 딥러닝 기반 얼굴인식이나 자세인식 관련 연구를 하기 때문에 이번 글의 주제에는 해당되지 않습니다.</p></div><div class="tab-pane" id="fields-4"><ul>
<li><strong>임베디드 환경에서의 가벼운 SLAM 알고리즘에 대한 수요</strong>가 높았습니다.<ul>
<li>이러한 환경 제약으로 인해 많은 경우 최신 딥러닝 기반 SLAM이나 graph-based SLAM에 대한 깊은 이해는 필요하지 않았습니다.</li>
<li>대신 ROS 사용유무를 떠나 <strong>하이레벨 SLAM 경험 (e.g. 설계)**과 **성능 튜닝 경험이 요구</strong>되었습니다.</li>
</ul>
</li>
<li>많은 기업들이 NVIDIA jetson 플랫폼을 사용하기 때문에 <strong>CUDA 가속 기술</strong>과 <strong>ARM기반 NEON SIMD 가속 기술</strong>을 보유하셨다면 큰 강점이 될 것 같습니다.</li>
<li>영상처리 뿐만이 아니라 <strong>항공역학, 제어, path planning 지식</strong>, 또는 <strong>초경량 딥러닝 기술</strong>을 이해하시는 분을 좀 더 좋아합니다.</li>
</ul>
<p>(제가 찾은 것으로는) 강남/판교 부근에서 드론 기업을 5곳정도 찾을 수 있었습니다. 각각의 기업들이 가지고 있는 BM이 확연하게 차이가 나서 흥미로웠습니다.</p></div><div class="tab-pane" id="fields-5"><ul>
<li><strong>Sensor fusion에 대한 수요</strong>가 높았습니다.</li>
<li>많은 기업들이 <strong>HD-Map 제작 및 사용</strong>을 하기 위해 ‘Map &amp; localization 엔지니어’라는 포괄적인 의미로 SLAM 엔지니어를 채용하였습니다.</li>
<li><strong>이종 센서간의 최적화 프로그래밍 경험</strong>이 가장 선호되었습니다.<ul>
<li>여기에 필요한 <strong>실차 센서 사용 및 통신 경험</strong>, <strong>데이터 association &amp; 퓨전 경험</strong>, <strong>자율주행 도메인에 대한 경험/이해</strong>가 요구되었습니다.</li>
<li>추가적으로 <strong>지리데이터 (e.g. GIS) 사용 경험</strong>, <strong>서버 분산처리 및 배치 프로그래밍</strong>, <strong>CUDA/OpenCL 가속</strong>, <strong>딥러닝 기술</strong>과 같은 기술을 보유하셨다면 큰 강점이 될 것 같습니다.</li>
</ul>
</li>
<li><strong>최신 자율주행 논문/기업/기술 트렌드를 파악</strong>하시는 것이 중요합니다.</li>
</ul>
<p>자율주행 기업들은 강남/판교 부근에서 상당히 많이 찾으실 수 있습니다. 지금까지 거론한 필드들 중에 가장 투자가 많이 들어간 필드라고 생각하고, 그만큼 회사들의 규모와 회사의 목표들도 굉장히 큰 편입니다.</p></div><div class="tab-pane" id="fields-6"><ul>
<li><strong>자율선박운행</strong>, <strong>자율비행</strong> 등 새로운 분야가 있었습니다.<ul>
<li>이처럼 초창기 사업 + 실험적인 기술에는 <strong>하나의 기술 분야에 충분한 경험</strong> + <strong>다양한 기술 분야에 대한 이해를 요구</strong>하였습니다.</li>
<li>물론 팀바이팀이겠지만, 솔직히 이런 포지션에는 시니어 급 포지션을 영입하는 것이 맞는 것 같고, 신입/주니어가 갈만한 포지션은 아니라고 생각됩니다.</li>
</ul>
</li>
</ul></div></div></div>

<p> </p>
<h3 id="소감"><a href="#소감" class="headerlink" title="소감"></a>소감</h3><blockquote>
<p><strong>아니 근데 반년전에는 visual-odometry 경험만 요구하다가 2021년 되었다고 갑자기 이렇게 많이 요구하는거면 대체?!</strong> <span class="github-emoji" alias="rofl" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f923.png?v8">🤣</span></p>
</blockquote>
<p> </p>
<h3 id="그래서-어떤-필드"><a href="#그래서-어떤-필드" class="headerlink" title="그래서 어떤 필드?"></a>그래서 어떤 필드?</h3><p>저는 <strong>자율주행=로보틱스&gt;드론 순서대로 선호</strong>도를 잡고 job search를 시작했습니다. </p>
<p>아무래도 이전 회사에서 모바일 기기로 컴퓨터 비전 계산을 하다보니 <strong>이번에는 그나마 조금 더 풍족한 컴퓨팅 자원</strong>에서 더 격한 (?) 알고리즘을 만들어보고 싶었습니다. 드론 분야는 이러한 이유로 자율주행과 로보틱스에 비해 선호도가 조금 밀리게 되었습니다. 온디바이스 기기는 자율주행이 아무래도 로보틱스 분야보다 더 좋은 사양을 가질 수 있지만, 두 분야 모두 <strong>클라우드 컴퓨팅을 사용할 가능성</strong>도 있었기 때문에 해당 경우에 대해서는 동일한 선호도를 두었습니다. VR/AR 분야에 올라온 공고들은 대부분 이전 기업의 경쟁사였기 때문에 지원을 고려하지 않았습니다.</p>
<p> </p>
<hr>
<h2 id="이력서-CV-Resume"><a href="#이력서-CV-Resume" class="headerlink" title="이력서 (CV/Resume)"></a>이력서 (CV/Resume)</h2><p>필드가 정해지고나서는 가장 먼저 이력서를 만들었습니다.</p>
<p>이직/면접 과정을 소개팅에 비교한다면 이력서는 ‘<strong>첫 인상</strong>‘에 해당한다고 생각합니다. 이력서는 그만큼 <strong>깔끔</strong>하게 다듬어야하고, 또 <strong>매력적</strong>이게 + 한번 만나보고싶게 만들어야합니다.</p>
<p>이력서를 적을때는 <strong>2가지가 준비</strong>되어야합니다.</p>
<ul>
<li>지원하는 직무에 <strong>큐레이션된 이력</strong></li>
<li>명쾌하게 내 의도를 전달하는 <strong>디자인</strong> (i.e. 내가 제일 찰떡이요, 날 뽑으시오!!)</li>
</ul>
<p> </p>
<h3 id="큐레이션된-이력"><a href="#큐레이션된-이력" class="headerlink" title="큐레이션된 이력"></a>큐레이션된 이력</h3><p>전반적인 이력 큐레이션에 관련해서는 <a href="https://changh95.github.io/20210528-work-history">이 글</a> 을 참조하시길 바랍니다.</p>
<p>위 글에서 시사하는 바와 같이 저는 <strong>지원하는 직무마다 하나씩 큐레이션해서 CV를 만들었습니다</strong>. </p>
<p>저는 다양한 필드에 지원하고 싶었는데, 필드마다 사용되는 SLAM 기술의 특성이 많이 다르기 때문에 <strong>‘SLAM 기술 경력’ 하나만으로 모든 필드에 다 지원할 수는 없기 때문입니다</strong>. 예를 들어, 드론 분야에서는 경량화된 빠른 VIO를 원하는데, GPU 서버를 써야하는 딥러닝 기반 semantic visual-lidar fusion SLAM 경력으로 지원하면 좋은 핏이 아니라고 볼 수 있습니다. 이러한 이유 때문에 저는 <strong>자XX닷컴, 원XX 등등과 같이 하나의 이력서를 만들고 지원포탈을 통해서 여러 회사에 뿌리는 서비스를 추천하지 않습니다</strong>.</p>
<p>이력서를 적기 전에 제 이력에 대해 평가를 해보았을 때, <strong>SLAM 개발 경험</strong>, <strong>SLAM 기술에 대한 이해도/관심도 표출</strong>, <strong>논문 실적</strong>이 부족해 보였습니다. 이걸 어떻게든 채워야 이직을 할 수 있다고 생각했습니다.</p>
<div class="tabs" id="work-history"><ul class="nav-tabs"><li class="tab active"><a href="#work-history-1">SLAM 개발 경험</a></li><li class="tab"><a href="#work-history-2">SLAM 기술에 대한 이해도/관심도</a></li><li class="tab"><a href="#work-history-3">논문 실적</a></li></ul><div class="tab-content"><div class="tab-pane active" id="work-history-1"><p><strong>부족한 SLAM 개발 경험</strong>을 채우기 위해 다음과 같은 액션을 취했습니다.</p>
<ol>
<li><a href="https://modulabs.co.kr/product/13th-wise-slam-study/"><strong>SLAM 구현 스터디</strong></a>를 기획해서 진행해봤지만 큰 성과는 없었습니다. </li>
<li>회사에서 SLAM 업무를 하려고 했습니다.<ul>
<li>…만 SLAM이 아닌 다른 업무가 배정되었습니다 <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span> </li>
</ul>
</li>
<li><strong>2개의 사이드 프로젝트를 진행</strong>했습니다.<ul>
<li>CubeSLAM에서 영감을 받아 딥러닝 + SLAM 개발</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L2NwcC1jdi1wcm9qZWN0LXRlbXBsYXRl">Visual-SLAM을 위한 보일러 플레이트 프로젝트 (cpp-cv-project-template)<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ol></div><div class="tab-pane" id="work-history-2"><p><strong>부족한 SLAM 기술에 대한 이해도/관심도 표출</strong>을 위해 다음과 같은 액션을 취했습니다.</p>
<ol>
<li><strong>기술 블로그</strong>를 더욱 채워가고 내용을 페이스북에 공유하였습니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9jdi1sZWFybi5jb20vRGVlcC1WaXN1YWwtU0xBTS0wLURlZXAtU0xBTS02MDNhMjZjNDdjMzg0OGE4OGI5MDU1OWQyNzNmOTVkYw==">Deep SLAM 글<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9jdi1sZWFybi5jb20vT3V0bGllci1Sb2J1c3QtZXN0aW1hdGlvbi0wLU91dGxpZXItZmEwYzljYjkyZmRjNGRlNzk5MDAxMTg3NjZjNTVjYmY=">Robust estimation 글<i class="fa fa-external-link-alt"></i></span></li>
<li>… 등등</li>
</ul>
</li>
<li><strong>SlideShare</strong>에 이전 SLAM 세미나 자료를 보강해서 공유하기도 하였습니다 <ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2xpZGVzaGFyZS5uZXQvSHl1bmdnaUNoYW5nL3Zpc3VhbHNsYW0taW4tMS1kYXk=">‘하루만에 Visual-SLAM 고수되기’ 자료<i class="fa fa-external-link-alt"></i></span>. </li>
</ul>
</li>
<li><strong>서베이 논문과 SLAM 논문</strong>을 읽고 정리했습니다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L3Zpc3VhbC1zbGFtLXJvYWRtYXA=">Visual-SLAM 로드맵<i class="fa fa-external-link-alt"></i></span>.</li>
</ul>
</li>
</ol></div><div class="tab-pane" id="work-history-3"><p><strong>부족한 논문 실적</strong>을 위해 아이디어를 내보고 여러 연구자분들에게 컨택을 해보았지만…</p>
<p>논문을 적을 실력도 여유도 부족했을 뿐 더러, 논문을 적어도 회사측에서 소속을 걸지 못하게 할 것 같은 느낌이 들었습니다.</p>
<p>실제로 survey paper라도 내보자, 그거도 안되면 white paper라도 내보자 했지만 전부 거절… <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span></p>
<p>현실적으로 불가능 할 것 같아 <strong>유일하게 논문 실적만큼은 포기</strong>하였습니다.</p></div></div></div>

<p> </p>
<p>부족한 부분을 어느정도 채운 후, <strong>Long CV를 작성</strong>했습니다.</p>
<p>그 후,  지원하려는 직무들마다 내용을 맞춰 <strong>Short CV에 적을 내용들을 정제</strong>했습니다. 내용을 정할 때 <strong>기업조사</strong>를 동시에 진행했는데, <strong>JD에서 언급한 키워드</strong>, <strong>기업의 인재상</strong>, <strong>기업/팀이 중요히 여기는 가치</strong>에 초점을 두며 내용을 정리하였습니다.</p>
<p> </p>
<h3 id="디자인"><a href="#디자인" class="headerlink" title="디자인"></a>디자인</h3><p><strong>Short CV는 2장으로 작성</strong>했습니다. 이는 제가 리서치+개발 포지션에 가기 위해 내용을 넣다보니 1장에 다 담기가 어려웠기 때문입니다. 또, 리서치 인력끼리 알아보는 시그니처(?)를 남기기 위해 <strong>CV를 일부러 LaTeX로 작성 (ㅋㅋ)</strong> 하였습니다. </p>
<p>내용상 중요한 순서대로 <strong>Summary-&gt;Employment-&gt;Projects-&gt;Skills-&gt;Education-&gt;Research Experiences</strong>를 적고나니 1장 반 정도가 되었는데, 나머지 부분은 블로그와 웨비나 경험과 같은 커뮤니티 경력으로 채웠습니다 (논문경력으로 채웠으면 좋았겠지만요 <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span>)</p>
<p>LaTeX와 같은 경우는 <a href="https://github.com/changh95/latex_resume_template_kor"><strong>템플릿</strong></a>을 만들기 굉장히 수월합니다. 아래와 같은 템플릿을 만들어서 <strong>동일한 구조의 새로운 CV</strong>를 빠르게 만들었습니다. 처음에는 5곳의 기업에 관심이 있어서 <strong>기업조사를 하며 5개의 short CV를 대충 빠르게 작성</strong>했고, <strong>지원서를 제출하기 직전에 내용과 디자인을 다듬어서 제출</strong>하였습니다. 이후 6곳의 기업에 추가로 관심이 생기면서 동일한 프로세스를 반복하였고, 총 6곳에 지원서를 제출하였습니다.</p>
<blockquote>
<p>해당 템플릿을 사용하시고 싶다면 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L2xhdGV4X3Jlc3VtZV90ZW1wbGF0ZV9rb3I=">링크<i class="fa fa-external-link-alt"></i></span>를 따라 들어가시면 됩니다 (도움되셨다면 스타를 꾸욱!). 경력기술서 템플릿도 포함되어있습니다!</p>
</blockquote>
<img src="/20210525-slam-field-job-offers/cv2.png" class="" title="CV2">

<p> </p>
<hr>
<h2 id="경력기술서"><a href="#경력기술서" class="headerlink" title="경력기술서"></a>경력기술서</h2><p>경력기술서는 대기업을 제외하고는 직접적으로 요구한 곳은 없었습니다. 그럼에도 <strong>지원서에 경력기술서를 첨부하였습니다</strong>. 이렇게 할 경우 면접관분들께서 사전에 질문 리스트를 좀 더 디테일하게 준비하실 수 있기 때문에 <strong>면접에서 더욱 심도깊은 이야기</strong>를 할 수 있습니다. 하지만 경력기술서에 모든걸 다 적어내버리면 반대로 면접에서 질문할 내용이 없어져버리기 때문에… <strong>유리한 질문을 끌어낼 수 있을만큼 적당하게 적는 것</strong>이 기술이라고 생각합니다.</p>
<blockquote>
<p>경력기술서 역시 아래에 보이는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L2xhdGV4X3Jlc3VtZV90ZW1wbGF0ZV9rb3I=">LaTeX 템플릿<i class="fa fa-external-link-alt"></i></span>를 사용해서 작성했습니다.</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/changh95/latex_resume_template_kor/main/work_experience_img.png"></p>
<p> </p>
<hr>
<h2 id="포트폴리오"><a href="#포트폴리오" class="headerlink" title="포트폴리오"></a>포트폴리오</h2><p><strong>포트폴리오는 따로 제출하지 않았습니다</strong>. 개인적인 생각이지만, 포트폴리오는 서류 단계에서 내는 것이 아니라고 생각합니다. <strong>포트폴리오는 발표를 위한 것</strong>이라고 생각합니다.</p>
<p>서류전형에서 합격하면 보통 면접에서 자기소개 시간에 발표를 부탁하는데, 이 때 발표 시간에 (보통 15/30분) 맞춰 포트폴리오 자료와 스크립트를 준비하고 리허설을 진행했습니다. 포트폴리오 디자인은 제가 평소에 사용하는 디자인을 사용했는데, 회색 배경에 은색 빛을 띄는 <span class="exturl" data-url="aHR0cHM6Ly9ub29ubnUuY2MvZm9udF9wYWdlLzIxNg==">배달의민족 한나체Pro 폰트<i class="fa fa-external-link-alt"></i></span>를 사용합니다. </p>
<p>제 포트폴리오는 <strong>크게 1. 자기소개, 2. 이력소개</strong> 로 나눠져있습니다.</p>
<p><strong>자기소개</strong>에서는 제가 <strong>관심가지는 기술 분야</strong>, <strong>저의 업무 스타일</strong>, <strong>지원 동기</strong> 에 대해서 소개했습니다. 종종 연말평가에서 받은 피드백을 인용하여 <strong>팀원으로써의 저</strong>를 소개하기도 했고, 지원하는 직무가 롱텀 목표가 있다면 <strong>제가 추구하는 커리어 방향</strong>도 함께 소개하였습니다.</p>
<p><strong>이력소개</strong>에서는 이력서에 적힌 프로젝트들을 소개하며, 제가 진행한 파트에만 집중하였습니다. 아래 예시에서 보이는 것 처럼 <strong>각 프로젝트들은 3페이지씩 소개</strong> 되었으며, 각 페이지마다 다음과 같으 내용을 담고 있습니다.</p>
<ul>
<li>프로젝트 목적과 사용된 기술의 이름을 명시</li>
</ul>
<img src="/20210525-slam-field-job-offers/portfolio-1.png" class="" title="portfolio1">

<ul>
<li>Flow chart를 통해 알고리즘 워크플로우를 소개</li>
</ul>
<img src="/20210525-slam-field-job-offers/portfolio-2.png" class="" title="portfolio2">

<ul>
<li>사진과 gif 이미지를 통해 실제 작동 장면 공유</li>
</ul>
<img src="/20210525-slam-field-job-offers/portfolio-3.png" class="" title="portfolio3">

<p> </p>
<hr>
<h2 id="지원서-Cover-letter"><a href="#지원서-Cover-letter" class="headerlink" title="지원서 / Cover letter"></a>지원서 / Cover letter</h2><p>스타트업, 중견기업/계열사, 대기업 별로 각각의 지원 과정 방법이 다릅니다.</p>
<div class="tabs" id="application"><ul class="nav-tabs"><li class="tab active"><a href="#application-1">스타트업</a></li><li class="tab"><a href="#application-2">중견기업/계열사</a></li><li class="tab"><a href="#application-3">대기업</a></li></ul><div class="tab-content"><div class="tab-pane active" id="application-1"><p>가장 간단한 과정을 가지고 있습니다.</p>
<p>많은 경우 CV를 이메일로 보내는게 다이고, 종종 석사논문을 요구하기도 합니다.</p>
<p>물론 이메일로 그냥 ‘지원합니다 + 첨부파일확인해주세요’라고 보내면 안됩니다. <strong>Cover letter</strong> 정도는 적어주는게 좋습니다.</p>
<p>지원하는 직무에 석사논문 주제가 연관된것이 아니라면 석사논문은 크게 중요하지 않은 것 같습니다 (면접관님들께서도 길어서 잘 안읽으시더라구요 <span class="github-emoji" alias="rofl" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f923.png?v8">🤣</span>) <strong>직무와 연관된 주제의 석사논문이라면 논문 전체를 첨부</strong>하고, <strong>그게 아니라면 abstract와 목차만 첨부</strong>하는 것도 괜찮을 것 같습니다.</p></div><div class="tab-pane" id="application-2"><p>자율형식의 CV를 제출하고, 추가적으로 정해진 질문들에 대한 답해야하는 유형입니다.</p>
<p><strong>‘왜 우리 회사에 지원하는가?’</strong>, <strong>‘본인의 장단점이 무엇인가?’</strong> 같은 질문들이 나옵니다.</p>
<p>종종 기출변형으로 ‘자신있는 기술들 + 숙련도를 적으시오’라던지, ‘본인을 시, 노랫말, 가사로 표현해보세요;;’ 같은 질문도 있습니다.</p>
<p><strong>충분한 시간을 가지시고 지원</strong>하시는게 좋습니다.</p>
<p>하지만 최근 중견기업/계열사 포지션은 <strong>경쟁이 치열하기때문에 빨리 행동하지 않으면 포지션이 사라질 수 있습니다.</strong></p></div><div class="tab-pane" id="application-3"><p>제일 귀찮은 작업이 많고 복잡한 유형입니다.</p>
<p><strong>정해진 규격에 맞춰서 모든걸 적어야 합니다</strong>. CV를 적은 이유가 없어지지만… CV도 첨부해야합니다 <span class="github-emoji" alias="rofl" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f923.png?v8">🤣</span> </p>
<p>부서마다 추가로 더 적어야하는 서류가 따로 있는 경우도 있습니다.</p>
<p>서류단계를 통과해도 1차면접 2차면접 전에 추가 서류를 요구하기도 합니다.</p>
<p>시스템도 조금 과하게 까탈스럽습니다. 저는 지원서 제출까지 2분 남았는데 석사논문과 CV를 업로드했더니 총 3mb가 넘는다는 이유로 첨부가 안되서… 빛보다 빠른 손으로 파일 압축해서 겨우 넣은 기억이 납니다 ㅋㅋ</p>
<p>꼭꼭꼭!! <strong>충분한 시간과 사전조사를 통해서 탄탄하게 준비</strong>하셔야합니다.</p></div></div></div>

<p> </p>
<p>스타트업이나 중견기업/계열사에 이메일을 통해 지원하실 경우 <strong>cover letter</strong>가 굉장히 중요하다고 봅니다. Cover letter는 <strong>본인의 배경</strong>, <strong>지원 동기</strong>, <strong>직무에 대한 적합성</strong>, <strong>열정</strong>, <strong>커뮤니케이션 능력</strong>도 보일 수 있는 귀중한 기회입니다. 저의 경우 아래와 같은 형식의 cover letter를 이용하였습니다.</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">안녕하세요,</span><br><span class="line"></span><br><span class="line">XXX 포지션에 지원하게 된 (이름))라고 합니다. YYY 기업이 해결하려고 하는 숙제와 해당 포지션이 담당하게 되는 업무의 내용이 굉장히 흥미롭게 다가왔고, Visual-SLAM 기술에 큰 관심을 가지고 있는 엔지니어로써 함께 문제를 해결해나가고 싶어서 지원하게 되었습니다. </span><br><span class="line"></span><br><span class="line">제 소개를 잠시 드리자면, 저는 (최근 경력 내용)을 하였습니다. 이 과정에서 (최근 경력에서 얻은 기술력/강점)하며 다양한 경험을 쌓았습니다. (직무의 적합성)라고 생각하여 이번 채용 공고를 통해 지원하게 되었습니다. 저는 (기타 장점)기술에 익숙하며, 이 경험을 토대로 YYY에서도 (목표, 열정)하고 싶습니다. </span><br><span class="line"></span><br><span class="line">지원 관련 서류는 첨부파일을 참조 부탁드립니다.</span><br><span class="line"></span><br><span class="line">지원을 검토해주셔서 감사합니다,</span><br><span class="line">(이름) 드림. </span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="기술면접-코딩테스트"><a href="#기술면접-코딩테스트" class="headerlink" title="기술면접 / 코딩테스트"></a>기술면접 / 코딩테스트</h2><p>Visual-SLAM 종사자시라면 아마 이번 글에서 가장 눈여겨보실 부분이 아닐까 생각합니다 ㅎㅎ</p>
<p>면접 프로세스는 대부분 2번을 보며 1차면접에서는 <strong>코딩테스트/기술면접</strong>, 2차면접에서는 <strong>컬쳐핏 면접</strong>을 진행합니다. 또, 종종 <strong>전화면접</strong> 또는 <strong>구현과제</strong>를 진행하기도 합니다.</p>
<p><strong>스타트업</strong>, <strong>중견기업/계열사</strong>, <strong>대기업</strong> 으로 나눠서 각각의 경험을 공유하겠습니다.</p>
<div class="tabs" id="tech-interview"><ul class="nav-tabs"><li class="tab active"><a href="#tech-interview-1">스타트업</a></li><li class="tab"><a href="#tech-interview-2">중견기업/계열사</a></li><li class="tab"><a href="#tech-interview-3">대기업</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tech-interview-1"><p><strong>스타트업</strong>에서는 다양한 종류의 1차 면접 방식이 있었습니다. 어떻게 진행될지는 정말 회사-바이-회사라고 생각합니다.</p>
<p>저는 다음과 같은 방식들의 면접을 겪었습니다.</p>
<ul>
<li>필기시험</li>
<li>실시간 질문/답변</li>
<li>인터넷과 Intellisense 끄고 코딩 후 제출</li>
<li>Google docs에서 실시간 코딩 (개인적으로 제일 떨렸습니다 ㅋㅋ)</li>
</ul>
<p>대체적으로 질문들은 <strong>영상처리, SLAM, C++에 대한 기본적인 지식/실력</strong>을 가지고 있는지 확인합니다. 다음과 같은 레벨의 질문들을 10~15개 정도 물어보시는 것 같습니다.</p>
<ul>
<li>Camera projection을 설명하세요 + 카메라 캘리브레이션은 어떻게 합니까?</li>
<li>자율주행/드론/AR/로보틱스 도메인의 특징에는 어떤게 있을까요? 본인이 하시던 분야와는 어떤 점이 다를까요?</li>
<li>자주 쓰시는 STL 컨테이너 중 하나를 설명해주세요. <code>std::map</code>과 <code>std::unordered_map</code>은 어떻게 다를까요?</li>
<li>피쳐 트랙킹에는 어떤 방법들이 있나요?</li>
<li>Feature-based SLAM과 Direct SLAM은 어떤 점이 다르나요?</li>
</ul>
<p>이후, <strong>이력서에 언급된 프로젝트들에 대해서 질문/답변</strong>을 진행합니다. 답변이 꼬리에 꼬리를 물고가기 때문에, 점점 더 내용이 깊게 들어갑니다. 충분히 답변이 되거나 답변을 하지 못할 때 까지 파고 들어가는 편 입니다. 예시를 들면</p>
<ul>
<li>나: 사전에 SLAM으로 만든 맵을 사용해서 실시간 visual localization을 수행하는 알고리즘을 만들었습니다.</li>
<li>면접관님: 피쳐는 어떤걸 사용했나요? 어떻게 매칭했나요?</li>
<li>나: 피쳐는 ORB를 사용했습니다. 매칭은 디스크립터 기반 매칭을 수행했습니다.</li>
<li>면접관님: 피쳐매칭이 잘못될 수도 있을텐데요,</li>
<li>나: 우선 Lowe ratio test로 비슷한 피쳐끼리의 매칭은 이미 걸러낸 상태였습니다. 이후 P3P+RANSAC을 이용해서 아웃라이어를 걸러냈습니다. RANSAC은 PROSAC 샘플링 기법에 inner-ransac을 만들어서 L-M 기법을 이용했구요. Tukey estimator까지 적용하고나니까 좋은 매칭 결과를 보였습니다.</li>
<li>면접관님: 그래도 잘 안되는 부분이 있었을텐데요,</li>
<li>나: FOV에 반복되는 피쳐만 가득한 경우에는 잘 안되었습니다 ㅠㅠ Global descriptor로도 풀어보려고 했는데 잘 안되었고, 다른 방법으로 풀어보려고 했지만 시간이 부족해서 해결하지 못했습니다.</li>
</ul></div><div class="tab-pane" id="tech-interview-2"><p><strong>중견기업/계열사</strong> 중 SLAM을 하는 곳은 사실 N사 계열사와 K사 계열사밖에 없다고 생각합니다 ㅋㅋ K사 계열사는 전문연구요원을 받지 않아서, N사 계열사는 스케줄 상 지원하기 어려워서 제가 직접 경험한 곳은 없었습니다. 하지만 굉장히 비슷한 컬쳐를 가진 기업과 면접을 보았기에 경험을 공유할 수 있다고 생각합니다.</p>
<p>중견기업/계열사는 스타트업에 비해 인재상이 더 뚜렷하게 잡혀있고, <strong>어떤 도메인의 제너럴리스트보다는 실제 경험이 풍부한 스페셜리스트</strong>를 뽑으려는 성향을 보였습니다. 그렇기 때문에 석사때 연구하신 분야가 회사에서 요구하는 기술과 일치하거나, 관련된 분야로 논문을 적은 경험이 있으시다면 굉장히 좋은 기회를 가졌다고 볼 수 있습니다. 다른 필드에서 분야를 전환해서 오시려는 분께는 특별한 킥이 있지않는 이상 난이도가 높다고 생각합니다.</p>
<p><strong>기본기</strong>를 중요하게 본다고 알려져있지만, 제가 느끼기로는 <strong>기술에 대해 깊게 + 로우레벨까지 고민해본적이 있는가</strong>를 보는 것 같습니다. 이러한 면에서 <strong>지원자의 진정성과 몰입도</strong>도 볼 수 있는 것 같구요. 제가 봤던 면접은 1시간 자기소개+질문답변 후 3시간 동안의 기술면접이였으며, 다음과 같은 분야의 질문들을 받았습니다.</p>
<ul>
<li>기초적인 수학 (선형대수, 확률과통계)</li>
<li>코딩테스트 (대부분 구현/시뮬레이션 문제)</li>
<li>컴퓨터 비전 기술 이해도 (경험, 이론)</li>
</ul>
<p>질문들의 수준은 <strong>스타트업에 비해 훨씬 깊은 지식을 요구하는 편</strong>이며, 해당 지식에 대해 모를 경우 <strong>생각의 흐름을 보려고 하는 문제</strong>가 많습니다. 기억나는 질문으로는 다음과 같은 것들이 있습니다.</p>
<ul>
<li>Fundamental matrix의 Dof는 몇개인가? 왜?</li>
<li>Maximum-likelihood estimation (MLE)와 Maximum-a-posteriori estimation (MAP estimation)의 차이점이 무엇인가?</li>
<li>Gauss-Newton과 Levenberg-Marquardt 방식이 다른 점은 무엇인가? 왜 gradient descent는 안쓰는가?</li>
<li>w,h 크기의 이미지가 있고, x,y 크기의 conv 마스크가 있고, 이 conv 마스크가 s stride 값으로 c 채널수 만큼 conv 연산을 한다면 총 연산 횟수는?</li>
<li>하얀 벽에 88이라는 숫자가 써있고, 다른 뷰에서 숫자를 바라본 이미지가 2개 있다. 이 두 이미지의 relative pose를 어떻게 구할 것인가?</li>
</ul>
<p>종종 전혀 모르는 분야에 대한 질문이 나올 때도 있습니다. <strong>절대 패닉하지 마시고 멘탈을 끝까지 잘 유지</strong>하셔야합니다.</p></div><div class="tab-pane" id="tech-interview-3"><p><strong>대기업</strong>의 경우는 L사에 지원하였습니다. S사도 경력직 지원했는데 알고보니 조건이 석+5년 / 박+2년이였어서 서탈했습니다 <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span></p>
<p>대기업은 기본적으로 가장 먼저 <strong>서류심사</strong> 후 <strong>인성검사</strong>를 진행하였습니다.</p>
<p><strong>인성검사</strong>는 따로 준비할 내용은 없었고 빠르게 본인의 업무 스타일과 성격에 맞춰서 답을 하면 됩니다.</p>
<p>이후 코딩테스트 대행 업체를 통해서 <strong>코딩테스트</strong>를 진행했습니다. L사의 경우는 <strong>프로그래머스</strong>를 이용하였습니다.</p>
<p>코딩테스트는 <strong>2시간에 3문제를 푸는 것이였고 굉장히 쉬웠습니다</strong>. 특이한 점으로는, 화면 공유 + 웹캠 모니터링 + 스마트폰을 이용해서 손과 무릎과 테이블 전체가 보이게해서 부정검사를 막습니다 (ㄷㄷ합니다). CS를 제대로 공부해본적 없는 제 인생에서 첫번째 제대로 된 코딩테스트였고, 이전부터 자료구조 책은 조금씩 읽긴 했지만 딱 이틀동안만 준비했는데 통과한걸 보면 확실히 쉬운 난이도라고 생각합니다 ㅎㅎ.</p>
<p>코딩테스트 준비는 <a href="https://www.youtube.com/playlist?list=PLRx0vPvlEmdAghTr5mXQxGpHjWqSz0dgC"><strong>동빈나의 ‘이것이 취업을 위한 코딩테스트다 with 파이썬’</strong></a> 재생목록의 반정도를 보았고, <strong>그리디/구현</strong>, <strong>DFS/BFS</strong> 위주로 보았습니다 (정말 명강의라고 생각합니다 ㅎㅎ). 물론 제가 본 코테가 유독 쉬웠을 수도 있고, 좀 더 난이도가 높은게 나올 수 있으니 대기업을 준비하시는 분들은 꼭 잘 준비하시길 바랍니다.</p>
<p>이후 <strong>온라인 대면 면접</strong>을 진행했습니다. 15분 자기소개 후 약 30분간 질문을 하셨고, <strong>질문들은 대부분 자기소개 내용에 기반</strong>하였습니다 (지식에 대한 질문이 많이 없었기 때문에 스타트업,중견기업/계열사 보다 훨씬 쉬웠습니다). ‘XXX 프로젝트를 개발하실 때 왜 YYY한 결정을 내리셨나요?’, ‘해당 기능은 직접 만들었나요? 아니면 라이브러리를 썼나요? 왜 라이브러리 안쓰고 굳이 직접 만드셨나요?’ 등이 있었습니다.</p>
<p>가장 기억에 남는 질문은 ‘본인을 로보티스트라고 생각하시나요? 아니면 비전러라고 생각하시나요?’ 였습니다. 솔직하게 ‘로보티스트라고 하기에는 kinematics, dynamics, 플래닝 등의 SLAM 외적인 분야를 잘 모릅니다. SLAM을 잘 하게 되어도 아마 다른 비젼쪽을 볼 것 같아서 비전러에 가까울 것 같다고 표현하겠습니다’ 라고 말씀드렸습니다. 당시에는 ‘아 맞다 나 로봇포지션 지원이였지,, ㅜㅜ’ 하며 떨어졌을거라고 생각했는데, 나중에 보니 통과했더라구요 ㅋㅋ 큰 의미를 두지는 않는 것 같습니다.</p></div></div></div>

<p> </p>
<hr>
<h2 id="구현과제"><a href="#구현과제" class="headerlink" title="구현과제"></a>구현과제</h2><p><strong>스타트업</strong>, <strong>중견기업/계열사</strong> 쪽에서는 <strong>구현과제</strong>를 요구하기도 하였습니다. </p>
<p>저는 구현과제를 2번 진행했는데, 첫번째 구현과제는 <strong>3일</strong>의 시간, 두번째 구현과제는 <strong>1주일</strong>의 시간이 주어졌습니다. 현재 직장에 다니신다면 일정을 어느정도 조정할 수 있고, 예기치 못한 일로 늦게 제출해야할 때는 연장도 가능했습니다 (하필 마감이랑 껴서 이틀 연장했던게 기억납니다 ㅠㅠ). </p>
<div class="tabs" id="homework"><ul class="nav-tabs"><li class="tab active"><a href="#homework-1">3일 과제</a></li><li class="tab"><a href="#homework-2">1주 과제</a></li></ul><div class="tab-content"><div class="tab-pane active" id="homework-1"><p>3일짜리 구현과제는 중견기업/계열사에서 받은 과제이며 2개의 문제가 있었습니다. </p>
<p>첫번째 문제는 <strong>multiple view geometry</strong> 기술로 문제를 해결해야하는 상황이 주어지는데, 일부러 필요한 정보와 불필요한 정보를 많이 섞어 문제를 꼬은듯 한 느낌을 받았습니다. 문제의 요점은 많은 정보로부터 중요한 정보를 직접 뽑아내고, MVG 이론을 적용하여 문제를 풀고 이를 코드로 구현하면 됩니다. </p>
<p>두번째 문제는 영상처리 시나리오였지만, 문제의 요점은 <strong>C++ 알고리즘과 자료구조를 사용하여 효율적인 코딩을 하는 구현문제</strong>였습니다. 저는 여기서 적절한 자료구조를 사용하지 않아서 비효율적인 코드를 작성했기 때문에 과제탈했습니다 <span class="github-emoji" alias="cry" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span>. </p>
<p>소스코드와 구현 리포트를 함께 제출하면 되는 방식이였습니다.</p></div><div class="tab-pane" id="homework-2"><p>1주일짜리 구현과제는 스타트업에서 받은 과제이며, 4개의 문제가 있었습니다. </p>
<p>기간이 많이 주어진 만큼 과제의 양도 많았는데, <strong>논문 구현</strong>, <strong>앱 구현</strong>, <strong>최적화 코드 작성</strong>, <strong>모듈 구현</strong> 이 있었습니다. </p>
<p><strong>난이도는 중견기업/계열사보다는 쉬운 편</strong>이라고 생각되지만, 분야가 훨씬 넓고 양이 많다보니 체력과 꾸준함이 요구되었습니다. </p>
<p>4문제 모두 노력할수록 더 잘 풀 수 있는 문제였기 때문에 노력~시간 비율을 맞추기가 쉽지 않았습니다.</p></div></div></div>

<img src="/20210525-slam-field-job-offers/homework.jpg" class="" title="homework">

<p>구현과제는 절대 쉽지 않은 과정이였지만, <strong>지원자의 입장에서는 입사 욕심이 나게 만들고 도전욕구를 끌어올리는 좋은 과정</strong>이라고 봅니다. 구현과제 문제를 이메일로 받아보았을 때 <strong>‘아 여기 들어가려면 이정도 실력은 있어야하는구나! 여기 계시는 분들은 잘 하시는 분들이 분명하다! 제대로 회사를 찾아왔구나!’ 라는 생각</strong>이 들었고, 구현과제를 진행하지 않고 단골 면접질문들만 물어보는 곳들에 비해 훨씬 좋은 인상을 받게 되었습니다. 물론 이런 생각이 들기위한 전제 조건은 ‘적절한 양의 과제 + 적절한 난이도의 과제’인 것이 좋다고 생각하며, 너무 양이 많거나 어려운 경우에는 지원자가 빠르게 포기해버릴 수도 있다고 생각합니다.</p>
<p> </p>
<hr>
<h2 id="컬쳐핏-면접-지원자의-질문"><a href="#컬쳐핏-면접-지원자의-질문" class="headerlink" title="컬쳐핏 면접 + 지원자의 질문"></a>컬쳐핏 면접 + 지원자의 질문</h2><h3 id="컬쳐핏-면접"><a href="#컬쳐핏-면접" class="headerlink" title="컬쳐핏 면접"></a>컬쳐핏 면접</h3><p>2차면접에서는 대부분 컬쳐핏 면접을 진행합니다.</p>
<p>개인적으로 컬쳐핏 면접은 ‘잘봤다/못봤다’의 개념이 없다고 생각합니다. 서류전형이 소개팅, 1차면접이 애프터라면, 컬쳐핏 면접은 사귈까 말까를 결정하는 과정이라고 생각합니다. <strong>회사측에서만 ‘우리 인재상이랑 잘 맞네요 오십시오!’가 아니라, 지원자 측에서도 ‘제가 딱 원했던 회사네요, 받아주십시오!’도 함께 오가야한다고 생각합니다</strong>. </p>
<p>그런면에서 저는 컬쳐핏 면접에서는 제가 물어볼 질문들을 미리 준비해가고, <strong>면접자분들이 질문하시는 만큼 역으로 면접자분들께 질문</strong>을 하는 편입니다. 이와 관련된 <span class="exturl" data-url="aHR0cHM6Ly9icnVuY2guY28ua3IvQGdvb2RnZGcvMTQy">좋은 아티클<i class="fa fa-external-link-alt"></i></span>이 있습니다. 지원자와 면접관이 서로에게 질문을 하고 핏이 잘 맞을 것 같다고 좋은 느낌이 오면 면접 분위기도 되게 화기애애해집니다.</p>
<p>제가 합격했던 면접의 분위기는 굉장히 화기애애했고, 이런 느낌으로 흘러갔습니다. (끄덕 -&gt; 끄덕)</p>
<img src="/20210525-slam-field-job-offers/nod.gif" class="" title="nod">

<p> </p>
<p>여러 컬쳐핏 면접을 거치면서 좋았던 점과 실망스러웠던 점들을 정리했습니다.</p>
<div class="tabs" id="culture-fit"><ul class="nav-tabs"><li class="tab active"><a href="#culture-fit-1">좋은 컬쳐핏 면접</a></li><li class="tab"><a href="#culture-fit-2">실망스러운 컬쳐핏 면접</a></li></ul><div class="tab-content"><div class="tab-pane active" id="culture-fit-1"><p>그 다음은 <strong>좋았던 컬쳐핏 면접</strong> 입니다.</p>
<p><strong>좋았던 컬쳐핏 면접</strong>에서는 다음과 같은 여러가지 특징이 있습니다.</p>
<p>첫번째로는 <strong>지원자의 성격과 습관등을 파악하려는 질문을 여러번 하는 것</strong> 입니다.</p>
<ul>
<li>‘지원자분은 팀에서 어떤 역할을 하고 있나요?’</li>
<li>‘지원자분은 어떤 타입의 사람과 잘 맞나요?’</li>
<li>‘어떤 작업을 했을 때 가장 뿌듯한가요?’</li>
<li>‘가장 피하고 싶은 작업이 있나요?’</li>
</ul>
<p>위와 같은 질문들은 지원자로써도 <strong>‘회사 측에서 뽑고싶어하는 인재상이 정확히 있구나’</strong> 라는 인상을 주게 되며, 그만큼 합격했을 경우 잘 맞는 팀이 될거라는 기대감이 생깁니다.</p>
<p>두번째로는 <strong>지원자가 회사에 바라는 점</strong>을 물어보는 것 입니다.</p>
<ul>
<li>‘지원자분은 어떤 회사가 좋은 회사라고 생각하시나요?’</li>
<li>‘커리어 목표가 어떻게 되시나요?’</li>
<li>‘우리에게 기대하는 점은 어떤 것이 있나요?’</li>
</ul>
<p>위와 같은 질문들은 지원자 입장에서는 <strong>회사가 직원들이 업무에 집중하는데에 서포트를 잘 해주고 함께 롱런할 수 있을 것이라는 느낌</strong>을 받습니다.</p></div><div class="tab-pane" id="culture-fit-2"><p>가장 실망스러웠던 것은 <strong>이미 1차면접에서 물어본 질문을 또 물어보시는 경우</strong>였습니다.</p>
<ul>
<li>‘왜 저희회사에 지원하셨나요?’ (1차 면접에서 물어봄)</li>
<li>‘본인의 장점/단점이 뭐라고 생각하세요?’ (1차 면접에서 물어봄)</li>
<li>‘프로젝트 설명 좀 부탁드릴게요’ (1차 면접에서 물어봄)</li>
</ul>
<p>위와 같은 질문을 또 다시 받았을 때, 지원자는 회사 내부에서 1차면접에 대한 협의가 전혀 없었던걸까, 하고 생각하게 합니다. <strong>똑같은 질문을 물어볼꺼면 왜 또 부른건지 시간이 아깝다</strong>고 느껴지게 됩니다. 당연히 회사에 대한 평가는 좋지 않습니다.</p>
<p><strong>형식적인 질문만 2~3개 물어보고 ‘저희 회사에 궁금한거 없으신가요?’와 같이 빠르게 면접을 끝낼 때</strong>도 실망스러운 인상이 남습니다. <strong>이미 내부적으로 결정이 난 것 같은데 본인을 왜 또 부른 것일까</strong>, 하는 생각이 들게 됩니다.</p></div></div></div>

<p> </p>
<h3 id="지원자의-질문"><a href="#지원자의-질문" class="headerlink" title="지원자의 질문"></a>지원자의 질문</h3><p>컬쳐핏 면접에서는 회사-&gt;지원자 방향의 질문도 있고, <strong>지원자-&gt;회사의 방향으로도 질문</strong>을 하게 됩니다.</p>
<p>회사 측에서 <strong>지원자로부터 질문을 받았을 때 자신있는 답변</strong>을 하실 때 지원자도 회사에 대해 신뢰를 가지게 됩니다.</p>
<p>면접이 끝날 때 쯤 보통 ‘<strong>질문있으신가요?</strong>‘라고 물어보시는데, 저는 스타트업이나 중견기업/계열사 면접에는 꼭 미리 준비해간 <strong>질문 리스트</strong>를 보여드렸습니다. 질문 리스트를 보여드릴 때 리액션이 많이 다양했습니다 - 놀라시거나, 대놓고 싫어하시거나, 흥미롭게 보시는 면접관님들이 계셨습니다. 제 질문 리스트에는 다음과 같은 질문들이 항상 있었습니다.</p>
<ul>
<li>제가 들어가면 어떤 직무를 맡게 될까요? 가장 빠른 마일스톤이 언제일까요? 제게는 어떤 기대를 하시나요?</li>
<li>팀 내 업무 스타일은 어떻게 되나요? 방향을 제시해주시는 시니어가 계시나요? 아니면 모두가 직접 책임을 가지고 작업을 진행하나요?</li>
<li>팀에서 형상관리는 어떻게 하시나요? CI/CD가 있나요? 코드리뷰는 어떻게 하시나요?</li>
<li>회사에서 시리즈 A/B/C/IPO 투자/상장 계획은 어떻게 되시나요?</li>
</ul>
<p>이러한 질문을 함으로써 <strong>지원자는 본인이 생각했던 회사의 이미지와 실제 회사의 모습이 일치한지 평가</strong>하게 됩니다. 생각했던 모습과 일치할수도 있고, 다를 수도 있습니다. 하지만 다르다는건 항상 나쁜게 아닙니다. 물론 지원자 입장에서는 회사측에서 자신감있게 다 갖춰져있다고 말씀해주시는게 제일 좋겠지만, 몇개씩은 없어도 사실 이해해줄 수 있습니다. 작은 사업체에서 CI/CD 인프라가 부족하다던지, 신생 팀이라서 업무 스타일이 확립이 안됬다던지, 시니어가 아직 없다던지의 상황은 충분히 상정 가능한 부분입니다. </p>
<p>이 과정에서 제일 중요한 부분은 지원자가 <strong>‘내가 여기에 입사해도 잘 적응할 수 있을까?’</strong> 를 판단하는 것이며, 이는 <strong>회사의 입장에서도 사실 매우 중요한 요소</strong>일 겁니다 (<strong>적응하지 못하면 몇달만에 나가버리니까요</strong>).</p>
<p> </p>
<p>위와 같은 질문들에 회사는 여러가지 방법으로 대답을 할 수 있습니다.</p>
<ul>
<li>솔직하게 좋은 점 + 부족한 점 다 이야기하기</li>
<li>좋은 점만 보여주기</li>
<li>좋은 점을 부풀려서 보여주기</li>
<li>기밀이라고 이야기하기</li>
<li>질문에 대해 거부감을 표시하기</li>
</ul>
<p>우선 <strong>제일 나쁜 신호</strong>는 ‘<strong>질문에 대해 거부감을 표시하는 것</strong>‘ 입니다. 지원자에게 이러한 모습은 ‘너는 질문하지 말고 일만 하면 돼’ 라는 느낌을 주며, 실제로 이런 태도를 보이는 곳들도 있습니다.</p>
<p>그 다음으로 나쁜 신호는 ‘<strong>좋은 점을 부풀려서 보여주기</strong>‘ 입니다. 면접관들도 지원자들이 어떠한 내용을 제대로 이해하지 못하는데 아는척 대답하는것을 싫어하듯이, <strong>지원자들도 회사측에서 없는데 있는척 하는 것을 보는 것을 굉장히 싫어합니다</strong>.</p>
<p>제 경험 중, 들어가면 할 업무도 기밀이라 말 못해주고, 마일스톤도 기밀, 형상관리와 코드리뷰는 ‘잘 하고 있다’라고 답변해주신 기업은 좋은 인상을 남기지 못했습니다. 회사 전략 상 기밀이 있을 수 있다는 점은 이해하지만, 지원자의 입장에서 지원하려는 직무가 어떤 직무인지 알 수 없다면 아무도 지원하려고 하지 않을 것 입니다 <span class="github-emoji" alias="sweat_smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">😅</span></p>
<p> </p>
<p>제가 받았던 답변 중 <strong>가장 좋은 인상을 남긴 답변</strong>은 다음과 같습니다. <strong>솔직하게 좋은점과 부족한 점을 이야기</strong>하면서, <strong>부족한 점을 어떻게 보완해나가겠다</strong>, 라는 점을 이야기하였습니다.</p>
<ul>
<li>입사하면 팀의 X번째 멤버일 것이다. 솔직히 말하면 팀이 생긴지 얼마 안되서 인프라도 아직 없고 시니어도 현재 채용중이다. 초반에는 어쩔수없이 업무가 다양할 수 있지만 최대한 빠르게 코어개발로 이끌어주겠다.</li>
<li>업무 스타일은 ‘기한과 퀄리티를 중요시 여긴다. 그것만 지킨다면 그 외로는 걱정할 것 없게 지원해주는 편’</li>
<li>형상관리/코드리뷰 방식은 팀바이팀이지만, 대부분 Git과 Github로 진행함. 기본적인 CI/CD 인프라는 있으며 필요에따라 확장 가능.</li>
<li>XXX년까지 AAA,BBB,CCC 제품을 만들 예정이며 그 후의 계획도 세워져있음. 회사는 계속 성장중이고, XXX년에 YYY한 투자/상장을 할 수 있을 것이라고 예상</li>
</ul>
<blockquote>
<p>이런 답변은 듣는 동시에 ‘잘부탁드립니다!’ 시전 + ‘입사일은 언제가 좋지’를 고민하게 됩니다</p>
</blockquote>
<img src="/20210525-slam-field-job-offers/sajangnim.png" class="" title="yess">

<p> </p>
<hr>
<h2 id="합격-목걸이-최종결정"><a href="#합격-목걸이-최종결정" class="headerlink" title="합격 목걸이 + 최종결정"></a>합격 목걸이 + 최종결정</h2><p>총 11곳의 리스트를 만들었지만, 7곳의 결과를 보았을 때 결정을 내리게 되었습니다.</p>
<p>결정을 내렸을 때의 상황은 다음과 같습니다.</p>
<ul>
<li>스타트업 3곳에서 오퍼 제안</li>
<li>대기업 S사 서류탈락, (중견기업/계열사의 문화를 가진) 스타트업 1곳에서 구현과제 후 탈락</li>
<li>대기업 L사 서류+인성검사+코딩테스트+1차면접 합격 (최종 면접 진행 중 이직 결정으로 인해 추가진행 없음), 스타트업 1곳에서 오퍼레터 수령 후 조정 불발</li>
</ul>
<p>감사하게도 저를 쓸모있게 봐주신 3곳의 회사로부터 오퍼를 받았으며, 총 <strong>7전 3승 2패 2무</strong>의 결과가 나왔습니다.</p>
<blockquote>
<p>SLAM 톡방에서 저를 봐오신 분들이라면 ‘맨날 입에 달고 살던 N사의 연구실은?’이라고 하실 수도 있겠지만 그곳은 계속 꿈만 꾸다가 스케줄이 맞지않아 지원하지도 못했습니다 <span class="github-emoji" alias="sweat_smile" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">😅</span>ㅋㅋ</p>
</blockquote>
<p> </p>
<p><strong>최종 결정</strong>은 생각보다 쉽게 진행되었습니다.</p>
<p>이 3곳의 회사들은 각각 다른 필드와 규모를 가졌고 장/단점이 극명하게 갈렸습니다.</p>
<p>하지만 <strong>장점만 보았을 때는 한곳이 특출나게 눈에 띄는 곳</strong>이 있었습니다. 장점들은 다음과 같습니다.</p>
<ul>
<li><strong>독보적인 기술력</strong><ul>
<li>‘오 잘하네’가 아니라 ‘이게 된다고?’를 말하게 하는 기술력</li>
</ul>
</li>
<li><strong>흥미로운 제품 아이템</strong><ul>
<li>국내 유일? 글로벌 경쟁사는 엄청난 곳이지만… 그래도 기술력 덕분에 비벼볼만하다는 생각이 들음</li>
<li>커리어 방향과 궁합이 잘 맞음</li>
</ul>
</li>
<li><strong>훌륭한 리더쉽 평가</strong><ul>
<li>충성충성!</li>
</ul>
</li>
</ul>
<p>장점은 모두가 만드는 것이기 때문에 쉽게 만들기 어려운 것이고, 단점이 있다면 내가 고치면 된다고 생각합니다 (근데 면접 과정 중에 딱히 단점이랄것도 없었습니다!).</p>
<p>커리어 방향과도 잘 맞고, 제품 아이템도 멋지게 성공할 것 같은 가능성이 보이기 때문에 쉽게 결정할 수 있었습니다.</p>
<p>필드는 위에서 목표했던 <strong>자율주행</strong> 필드입니다.</p>
<p>최종 결정한 회사에서는 오퍼레터를 보내주시기보다는 희망처우를 먼저 물어보셨는데… 저는 연봉협상 이런거 잘 할줄 몰라서 다른 오퍼레터들에서 제안하신거보다 티끌만큼만 얹어서 말씀드렸습니다.</p>
<p>그랬더니 거의 딱 그 내용으로 오퍼레터가 와서 받자마자 <strong>5분만에 ‘저 갈게요!!!!’를 외쳤습니다</strong> ㅋㅋㅋ</p>
<img src="/20210525-slam-field-job-offers/new_me.jpg" class="" title="yesssss">

<p> </p>
<hr>
<h2 id="마치며"><a href="#마치며" class="headerlink" title="마치며"></a>마치며</h2><p>이렇게 지난 몇달간의 이직을 위한 몸부림에 대해 말씀드렸습니다. 다른 분들에 비해 적은 수의 경험만 가지고 말한 감이 있어서 조금 조심스럽기도 합니다. 아무쪼록 긴 글인데 읽어주셔서 감사합니다! <span class="github-emoji" alias="smile_cat" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f638.png?v8">😸</span></p>
<p>이번 해 1~3월에 이직하는 것을 꿈꿨는데, 생각보다 준비할 것도 많았고 난이도도 높다는 것을 느꼈습니다. 하지만 이 과정 속에서도 많이 성장했다고 생각합니다.</p>
<p>SLAM 업계가 생각보다 엄청나게 빠른 속도로 성장하고 있다는 것도 느꼈고, 여기에 맞춰서 나 역시 뒤쳐지지 말고 빠르게 성장해나가야겠다는 생각도 듭니다.</p>
<p>2021년 이직을 준비하시는 분들께서도 좋은 운이 깃드시길 바라고, 또 <strong>독보적이고, 미래적이고, 멋진 SLAM을 하고 싶으신 분들은 제게 연락부탁주셔요! 함께해요!!</strong></p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>일상</tag>
        <tag>후기</tag>
        <tag>이직</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2021 튜토리얼 / 워크샵 리스트</title>
    <url>/20210530-icra-2021/</url>
    <content><![CDATA[<h2 id="Workshops-Tutorials"><a href="#Workshops-Tutorials" class="headerlink" title="Workshops / Tutorials"></a>Workshops / Tutorials</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9ETDNOd2hMUXZlbw==">Digital Twins for Robots in Industrial Applications<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMODY4dHdzeDdPamZMYXVBT3ZjZk5zN2xPOU5odUtyeWM=">Opportunities and Challenges with Autonomous Racing<i class="fa fa-external-link-alt"></i></span><ul>
<li>9:00 GMT-04</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMZVhXei1nMklmOTY0SVcyVFh2eGltMlR1ZWJUMGRLTUI=">Perception and Action in Dynamic Environments<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMODY4dHdzeDdPamZMYXVBT3ZjZk5zN2xPOU5odUtyeWM=">Robust Perception For Autonomous Field Robots in Challenging Environments<i class="fa fa-external-link-alt"></i></span><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9zNUxMME5ZYkktVQ==">Davide Scaramuzza - Robust Perception For Cars And Drones<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS96UjJ1UjFoQ2NMMA==">CJ Taylor - UPSLAM : Union of Panoramas SLAM<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9LSDVfZUNpdkc3TQ==">Sebastian Scherer - Robust Navigation with Visual and Thermal Sensors in Degraded Visual Environments<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9mVHd0el9hWG5EOA==">Jeanette Bohg - Detect, Reject, Correct: Cross-modal Compensation of Corrupted Sensors<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS93Zy1ELUtUYnRudw==">Claire Tomlin - Learning-Based Waypoint Navigation: a Viewpoint on Perception, Planning, and Control<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9LRnlsM01sTHYxWQ==">Larry Matthies - Terrain-relative navigation for guided descent on Titan<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9ldGkwVHh1aTg5aw==">Sanjiv Singh - Perceptual robustness to obscurants and the world itself<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9xRkF5VktaMDY4TQ==">Tim Barfoot - Dark, Damp, and Dynamic: Recent Progress on Robotic Localization in Challenging Environments<i class="fa fa-external-link-alt"></i></span></li>
<li>9:00 GMT-04</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj14bERidXc2c2thZyZhYl9jaGFubmVsPVJvYm90UGVyY2VwdGlvbmFuZE5hdmlnYXRpb25Hcm91cA==">Workshop on Visual-Inertial Navigation Systems<i class="fa fa-external-link-alt"></i></span><ul>
<li>9:00 GMT-04</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYWVyaWFsLXJvYm90aWNzLXdvcmtzaG9wLmNvbS9hZ2VuZGEuaHRtbA==">Resilient and Long-Term Autonomy for Aerial Robotic Systems<i class="fa fa-external-link-alt"></i></span><ul>
<li>8:00 GMT-04</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><ul>
<li>Optimization-Based Visual-Inertial SLAM Tightly Coupled with Raw GNSS Measurements</li>
<li>LiTAMIN2: Ultra Light LiDAR-Based SLAM Using Geometric Approximation Applied with KL-Divergence</li>
<li>Compositional and Scalable Object SLAM</li>
<li>Visual Place Recognition Via Local Affine Preserving Matching</li>
<li>Towards Real-Time Semantic RGB-D SLAM in Dynamic Environments</li>
<li>Voxelized GICP for Fast and Accurate 3D Point Cloud Registration</li>
<li>Markov Parallel Tracking and Mapping for Probabilistic SLAM</li>
<li>Avoiding Degeneracy for Monocular Visual SLAM with Point and Line Features</li>
<li>TT-SLAM: Dense Monocular SLAM for Planar Environments</li>
<li>OV2SLAM : A Fully Online and Versatile Visual SLAM for Real-Time Applications</li>
<li>DOT: Dynamic Object Tracking for Visual SLAM</li>
<li>DefSLAM: Tracking and Mapping of Deforming Scenes from Monocular Sequences (I)</li>
<li>An Equivariant Filter for Visual Inertial Odometry</li>
<li>Run Your Visual-Inertial Odometry on NVIDIA Jetson: Benchmark Tests on a Micro Aerial Vehicle</li>
<li>Simple but Effective Redundant Odometry for Autonomous Vehicles</li>
<li>Revisiting Visual-Inertial Structure-From-Motion for Odometry and SLAM Initialization</li>
<li>RigidFusion: Robot Localisation and Mapping in Environments with Large Dynamic Rigid Objects</li>
<li>Tight Integration of Feature-Based Relocalization in Monocular Direct Visual Odometry</li>
<li>Adaptive Robust Kernels for Non-Linear Least Squares Problems</li>
<li>A Front-End for Dense Monocular SLAM Using a Learned Outlier Mask Prior</li>
<li>HyperMap: Compressed 3D Map for Monocular Camera Registration</li>
<li>Robust Skin-Feature Tracking in Free-Hand Video from Smartphone or Robot-Held Camera, to Enable Clinical-Tool Localization and Guidance</li>
<li>VOLDOR-SLAM: For the Times When Feature-Based or Direct Methods Are Not Good Enough</li>
<li>ROBIN: A Graph-Theoretic Approach to Reject Outliers in Robust Estimation Using Invariants</li>
<li>Kimera-Multi: A System for Distributed Multi-Robot Metric-Semantic Simultaneous Localization and Mapping</li>
<li>Semantic SLAM with Autonomous Object-Level Data Association</li>
<li>Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search</li>
<li>Structure Reconstruction Using Ray-Point-Ray Features: Representation and Camera Pose Estimation</li>
<li>Lightweight 3-D Localization and Mapping for Solid-State LiDAR</li>
<li>BALM: Bundle Adjustment for Lidar Mapping</li>
<li>Tactile SLAM: Real-Time Inference of Shape and Pose from Planar Pushing</li>
<li>SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal Scenes</li>
<li>Direct Sparse Mapping (I)</li>
<li>Simultaneous Multi-Level Descriptor Learning and Semantic Segmentation for Domain-Specific Relocalization</li>
<li>Pose Estimation for Vehicle-Mounted Cameras Via Horizontal and Vertical Planes</li>
<li>Lightweight Semantic Mesh Mapping for Autonomous Vehicles</li>
<li>A Complete, Accurate and Efficient Solution for the Perspective-N-Line Problem</li>
<li>Accelerating Robot Dynamics Gradients on a CPU, GPU, and FPGA</li>
<li>Robust Place Recognition Using an Imaging Lidar</li>
<li>A Switching-Coupled Backend for Simultaneous Localization and Dynamic Object Tracking</li>
<li>SLAAM: Simultaneous Localization and Additive Manufacturing (I)</li>
<li>Asynchronous Multi-View SLAM</li>
<li>LVI-SAM: Tightly-Coupled Lidar-Visual-Inertial Odometry Via Smoothing and Mapping</li>
<li>Distributed Client-Server Optimization for SLAM with Limited On-Device Resources</li>
<li>IMU Data Processing for Inertial Aided Navigation: A Recurrent Neural Network Based Approach</li>
<li>Robust Semantic Map Matching Algorithm Based on Probabilistic Registration Model</li>
<li>Accurate and Robust Scale Recovery for Monocular Visual Odometry Based on Plane Geometry</li>
<li>Greedy-Based Feature Selection for Efficient LiDAR SLAM</li>
<li>Road Mapping and Localization Using Sparse Semantic Visual Features</li>
<li>RoadMap: A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving</li>
<li>Visual Semantic Localization Based on HD Map for Autonomous Vehicles in Urban Scenarios</li>
<li>Hybrid Bird’s-Eye Edge Based Semantic Visual SLAM for Automated Valet Parking</li>
<li>CamVox: A Low-Cost and Accurate Lidar-Assisted Visual SLAM System</li>
<li>PSF-LO: Parameterized Semantic Features Based Lidar Odometry</li>
<li>PicoVO: A Lightweight RGB-D Visual Odometry Targeting Resource-Constrained IoT Devices</li>
<li>CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth</li>
<li>Deep Online Correction for Monocular Visual Odometry</li>
<li>Robust Improvement in 3D Object Landmark Inference for Semantic Mapping</li>
<li>YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
        <category>1.4 Robotics</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>ICRA</tag>
      </tags>
  </entry>
  <entry>
    <title>STL containers</title>
    <url>/20201125-stl-containers/</url>
    <content><![CDATA[<h1 id="std-array"><a href="#std-array" class="headerlink" title="std::array"></a>std::array</h1><h2 id="C-style-array랑-다른-점"><a href="#C-style-array랑-다른-점" class="headerlink" title="C-style array랑 다른 점"></a>C-style array랑 다른 점</h2><ul>
<li>자동 메모리 할당 / 해제</li>
<li>자동 에러 탐지<ul>
<li>out of range 에러 등등</li>
</ul>
</li>
<li>Deep copy 기능 추가<ul>
<li>자동으로 deep copy를 사용함.</li>
<li>복사 작업을 피하고 싶다면, reference나 const를 이용해서 추가하면 됨.</li>
</ul>
</li>
</ul>
<h2 id="사용-예제"><a href="#사용-예제" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Init</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="keyword">int</span>, 100&gt; arr;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="keyword">int</span>, 5&gt; arr1 = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line"></span><br><span class="line"><span class="comment">// Element access</span></span><br><span class="line">arr[<span class="number">0</span>] = <span class="number">10</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; arr[<span class="number">0</span>] &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; arr1.front() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; arr1.back() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *(arr1.data() + <span class="number">1</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Iterator</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = arr1.begin(); it != arr1.end(); it++)</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *it &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> elem: arr1)</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; elem &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exception handling using .at()</span></span><br><span class="line"><span class="keyword">try</span></span><br><span class="line">{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; arr1.at(<span class="number">6</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;    </span><br><span class="line">}</span><br><span class="line"><span class="keyword">catch</span> (<span class="keyword">const</span> <span class="built_in">std</span>::out_of_range&amp; exception)</span><br><span class="line">{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; exception.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-vector"><a href="#std-vector" class="headerlink" title="std::vector"></a>std::vector</h1><h2 id="std-array랑-다른-점"><a href="#std-array랑-다른-점" class="headerlink" title="std::array랑 다른 점"></a>std::array랑 다른 점</h2><ul>
<li><code>std::array</code>의 고정된 크기랑은 다르게 dynamic한 크기를 가짐.</li>
</ul>
<h2 id="특징"><a href="#특징" class="headerlink" title="특징"></a>특징</h2><ul>
<li>새로운 데이터 원소를 채워넣는데에는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>만큼 소요.</li>
<li><code>size</code>와 <code>capacity</code>를 가지고 있음.<ul>
<li>size는 실제로 데이터가 채워진 메모리의 양.</li>
<li>capacity는 현재 vector에 할당된 메모리의 양.<ul>
<li>capacity가 다 차게 되면, 2*size만큼 새로 capacity를 할당.</li>
<li>새로 할당된 메모리로 기존의 정보를 모두 이동 (i.e. 복사 이동)</li>
<li>이 경우, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>만큼 소요.</li>
</ul>
</li>
</ul>
</li>
<li><code>insert()</code>는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>만큼 소요.</li>
<li></li>
</ul>
<h2 id="사용-예제-1"><a href="#사용-예제-1" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec; <span class="comment">// 빈 vector</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">vec</span><span class="params">(<span class="number">10</span>)</span></span>; <span class="comment">// 크기가 10인 벡터 선언</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">vec</span><span class="params">(<span class="number">10</span>, <span class="number">0</span>)</span></span>; <span class="comment">// 크기가 10이고 0으로 초기화된 벡터 선언</span></span><br><span class="line"></span><br><span class="line">vec.insert(<span class="keyword">int</span>.begin(), <span class="number">0</span>); <span class="comment">// 중간에 추가</span></span><br><span class="line">vec.push_back(<span class="number">10</span>); <span class="comment">// 맨뒤에 추가</span></span><br><span class="line">vec.insert(<span class="built_in">std</span>::find(vec.begin(), vec.end(), <span class="number">10</span>), <span class="number">9</span>); <span class="comment">// 10 앞에 9 추가.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Image&gt; vecImg;</span><br><span class="line">vecImg.emplace(vecImg.begin()+<span class="number">1</span>, img());</span><br><span class="line">vecImg.emplace_back(img()); <span class="comment">// emplace_back()은 push_back()과 다르게 복사 연산이 들어가지 않음. 큰 구조체를 넣을 때 constructor와 함께 사용하면 좋음.</span></span><br><span class="line"></span><br><span class="line">vec.pop_back();</span><br><span class="line">vec.erase(vec.begin()+<span class="number">5</span>); <span class="comment">// 6번째 원소 지우기</span></span><br><span class="line">vec.erase(vec.begin() +<span class="number">2</span>, vec.begin() +<span class="number">4</span>) <span class="comment">// 2번째 부터 4번째 원소까지 지우기</span></span><br><span class="line"></span><br><span class="line">vec.clear() <span class="comment">// 전부 비우기.</span></span><br><span class="line">vec.reserve(<span class="number">100</span>) <span class="comment">// capacity를 100만큼 할당하기.</span></span><br><span class="line">vec.shrink_to_fit() <span class="comment">// 현재 사용중인 메모리에 효율적이도록 빈 메모리를 해제하기.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-forward-list"><a href="#std-forward-list" class="headerlink" title="std::forward_list"></a>std::forward_list</h1><h2 id="Contiguous-data-structure의-단점"><a href="#Contiguous-data-structure의-단점" class="headerlink" title="Contiguous data structure의 단점"></a>Contiguous data structure의 단점</h2><ul>
<li>데이터 중간에 자료를 추가하거나 뺄 때 비효율적.</li>
</ul>
<h2 id="std-forward-list-특징"><a href="#std-forward-list-특징" class="headerlink" title="std::forward_list 특징"></a>std::forward_list 특징</h2><ul>
<li>Linked list 자료구조에 간편한 기능들을 추가한 클래스.</li>
<li>최앞단 원소만 바로 접근 가능.<ul>
<li><code>push_front()</code> 또는 <code>insert_after()</code>을 사용해서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>으로 추가 가능.</li>
</ul>
</li>
<li><code>remove(x)</code>를 통해 list 내부에 x라는 값을 가진 원소를 모두 없앰.<ul>
<li>list 내부에 있는 데이터들이 <code>==operator</code>가 안된다면 컴파일러 에러.</li>
</ul>
</li>
<li><code>remove_if(comp)</code>를 사용해서 list 내부에 comp 함수의 조건을 맞춘 원소를 모두 없앰.<ul>
<li>람다 함수를 이용해서 만들면 좋음.</li>
</ul>
</li>
<li><code>remove()</code>와 <code>remove_if()</code>는 전체를 순회하기 때문에 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>이 걸림.</li>
<li><code>sort()</code>를 쓸 수 있지만, <code>std::vector</code> 등에서 쓰는 sort와 다름.<ul>
<li><code>&lt;operator</code>를 통해 사용하거나 커스텀 cmp 함수를 통해 사용 가능.</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.812ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4779 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1752, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2050, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2535, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(3012, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3401, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4001, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4390, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 만큼 소요됨.</li>
</ul>
</li>
<li><code>reverse()</code>로 순서를 뒤집을 수 있음. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
<h2 id="사용-예제-2"><a href="#사용-예제-2" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">std</span>::forward_list&lt;<span class="keyword">int</span>&gt; lst = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>};</span><br><span class="line"></span><br><span class="line">lst.push_front(<span class="number">0</span>); <span class="comment">// {0,1,2,3}</span></span><br><span class="line">lst.insert_after(lst.begin(), <span class="number">0</span>); <span class="comment">// 맨 처음 원소 뒤에 0 추가. {0,0,1,2,3}</span></span><br><span class="line"></span><br><span class="line">lst.pop_front() <span class="comment">// {0,1,2,3}</span></span><br><span class="line">lst.erase_after(lst.begin()); <span class="comment">// {1,2,3}</span></span><br><span class="line">lst.erase_after{lst.begin() + <span class="number">1</span>, lst_end()} <span class="comment">// {1}</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::forward_list&lt;Image&gt; lstImg;</span><br><span class="line">lst.emplace_front(Image());</span><br><span class="line">lst.emplace_after(lst.begin(),Image());</span><br><span class="line"></span><br><span class="line">lst.reverse(); <span class="comment">// {3,2,1,0}</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::forward_list&lt;<span class="keyword">int</span>&gt; lst1 = {<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>};</span><br><span class="line">lst.unique(); <span class="comment">// {0,1,2,3}</span></span><br><span class="line">lst.unique([](<span class="keyword">int</span>, a, <span class="keyword">int</span> b){<span class="keyword">return</span> (b-a) &lt;<span class="number">2</span>;}); <span class="comment">// {}. 특정 원소가 바로 앞 원소보다 2 이상 크지 않으면 삭제.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::forward_list&lt;<span class="keyword">int</span>&gt; lst2 = {<span class="number">100</span>, <span class="number">50</span>, <span class="number">70</span>, <span class="number">0</span>, <span class="number">-100</span>};</span><br><span class="line">lst2.sort(); <span class="comment">// {-100, 0, 50, 70, 100}</span></span><br><span class="line">lst2.sort(<span class="built_in">std</span>::greater&lt;<span class="keyword">int</span>&gt;()); <span class="comment">// {100, 70, 50, 0, -100};</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-list"><a href="#std-list" class="headerlink" title="std::list"></a>std::list</h1><h2 id="구현-방법"><a href="#구현-방법" class="headerlink" title="구현 방법"></a>구현 방법</h2><ul>
<li>Doubly-linked list<ul>
<li>각 node가 이전 node와 다음 node에 대한 포인터 값을 가지고 있음.</li>
</ul>
</li>
<li><code>std::forward_list</code>가 지원하지 못하는 <code>push_back()</code>, <code>emplace_back()</code>, <code>pop_back()</code>을 할 수 있음.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1778 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(889, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1389, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>으로 작동함.</li>
</ul>
</li>
<li>기존의 <code>remove()</code>, <code>remove_if()</code>, <code>sort()</code>, <code>unique()</code>, <code>reverse()</code> 등도 사용할 수 있음.<ul>
<li>훨씬 편리하기 때문에 대부분의 경우 <code>std::forward_list</code>보다 <code>std::list</code>를 더 많이 사용함.</li>
</ul>
</li>
<li>Bidirectional iterator 사용.</li>
</ul>
<h2 id="사용-예제-3"><a href="#사용-예제-3" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; lst = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line">lst.push_back(<span class="number">6</span>); <span class="comment">// {1,2,3,4,5,6}</span></span><br><span class="line">lst.insert(<span class="built_in">std</span>::next(<span class="built_in">list</span>.begin()),<span class="number">0</span>); <span class="comment">// {1,0,2,3,4,5,6}</span></span><br><span class="line">lst.pop_back(); <span class="comment">// {1,0,2,3,4,5}</span></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="Iterators-simple"><a href="#Iterators-simple" class="headerlink" title="Iterators (simple)"></a>Iterators (simple)</h1><h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><img src="/20201125-stl-containers/iterators.png" class="" title="iterators">
<ul>
<li><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3BsdXNwbHVzLmNvbS9yZWZlcmVuY2UvaXRlcmF0b3Iv">이미지 출처<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>Iterator의 종류는 사용하는 컨테이너에 따라 결정된다.</p>
</li>
<li><p><code>std::vector</code>, <code>std::array</code>는 contiguous data structure이기 때문에 특정 위치의 데이터 원소에 곧바로 접근 가능.</p>
<ul>
<li>Random access iterator</li>
</ul>
</li>
<li><p><code>std::forward_list</code>는 역방향이 존재하지 않음.</p>
<ul>
<li>Forward iterator</li>
</ul>
</li>
<li><p><code>std::list</code>는 양방향 움직일 수 있음.</p>
<ul>
<li>Bidirectional iterator</li>
</ul>
</li>
<li><p><code>advance()</code>, <code>next()</code>, <code>prev()</code>로 움직일 수 있음.</p>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> it = vec.begin();</span><br><span class="line">it += <span class="number">3</span>;</span><br><span class="line"><span class="built_in">std</span>::advance(it, <span class="number">-2</span>);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; *it &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::forward_list&lt;<span class="keyword">int</span>&gt; lst = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line"><span class="keyword">auto</span> it lst.begin();</span><br><span class="line"><span class="comment">// it += 3; // Error, since std::forward_list does not support random access</span></span><br><span class="line"><span class="built_in">std</span>::advance(it, <span class="number">3</span>);</span><br><span class="line"><span class="comment">// std::advance(it, -1); // Error, since std::forward_list does not support  backward iteration</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-deque"><a href="#std-deque" class="headerlink" title="std::deque"></a>std::deque</h1><h2 id="구현-1"><a href="#구현-1" class="headerlink" title="구현"></a>구현</h2><ul>
<li>Deque: Double-ended queue 의 약자.</li>
<li>성능:<ul>
<li><code>push_front()</code>, <code>pop_front()</code>, <code>push_back()</code>, <code>pop_back()</code>이 모두 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 작동.</li>
<li>Random access도 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 작동.</li>
<li><code>insert()</code>와 <code>delete()</code>는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 작동.</li>
</ul>
</li>
<li>메모리 구조:<ul>
<li>크기가 같은 여러개의 메모리 청크를 사용.</li>
<li>이 메모리 청크들의 주소를 연속적인 메모리 구조에 저장.<ul>
<li>새로운 메모리 공간을 할당할 때, 기존의 메모리는 그대로 있음.</li>
</ul>
</li>
</ul>
</li>
<li>지원 함수:<ul>
<li><code>push_front()</code>, <code>pop_front()</code>, <code>emplace_front()</code></li>
<li><code>push_back()</code>, <code>pop_back()</code>, <code>emplace_back()</code></li>
<li><code>insert()</code>, <code>emplace()</code>, <code>delete()</code></li>
<li><code>erase()</code></li>
<li><code>shrink_to_fit()</code></li>
<li><code>capacity()</code>는 지원 안됨!</li>
</ul>
</li>
<li><code>std::vector</code>와는 메모리 할당 방식이 다름.</li>
</ul>
<div class="video-container"><iframe src="https://www.youtube.com/embed/3ibj354Y8vg" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h2 id="사용-예제-4"><a href="#사용-예제-4" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">deque</span>&lt;<span class="keyword">int</span>&gt; deq = {<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>};</span><br><span class="line"></span><br><span class="line">deq.push_front(<span class="number">0</span>);</span><br><span class="line">deq_push_back(<span class="number">6</span>);</span><br><span class="line"></span><br><span class="line">deq.insert(deq.begin() + <span class="number">2</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">deq.pop_back();</span><br><span class="line">deq.pop_front();</span><br><span class="line"></span><br><span class="line">deq.erase(deq.begin() + <span class="number">1</span>);</span><br><span class="line">deq.erase(deq.begin() + <span class="number">3</span>, deq.end());</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-stack-container-adapter"><a href="#std-stack-container-adapter" class="headerlink" title="std::stack (container adapter)"></a>std::stack (container adapter)</h1><h2 id="구현-2"><a href="#구현-2" class="headerlink" title="구현"></a>구현</h2><ul>
<li><code>std::deque</code>를 기본 컨테이너로 사용하여 만든 래퍼 클래스.<ul>
<li>원소 저장 공간을 재할당할때 <code>std::vector</code>처럼 전체 원소를 이동할 필요가 없기 때문.</li>
<li>하지만 원한다면 <code>std::stack&lt;int, std::vector&lt;int&gt;&gt;</code>와 같은 형태로 만들 수 있음.</li>
<li><code>std::stack&lt;int, std::list&lt;int&gt;&gt;</code>도 가능.</li>
</ul>
</li>
<li>LIFO (Last In First Out) 구조를 가지고 있음.<ul>
<li><code>std::deque</code>는 push_front()를 지원…</li>
<li><code>std::stack</code>은 해당 기능을 꺼둠. 왜냐하면 stack은 push_front같은거 지원하지 않기 때문.</li>
</ul>
</li>
<li>모든 연산의 시간 복잡도가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
</ul>
<p> </p>
<hr>
<h1 id="std-queue-container-adapter"><a href="#std-queue-container-adapter" class="headerlink" title="std::queue (container adapter)"></a>std::queue (container adapter)</h1><h2 id="구현-3"><a href="#구현-3" class="headerlink" title="구현"></a>구현</h2><ul>
<li>FIFO (First In First Out) 구조<ul>
<li><code>std::queue</code>에서의 <code>push()</code>는 <code>std::stack</code>에서의 <code>push_back()</code>을 의미</li>
<li><code>std::queue</code>에서의 <code>pop()</code>는 <code>std::stack</code>에서의 <code>pop_front()</code>를 의미</li>
</ul>
</li>
<li>기본적으로 <code>std::deque</code>를 사용해서 만든 래퍼 클래스.<ul>
<li>모든 함수가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>으로 사용 가능. </li>
</ul>
</li>
</ul>
<h2 id="사용-예제-5"><a href="#사용-예제-5" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">q.push(<span class="number">1</span>);</span><br><span class="line">q.push(<span class="number">2</span>);</span><br><span class="line">q.push(<span class="number">3</span>); <span class="comment">// {1,2,3}</span></span><br><span class="line"></span><br><span class="line">q.pop(); <span class="comment">// {2,3}</span></span><br><span class="line">q.push(<span class="number">4</span>); <span class="comment">// {2,3,4}</span></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="std-priority-queue"><a href="#std-priority-queue" class="headerlink" title="std::priority_queue"></a>std::priority_queue</h1><h2 id="구현-4"><a href="#구현-4" class="headerlink" title="구현"></a>구현</h2><ul>
<li>Priority queue, 또는 Heap이라고도 불림.</li>
<li>가장 작거나 (또는 가장 큰) 원소에 빠르게 접근 할 수 있는 자료구조.</li>
<li>최소/최대 원소에 접근하는데는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2041 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1152, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1652, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>원소 삽입/제거는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.455ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4179 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3401, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3790, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><ul>
<li>원소 제거는 최소/최대 원소에 대해서만 가능.</li>
</ul>
</li>
<li><code>std::vector</code>를 기본 컨테이너로 사용해서 구현.</li>
<li>기본적으로 <code>std::less</code>를 사용해서 max heap이 구현되어있음</li>
</ul>
<h2 id="사용-예제-6"><a href="#사용-예제-6" class="headerlink" title="사용 예제"></a>사용 예제</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Creates a max heap</span></span><br><span class="line"><span class="built_in">priority_queue</span> &lt;<span class="keyword">int</span>&gt; pq;</span><br><span class="line">pq.push(<span class="number">5</span>);</span><br><span class="line">pq.push(<span class="number">1</span>);</span><br><span class="line">pq.push(<span class="number">10</span>);</span><br><span class="line">pq.push(<span class="number">30</span>);</span><br><span class="line">pq.push(<span class="number">20</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// One by one extract items from max heap</span></span><br><span class="line"><span class="comment">// 30 20 10 5 1 </span></span><br><span class="line"><span class="keyword">while</span> (pq.empty() == <span class="literal">false</span>)</span><br><span class="line">{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; pq.top() &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    pq.pop();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a min heap</span></span><br><span class="line"><span class="built_in">priority_queue</span> &lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt; &gt; pq1;</span><br><span class="line">pq1.push(<span class="number">5</span>);</span><br><span class="line">pq1.push(<span class="number">1</span>);</span><br><span class="line">pq1.push(<span class="number">10</span>);</span><br><span class="line">pq1.push(<span class="number">30</span>);</span><br><span class="line">pq1.push(<span class="number">20</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// One by one extract items from min heap</span></span><br><span class="line"><span class="comment">// 1 5 10 20 30 </span></span><br><span class="line"><span class="keyword">while</span> (pq1.empty() == <span class="literal">false</span>)</span><br><span class="line">{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; pq1.top() &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    pq1.pop();</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Data structures</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>NVIDIA Jetson user forum</title>
    <url>/20201212-jetson/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9mb3J1bXMuZGV2ZWxvcGVyLm52aWRpYS5jb20vYy9hZ3gtYXV0b25vbW91cy1tYWNoaW5lcy9qZXRzb24tZW1iZWRkZWQtc3lzdGVtcy83MA==">https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/70<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20201212-jetson/jetson.png" class="" title="jetson">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>NVIDIA Jetson</tag>
      </tags>
  </entry>
  <entry>
    <title>Algebra + Calculus 공부 노트 (Paul&#39;s maths notes, Calculus made easy) 레퍼런스</title>
    <url>/20201212-maths-notes/</url>
    <content><![CDATA[<h1 id="Calculus-made-easy"><a href="#Calculus-made-easy" class="headerlink" title="Calculus made easy"></a>Calculus made easy</h1><p><span class="exturl" data-url="aHR0cHM6Ly9jYWxjdWx1c21hZGVlYXN5Lm9yZy8=">https://calculusmadeeasy.org<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20201212-maths-notes/calculus.png" class="" title="calculus">

<p>&nbsp;</p>
<hr>
<h1 id="Paul’s-online-maths-note"><a href="#Paul’s-online-maths-note" class="headerlink" title="Paul’s online maths note"></a>Paul’s online maths note</h1><p><span class="exturl" data-url="aHR0cHM6Ly90dXRvcmlhbC5tYXRoLmxhbWFyLmVkdS8=">https://tutorial.math.lamar.edu<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>Lamar 대학 수학강의노트</li>
<li>Algebra, Calculus I/II/III, Differential equations 다 있음</li>
</ul>
<img src="/20201212-maths-notes/paul.png" class="" title="paul">]]></content>
      <categories>
        <category>4. Maths</category>
      </categories>
      <tags>
        <tag>Maths</tag>
        <tag>Algebra</tag>
        <tag>Calculus</tag>
      </tags>
  </entry>
  <entry>
    <title>의료쪽 공부 레퍼런스</title>
    <url>/20201212-medicine-references/</url>
    <content><![CDATA[<h1 id="WebSurg-IRCAD-Online-university"><a href="#WebSurg-IRCAD-Online-university" class="headerlink" title="WebSurg (IRCAD Online university)"></a>WebSurg (IRCAD Online university)</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cud2Vic3VyZy5jb20v">https://www.websurg.com<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>수술영상 및 다양한 외과 기법들 수업 영상 모음집</li>
</ul>
<p>&nbsp;</p>
<hr>
<h1 id="BioDigital-Human"><a href="#BioDigital-Human" class="headerlink" title="BioDigital Human"></a>BioDigital Human</h1><p><span class="exturl" data-url="aHR0cHM6Ly9odW1hbi5iaW9kaWdpdGFsLmNvbS9sb2dpbj9yZXR1cm5Vcmw9L2V4cGxvcmU=">https://human.biodigital.com/login?returnUrl=/explore<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>온라인 human anatomy navigator</li>
</ul>
<img src="/20201212-medicine-references/biodigital.png" class="" title="biodigital">

<p>&nbsp;</p>
<hr>
<h1 id="Medical-dictionary"><a href="#Medical-dictionary" class="headerlink" title="Medical dictionary"></a>Medical dictionary</h1><p><span class="exturl" data-url="aHR0cHM6Ly9tZWRpY2FsLWRpY3Rpb25hcnkudGhlZnJlZWRpY3Rpb25hcnkuY29tLw==">https://medical-dictionary.thefreedictionary.com<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>온라인 메디컬 사전</li>
</ul>
]]></content>
      <categories>
        <category>5. Medicine</category>
      </categories>
      <tags>
        <tag>Medicine</tag>
        <tag>WebSurg</tag>
        <tag>IRCAD</tag>
        <tag>BioDigital Human</tag>
      </tags>
  </entry>
  <entry>
    <title>Metallurgy 공부 레퍼런스</title>
    <url>/20201212-metallurgy/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly93d3cucGhhc2UtdHJhbnMubXNtLmNhbS5hYy51ay90ZWFjaGluZy5odG1s">https://www.phase-trans.msm.cam.ac.uk/teaching.html<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>케임브리지 대학 metallurgy 공부 자료.</li>
<li>옛날에 Iron-ferrous composition 쪽 공부할 때 진짜 재밌었는데!</li>
</ul>
<img src="/20201212-metallurgy/metallurgy.png" class="" title="metallurgy">]]></content>
      <categories>
        <category>6. Manufacturing Engineering</category>
      </categories>
      <tags>
        <tag>Metallurgy</tag>
      </tags>
  </entry>
  <entry>
    <title>SOTA 논문 팔로우업 - Papers with code 페이지 레퍼런스</title>
    <url>/20201212-paperswithcode/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20v">https://paperswithcode.com<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20201212-paperswithcode/paperswithcode.png" class="" title="paperswithcode">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>PapersWithCode</tag>
      </tags>
  </entry>
  <entry>
    <title>PRML (Pattern recognition and Machine Learning) 한국어 정리 레퍼런스</title>
    <url>/20201212-prml-kor/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cDovL25vcm1hbjMuZ2l0aHViLmlvLw==">http://norman3.github.io<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20201212-prml-kor/prml.png" class="" title="prml">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>Maths</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>수학공부에 좋은 웹사이트 - Wolfram mathworld 레퍼런스</title>
    <url>/20201212-wolfram-mathworld/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9tYXRod29ybGQud29sZnJhbS5jb20v">https://mathworld.wolfram.com<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20201212-wolfram-mathworld/mathworld.png" class="" title="mathworld">
]]></content>
      <categories>
        <category>4. Maths</category>
      </categories>
      <tags>
        <tag>Maths</tag>
        <tag>Wolfram mathworld</tag>
      </tags>
  </entry>
  <entry>
    <title>Tartan Series 2021 - Challenges in SLAM - What&#39;s ahead (Prof. Sebastian Scherer)</title>
    <url>/20210529-tartan-scherer/</url>
    <content><![CDATA[<p>강의 링크 - <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9hY1lCU3JEcEVkUQ==">https://youtu.be/acYBSrDpEdQ<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="SLAM이-처음이라면…"><a href="#SLAM이-처음이라면…" class="headerlink" title="SLAM이 처음이라면…"></a>SLAM이 처음이라면…</h2><ul>
<li>선형대수 공부자료 - <span class="exturl" data-url="aHR0cDovL3Zpc2lvbi5zdGFuZm9yZC5lZHUvdGVhY2hpbmcvY3MxMzFfZmFsbDE2MTcvbGVjdHVyZXMvbGVjdHVyZTJfbGluYWxnX3Jldmlld19jczEzMV8yMDE2LnBkZg==">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>확률과 통계 공부자료 - <span class="exturl" data-url="aHR0cDovL2hlYWx5LmNyZWF0ZS5zdGVkd2FyZHMuZWR1L0NoZW1pc3RyeS9DSEVNNDM0MS9CYXllc1ByaW1lcjIucGRm">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>AirLab Summer School - <span class="exturl" data-url="aHR0cDovL3RoZWFpcmxhYi5vcmcvc3VtbWVyMjAyMC8=">링크<i class="fa fa-external-link-alt"></i></span></li>
<li>CVPR 2020 SLAM 워크샵 - <span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvdmlzbG9jc2xhbWN2cHIyMDIwL2ludml0ZWQtc3BlYWtlcnM=">링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p> </p>
<hr>
<h2 id="SLAM에서-자주-쓰는-단어들"><a href="#SLAM에서-자주-쓰는-단어들" class="headerlink" title="SLAM에서 자주 쓰는 단어들"></a>SLAM에서 자주 쓰는 단어들</h2><p><img src="./1.png" alt="terms"></p>
<ul>
<li>Pose<ul>
<li>위치 (Position) + 방향 (Orientation)</li>
</ul>
</li>
<li>Odometry<ul>
<li>두 pose간의 상대적 차이</li>
</ul>
</li>
<li>Localization<ul>
<li>내가 지도 상 어디에 있는지 풀어내는 문제</li>
</ul>
</li>
<li>Mapping<ul>
<li>지도를 그려내는 문제</li>
</ul>
</li>
<li>Drift<ul>
<li>연속적으로 위치를 추정할 때 쌓여가는 pose에 대한 오차</li>
</ul>
</li>
<li>Loop closure<ul>
<li>동일한 장소로 돌아왔을 때 생기는 loop 내부에서 drift를 해소하는 방법</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM이란"><a href="#SLAM이란" class="headerlink" title="SLAM이란?"></a>SLAM이란?</h2><ul>
<li>기술적 의미: Localization ⇄ Mapping 루프를 통해 위치/맵 추정을 하는 것.<ol>
<li>먼저 나의 위치를 확인 (localization)</li>
<li>맵을 증축함 (mapping)</li>
<li>증축된 맵에서 나의 위치를 확인 (localization)</li>
<li>맵을 증축함 (mapping)</li>
<li>…</li>
</ol>
</li>
<li>일반적으로 보는 의미: 다양한 센서들로 다양한 추정을 함<ul>
<li>카메라, 레이더, 라이다, GPS 등과 같이 다양한 센서 값을 어떻게 조합해서 위치/맵 추정을 할 것인가?</li>
<li>움직이는 객체가 있을 경우에는? Semantic 정보도 추출할 수 있는지? </li>
<li>종종 ‘위치 추정’ 기술을 쉽게 부르기 위해서 SLAM이라고 부르기도 함 (사실 알고보면 그냥 odometry나 mapping인 경우도 많음)</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-기술의-발전-방향"><a href="#SLAM-기술의-발전-방향" class="headerlink" title="SLAM 기술의 발전 방향"></a>SLAM 기술의 발전 방향</h2><ul>
<li>SLAM 성능<ul>
<li>Robustness의 발전<ul>
<li>Visual / geometric degeneration / challenging scenes (e.g. 안개, 비/눈, 화재, 탄광)</li>
</ul>
</li>
<li>Efficiency의 발전<ul>
<li>Lightweight image localization</li>
<li>Dense reconstruction</li>
</ul>
</li>
<li>Accuracy의 발전<ul>
<li>High resolution</li>
</ul>
</li>
</ul>
</li>
<li>신기능<ul>
<li>Collaborative SLAM<ul>
<li>여러 대의 agent가 돌아다니면서 각각 맵을 만드는 것.</li>
</ul>
</li>
<li>Semantic SLAM<ul>
<li>SLAM을 하면서 semantic 정보도 동시에 추출하는 것.</li>
</ul>
</li>
<li>High resolution map<ul>
<li>거대 공간 (i.e. large scale)에서 mm단위의 정확한 맵 생성</li>
</ul>
</li>
<li>Unified map representation<ul>
<li>카메라로 읽던, 라이다로 읽던, 레이더로 읽던 호환되는 맵</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-방법론-개요"><a href="#SLAM-방법론-개요" class="headerlink" title="SLAM 방법론 개요"></a>SLAM 방법론 개요</h2><p><img src="./2.png" alt="Overview"></p>
<blockquote>
<p>위 이미지에서 설명된 Visual / LiDAR 외로 GPS, 레이더, sonar 등 다른 방식들도 있음.</p>
</blockquote>
<h3 id="LiDAR-SLAM"><a href="#LiDAR-SLAM" class="headerlink" title="LiDAR SLAM"></a>LiDAR SLAM</h3><p><img src="./3.png" alt="LiDAR SLAM"></p>
<ul>
<li>굉장히 정확한 편</li>
<li>공간의 생김새에 따라 정확도/안정성이 변화함</li>
</ul>
<h3 id="Visual-SLAM"><a href="#Visual-SLAM" class="headerlink" title="Visual SLAM"></a>Visual SLAM</h3><p><img src="./4.png" alt="Visual SLAM"></p>
<ul>
<li>저렴한 센서</li>
<li>공간에 존재하는 texture에 따라 정확도/안정성이 변화함</li>
<li>3가지 Visual SLAM 방법론이 존재함<ul>
<li>Sparse SLAM: Sparse local feature를 (e.g. SIFT, ORB) 기반으로 SLAM 추정</li>
<li>Semi-dense SLAM : Sparse와 Dense SLAM의 중간</li>
<li>Dense SLAM: 이미지 전체의 정보를 사용해서 SLAM 추정</li>
</ul>
</li>
</ul>
<h3 id="RGB-D-Visual-Inertial-SLAM"><a href="#RGB-D-Visual-Inertial-SLAM" class="headerlink" title="RGB-D / Visual-Inertial SLAM"></a>RGB-D / Visual-Inertial SLAM</h3><p><img src="./5.png" alt="RGB-D/Visual-Inertial SLAM"></p>
<ul>
<li>센서 1개만 쓰면 잘 안될때가 많다 (센서들마다 잘 되는곳/안되는 곳이 있다)<ul>
<li>여러개의 센서를 써서 잘 안되는 부분을 커버한다.</li>
<li>e.g. Visual + IMU 방식은 monocular 방식이 (i.e. 1개 카메라) 풀지 못하는 metric scale 복원 (i.e. 실제 m단위 스케일) 작업이 가능하다.</li>
</ul>
</li>
</ul>
<h3 id="“Deep”-Visual-SLAM"><a href="#“Deep”-Visual-SLAM" class="headerlink" title="“Deep” Visual SLAM"></a>“Deep” Visual SLAM</h3><p><img src="./6.png" alt="Deep Visual SLAM"></p>
<ul>
<li>뉴럴넷을 사용해서 state estimation을 진행</li>
<li>기존의 방식이 가지던 한계점을 뉴럴넷이 극복할 수 있을 것이라는 희망</li>
<li>Semantic 정보를 추출해내려는 시도</li>
<li>현재 이러한 방식으로 풀려고 하는 문제들<ul>
<li>텍스처가 전혀 없는 곳에서 SLAM을 할 수 있을까?</li>
<li>High dynamic range 환경에서 SLAM을 할 수 있을까?</li>
<li>모션 블러가 있는 곳에서 SLAM을 할 수 있을까?</li>
<li>움직이는 객체가 많은 곳에서 SLAM을 할 수 있을까?</li>
</ul>
</li>
</ul>
<h3 id="Multi-Sensor-Fusion"><a href="#Multi-Sensor-Fusion" class="headerlink" title="Multi-Sensor Fusion"></a>Multi-Sensor Fusion</h3><p><img src="./7.png" alt="Multi sensor fusion"></p>
<ul>
<li>여러개의 센서를 사용<ul>
<li>많은 정보를 얻음</li>
<li>센서들마다 가진 단점을 서로 상쇄</li>
</ul>
</li>
</ul>
<hr>
<h2 id="SLAM-성능을-측정하는-방법"><a href="#SLAM-성능을-측정하는-방법" class="headerlink" title="SLAM 성능을 측정하는 방법"></a>SLAM 성능을 측정하는 방법</h2><h3 id="데이터셋"><a href="#데이터셋" class="headerlink" title="데이터셋"></a>데이터셋</h3><p><img src="./8.png" alt="Datasets"></p>
<ul>
<li>KITTI, EuRoC, TUM 데이터셋은 SLAM 연구자들 사이에서 많이 쓰이는 데이터셋임.</li>
<li>하지만 위 데이터셋은 굉장히 단조로움<ul>
<li>환경, 라벨, 모션 패턴이 모두 단조로움.</li>
<li>다양한 데이터가 없기 때문에, 위 데이터셋으로 뉴럴넷을 학습할 경우 특정 컨디션에 오버핏 할 가능성이 높음.</li>
</ul>
</li>
<li>그러니 <span class="exturl" data-url="aHR0cHM6Ly90aGVhaXJsYWIub3JnL3RhcnRhbmFpci1kYXRhc2V0Lw==">Tartan Dataset<i class="fa fa-external-link-alt"></i></span>을 써라! (결국 본인 랩실 홍보 ㅋㅋ)<ul>
<li>Urban, rural, domestic, public, scifi 씬 등 20개 환경 구비</li>
<li>RGB, Depth, Segmentation, Flow, Pose 등 데이터 구비</li>
<li>Random translation / rotation의 500+ 모션 구비</li>
<li>빡센 환경 구비 (e.g. 연기, 안개, 어두운 밤 + 밝은 불빛, 비, 눈)</li>
</ul>
</li>
</ul>
<h3 id="측정-metric"><a href="#측정-metric" class="headerlink" title="측정 metric"></a>측정 metric</h3><ul>
<li><strong>Absolute Trajectory Error (ATE)**와 **Relative Pose Error (RPE)</strong> 기반으로 성능을 측정하는 경우가 많음.<ul>
<li>ATE와 RPE는 중간에 끊기는 데이터 (i.e. tracking lost)에 대응하지 못함.</li>
<li>Challenging scene에서는 끊기는 경우가 허다함.</li>
</ul>
</li>
<li>AirLab에서는 robustness에 대한 새로운 척도를 제안<ul>
<li>Valid Rate = Length of valid trajectory / Total length</li>
<li>Valid = Successfully initialised + not lose tracking<ul>
<li>‘Valid’ 척도는 사실 굉장히 주관적임…</li>
</ul>
</li>
<li>더 좋은 metric을 만들려고 연구중이라고 함.</li>
<li>어찌되었건 이 metric으로 봤었을 때, ORB-SLAM은 KITTI 벤치마크 기준 잘된다고 알려져있으나 Tartan Dataset에서는 완전 말아먹는 모습을 보임.</li>
</ul>
</li>
<li>TartanAir-V2를 만들고 있다고 함.<ul>
<li>더 많은 환경을 만들 예정.</li>
<li>IMU, LiDAR, Spherical, Fisheye, Event, Radar 등을 추가할 예정.</li>
<li>Dynamic object를 넣을 예정</li>
<li>GRound robot motion 패턴도 추가할 예정.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="TartanVO-Learning-based-Visual-Odometry"><a href="#TartanVO-Learning-based-Visual-Odometry" class="headerlink" title="TartanVO: Learning-based Visual Odometry"></a>TartanVO: Learning-based Visual Odometry</h2><p><img src="./9.png" alt="TartanVO"><br><img src="./10.png" alt="TartanVO"></p>
<ul>
<li>TartanAir 처럼 다양한 환경/모션 정보가 있는 데이터셋이 있으니, SLAM 뉴럴넷을 학습해볼 수 있겠다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9OUTFVRWgzdGhiVQ==">TartanVO<i class="fa fa-external-link-alt"></i></span></li>
<li>기존의 Feature detection / matching 단계를 하나의 뉴럴넷으로 대체</li>
<li>기존의 Motion estimation 과정을 deep optical flow + pose 뉴럴넷으로 대체</li>
</ul>
</li>
<li>극심한 모션 블러에도 실패하지 않음 (기존의 SLAM 방식은 tracking lost가 남)</li>
</ul>
<p> </p>
<hr>
<h2 id="Super-Odometry-IMU-centric-LiDAR-Visual-Inertial-Estimator-for-Challenging-Environments"><a href="#Super-Odometry-IMU-centric-LiDAR-Visual-Inertial-Estimator-for-Challenging-Environments" class="headerlink" title="Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments"></a>Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments</h2><p><img src="./11.png" alt="Super Odometry"></p>
<ul>
<li>Robustness를 높이는 방법 중 하나는 다양한 종류의 센서를 사용하는 것임.</li>
<li>Super Odometry는 IMU를 중심으로 카메라와 라이다 같은 센서들을 퓨전해서 사용함.<ul>
<li>왜 IMU 중심으로 사용하는가?<ul>
<li>카메라는 texture가 부족할 때 실패함</li>
<li>라이다는 geometry가 부족할 때 실패함</li>
<li>이에 반해 IMU는 전혀 외적 요인을 타지 않음</li>
</ul>
</li>
<li>특정 센서가 실패하는 상황이 올 때, 해당 센서로 부터 오는 정보는 사용하지 않고 잘 작동하고 있는 센서에서만 정보를 받아와서 다양한 상황에서도 안정적으로 위치추정을 할 수 있음.</li>
</ul>
</li>
<li>예시로, 2가지 상황을 보여줌<ul>
<li>평평한 벽이 좌우로 나있는 긴 복도에서는 라이다가 실패함. 하지만 카메라 정보로부터 odometry를 계속 받기때문에 잘 됨.</li>
<li>어두운 방에 들어가면 아무것도 보이지 않아 카메라가 실패함. 하지만 라이다 정보를 계속 받아 odometry를 잘 유지함.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Visual-localization"><a href="#Visual-localization" class="headerlink" title="Visual localization"></a>Visual localization</h2><ul>
<li>Visual localization = “2D 이미지만 가지고 3D 공간에서 내가 어디있는지 알 수 있을까?”<ul>
<li>3D 공간에 대한 2D appearance 정보는 전혀 없고, 3D 포인트 클라우드만 있을 때는 어떻게 할까?</li>
<li>비슷한 공간들에 대한 place recognition은 어떻게 할까?</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="2D-3D-Line-매칭-기반-Localization"><a href="#2D-3D-Line-매칭-기반-Localization" class="headerlink" title="2D-3D Line 매칭 기반 Localization"></a>2D-3D Line 매칭 기반 Localization</h3><p><img src="./12.png" alt="2D-3D Line Correspondence based localization"><br><img src="./13.png" alt="Line detector"></p>
<ul>
<li>3D LiDAR map에서 카메라로 어떻게 위치를 찾을 수 있을까?<ul>
<li>Cross modality!</li>
</ul>
</li>
<li>Edge 정보를 이용해서 카메라 이미지 상의 2D line과 포인트 클라우드 지도상의 3D line을 매칭해보자!<ul>
<li>이를 위해 새롭게 딥러닝 기반 line detector를 만들었다.<ul>
<li>어떠한 카메라에서도 잘 작동한다 (e.g. pinhole, fisheye, spherical cameras)</li>
<li>Intrinsic에 대해 전혀 영향을 받지 않는다.</li>
</ul>
</li>
</ul>
</li>
<li>기존의 SLAM/odometry 방식보다 훨씬 적은 drift를 가지면서 할 수 있다<ul>
<li>(당연할걸수도…? localization은 drift를 가질 수 없음)</li>
</ul>
</li>
<li>이 방식의 단점은, line feature는 실내처럼 사람이 만든 공간에서는 많이 나타나지만 자연공간에서는 많이 나타나지 않다는 점임.</li>
<li>참고논문:<ul>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS9wdWJsaWNhdGlvbnMvbGluZS1iYXNlZC0yZC0zZC1yZWdpc3RyYXRpb24tYW5kLWNhbWVyYS1sb2NhbGl6YXRpb24taW4tc3RydWN0dXJlZC1lbnZpcm9ubWVudHMv">Line-based 2D-3D registration and camera localization in structured envrionemnts<i class="fa fa-external-link-alt"></i></span></li>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDA3NDA=">Monocular camera localization in prior LiDAR Maps with 2D-3D correspondences<i class="fa fa-external-link-alt"></i></span></li>
<li>Yu 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMTEuMDMxNzQ=">ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="시점변화-조명변화-날씨변화에-강인한-descriptor-learning"><a href="#시점변화-조명변화-날씨변화에-강인한-descriptor-learning" class="headerlink" title="시점변화/조명변화/날씨변화에 강인한 descriptor learning"></a>시점변화/조명변화/날씨변화에 강인한 descriptor learning</h3><p><img src="./14.png" alt="Cross-domain (Image-to-LiDAR) descriptor learning module "></p>
<ul>
<li>자연공간에서는 어떤 방식으로 매칭을 해야할까?<ul>
<li>어떤 방식이 좋은지 모르겠다면… 그 방식을 딥러닝으로 학습하도록 하자.</li>
</ul>
</li>
<li>Visual-LiDAR 도메인 사이에서 사용될 수 있는 descriptor를 학습하였다.<ul>
<li>전체 네트워크는 matching module과 similarity module로 나눠진다.</li>
</ul>
</li>
<li>참고 논문:<ul>
<li>Yin 2019 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDIuMTAwNTg=">A multi-domain feature learning method for visual place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2020 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzNDE3Mjc=">SeqSphereVLAD: Sequence Matching Enhanced Orientation-invariant Place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2021 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzNjEzMTY=">FusionVLAD: A Multi-View Deep Fusion Networks for Viewpoint-Free 3D Place recognition<i class="fa fa-external-link-alt"></i></span></li>
<li>Yin 2021 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMTI4ODM=">i3dLoc: Image-to-range Cross-domain Localization Robust t oinconsistent Environmental Conditions<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="Lifelong-Graph-Learning을-이용한-feature-matching-place-recognition"><a href="#Lifelong-Graph-Learning을-이용한-feature-matching-place-recognition" class="headerlink" title="Lifelong Graph Learning을 이용한 feature matching / place recognition"></a>Lifelong Graph Learning을 이용한 feature matching / place recognition</h3><p><img src="./15.png" alt="lifelong graph learning"></p>
<ul>
<li>SLAM에서 loop closure를 할 때 place recognition을 통해서 이전에 와본 장소로 왔다는것을 인식하고, local feature matching을 통해 localization을 한다. 이후 graph optimization을 한다.</li>
<li>이 때 사용되는 local feature extraction과 matching 과정을 lifelong graph learning을 이용해서 개선시킨다.<ul>
<li>Memory module을 통해 실시간으로 매칭을 개선한다</li>
<li>Memory replay를 통해 기억이 사라지는 것 (i.e. catastrophic forgetting)을 방지한다</li>
<li>참고논문: Wang 2021 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDkuMDA2NDc=">Lifelong Graph Learning<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Large-scale-sub-millimeter-mapping-with-lightweight-and-low-cost-weight-sensors"><a href="#Large-scale-sub-millimeter-mapping-with-lightweight-and-low-cost-weight-sensors" class="headerlink" title="Large scale sub-millimeter mapping (with lightweight and low-cost weight sensors)"></a>Large scale sub-millimeter mapping (with lightweight and low-cost weight sensors)</h2><p><img src="./16.png" alt="HARD"></p>
<ul>
<li>위 사진들과 같이 고해상도 SLAM이 작동하기 어려운 공간들이 있다.</li>
<li>위와 같은 환경에서 강인하게 SLAM을 작동하기 위해서 2가지 전략을 사용한다.<ol>
<li>최대한 Field of view (i.e. 시야 범위)를 넓힌다<ul>
<li>넓은 범위를 봄으로써 특정 공간에서 degenerate case가 나타나도 다른 시야 범위의 공간정보를 이용해서 실패를 방지한다.</li>
</ul>
</li>
<li>다양한 센서들을 퓨전해서 좋은 sparse model을 얻은 후, dense 모델로 재구성한다.<ul>
<li>카메라+라이다+IMU로 sparse reconstruction을 먼저 한다.</li>
<li>이후 stereo depth와 laser range projection을 퓨전해서 dense reconstruction을 수행한다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<p> </p>
<h3 id="카메라-라이다-IMU-퓨전"><a href="#카메라-라이다-IMU-퓨전" class="headerlink" title="카메라+라이다+IMU 퓨전"></a>카메라+라이다+IMU 퓨전</h3><p><img src="./17.png" alt="recon"><br><img src="./18.gif" alt="recon"></p>
<ul>
<li>많이 사용되는 COLMAP, OpenMVG와 같은 이미지 기반 reconstruction 기법에서는 제대로 scale을 측정하지 못하는 경우가 많다.</li>
<li>위의 연구는 joint camera-lidar optimisation을 통해 scale 정보까지 확실하게 추정할 수 있다.</li>
<li>참고자료:<ul>
<li>Zhen 2020 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTEuMDMzNjk=">LiDAR-enhanced Structure-from-Motion<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2019 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDcuMDA5MzA=">A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3-D Reconstructions<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2019 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS93cC1jb250ZW50L3VwbG9hZHMvMjAxOS8wNi9yb290LnBkZg==">Estimating the localizability in tunnel-like environments using LiDAR and UWB<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2018 - <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MTAuMTI1MTU=">A Unified 3D Mapping Framework Using a 3D or 2D LiDAR<i class="fa fa-external-link-alt"></i></span></li>
<li>Zhen 2017 - <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS93cC1jb250ZW50L3VwbG9hZHMvMjAxNy8wNC9lc2tmLnBkZg==">Robust localization and localizability estimation with a rotating laser scanner<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="저해상도-이미지-기반-고해상도-dense-stereo-reconstruction"><a href="#저해상도-이미지-기반-고해상도-dense-stereo-reconstruction" class="headerlink" title="저해상도 이미지 기반 고해상도 dense stereo reconstruction"></a>저해상도 이미지 기반 고해상도 dense stereo reconstruction</h3><p><img src="./19.png" alt="ORStereo"><br><img src="./20.png" alt="ORStereo"></p>
<ul>
<li>Stereo reconstruction을 잘 하는 방법에는 ‘4K의 고해상도 카메라’ + ‘넓은 baseline’을 가지고 촬영하면 된다.<ul>
<li>이렇게 하면 sub-millimeter 정확도까지 얻어낼 수 있다.</li>
</ul>
</li>
<li>기존의 방식으로는 수천만개의 픽셀들에 대해 disparity 계산을 할 수 없다 (딥러닝을 쓰던 안쓰던 RAM을 너무 많이 차지할것이다)</li>
<li>제안하는 연구는, 기존의 4K 이미지를 다운샘플링해서 딥러닝 기반 방식으로 disparity와 occlusion을 구한 다음에, disparity를 4K로 업샘플링해서 기존의 4K 이미지와 부분적으로 비교하여 disparity와 occlusion을 얻어낸다.<ul>
<li>이런 방식으로 하면 기존의 저해상도 이미지에서 얻은 disparity 정보보다 훨씬 더 많은 디테일을 얻을 수 있다.</li>
<li>이 정교한 disparity 정보를 dense reconstruction에 사용하면, 위와 같이 굉장히 작은 균열 (sub-millimeter)도 검출할 수 있다.</li>
</ul>
</li>
<li>참고자료:<ul>
<li>Hu 2020 - <a href="">Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction</a></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Open-questions"><a href="#Open-questions" class="headerlink" title="Open questions"></a>Open questions</h2><ul>
<li>현재 SLAM을 기반으로 perception, planning, control이 이뤄지고 있는 로봇들이 많다.<ul>
<li>이러한 시스템에는 Metric SLAM이 필수적인데, 막상 연구개발 해보니까 이게 쉽지 않다.</li>
<li>Metric SLAM을 대체할 수 있는 것이 있을까?</li>
<li>만약에 SLAM이 우리가 생각했던대로 성공할 수 없는 기술이라면???</li>
</ul>
</li>
<li>‘강인함’을 재는 척도는 어떤게 좋을까?</li>
<li>맵의 품질은 어떻게 재야할까?</li>
<li>딥러닝을 SLAM에 진짜 쓸 수 있는걸까?<ul>
<li>Generalization은 어떻게 해야할까?</li>
<li>Online learning 방식이 좋을까? 아니면 시뮬레이션에서 더 학습하고 오는게 좋을까?<ul>
<li>시뮬레이션을 쓴다면 domain adaptation은?</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="참가자-질문"><a href="#참가자-질문" class="headerlink" title="참가자 질문"></a>참가자 질문</h2><ul>
<li>Ground truth는 어떻게 얻는가?<ul>
<li>실외: Survey scanner로 맵을 땀. GPS로 위치 추정</li>
<li>실내: Survey scanner / 라이다로 맵을 땀. 모션캡처로 위치 추정</li>
</ul>
</li>
<li>Edge 기기에서 잘 도는 SLAM 추천해주세요!<ul>
<li>무슨 센서를 쓰는지, 어떤 정보를 얻고싶은지에 따라 다르다. 하나의 SLAM으로 다 해먹을 수 없음.</li>
</ul>
</li>
<li>Visual place recognition과 SLAM의 접점은 어떻게 되는가?<ul>
<li>증강현실 등에서 robust localization은 중요하다 (i.e. global cooridinate를 알아야하는 경우)</li>
<li>3D map은 있는데, 2D 이미지 센서로 연결점을 만들어야할 때 중요하다.</li>
<li>Loop closure할 때도 중요하다.<ul>
<li>라스트마일 배달로봇, 맹인 가이드 로봇 등에 중요하다.</li>
</ul>
</li>
</ul>
</li>
<li>인턴십 뽑나요! (ㅋㅋ)<ul>
<li>여름 인턴십 프로그램이 있다 (학부생)</li>
</ul>
</li>
<li>슬램 성능은 어떻게 재나요?<ul>
<li>ATE, RPE</li>
<li>맵에 대한 정보는 조금 더 연구가 필요함.</li>
</ul>
</li>
<li>안개끼고 눈내리는 곳에서 센서는 뭘 써야할까요?<ul>
<li>Thermal camera + IMU + radar 좋은듯!</li>
</ul>
</li>
<li>Brownout effect에는 어떻게 대응하나요?<ul>
<li>(Brownout effect = 먼지가 엄청 많은 곳)</li>
<li>필터로 어느정도 제거해줄 수 있음.</li>
<li>하지만 너무 심한 경우에는 그냥 카메라/라이다 센서 데이터를 사용하지 않고 레이더만 사용한다던가</li>
<li>또는 특정 빛의 스펙트럼을 받는 카메라를 사용할 수 있음.</li>
</ul>
</li>
<li>SLAM에서 traditional / learning-based 중에 어떤걸 써야할까? (Prof. Kaess 질문)<ul>
<li>글쎄… 프론트엔드에서는 하나만 쓰는거보다는 엔지니어링 관점에서 섞어쓰면 어떨까 생각한다.<ul>
<li>피쳐가 있을 때는 traditional, 없을때는 learning-based를 쓰면 어떨까?</li>
</ul>
</li>
<li>백엔드에서는… 그건 당신이 더 전문가잖아! (ㅋㅋㅋ)<ul>
<li>기존의 bundle adjustment, pose graph optimisation, scan matching 등은 잘됨.<ul>
<li>지금 현재 방법에서 속도를 더 빠르게 하는 것 외로는 다른 어떤 방식이 있는지 잘 모르겠음.</li>
<li>현재 방식이 잘 안되는 경우는 대부분 ‘gaussian model assumption’에서 나오는 것이라고 봄.</li>
</ul>
</li>
<li>딥러닝 방식으로 시도를 하기도 했지만… 아직 인정해줄만큼 성과가 안나옴. 하지만 길게 보았을때 어떻게 될지는 모르겠음</li>
</ul>
</li>
</ul>
</li>
<li>센서퓨전을 할 때 어떤 방법을 써야하는가? ~~~한 케이스가 있는데, EKF를 써야할지 UKF를 써야할지 모르겠다.<ul>
<li>왠만한 경우에는 EKF도 잘 된다.</li>
<li>UKF를 쓰고 싶으면 ‘Unscented’ 파트가 진짜 필요한지 생각해봐야한다.<ul>
<li>Non-Gaussian인가?</li>
</ul>
</li>
</ul>
</li>
<li>Loosely-coupled vs Tightly-coupled의 차이가 무엇인가?<ul>
<li>Loosely-coupled: 여러개의 센서가 각각 odometry를 계산하고, 그걸 합친다.</li>
<li>Tightly-coupled: 여러개의 센서 값을 기반으로 하나의 odometry를 계산한다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Deep SLAM</tag>
        <tag>Tartan</tag>
        <tag>AirLab</tag>
        <tag>CMU</tag>
        <tag>Sebastian Scherer</tag>
      </tags>
  </entry>
  <entry>
    <title>Tartan Series 2021 - Factor Graphs and Robust Perception (Prof. Michael Kaess)</title>
    <url>/20210604-tartan-kaess/</url>
    <content><![CDATA[<h2 id="Factor-graphs"><a href="#Factor-graphs" class="headerlink" title="Factor graphs"></a>Factor graphs</h2><ul>
<li>2001년에 확립된 개념<ul>
<li>Kschischang 2001 - <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkxMDU3Mg==">Factor graphs and the sum-product algorithm<i class="fa fa-external-link-alt"></i></span></li>
<li>Factor graph는 어떤 함수의 factorisation을 표현하는 <span class="exturl" data-url="aHR0cHM6Ly9nbWx3amQ5NDA1LmdpdGh1Yi5pby8yMDE4LzA4LzIzL2FsZ29yaXRobS1iaXBhcnRpdGUtZ3JhcGguaHRtbA==">이분 그래프<i class="fa fa-external-link-alt"></i></span>이다.</li>
</ul>
</li>
</ul>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.041ex" xmlns="http://www.w3.org/2000/svg" width="24.68ex" height="5.191ex" role="img" focusable="false" viewBox="0 -950 10908.7 2294.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mrow" transform="translate(477, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1364.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(1809.2, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(3147.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3592.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4638.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mo" transform="translate(5782.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(6838.4, 0)"><g data-mml-node="mo"><path data-c="220F" d="M220 812Q220 813 218 819T214 829T208 840T199 853T185 866T166 878T140 887T107 893T66 896H56V950H1221V896H1211Q1080 896 1058 812V-311Q1076 -396 1211 -396H1221V-450H725V-396H735Q864 -396 888 -314Q889 -312 889 -311V896H388V292L389 -311Q405 -396 542 -396H552V-450H56V-396H66Q195 -396 219 -314Q220 -312 220 -311V812Z"></path></g><g data-mml-node="TeXAtom" transform="translate(33.7, -1100) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(412, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1079, 0)"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g></g></g><g data-mml-node="msub" transform="translate(8283, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="TeXAtom" transform="translate(490, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(9114.4, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="58" d="M270 0Q252 3 141 3Q46 3 31 0H23V46H40Q129 50 161 88Q165 94 244 216T324 339Q324 341 235 480T143 622Q133 631 119 634T57 637H37V683H46Q64 680 172 680Q297 680 318 683H329V637H324Q307 637 286 632T263 621Q263 618 322 525T384 431Q385 431 437 511T489 593Q490 595 490 599Q490 611 477 622T436 637H428V683H437Q455 680 566 680Q661 680 676 683H684V637H667Q585 634 551 599Q548 596 478 491Q412 388 412 387Q412 385 514 225T620 62Q628 53 642 50T695 46H726V0H717Q699 3 591 3Q466 3 445 0H434V46H440Q454 46 476 51T499 64Q499 67 463 124T390 238L353 295L350 292Q348 290 343 283T331 265T312 236T286 195Q219 88 218 84Q218 70 234 59T272 46H280V0H270Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="6A" d="M98 609Q98 637 116 653T160 669Q183 667 200 652T217 609Q217 579 200 564T158 549Q133 549 116 564T98 609ZM28 -163Q58 -168 64 -168Q124 -168 135 -77Q137 -65 137 141T136 353Q132 371 120 377T72 385H52V408Q52 431 54 431L58 432Q62 432 70 432T87 433T108 434T133 436Q151 437 171 438T202 441T214 442H218V184Q217 -36 217 -59T211 -98Q195 -145 153 -175T58 -205Q9 -205 -23 -179T-55 -117Q-55 -94 -40 -79T-2 -64T36 -79T52 -118Q52 -143 28 -163Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(1405.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></svg></mjx-container></p>
<ul>
<li>Factor graph는 <strong>Variable node</strong>와 <strong>Factor node</strong>로 이뤄진다.<ul>
<li>로보틱스의 경우, variable node는 robot position, landmark position 등이 될 수 있다.</li>
<li>Factor node는 measurement로부터 (i.e. 센서 값으로부터) 계산된 constraint가 된다.</li>
</ul>
</li>
<li>Factor graph를 이용해서 sum-product 문제를 풀 수 있다.<ul>
<li>Sum-product 문제에는 belief propagation, Viterbi, Kalman filter, turbo-codes, fast Fourier transform 등 다양한 알고리즘들이 있다.</li>
<li>이 문제들을 factor graph를 사용해서 풀 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Factor-graphs-for-robust-perception"><a href="#Factor-graphs-for-robust-perception" class="headerlink" title="Factor graphs for robust perception"></a>Factor graphs for robust perception</h2><p><img src="./1.png" alt="sensors"></p>
<ul>
<li>강인하게 localization, mapping, control 문제를 풀기 위해서는 여러가지 센서를 사용하는데, 이 때 factor graph를 통해서 다양한 센서들로부터 들어오는 measurement 값들과 최적화를 위한 constraint 값들을 모델링 할 수 있다.<ul>
<li>그리고 이 모델들을 기반으로 문제를 해결할 수 있다 (i.e. inference).</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="SLAM-and-factor-graphs"><a href="#SLAM-and-factor-graphs" class="headerlink" title="SLAM and factor graphs"></a>SLAM and factor graphs</h2><p><img src="./2.png" alt="SLAM"></p>
<ul>
<li>SLAM에서 factor graph를 이용하는 방법은 다음과 같다.</li>
<li>처음 로봇이 켜지면 아무런 정보도 없다.</li>
<li>로봇이 카메라나 라이다 정보를 통해 어떠한 landmark와 연관된 정보를 얻게 된다.<ul>
<li>이 때, 로봇의 입장에서 landmark의 위치를 추론할 수 있게 해주는 정보를 ‘landmark measurement’라고 한다.<ul>
<li>카메라에서는 local feature의 (e.g. SIFT, ORB) 픽셀 위치가 될 수 있다.</li>
<li>라이다에서는 3D point의 위치가 될 수 있다.</li>
</ul>
</li>
<li>추론된 landmark는 landmark 1,2,3 이렇게 저장된다.</li>
</ul>
</li>
<li>로봇이 움직이면서 본인의 운동량에 대한 정보를 얻게 된다.<ul>
<li>Wheel odometry를 통해 이동한 거리에 대한 정보를 얻을 수 있다.</li>
<li>IMU 센서 값을 dead reckoning을 통해 이동한 거리에 대한 정보를 얻을 수 있다.</li>
<li>카메라/라이다 odometry를 통해 이동한 거리에 대한 정보를 얻을 수 있다.</li>
</ul>
</li>
<li>로봇이 움직이면서 새로운 landmark measurement를 가지게 되고, 새로운 landmark 정보를 계속 얻게 된다.<ul>
<li>하지만 landmark measurement / odometry measurement에는 오차 값이 포함되므로 로봇의 위치에는 오차가 점점 쌓인다.</li>
</ul>
</li>
<li>이 후, 로봇이 이전에 보았던 landmark를 다시 보게되었을 때 loop closure가 발동하게 된다.<ul>
<li>Loop closure가 발동하면, 해당 landmark를 보았던 시점에서부터 loop closure가 발동한 시점까지 이어져있는 모든 node/edge들은 최적화되면서 오차를 해소할 수 있다 (i.e. 로봇의 위치와 맵을 보정한다).</li>
</ul>
</li>
</ul>
<p> </p>
<p><img src="./3.png" alt="SLAM"></p>
<ul>
<li>위에서 설명한 SLAM의 과정을 factor graph로 표현하면 다음과 같다.</li>
<li>중요한 점은, variable node 사이에 factor node가 올라가있는 형태가 아니다.<ul>
<li>즉, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 975.6 607.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>과 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="1.587ex" height="1.91ex" role="img" focusable="false" viewBox="0 -694 701.6 844"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mn" transform="translate(298, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>이 하나의 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.899ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1281.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>이라는 edge로 이어져있는 것이 아니다.</li>
<li>실제로는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 975.6 607.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.899ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1281.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>과 이어져있고, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.899ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1281.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(878, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>이 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="1.587ex" height="1.91ex" role="img" focusable="false" viewBox="0 -694 701.6 844"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mn" transform="translate(298, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>과 이어져있는 것이다.</li>
<li>Edge는 node들 사이에 있는 것이며, 하나에 factor에는 여러개의 variable이 연결될 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<p><img src="./4.png" alt="SLAM"></p>
<ul>
<li>다양한 문제에 Factor graph를 적용한 모습이다.</li>
<li>예시로 보면, SLAM과 Structure from motion과 pose graph optimisation이 다른 형태를 가진 것을 볼 수 있다.<ul>
<li>SLAM은 landmark와 pose 정보를 모두 최적화 하는 것이다.</li>
<li>Structure from motion은 SLAM에서 odometry measurement가 없는 형태이다 (즉, 시간 정보를 가지고 있지 않다).</li>
<li>Pose graph optimisation은 SLAM에서 landmark 정보를 제거한 후 pose 값만 가지고 최적화를 하는 것이다.<ul>
<li>그렇기 때문에 더 부정확할 수 밖에 없다.</li>
<li>하지만 계산량이 훨씬 적다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Tartan</tag>
        <tag>AirLab</tag>
        <tag>CMU</tag>
        <tag>Michael Kaess</tag>
        <tag>Factor graphs</tag>
        <tag>Graph-based SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Juric 2021 - A Comparison of Graph Optimization Approaches for Pose Estimation in SLAM</title>
    <url>/20210607-solver-comp/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9sYW1vci5mZXIuaHIvaW1hZ2VzLzUwMDM2NjA3LzIwMjEtYWp1cmljLWNvbXBhcmlzb24tbWlwcm8ucGRm">Juric 2021 - A Comparison of Graph Optimization Approaches for Pose Estimation in SLAM 논문<i class="fa fa-external-link-alt"></i></span>의 리뷰입니다.</p>
<p> </p>
<hr>
<h2 id="목적"><a href="#목적" class="headerlink" title="목적"></a>목적</h2><ul>
<li>다양한 SLAM solver 프레임워크들의 성능을 비교합니다.<ul>
<li>g2o, ceres-solver, GTSAM, SE-Sync</li>
<li>이 solver 라이브러리들에 대한 설명은 <a href="https://changh95.github.io/20210607-solvers/">Graph-based SLAM 입문 + Solver 프레임워크 소개 (Ceres-solver, g2o, GTSAM, SE-Sync)</a> 글 참조.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="실험환경"><a href="#실험환경" class="headerlink" title="실험환경"></a>실험환경</h2><ul>
<li>인텔 8코어 i7-6700HQ. 2.60GHz<ul>
<li>16GB 램</li>
<li>Ubuntu 20.04</li>
</ul>
</li>
<li>g2o, ceres-solver, GTSAM은 Levenberg-Marquardt (L-M) solver 사용</li>
<li>SE-Sync는 Riemannian trust-region 기법 (RTR) 사용 (L-M 미지원)</li>
<li>100 iteration만 가능<ul>
<li>Early stop<ul>
<li>L-M의 경우 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="4.42ex" height="2.005ex" role="img" focusable="false" viewBox="0 -864 1953.7 886"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1000, 393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g></g></g></g></g></svg></mjx-container> 까지 떨어지면 early-stop.</li>
<li>RTR의 경우 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="4.42ex" height="2.005ex" role="img" focusable="false" viewBox="0 -864 1953.7 886"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1000, 393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></g></svg></mjx-container> 까지 떨어지면 early-stop.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="데이터셋"><a href="#데이터셋" class="headerlink" title="데이터셋"></a>데이터셋</h2><ul>
<li>Real-world<ul>
<li>INTEL, MIT, Garage, Cubicle, Rim</li>
</ul>
</li>
<li>Simulation<ul>
<li>M3500 (a,b,c), Sphere-a, Torus, Cube</li>
</ul>
</li>
</ul>
<p>아래는 각각의 데이터셋이 가지고 있는 node와 edge들 (i.e. constraints들의 수)</p>
<img src="/20210607-solver-comp/datasets.png" class="" title="datasets">

<p> </p>
<hr>
<h2 id="실험-결과"><a href="#실험-결과" class="headerlink" title="실험 결과"></a>실험 결과</h2><img src="/20210607-solver-comp/result.png" class="" title="result">

<ul>
<li>빨간색: 제일 빠른 프레임워크<ul>
<li>SE-Sync가 제일 많은 빨간색을 가짐</li>
</ul>
</li>
<li>초록색: 가장 정확한 프레임워크<ul>
<li>SE-Sync가 제일 많은 초록색을 가짐</li>
</ul>
</li>
<li>파란색: 값은 잘 나오지만, 시각화해서 보면 완전 잘못된 결과 (i.e. local minima converge)<ul>
<li>Ceres와 GTSAM이 파란색이 많은 편</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><ul>
<li>SE-Sync<ul>
<li><strong>가장 빠른 프레임워크</strong><ul>
<li>다른 프레임워크에 비해 몇십배~몇백배 빠른 경우도 있었다.</li>
<li>추가적인 verification step을 거치면 global optimality도 얻을 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>g2o<ul>
<li>제일 오래 걸리는 편</li>
<li>간단한 2D 데이터셋에서는 잘 됨</li>
</ul>
</li>
<li>Ceres<ul>
<li>모든 면에서 딱 중간 - <strong>적당히 빠르고, 적당히 정확함</strong>.</li>
</ul>
</li>
<li>GTSAM<ul>
<li><strong>SE-Sync랑 성능이 동급</strong></li>
<li>하지만 <strong>노이즈가 섞여있을 때 local minima에 빠짐</strong>.</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>기본적으로 SE-Sync를 쓰는게 제일 좋을듯</strong>.<br>탄탄한 frontend를 기반으로 <strong>좋은 initial guess가 보장된다면 GTSAM을 사용</strong>해도 무방.</p>
</blockquote>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Solver</tag>
        <tag>GTSAM</tag>
        <tag>Ceres-solver</tag>
        <tag>g2o</tag>
        <tag>SE-Sync</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph-based SLAM 입문 + Solver 프레임워크 소개 (Ceres-solver, g2o, GTSAM, SE-Sync)</title>
    <url>/20210607-solvers/</url>
    <content><![CDATA[<h2 id="Optimisation-기반-SLAM"><a href="#Optimisation-기반-SLAM" class="headerlink" title="Optimisation 기반 SLAM"></a>Optimisation 기반 SLAM</h2><p>SLAM 기술의 코어는 non-linear한 특성의 센서 데이터를 기반으로 map과 pose에 대한 최적의 state를 추정하는 것이다.</p>
<p>이전에는 Kalman filter나 particle filter 기반의 알고리즘들을 사용하여 observation model과 motion model이 확률/수치적으로 동의할 수 있는 최적의 state를 찾았다. 하지만 최근에는 <strong>batch optimisation 기술</strong>을 이용하여 최적의 state를 찾아내는 방식을 사용하는데, 이는 컴퓨터의 발전으로 인해 무거운 batch optimisation 게산을 실시간으로 수행할 수 있게 되었기 때문이다. Batch optimisation 방식은 filter 방식에 비교했을 때 절대적인 계산량은 더 많지만, <strong>동일한 계산량 (i.e. CPU 사이클)에서 더 높은 정확도를 보여주기 때문</strong>에 컴퓨팅 파워가 받쳐주는 플랫폼에서는 optimisation 기법을 선호하는 경우가 많다.</p>
<blockquote>
<p>이번 글에서는 이러한 optimisation 기반 SLAM solver에 대해 이야기한다.</p>
</blockquote>
<p>Optimisation 기반 SLAM 방법론 중 가장 유명한 방법은 <strong>Graph-based SLAM</strong>이며, 보통 <strong>Frontend</strong>와 <strong>Backend</strong> 두가지 단계를 거친다.</p>
<p><strong>Frontend</strong>는 센서 값들로부터 현재 <strong>로봇의 이동치</strong>와 <strong>주변 환경</strong>에 대한 정보를 계산하는데, 이 때 이동치는 motion model을, 주변 환경 정보는 observation model을 사용해서 계산한다. Graph-based SLAM에서는 로봇의 위치와 주위 랜드마크의 위치를 node로 표현하는데, 우리가 계산한 <strong>이동치는 로봇의 위치를 표현하는 node끼리를 이어주는 motion constraint</strong>, 그리고 <strong>주변 환경의 위치 정보는 로봇-랜드마크 간의 상대적인 관계를 표현하는 node를 이어주는 observation constraint</strong>로 표현된다. Frontend는 이렇게 새로운 node를 만들고, 또 node 끼리의 constraint도 이어주기 때문에 graph construction이라고 불리기도 한다.</p>
<p><img src="http://jinyongjeong.github.io/images/post/SLAM/lec13_least_square_SLAM/graph1.png"></p>
<p><strong>Backend</strong>는 frontend에서 생성된 <strong>constraint들을 기반으로 최적의 (i.e. optimal) pose &amp; map을 계산</strong>한다. Graph solving에 가장 보편적인 방법은 <strong>least squares</strong>로써, 선형대수에서 이야기하는 Ax=b 형태의 문제에서 A가 graph matrix, b가 센서 정보일 때, x가 무엇이 나와야하는지 찾는 것이라고 보면 된다. 이러한 Graph matrix는 엄청나게 거대하기 때문에 단번에 푸는 것은 불가능하고 (특히나 SLAM에서 요구하는 실시간 계산 조건에서는!), 그렇기 때문에 <strong>iterative</strong>하게 풀어야한다. <strong>우리가 사용하는 센서들은 non-linear</strong>하기 때문에 iterative solving에도 매번 <strong>linearization 과정</strong>이 들어가야한다.</p>
<p><img src="http://jinyongjeong.github.io/images/post/SLAM/lec13_least_square_SLAM/graph2.png"></p>
<blockquote>
<p>Backend를 좀 더 자세히 이해하고 싶다면 다음 글을 참조하면 좋다. 1.’<a href="https://changh95.github.io/20210313-ba/">Bundle adjustment란?</a>‘, 2. ‘<a href="https://changh95.github.io/20210314-nonlinear-optimisation/">비선형 최적화 Non-linear optimisation - Gradient descent, Newton-Raphson, Gauss-Newton, Levenberg-Marquardt</a>‘</p>
</blockquote>
<p>Backend의 과정을 정리해보자면 다음과 같은 단계가 있다.</p>
<ul>
<li>Constraint 기반으로 least squares optimisation problem 생성</li>
<li>Linearization</li>
<li>Iterative solving</li>
</ul>
<p>이 모든 과정을 쉽게 계산하기 위해 우리는 <strong>Solver 라이브러리</strong>들을 사용한다.</p>
<p> </p>
<hr>
<h2 id="Solver-라이브러리-소개"><a href="#Solver-라이브러리-소개" class="headerlink" title="Solver 라이브러리 소개"></a>Solver 라이브러리 소개</h2><p>이러한 optimisation 계산을 빠르게 구현할 수 있도록 여러 solver 프레임워크들이 개발되었다. 가장 유명한 것들로는 다음과 같은 라이브러리들이 있다.</p>
<ul>
<li>Freiburg 대학의 <span class="exturl" data-url="aHR0cHM6Ly9vcGVuc2xhbS1vcmcuZ2l0aHViLmlvL2cybw==">g2o<i class="fa fa-external-link-alt"></i></span></li>
<li>Google의 <span class="exturl" data-url="aHR0cDovL2NlcmVzLXNvbHZlci5vcmcv">ceres-solver<i class="fa fa-external-link-alt"></i></span></li>
<li>Georgia Tech의 <span class="exturl" data-url="aHR0cHM6Ly9ndHNhbS5vcmcv">GTSAM<i class="fa fa-external-link-alt"></i></span></li>
<li>MIT의 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RhdmlkLW0tcm9zZW4vU0UtU3luYw==">SE-Sync<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p> </p>
<div class="tabs" id="solvers"><ul class="nav-tabs"><li class="tab active"><a href="#solvers-1">g2o</a></li><li class="tab"><a href="#solvers-2">Ceres-solver</a></li><li class="tab"><a href="#solvers-3">GTSAM</a></li><li class="tab"><a href="#solvers-4">SE-Sync</a></li></ul><div class="tab-content"><div class="tab-pane active" id="solvers-1"><p>g2o는 graph 형태의 nonlinear 함수들을 최적화 할 수 있는 오픈소스 프레임워크이다.</p>
<p>2011년 릴리즈 당시 저자들은 SOTA 알고리즘들의 성능에 준한다고 이야기했고, 현재도 지속적으로 업데이트되면서 다양한 모던 프로세서에 대해 호환성도 가지고 있다. </p>
<p>프레임워크에서 Pose graph optimisation을 푸는 방법으로는 Gauss-Newton, Levenberg-Marquardt, Doglog 방식을 지원하는데, 이는 <strong>왠만한 SLAM 문제는 다 풀 수 있다</strong>고 볼 수 있다. <strong>사용하기 굉장히 쉬운 편</strong>이기 때문에 Visual-SLAM 연구/개발에서 많이 사용하며, <strong>ORB-SLAM과 SVO에서도 사용</strong>되었다. </p>
<p>특정 모듈들이 LGPL3+와 GPL3+ 라이센스를 가지고 있지만, 특정 linear solver 및 뷰어에 한정되어있다. 연구 쪽에서는 오픈소스하는 경우가 많기 때문에 GPL-v3 라이센스는 문제가 되지 않아 <strong>연구와 궁합이 잘 맞고</strong>, 상용쪽에서는 최적의 성능을 위해 어차피 뷰어를 잘 사용하지 않기 때문에 <strong>상용에도 궁합이 잘 맞는 편</strong>이다.</p></div><div class="tab-pane" id="solvers-2"><p>Ceres-solver는 대형 nonlinear least squares 최적화 문제를 풀기위해 만든 오픈소스 라이브러리이다. Bundle adjustment 기법을 사용하는 SLAM과 SfM (i.e. Structure from Motion) 분야에서 많이 사용한다.</p>
<p><strong>사용하기 쉬운 편</strong>이며 <strong>뛰어난 성능</strong>을 가졌고 <strong>portable</strong>하다는 장점을 가지고 있다. 하지만 무엇보다 가장 큰 장점은 <strong>최적화에 필요한 objective function을 유저가 직접 만들 수 있다는 점 (i.e. custom function)**인데, 이는 새로운 아이디어/방법론을 연구할 때 엄청난 이점이 된다. 여기에 **linearization을 쉽게 할 수 있는 auto-diff 기능</strong>까지 있으니 <strong>기능과 편의성을 동시에 잡았다</strong>고 볼 수 있다.</p>
<p>지원하는 solver는 trust region solver (i.e. Levenberg-marquardt, dogleg)와 line search solver가 있다.</p>
<p><strong>센서퓨전을 이용해서 tightly-fused 알고리즘들은 왠만하면 Ceres를 이용</strong>하며, Camera + IMU를 사용하는 <strong>OKVIS와 VINS-Mono가 Ceres 를 사용</strong>한다.</p></div><div class="tab-pane" id="solvers-3"><p>GTSAM은 로보틱스 환경에서 센서퓨전 최적화 문제를 풀기 위해 만든 오픈소스 라이브러리이다.</p>
<p>GTSAM은 <strong>factor graph</strong>를 이용해서 최적화 문제를 모델링하는데, 다른 라이브러리들에 비해 내부적으로 matrix sparsity를 잘 이용해서 <strong>계산 효율이 가장 좋다</strong>고 볼 수 있다. </p>
<p>다른 라이브러리들이 지원하는 Gauss-Newton, Levenberg-Marquardt, dogleg에 더불어 conjugate gradient optimizer와 <strong>iSAM solver</strong> (i.e. incremental smoothing and mapping) 기능을 가지고 있다. 최근 graph-based SLAM 형태로 VIO를 구현할 때는 <strong>IMU preintegration 기술</strong>의 사용이 필수적인데, 이러한 기능을 쉽게 사용 가능한 함수 API로 구현해두었다. </p>
<p>현재 구현 단계에서도 <strong>연구/개발 양쪽 다 쉽게 사용</strong>할 수 있지만, 최근 OpenSAM이라는 이름으로 좀 더 industry에서 사용하기 좋은 형태를 만들고 있다는 소식이 있다.</p></div><div class="tab-pane" id="solvers-4"><p>SE-Sync는 special Euclidean group (i.e. 3D pose) 최적화 문제에서 <strong>global minima를 보장하는 certifiably correct algorithm</strong>을 이용한 최적화 라이브러리이다.</p>
<p>2D/3D geometry fitting, graph-based SLAM, camera motion estimation 등에서 사용할 수 있다.</p>
<p>가장 최신의 프레임워크이라 <strong>아직 유저층이 두텁지는 않다</strong>.</p>
<p>다른 프레임워크들에 비해 <strong>가장 진보된 알고리즘</strong>을 가지고 있다. 예를 들어, large scale optimisation을 풀 때는 많은 constraint들로 인해 non-convex한 manifold가 구축되는데 (i.e. local minima가 많이 생기는데), 이 문제를 좀 더 <strong>풀기 쉬운 convex 형태의 문제로 근사시켜 global minima를 계산하는 semidefinite relaxation 기법</strong> 등이 있다. (Trust region 방법으로 truncated-Newton Riemannian method를 사용한다고 하는데… 뭔지 잘 모르겠다 이름은 멋있는데!)</p></div></div></div>

<hr>
<h2 id="어떤것을-써야하는가"><a href="#어떤것을-써야하는가" class="headerlink" title="어떤것을 써야하는가?"></a>어떤것을 써야하는가?</h2><p><a href="https://changh95.github.io/20210607-solver-comp/">Juric 2021 논문</a>에서 발표한 내용은 다음과 같다.</p>
<ul>
<li>가장 빠른걸 원한다면 SE-Sync<ul>
<li>SLAM 프론트엔드에서 노이즈를 잘 잡아낸다면 GTSAM도 상관 없음</li>
</ul>
</li>
<li>모든 성능면에서 평균적인건 Ceres-solver</li>
<li>g2o는 쉬운 문제에서 잘 됨</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Solver</tag>
        <tag>GTSAM</tag>
        <tag>Ceres-solver</tag>
        <tag>g2o</tag>
        <tag>SE-Sync</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2021 튜토리얼 / 워크샵 리스트</title>
    <url>/20210619-cvpr-2021/</url>
    <content><![CDATA[<h2 id="Workshop"><a href="#Workshop" class="headerlink" title="Workshop"></a>Workshop</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9HTklvclY1RFJBYw==">Vision Meets Mapping 3 (VMM3) – Computer Vision for Location-based Reasoning and Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>19일 10:50PM - 7:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9DT2dFUXVxVEF1Zw==">Autonomous Driving: Perception Prediction and Planning<i class="fa fa-external-link-alt"></i></span><ul>
<li>19일 11:00PM - 7:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS81aHVrcC14TWMwZw==">New frontiers in data-driven autonomous driving<i class="fa fa-external-link-alt"></i></span><ul>
<li>19일 11:00PM - 3:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vcGxheWxpc3Q/bGlzdD1QTGpMTGZ4RjdJQXJHMFJoZTdwb09WWEZONHU1VktvRXpz">Normalization Techniques in Deep Learning: Methods, Analyses, and Applications<i class="fa fa-external-link-alt"></i></span><ul>
<li>19일 11:00PM - 3:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMZVhXei1nMklmOTVtak5wQS15LVdJb0Rhb0I4V3RtRTc=">Workshop on Event-based Vision<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9TVlNOVTZ2S3d3WQ==">DynaVis: The 3rd International Workshop on Dynamic Scene Reconstruction<i class="fa fa-external-link-alt"></i></span><ul>
<li>20일 0:30AM - 4:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zY2VuZS11bmRlcnN0YW5kaW5nLmNvbS90YWxrcy5odG1s">3D Scene Understanding for Vision, Graphics, and Robotics<i class="fa fa-external-link-alt"></i></span><ul>
<li>20일 4:00AM - 8:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS80VlBNMFBQdjRxOA==">Computational Cameras and Displays<i class="fa fa-external-link-alt"></i></span><ul>
<li>20일 10:40PM - 7:30AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1NZEQ0VU1zaGwxUQ==">Leave Those Nets Alone: Advances in Self-Supervised Learning<i class="fa fa-external-link-alt"></i></span><ul>
<li>20일 11:00PM - 3:30AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vYy9XQURhdENWUFI=">Workshop on Autonomous Driving<i class="fa fa-external-link-alt"></i></span><ul>
<li>20일 11:50PM - 8:30AM</li>
</ul>
</li>
<li>[All About Self-Driving]<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8tbnI3LVBRUVlydw==">Part1<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9QM21sQTBtaFVXYw==">Part2<i class="fa fa-external-link-alt"></i></span></li>
<li>21일 0:45AM - 10:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc3JpLmNvbS9jb21wdXRlci12aXNpb24vY3Zwci0yMDIxLXR1dG9yaWFsLW9uLWNyb3NzLXZpZXctYW5kLWNyb3NzLW1vZGFsLXZpc3VhbC1nZW8tbG9jYWxpemF0aW9uLw==">Cross-view and Cross-modal Visual Geo-Localization<i class="fa fa-external-link-alt"></i></span><ul>
<li>21일 3:00AM - 7:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9qcl9iS21oNllVWQ==">ScanNet Indoor Secene Understanding Challenge<i class="fa fa-external-link-alt"></i></span><ul>
<li>21일 0:50AM - 4:30AM</li>
</ul>
</li>
<li>Interpretable Machine Learning for Computer Vision<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9rQVN2dy1sekxCaw==">Video1<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS94N1U1cUM2ZU1uRQ==">Video2<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9CUTA2RXlkTEYwUQ==">Video3<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9QdFJVMkI2SW1sNA==">Video4<i class="fa fa-external-link-alt"></i></span></li>
<li>21일 3:00AM - 4:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9KZVRaYkN1eWVNOA==">Visual Odometry &amp; Computer Vision Applications Based on Location Clues<i class="fa fa-external-link-alt"></i></span><ul>
<li>21일 3:00AM - 7:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS85Y1ZWOW1fYjVZcw==">Image Matching Workshop: Local Features and Beyond<i class="fa fa-external-link-alt"></i></span><ul>
<li>25일 10:00AM - 1:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZXZlbnRzY3JpYmUubmV0LzIwMjEvaW5jbHVkZXMvdHJhY2tpbmcvY2xpY2tUcmFja2luZy5hc3A/bGZwPWQzTmpRaXR5UzNkWU1taHZha0Z4YldOSFkwdDJSVkZoYVdWU1ZFZ3laM2huUVRCVk9EUnhOQzlTWms1T2FuWm1jR2RYVXpFdmNrRkdSa05vVVhOWVIyeFRWbTlHVGxKVlIyOW1aVXRLYjBsRWRETnZVV3g0ZVZOVVZscHZVVzlvTkV0NUt6ZGFRbEZUTUdSclp6aFVaVTVoZFZkTFJEVnhSMUV6YlM5WFpYQktLemd5VmpGUU1WUnVOSFJvZDB4cVZYQnljREZvTTFRelIyWkNNa0pzU2twVldVRlZTRlJrWnpGQlBRPT0=">Learning to Generate 3D Shapes and Scenes<i class="fa fa-external-link-alt"></i></span><ul>
<li>25일 11:00PM - 3:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9tQWI3NXNLbFVpUQ==">Omnidirectional Computer Vision<i class="fa fa-external-link-alt"></i></span><ul>
<li>25일 10:00PM - 7:00AM</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9jak1LOWZPMWpBQQ==">Frontiers of monocular 3D perception<i class="fa fa-external-link-alt"></i></span><ul>
<li>26일 12:00AM - 4:00AM</li>
</ul>
</li>
</ul>
<p>##</p>
<ul>
<li>Swin Transformer and 5 reasons to use Transformer and attention in computer vision</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>CVPR</tag>
      </tags>
  </entry>
  <entry>
    <title>효율적인 이미지 버퍼 구현 (C++17 멀티 쓰레드)</title>
    <url>/20210625-multithread-image-buffer/</url>
    <content><![CDATA[<h2 id="Naive-구현-Sequential-method"><a href="#Naive-구현-Sequential-method" class="headerlink" title="Naive 구현 - Sequential method"></a>Naive 구현 - Sequential method</h2><p>OpenCV를 이용해서 실시간 이미지 프로세싱을 구현할 때, 우리는 보통 <code>cv::VideoCapture</code> 기능을 이용한다. 이 기능은 두가지 기능을 지원하는데, 첫째는 기존에 촬영해둔 이미지 데이터셋을 읽어들이는 것이고, 둘째는 PC에 연결된 카메라로부터 실시간 비디오 스트림을 받아오는 것이다.</p>
<p>종종 실시간 프레임~프레임 트랙킹 알고리즘을 구현할 때, 프레임간의 간격이 짧은 (i.e. FPS가 높은) 데이터가 요구된다.</p>
<p>데이터셋을 기반으로 개발을 하고 있다면 어렵지 않게 높은 FPS를 얻을 수 있다. 단순히 높은 FPS의 카메라로 촬영을 해두고, 개발은 ‘이미지 읽기 -&gt; 알고리즘 돌리기 -&gt; 다음 이미지 읽기 -&gt; 알고리즘 돌리기’ 를 하면 되기 때문이다. 알고리즘이 실시간에 돌아가던, 10초씩 돌아가던 우리의 이미지는 여전히 실시간의 데이터를 담고 있다.</p>
<p>카메라로부터 실시간 데이터를 가져오는 경우는 좀 더 복잡하다. 이미지를 읽고 -&gt; 알고리즘을 수행하고 -&gt; 이미지를 읽는… 이런 형태의 파이프라인을 가지고 있다면, 알고리즘을 수행하는데에 걸리는 시간이 길어질수록 그 다음 이미지를 취득하는데에 더 오래 걸릴 것이다. 이 경우, 개발 단계에서도 알고리즘이 실시간성을 유지해야하기 되는데, 대부분 개발 초기단계에서는 최적화가 되지 않은 코드가 많기 때문에 개발이 어렵게 된다.</p>
<p>실시간 웹캠을 이용하면 데이터셋 기반 방식보다 더 다양한 실험을 빠르게 할 수 있다는 장점이 있기 때문에 이 방법을 포기하기에는 너무 아깝다. 실시간으로 데이터를 받으면서, 알고리즘 실험은 그대로 할 수 있으면 어떨까?</p>
<h2 id="Thread-기반-구현"><a href="#Thread-기반-구현" class="headerlink" title="Thread 기반 구현"></a>Thread 기반 구현</h2><p>아래 방법은 C++ 멀티쓰레딩 방식을 이용해서 실시간 이미지를 받아서 메모리에 저장하는 코드 구현이다. 2개의 쓰레드가 동시에 돌아가는데, 첫번째는 <code>capture()</code>를 실행하는 t1 쓰레드, 두번째는 <code>process()</code>를 실행하는 t2 쓰레드이다.</p>
<p>t1은 독립된 쓰레드에서 <code>cv::VideoCapture</code> 기반으로 카메라에서 이미지를 읽는다. 읽은 이미지는 <code>buffer</code>라는 queue에 순차적으로 저장된다.</p>
<p>t2는 다른 쓰레드에서 수행되는 작업이다. 실시간 트랙킹 알고리즘 등이 이 쓰레드에 구현될 수 있다. 아래 코드에서는 간단하게 rgb2gray가 구현되었고, 시간 제약을 두기 위해 waitKey에 40ms를 넣었다. 이미지를 읽고나</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">queue</span>&lt;cv::Mat&gt; buffer;</span><br><span class="line"><span class="built_in">std</span>::mutex mutex;</span><br><span class="line"><span class="keyword">bool</span> isRunning = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rgb2gray</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cv::Mat frameGray;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer.empty())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!isRunning) <span class="comment">// Program termination</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        mutex.lock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> img = buffer.front();</span><br><span class="line">        buffer.pop();</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Pop:&quot;</span> &lt;&lt; buffer.size() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">        mutex.unlock();</span><br><span class="line"></span><br><span class="line">        cv::cvtColor(img, frameGray, cv::COLOR_RGB2GRAY);</span><br><span class="line">        cv::imshow(<span class="string">&quot;lol&quot;</span>, frameGray);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">27</span> == cv::waitKey(<span class="number">40</span>))</span><br><span class="line">            isRunning = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">capture</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">cv::VideoCapture <span class="title">cap</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line">    cap.open(<span class="number">0</span>);</span><br><span class="line">    cv::Mat frame;</span><br><span class="line">    cv::Mat frameClone;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (isRunning)</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// if (buffer.size() &gt; 100)</span></span><br><span class="line">        <span class="comment">//     buffer.pop();</span></span><br><span class="line"></span><br><span class="line">        cap.read(frame);</span><br><span class="line">        frameClone = frame.clone();</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="built_in">std</span>::scoped_lock <span class="title">lock</span><span class="params">(mutex)</span></span>;</span><br><span class="line">        buffer.push(frameClone);</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; buffer.size() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    isRunning = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">t1</span><span class="params">(&amp;capture)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">t2</span><span class="params">(&amp;process)</span></span>;</span><br><span class="line"></span><br><span class="line">    t1.join();</span><br><span class="line">    t2.join();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Thread</tag>
        <tag>Image processing</tag>
      </tags>
  </entry>
  <entry>
    <title>새 일렉기타 장만! PRS CE24</title>
    <url>/20210627-guitar/</url>
    <content><![CDATA[<h2 id="기타"><a href="#기타" class="headerlink" title="기타!!!!"></a>기타!!!!</h2><p>10년 넘게 꿈꾸던 드림 기타를 장만했다!!</p>
<p>PRS사의 CE24 세미할로우 모델이다!</p>
<p><strong>신난다!!!!!</strong> <span class="github-emoji" alias="smile_cat" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f638.png?v8">😸</span></p>
<img src="/20210627-guitar/guitar.png" class="" title="guitar">


<p> </p>
<hr>
<h2 id="이-기타를-고르게-된-이유"><a href="#이-기타를-고르게-된-이유" class="headerlink" title="이 기타를 고르게 된 이유"></a>이 기타를 고르게 된 이유</h2><p>이번에 기타를 고르면서 찾던 기능들은 다음과 같다.</p>
<ul>
<li>이쁜 룩!</li>
<li>24프렛 (특별한 경우 22프렛도 괜춘)</li>
<li>싱/험 전환</li>
<li>트레몰로</li>
<li>세미 할로우</li>
<li>편한 연주감 (높은 빌드 퀄리티)</li>
<li>좋은 클린 톤 + 범용성 좋은 드라이브 톤</li>
</ul>
<p>그렇게 후보로 뽑힌 모델은 다음과 같다 (아래 사진 참조).</p>
<ul>
<li>Strandberg의 Salen Deluxe</li>
<li>PRS US Core 라인</li>
<li>Dusenberg Starplayer TV</li>
</ul>
<img src="/20210627-guitar/guitar3.png" class="" title="guitar3">

<p> </p>
<p> </p>
<p> </p>
<h3 id="Strandberg를-안-고른-이유"><a href="#Strandberg를-안-고른-이유" class="headerlink" title="Strandberg를 안 고른 이유"></a>Strandberg를 안 고른 이유</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/WWpU9fcQG3c" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p><strong>솔직히 제일 끌렸던 모델</strong>이다. 험버커 소리보다는 텔레 소리를 좋아하는 편이기도 하고, 또 제일 특이하게 생겼기 때문이다 ㅋㅋ 하지만 안타깝게도 현재 Salen Deluxe 모델은 메이플 탑을 가공하는 기계에서 결함이 있어서, 작년 하반기부터 내년 상반기까지는 생산이 중단되어있다고 한다 ㅠㅠ</p>
<p>아쉬운대로 기타샵에 가서 동일한 쉐입의 다른 모델을 쳐봤다. </p>
<p>근데 왠걸, 내 손이랑 하나도 맞지 않는 기타였던 것이다 ㅋㅋ 가볍고 몸에 착 붙는 기타인건 확실했지만, <strong>넥이 너무 딱딱했다</strong>. Fanned fretboard 이기 때문에 각각의 음들에 대한 액션은 마음에 들었지만, 코드를 치기가 조금 어려웠다.</p>
<p>그런고로 바로 Strandberg는 아웃!</p>
<p> </p>
<h3 id="Dusenberg를-안-못-고른-이유"><a href="#Dusenberg를-안-못-고른-이유" class="headerlink" title="Dusenberg를 안 (못) 고른 이유"></a>Dusenberg를 안 (못) 고른 이유</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/UzL5fYyptdI" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p>소리가 좋고, 픽업도 다른 기타랑 다르게 생겼다. 무엇보다… 저 금장 하드웨어가 너무 멋지다 헉헉 <span class="github-emoji" alias="crying_cat_face" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63f.png?v8">😿</span></p>
<p>다만 레스폴 쉐입이라서 걱정이 되었다. 레스폴 쉐입은 하이프렛에서 넥 포켓이 두껍기 때문에 속주가 어렵다.</p>
<p>이런 경우에는 직접 쳐보고 나서 판단해야하는데… 국내 신품 매물도 없고 심지어 중고 매물도 없었다. 솔직히 손에 잘 맞는다고 해도, 금장 하드웨어에 생기는 녹을 관리할 자신이 없었다 ㅋㅋ 그래서 그냥 포기했다.</p>
<p>는… <span class="exturl" data-url="aHR0cDovL3d3dy5idXp6YmVlLmNvLmtyL3Nob3AvZ29vZHMvZ29vZHNfdmlldy5waHA/Z29vZHNubz0yMjc3OSZjYXRlZ29yeT0=">어…? 매물이 들어왔네…?<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<h3 id="PRS를-고른-이유"><a href="#PRS를-고른-이유" class="headerlink" title="PRS를 고른 이유"></a>PRS를 고른 이유</h3><div class="video-container"><iframe src="https://www.youtube.com/embed/50j4UCT3wHI" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p>PRS 제품 라인업에는 사실 위의 조건들을 충족하는 모델들이 많이 있다. 이번에 고르는 기타는 인생기타가 될 예정이라 US Core 라인 이상을 찾아보았다. </p>
<p>US Core 라인에서 유명한 Custom 24, Studio, McCarty 594, 509의 경우 세미할로우의 경우 최소 500만원 이상을 잡아야해서 사실 부담이 되었다. 그렇다고 가격을 낮추자니 퀄리티가 낮아져서 아쉬웠다.</p>
<p>그러다가 CE24 모델을 보게 되었다. CE24는 PRS의 US Core 라인 중 가장 하위모델인데, 세미할로우도 200만원 중반 ~ 300만원 초 부터 시작한다. US Core의 다른 모델들보다 퀄리티가 떨어진다는 말이 있었는데, 2020년부터 생산 공장이 통합되어서 사용되는 재료 외의 빌드 퀄리티는 차이가 나지 않는다고 한다. </p>
<p>다른 PRS US Core 라인 기타들은 넥쓰루 공법인 반면 CE24는 볼트온 방식을 사용해서 저렴하다고 한다. 이렇게 보면 볼트온 방식은 소리가 구려서 저렴한거라고 볼 수 있는데, 그렇게 따지면 1000만원대를 오가는 펜더 커스텀샵도 다 볼트온 방식이다 ㅋㅋ… 볼트온이라고 소리가 나쁠 이유는 없을 것 같다.</p>
<p>CE24를 고르게 된 이유는 사실 <strong>‘나무 구성이 Custom 24보다 좋아서’</strong> 이다. CE24는 Custom 24와 동일한 마호가니 바디 + 메이플 탑 재료 구성을 가지고 있다. Custom 24는 CE24보다 메이플 탑을 좀 더 두껍게 얹기 때문에 좀 더 강한 패턴이 나타난다고 하는데, 찾아보니 Custom 24와 CE24의 두께는 같았다 (!). 이 뜻은, Custom 24는 마호가니:메이플 비율에서 메이플을 더 많이 가져간다는 뜻이 되고, 역으로 CE24는 마호가니를 더 많이 가져간다는 뜻이 된다. 마호가니 목재는 서스테인이 길고 따뜻한 소리가 나기 때문에 부드러운 클린/드라이브 톤을 가져갈 수 있는데, 이는 세미할로우 형태의 기타와 궁합이 더 잘 맞게 된다. 즉, <strong>세미 할로우의 경우 Custom 24보다 CE24가 가격도 더 저렴한데 오히려 더 좋은 소리를 낼 가능성</strong>이 있다 (???)</p>
<p>가성비는 못참지! CE24로 결정했다.</p>
<p> </p>
<hr>
<h2 id="계획-어떤-음악을-할것인가"><a href="#계획-어떤-음악을-할것인가" class="headerlink" title="계획 - 어떤 음악을 할것인가?"></a>계획 - 어떤 음악을 할것인가?</h2><p>오랜만에 일렉을 치게되다보니, 특정 장르를 골라서 그것만 파기보다는 여러 장르를 접해보고 싶다.</p>
<p>많이 좋아했던 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8yeUxOdGVCazdIbw==">SRV<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9KWlhSQjlCdUxoaw==">헨드릭스<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9WQzAyd0dqNWdQdw==">제프벡<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9KX2kwLUtTUHdsZw==">에릭존슨<i class="fa fa-external-link-alt"></i></span>을 커버하는 것 부터 락, 메탈, EDM 커버까지 하고싶다.</p>
<p>최근에 관심을 가지게 된 분야는 <strong>앰비언트 음악</strong>이다. 전체적으로 차분한 느낌을 가지는 음악이며, 듣는이에게는 편안한 느낌을 준다 (마치 노동요?). </p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/gvBuTYCZ1IY" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<p>앰비언트 음악은 의외로 연주자에게는 곡 설계 능력, 소리에 대한 이해와 연주 능력을 요구한다. </p>
<p>느리지만 지루하지 않게 곡을 설계해야하고, 편안하지만 뻔하지 않게 톤을 만들어야한다. </p>
<p>이러한 톤은 리버브/딜레이와 같은 공간계 이펙터를 통해 만들어야하는데, 한번 삑사리가 나면 엄청 오랫동안 그 삑사리가 들리기 때문에 실수하면 안된다 (<span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9zT0VzMzhNY1hQUQ==">관련 드립<i class="fa fa-external-link-alt"></i></span>도 있다 ㅋㅋ).</p>
<p> </p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/ZWREt_fRcC0" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<p>그래서 요즘은 톤 공부를 하면서 시간을 보내고 있다. </p>
<p>다른 종류의 딜레이 스택킹, 리버브 모듈레이션, transparent 오버드라이브 스택킹 등등 공부하는데 꿀잼이다 ㅎㅎ </p>
<p>기회가 된다면 페달보드도 맞춰보고 싶은데, 최근 눈여겨보고 있는 브랜드는 Chase bliss audio와 Strymon이다. </p>
<p>우선은 아래와 같이 짰다 (지금 다시 보니까, 드라이브 계열은 아마 바뀌지 않을까 싶다!)</p>
<p> </p>
<img src="/20210627-guitar/effects.jpg" class="" title="effects">]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.4 Guitar</category>
      </categories>
      <tags>
        <tag>일상</tag>
        <tag>Guitar</tag>
        <tag>이펙터</tag>
      </tags>
  </entry>
  <entry>
    <title>MS - Beginner&#39;s Series to Rust</title>
    <url>/20210628-MS-rust/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/videoseries?list=PLlrxD0HtieHjbTjrchBwOVks_sr8EVW1x" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.4 Rust</category>
      </categories>
      <tags>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title>기타 이펙터 - 관심있는 오버드라이브</title>
    <url>/20210629-guitar-overdrive/</url>
    <content><![CDATA[<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><ul>
<li>듀얼 오버드라이브, 또는 2오버드라이브 스택킹을 할 페달을 찾고있다.</li>
</ul>
<h2 id="Wampler-Dual-Fusion"><a href="#Wampler-Dual-Fusion" class="headerlink" title="Wampler Dual Fusion"></a>Wampler Dual Fusion</h2><ul>
<li>지난 몇년 전 부터 많이 나타나기 시작한 듀얼 오버드라이브 페달 패턴<ul>
<li>이 페달의 v1은 거의 10년 전에 나타난듯</li>
</ul>
</li>
<li>Wampler의 Euphoria 드라이브 + Paisley 드라이브를 붙힘<ul>
<li>Euphoria는 transparent 오버드라이브</li>
<li>Paisley는 색깔이 있는 편</li>
</ul>
</li>
<li>Tom Quayle 시그니처 오버드라이브</li>
</ul>
<div class="video-container"><iframe src="https://www.youtube.com/embed/RRhhqKf-EZQ" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
<div class="video-container"><iframe src="https://www.youtube.com/embed/7yPBoKITPXY" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h2 id="Origian-Revival-Drive"><a href="#Origian-Revival-Drive" class="headerlink" title="Origian Revival Drive"></a>Origian Revival Drive</h2><ul>
<li>엄청 비싸다 ㄷㄷ,, 80만원…<ul>
<li>그만큼 이쁨</li>
</ul>
</li>
<li>듀얼 오버드라이브<ul>
<li>하나는 튜브, 하나는 실리콘 패턴이라고 한다.</li>
<li>실리콘 패턴이 아무래도 transparent 오버 계열인거 같다.</li>
</ul>
</li>
<li>드라이브가 굉장히 깨끗한듯!</li>
</ul>
<div class="video-container"><iframe src="https://www.youtube.com/embed/nSRvly1M2ww" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h2 id="JHS-double-barrel"><a href="#JHS-double-barrel" class="headerlink" title="JHS double barrel"></a>JHS double barrel</h2><ul>
<li>듀얼 오버드라이브</li>
</ul>
<div class="video-container"><iframe src="https://www.youtube.com/embed/7oRK1IxrGjs" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h2 id="Klon-KTR"><a href="#Klon-KTR" class="headerlink" title="Klon KTR"></a>Klon KTR</h2><ul>
<li>Klon Centaur의 카피.<ul>
<li>Centaur는 희소성 때문에 가격이 엄청 높다 ㄷㄷ,, 300만원?</li>
</ul>
</li>
<li>클론 계열의 부드러운 드라이브 느낌이 잘 살아있음.<ul>
<li>생톤을 잘 살리는 쪽이라면 가볍게 RC booster 같은거랑 엮어서 다이나믹한 드라이브를 만들 수 있을 듯.</li>
</ul>
</li>
</ul>
<h2 id="Macron-Audio-드라이브-계열들"><a href="#Macron-Audio-드라이브-계열들" class="headerlink" title="Macron Audio 드라이브 계열들"></a>Macron Audio 드라이브 계열들</h2><ul>
<li>국산 이펙터. 싱글페달임.</li>
<li>Astrum + Viento를 섞어서 적당한 리드톤 만들 수 있을듯?</li>
<li>쎈 드라이브는 Helios로… (메사부기 톤)</li>
<li>근데 다 해서 80만원 ㄷㄷ</li>
</ul>
<div class="video-container"><iframe src="https://www.youtube.com/embed/qiHppO46X3I" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.4 Guitar</category>
      </categories>
      <tags>
        <tag>일상</tag>
        <tag>Guitar</tag>
        <tag>이펙터</tag>
      </tags>
  </entry>
  <entry>
    <title>기타 이펙터 - 오버드라이브 계열과 기초 이론 공부</title>
    <url>/20210630-overdrive-study/</url>
    <content><![CDATA[<h2 id="드라이브가-걸리는-원리"><a href="#드라이브가-걸리는-원리" class="headerlink" title="드라이브가 걸리는 원리"></a>드라이브가 걸리는 원리</h2><p>원래 앰프의 본래 역할은 말 그대로 소리 신호를 증폭해주는 역할을 한다 (amplifier). 깔끔한 소리가 들어가면, 깔끔한 소리가 훨씬 크게 나야하는 것이 앰프의 역할이다.</p>
<p>하지만 어떠한 이유에서 소리 신호를 특정 임계치 이상 증폭시킬 경우 찢어지는 소리가 난다. 이러한 현상에는 여러가지 이유가 있는데 가장 많이 보이는 이유 2개를 설명한다.</p>
<ul>
<li>앰프 내부의 회로가 더 큰 소리를 감당하지 못해서 클리핑이 나는 경우</li>
<li>스피커가 가용할 수 있는 소리보다 더 큰 소리를 낼 수 없어서 불안정하게 진동을 하기 때문에 소리가 찢어지는 경우</li>
</ul>
<p>전자의 경우를 그림으로 설명하면 아래와 같다.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2011/05/Clipping_waveform.jpg?trim=1,1&bg-color=000&pad=1,1"></p>
<p>신호이론 상 가용치보다 더 큰 신호를 받으면, 해당 앰프가 낼 수 있는 최대 임계치 밑으로 신호가 모이면서 saturation 현상이 발생한다. </p>
<p>이 때 saturation이 되는 모양은 soft clipping / hard clipping의 두가지 다른 패턴으로 나눠지게 된다. Soft clipping은 파동의 형태가 원래의 모양과 비슷한 것을 볼 수 있고 (i.e. 파형 전체의 크기가 변하는 것), hard clipping은 peak/trough가 주로 깎이는 것을 볼 수 있다. 이렇게 급격하게 입력 소리 파형의 모양이 바뀌는 것일수록 스피커에서는 안정적으로 반응하기 힘들기 때문에 더 찢어지는 소리가 나기도 한다.</p>
<p>어찌되었건 이런 식으로 소리를 찌그러트리는 것을 ‘드라이브를 건다’ 라는 표현을 쓴다. </p>
<p>얼마나 드라이브가 걸리냐에 따라서 Overdrive / distortion 등으로 나뉘며, 드라이브 특유의 소리에 따라서 crunch, fuzz 등의 드라이브 타입이 나뉘기도 한다. </p>
<p>각 타입에서도 제조사의 톤 특색에 맞춰서 이름이 있다.</p>
<p>보통 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9UMzh2My1TU0djTQ==">블루스<i class="fa fa-external-link-alt"></i></span> 등에서 들리는 정도의 드라이브 소리를 overdrive, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS85ZDR1aTlxN2VETQ==">락/메탈<i class="fa fa-external-link-alt"></i></span>에서 들리는 정도의 드라이브 소리를 distortion이라고 한다.</p>
<p>기타 이펙터 중에는 overdrive / distortion 계열 이펙터들이 있는데, <strong>실제로 앰프의 소리를 엄청나게 키우지 않고도 소리의 파형을 변형시켜 이와 같이 찢어지는 소리를 내게 해주는 장치</strong>이다.</p>
<p> </p>
<h2 id="Bass-Mid-Treble의-영역"><a href="#Bass-Mid-Treble의-영역" class="headerlink" title="Bass / Mid / Treble의 영역"></a>Bass / Mid / Treble의 영역</h2><p>기타 이펙터나 앰프를 보면 Bass/Mid/Treble을 나눠서 소리를 조절할 수 있다. Bass는 낮은 소리 영역, Mid는 중간 소리 영역, Treble은 높은 소리 영역을 뜻하며, 각각 담당하는 스펙트럼 영역은 다음과 같다.</p>
<ul>
<li>Bass는 20~180 Hz</li>
<li>Mid는 180~1200 Hz</li>
<li>Treble은 1200~10,000 Hz</li>
</ul>
<p>(다른 자료에서는 5000Hz부터 Treble이라는 말도 있다. 그냥 대충 이 언저리 쯤에서 mid~treble의 영역을 가르는 것 같다.)</p>
<p><img src="https://i.pinimg.com/originals/a9/f5/75/a9f57539217d75396452b6e0f50c3c6e.png"></p>
<p>오버드라이브를 공부할 때 이러한 이유를 알아야하는 이유는 다음과 같다.</p>
<ul>
<li>오버드라이브 이펙터마다 강조하는 음역대가 다르다</li>
<li>다양한 악기들과 합주를 할 때, 내 음역대의 상태에 따라 내 소리가 묻히거나 앞서게 된다.<ul>
<li>내가 리드기타라면 당연히 앞서야한다.</li>
<li>내가 백킹기타라면 너무 앞서면 안된다.</li>
</ul>
</li>
</ul>
<p>내가 원하는 소리를 내기 위해서는 어떤 오버드라이브가 필요한지 알아야 할 필요가 있다. 기타샵에 가면 정말 많은 오버드라이브 이펙터들이 있는데, 어떤 이펙터가 어떤 종류의 소리를 내고 합주에는 어떤 영향을 끼치는지 알면 큰 도움이 된다.</p>
<p> </p>
<h2 id="TS-타입-Tube-Screamer"><a href="#TS-타입-Tube-Screamer" class="headerlink" title="TS 타입 (Tube Screamer)"></a>TS 타입 (Tube Screamer)</h2><img src="https://sc1.musik-produktiv.com/pic-003470039xl/ibanez-ts9-tube-screamer.jpg" width="500" height="500" alt="[alternative_text]" title="[이예에~~]">

<p>아마 가장 유명한 드라이브 계열은 TS 계열이지 않을까?</p>
<p>TS 계열 드라이브 이펙터들 중에서는 Ibanez에서 만든 TS9과 TS808 등이 가장 유명하고, 그 후의 제품들은 리이슈나 리메이크 또는 비슷한 소리를 가진 제품들이 많이 나왔다. TS계열 이펙터를 알아보는 하나의 방법은 ‘초록색 색깔’ 이라는 점인데, 많은 TS계열 제품들이 TS808/TS-9이 가진 초록색 색깔을 그대로 가져가기 때문이다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9rc2hyY3ROeEVRRT90PTE1MQ==">TS계열 이펙터들을 비교하는 영상<i class="fa fa-external-link-alt"></i></span>을 보면 초록색 이펙터가 많다 ㅋㅋ</p>
<p>오리지널 TS808 / TS-9의 드라이브 톤은 굉장히 빈티지한 드라이브 톤을 만든다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9BYktlX25hc2gtZw==">Stevie Ray Vaughan<i class="fa fa-external-link-alt"></i></span>가 TS 계열의 드라이브를 적극 활용해서 블루스에 맞는 오버드라이브 톤을 만들어낸다. 하지만 이런 톤은 모던 음악과는 잘 어울리지 않기 때문에 일반적으로는 다른 계열의 드라이브를 사용한다.</p>
<p>그러면 TS계열은 왜 유명한것일까?</p>
<p>TS계열의 <strong>진짜 면모는 ‘아주 약간의 드라이브’를 건 상태에서 부스팅을 할 때</strong> 나타난다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9oVDlOaGpYZDlMTT90PTk4">Eric Johson<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9uY2lqdjFXQ25Waz90PTU5">John Mayer<i class="fa fa-external-link-alt"></i></span>과 같이 유명한 톤은 다 TS계열 부스팅의 덕을 보았다. Drive 노브는 12시를 넘기지 않고 Level 노브를 높혀서 드라이브가 살짝 묻은 상태로 볼륨을 높히는 것인데, TS계열 특성상 아래 소리 스펙트럼 그래프에서 보이는 것 처럼 <strong>bass와 treble은 깎이면서 mid 볼륨이 엄청나게 커진다</strong>. 이때 이 커진 mid를 <strong>크런치나 퍼즈를 연결하면 후발 드라이브의 효과가 극대화</strong>된다. 또, <strong>mid가 커지면 합주를 할 때도 소리가 두각을 나타내기 때문에 리드기타 톤을 만들기 좋다</strong>.</p>
<img src="https://heartfx.net/ts9/frtonmax.png" width="500" height="500" alt="[alternative_text]" title="[이예에~~]">

<p>물론 mid가 커진다고 모든 기타에서 좋은 소리가 나는 것은 아니다. 이미 mid가 빵빵한 기타 소리에 mid를 더 밀어주면 굉장히 듣기 거북한 소리가 나게 된다. 그렇기 때문에 보통 <strong>스트랫이나 텔레처럼 mid가 많이 부족한 기타가 TS계열을 썼을 때 큰 효과를 받는 편</strong>이다. 아주 niche한 예시로, <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS8xZ21HQm9aV3h3ND90PTIzOQ==">메사부기 앰프<i class="fa fa-external-link-alt"></i></span>를 사용하면서 메탈 음악을 할 때 TS계열 드라이브를 이용해서 mid를 펌핑하고 게인을 미는 경우가 있다.</p>
<p>TS계열 드라이브는 자글자글한 소리를 내는데, 이는 굵기가 확실하게 들리는 드라이브를 좋아하는 사람들에게는 좋지 않은데, <strong>어떤 드라이브던지 자글자글한 드라이브 감을 추가</strong>해버리기 때문이다.</p>
<p> </p>
<h2 id="KLON-계열"><a href="#KLON-계열" class="headerlink" title="KLON 계열"></a>KLON 계열</h2><img src="https://online.berklee.edu/takenote/wp-content/uploads/2013/01/klon-centaur.jpg" width="500" height="500" alt="[alternative_text]" title="[이예에~~]">

<p>위 사진의 이펙터는 KLON Centaur인데, 가격은 4천 달러를 넘어간다. 물론 저 가격이 말도 안된다고 생각하는 사람들이 많아서 소리를 카피한 훨씬 저렴한 이펙터들도 많다 (KLON의 클론… 후후…). 오죽하면 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS84dGpRWFRNZnhZWQ==">블라인드 챌린지<i class="fa fa-external-link-alt"></i></span>도 있었다 ㅋㅋ</p>
<p>Hard clipping 드라이브라 컴프감이 들어가있는데, 이 <strong>컴프감이 굉장히 자연스럽다</strong>.</p>
<p>또, 드라이브에 적당히 자연스럽게 클린 시그널도 섞여들어가있는데, <strong>강하게 치면 드라이브가 좀 더 섞이고 약하게 치면 또 부드러운 클린 섞인 소리가 나온다</strong>.</p>
<p>TS계열이 미드부스팅 특화였다면, KLON은 <strong>특색이 없는 게인 부스터</strong>라고 볼 수 있다. Bass/Mid/Treble을 거의 인풋 그대로 가져가기 때문에 (i.e. 특색이 없기 때문에), <strong>아무 앰프게인/ 오버드라이브 이펙터와 섞어도 자연스럽게 드라이브를 얹어줄 수 있다</strong>. 이러한 점 때문에 인기가 많은 것 같다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9aMHBTdDVhR21fZw==">소리 샘플<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<h2 id="BB-계열-Blues-Breaker"><a href="#BB-계열-Blues-Breaker" class="headerlink" title="BB 계열 (Blues Breaker)"></a>BB 계열 (Blues Breaker)</h2><img src="https://images.reverb.com/image/upload/s--3f35PQ2W--/a_exif,c_limit,e_unsharp_mask:80,f_auto,fl_progressive,g_south,h_1600,q_80,w_1600/v1444947257/irgvvo2soqldx3spmhoa.jpg" width="500" height="500" alt="[alternative_text]" title="[이예에~~]">

<p>KLON처럼 <strong>특색이 없는 드라이브</strong>라고 한다. 대신 다른 점이라면 <strong>soft clipping 오버드라이브</strong>라는 점이다.</p>
<p>KLON보다 좀 더 두드러지는 드라이브를 가지고 있는데, 오리지널 Marshall Blues breaker 이펙터는 게인량이 그렇게 크지 않다고 한다. 소프트한 블루스 이상의 드라이브를 얻기 어렵다고 하기 때문에, <strong>최근 BB계열 이펙터들은 이러한 점을 개선해서 게인량을 늘렸다</strong>고 한다. <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS83WlM5U1FCRW90dz90PTY2NA==">소리 샘플<i class="fa fa-external-link-alt"></i></span></p>
<p>하지만 소프트한 블루스에는 이것만큼 좋은게 없다는 이야기도 있는데, 존메이어의 백킹톤이 왠만하면 BB에서 나온다는 이야기가 있다 (리드톤은 아마 TS계열 부스트가 붙지 않을까 싶다). <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9jWUhYTWF6eWhXSQ==">소리 샘플<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.4 Guitar</category>
      </categories>
      <tags>
        <tag>일상</tag>
        <tag>Guitar</tag>
        <tag>이펙터</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust estimation 시리즈 (0) - Outlier란?</title>
    <url>/20210716-outlier-0/</url>
    <content><![CDATA[<h2 id="Outlier란"><a href="#Outlier란" class="headerlink" title="Outlier란?"></a>Outlier란?</h2><p>Outlier는 통계 단어로써 ‘<strong>이상치</strong>‘, 즉 우리가 기대하지 않았떤 잘못된 데이터를 뜻한다. ‘이상치’는 ‘노이즈’와 다른 의미를 가진다. 우리는 보통 어떤 특정 분포의 데이터를 기대하고 알고리즘을 선택하는데, 노이즈는 우리가 기대한 분포 안에서 나타나는 데이터들간의 변화이고, <strong>이상치는 이 분포를 아득히 뛰어넘어 완전히 다른 분포를 가진다</strong>. 따라서 Outlier를 제거한다는 것은 우리가 기대했던 올바른 데이터들만으로 계산을 한다는 것이다.</p>
<p>반대로, 이 outlier가 아닌 올바른 데이터는 <strong>inlier</strong>라고 한다.</p>
<p>많은 컴퓨터 알고리즘에서 황금룰처럼 적용되는 Good data in → Good data out은 비전 알고리즘에도 똑같이 적용된다. 하지만 다른 분야에 비해 유독 컴퓨터 비전에서는 outlier 제거가 더 중요한 것 같다. 왜냐하면 우리가 자주 사용하는 몇개의 컴퓨터 비전 알고리즘들은 <strong>outlier가 없는 상황에서만 잘 작동하는 closed-solution</strong>인 경우가 많기 때문이다.</p>
<p>Fundamental matrix를 구할때나 homography matrix를 구할 때를 생각해보자. 이 매트릭스 계산을 하기 위해서는 몇개의 image feature pair가 필요하다. 이 계산을 하기 위해 우리가 쓸 수 있는 image feature pair가 몇백개씩 있어도, 우리는 이 중 아무 feature pair나 막 골라서 사용할 수 없다. 그 이유는 fundamental / homography matrix의 계산식에 혹여나 잘못 매칭된 feature pair가 사용될 경우 처참할 정도로 잘못된 결과값이 나오기 때문이다.</p>
<img src="/20210716-outlier-0/Untitled.png" class="" title="F/H Mat">

<p>그러면 이번에는 우리가 어떠한 방식으로 outlier를 전부 제거해서 좋은 데이터만 남았다고 해보자. Outlier가 없으니 이제 좋은 결과가 나올 것이다. 하지만 컴퓨터 비전 데이터 특성 상 모든 데이터에 noise가 조금씩 끼어있다. 어떤 데이터는 다른 데이터보다 noise가 더 끼어있을 수 있다. 이 때문에 좋은 데이터들 사이에서도 어떤 조합으로 계산하냐에 따라서 정확도가 달라지기도 하는데, 우리는 또 최적의 조합을 찾기 위해 이번에는 ‘가장 적은 노이즈를 가지는 데이터 조합’을 찾는 노력을 들인다.</p>
<p><strong>정확한 결과를 얻기 위해서는 outlier와 noise가 섞인 데이터에서 최적의 데이터를 솎아내야한다</strong>. Input 데이터의 양은 유한하기 때문에, 시간만 있다면 모든 경우의 수를 찾아보아 결국에는 최적의 값을 찾아낼 수 있다. 하지만 Real-time 컴퓨터 비전에서는 주어진 시간이 그렇게 많지 않다. 그러면 어떤 방법을 사용해야 빠르게 좋은 데이터만을 솎아낼 수 있을까?</p>
<p>수많은 연구를 통해 발견한 것은, outlier와 noise가 섞인 데이터로부터 곧바로 최적의 데이터를 솎아내는 것보다, outlier의 특성과 noise의 특성을 찾아내 부적절한 데이터를 우선 제거하고난 후 남아있는 데이터로부터 정확한 값을 추론하는 것이 훨씬 빠르다는 것이다. 그렇기 때문에 컴퓨터 비전에서는 수많은 outlier 제거 및 noise 제거 단계를 거친다. Gaussian filter를 사용해서 픽셀 노이즈를 줄여주는 것도, feature matching에 여러가지 threshold를 주는 것도, RANSAC을 통해 올바른 데이터를 골라내는 것도, weighted least squares optimization을 통해서 특정 데이터에 가중치를 주는 것도 모두 outlier 제거 및 noise 제거로 볼 수 있다. 이러한 계산들이 종종 Visual-SLAM에서 전체 계산량의 70~80%를 이루기도 하는데, 이들이 사용되는 이유는 오로지  <strong>‘빠르고 정확한 계산을 위한 outlier 및 noise 제거’</strong> 라고 볼 수 있다.</p>
<p>반대로 생각하면, 더 효율적인 방법으로 outlier와 noise를 제거할 수 있으면 더욱 visual-SLAM의 본질에 다가간다고 생각한다. 그렇기 때문에 이번 포스팅 시리즈에서는 여러가지 outlier 제거 방법에 대해 깊게 알아보기로 한다.</p>
<p> </p>
<hr>
<h2 id="컴퓨터-비전에서의-Outlier-정보-특성"><a href="#컴퓨터-비전에서의-Outlier-정보-특성" class="headerlink" title="컴퓨터 비전에서의 Outlier 정보 특성"></a>컴퓨터 비전에서의 Outlier 정보 특성</h2><blockquote>
<p>질문: Input data로 수많은 데이터가 들어왔을 때, 우리는 이 중 어떤 데이터가 outlier이고 inlier인지 알 수 있을까? </p>
</blockquote>
<p> </p>
<h3 id="Naive-답변-1-데이터-분포를-분석하기"><a href="#Naive-답변-1-데이터-분포를-분석하기" class="headerlink" title="Naive 답변 1 - 데이터 분포를 분석하기"></a>Naive 답변 1 - 데이터 분포를 분석하기</h3><p>가장 쉽게 생각할 수 있는 방법은 <strong>Raw 데이터의 분포</strong>를 보는 것일 것이다.</p>
<p>분포를 보았을 때, 하나의 큰 데이터 분포와 띄엄띄엄 떨어진 데이터들이 있다면, 보통 그 한곳에 모인 큰 데이터 분포를 inlier로 생각하고 띄엄띄엄 떨어진 데이터들이 outlier로 생각될 것이다. 왜냐하면, inlier 데이터는 closed-form에서 잘 작동한다는 전제를 가졌기 때문에 inlier 데이터들은 서로 분포에서 가깝게 나타난다는 특성이 있기 때문이다. 반대로, outlier 데이터는 일정한 분포를 가지지 못한 경우가 많은데, 이는 outlier가 우리가 예상하지 못한 현상에서 오는 경우가 많기 때문이다.</p>
<p>여기까지가 ‘보통의’ 데이터를 받았을 때 우리가 생각하는 방법이다. 통계적으로도 이렇게 생각하는 방식이 일반적이다. <strong>하지만 컴퓨터 비전에서 만큼은 이 방법이 잘 맞지 않는다</strong>. 위에서 설명한 방법은 아래와 같은 상황에서 성립하지 않게 되는데, 컴퓨터 비전에서는 이러한 상황이 종종 나타나곤 한다. 즉, <strong>컴퓨터 비전에서는 input 데이터의 분포만 가지고 어떤 데이터가 inlier이고 outlier인지 쉽게 구분할 수가 없다.</strong></p>
<ul>
<li>Outlier의 수가 inlier의 수 보다 많은 경우<ul>
<li>Image motion blur</li>
</ul>
</li>
<li>Outlier들끼리도 분포를 이루는 경우<ul>
<li>거울에서 반사된 이미지, specular reflection</li>
</ul>
</li>
<li>Inlier 데이터에 노이즈가 많이 껴있는 경우<ul>
<li>Object motion blur, pixel noise</li>
</ul>
</li>
<li>Inlier 데이터 자체가 없는 경우<ul>
<li>Occlusion</li>
</ul>
</li>
</ul>
<img src="/20210716-outlier-0/Untitled%201.png" class="" title="Outlier가 나타날 때">

<p> </p>
<h3 id="Naive-답변-2-Model-fitting-via-least-squares"><a href="#Naive-답변-2-Model-fitting-via-least-squares" class="headerlink" title="Naive 답변 2 - Model fitting via least squares"></a>Naive 답변 2 - Model fitting via least squares</h3><p>위의 질문에 대해 계속해서 답해보자. 우리는 컴퓨터 비전에서는 input 데이터의 분포만 가지고 inlier와 outlier를 구분할 수 없다는 것을 깨닳았다. 즉, Inlier와 outlier를 구분하기 위해서는 실제로 이 데이터들을 계산식에 넣어보는 방법 (trial-and-error) 밖에 없다는 것을 알았다. 이제 우리가 몇개의 데이터를 샘플링해서 계산식에 넣었다고 해보자. 이 데이터들이 좋은 데이터인지 아닌지 이제 어떻게 알 수 있을까?</p>
<p>가장 쉬운 방법은 (fundamental matrix나 homography matrix와 같은) <strong>모델을 계산해서 샘플링 되지 않은 모든 데이터들이 이 모델과 동의하는지를 보는 것</strong>이다. 각각의 데이터와 모델간의 차이 (error, 또는 residual)를 구하고, 이 에러들을 모두 더한다.</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex" xmlns="http://www.w3.org/2000/svg" width="24.415ex" height="6.354ex" role="img" focusable="false" viewBox="0 -1562.5 10791.3 2808.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(509.9, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="msubsup" transform="translate(1610.7, 0)"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(451, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(451, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2743, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(3798.8, 0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(509.9, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="msup" transform="translate(5409.4, 0)"><g data-mml-node="mrow"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(490, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1395.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2395.4, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mrow" transform="translate(2945.4, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1255, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mo" transform="translate(4589.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(4978.3, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></g></svg></mjx-container></p>
<img src="/20210716-outlier-0/Untitled%202.png" class="" title="Outlier가 나타날 때">

<p>두 데이터를 이으면 ‘선’이라는 모델이 나온다.<br>이 모델에 대해 모든 데이터의 residual 합이 최소가 되는 데이터의 짝을 구하면 된다.</p>
<p>에러의 총합이 내가 ‘에러가 이쯤 안나오면 inlier로 계산한 것이겠다~’ 하고 정한 threshold를 넘지 않으면, 그러면 그 데이터들은 inlier로 고려할 수 있다. 수식으로 표현하면, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.774ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 784 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>는 데이터 값, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.964ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2194 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(939, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1805, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>는 모델의 값이 되며, 그 차이를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> (residual) 이라고 부른다. 큰 에러에 대해 비중을 더 주기 위해서 제곱을 하기도 하고, 아니면 그냥 abs 값을 주기도 하는데, 전자를 <strong>Sum of squared errors (SSE)</strong> 후자를 Sum of absolute error (SAE) 라고 한다. 여기서 우리가 정한 threshold가 너무 크면 outlier가 inlier로 인식될 수도 있고, 너무 작으면 실제로 아무 모델도 구해지지 않을 수도 있다.</p>
<p>하지만 이 방법 역시 극단적인 outlier가 존재하거나, outlier 데이터 사이에도 패턴이 존재하거나, 또는 inlier 패턴이 존재하지 않을경우 작동하지 않는다. </p>
<img src="/20210716-outlier-0/Untitled%203.png" class="" title="asdf">

<p>극단적인 outlier가 존재할 경우의 예시.<br>대다수의 데이터가 M1 모델과 동의함에도 불구하고, outlier의 residual이 너무 크다.<br>M2 모델은 단 하나의 데이터만 동의하는데도, 전체 residual의 합이 M1 보다 작다.<br>그러므로 대다수의 데이터가 M1에 동의하는데도, M2가 선택되는 이상한 현상을 볼 수 있다.</p>
<img src="/20210716-outlier-0/Untitled%204.png" class="" title="asdjfklsadf">

<p>M1과 M2 둘 중 하나는 outlier 데이터만으로 만들어진 모델이다.<br>어떤 모델이 outlier 데이터를 가지고 있는지 모르는 경우, 우리는 어떻게 해야 올바른 모델을 고를 수 있을까?<br>더 많은 데이터를 포함하는 모델이 항상 inlier는 아니다. Outlier 데이터의 수가 inlier 데이터의 수 보다 많으면 오히려 반대로 고르게 되는 것이다.</p>
<img src="/20210716-outlier-0/Untitled%205.png" class="" title="sdjkfl">

<p>여기에 찍힌 데이터가 모두 inlier인 경우에, 우리는 어떤 모델을 골라야하는가?<br>빨간색과 노란색은 둘 다 데이터를 지나가지만, 전체 비율 50%의 데이터와밖에 동의하지 않는다.<br>초록색은 residual이 최소화되지만, 단 하나의 데이터도 직접적으로 모델과 동의하지 않는다.</p>
<p> </p>
<hr>
<h2 id="Robust-parameter-estimation"><a href="#Robust-parameter-estimation" class="headerlink" title="Robust parameter estimation"></a>Robust parameter estimation</h2><p>방금 전 예시에서 본 방법들은 모두 outlier의 영향으로 올바른 모델 선정에 실패했다. 사실 Prior data distribution 분석과 least square model fitting 방법 모두 outlier가 없을때는 잘 작동하는 방법이다. 이 두 방법이 실패한 이유는 순전히 outlier의 영향 때문이다.</p>
<p>그렇다면, outlier가 될만한 요소들을 제거하고 계산해보면 되지 않을까? 이렇게 <strong>outlier를 제거하고 모델을 추정하는 기법을 Robust parameter estimation</strong>이라고 한다. 여기서 <strong>robust</strong>-는 outlier가 끼어있어도 강인하게 추정하는- 이라는 뜻을 가지고, <strong>parameter</strong>는 모델을 구성하는 model parameter를 뜻한다.</p>
<p> </p>
<h3 id="방법-1-Consensus-search-within-a-threshold-similar-to-RANSAC-least-square-optimization"><a href="#방법-1-Consensus-search-within-a-threshold-similar-to-RANSAC-least-square-optimization" class="headerlink" title="방법 1 - Consensus search within a threshold (similar to RANSAC) + least square optimization"></a>방법 1 - Consensus search within a threshold (similar to RANSAC) + least square optimization</h3><p>이 방법은 RANSAC에서 사용하는 방식에서 조금 더 나아간 방식이 되겠다 (RANSAC을 이해하지 못해도 괜찮다. 이후 포스팅에서 RANSAC 내용을 커버할 것이다). 데이터 샘플링을 통해서 어떠한 모델을 추정해낸다. 그리고 추정된 모델에서 ±로 threshold를 만들어준다.</p>
<p>우리는 이제 다른 데이터는 보지도 않고, 이 threshold  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.054ex" height="1.072ex" role="img" focusable="false" viewBox="0 -452 466 474"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g></g></g></svg></mjx-container> 안에서 몇개의 데이터가 모델에 동의하는지 숫자를 셀 것이다. 아래 이미지에서는 두가지 조합을 비교했다. M1은 11개의 inlier, M2는 5개의 inlier를 가지기 때문에, M1이 더 선호된다. 이 경우는 아까 least square model fitting이 실패했던 ‘극단적인 outlier’ 예시의 문제점을 돌파한다. 물론 threshold를 잘 조절하는게 사용자의 몫이다.</p>
<p>이 inlier들 사이에서 좀 더 좋은 모델을 찾을 수 없을까? 라고 질문이 될 수 있다. 이 방법을 해결하기 위해서는 모든 inlier 데이터들에 대한 residual을 최소화하는 모델 파라미터를 찾으면 된다. 즉, least square optimization을 수행하면 되는 것이다. 이렇게까지하면 outlier 제거 후 noise 제거까지 한 것이 되겠다.</p>
<p>하지만 이 방법도 단점은 있다. Inlier의 데이터 노이즈가 심하다면, threshold  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.054ex" height="1.072ex" role="img" focusable="false" viewBox="0 -452 466 474"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g></g></g></svg></mjx-container> 안에 inlier 수가 적게 나타날 수가 있어 소수의 inlier로만 모델을 추정하여 높은 에러를 가지거나, 또는 outlier에서 나타난 패턴에서 모델을 찾을 수도 있다.</p>
<img src="/20210716-outlier-0/Untitled%206.png" class="" title="sdfdf">

<p>데이터로부터 모델을 추정하고, threshold  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.054ex" height="1.072ex" role="img" focusable="false" viewBox="0 -452 466 474"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D700" d="M190 -22Q124 -22 76 11T27 107Q27 174 97 232L107 239L99 248Q76 273 76 304Q76 364 144 408T290 452H302Q360 452 405 421Q428 405 428 392Q428 381 417 369T391 356Q382 356 371 365T338 383T283 392Q217 392 167 368T116 308Q116 289 133 272Q142 263 145 262T157 264Q188 278 238 278H243Q308 278 308 247Q308 206 223 206Q177 206 142 219L132 212Q68 169 68 112Q68 39 201 39Q253 39 286 49T328 72T345 94T362 105Q376 103 376 88Q376 79 365 62T334 26T275 -8T190 -22Z"></path></g></g></g></svg></mjx-container> 안에서 inlier의 수를 센다.<br>그 후 가장 높은 inlier의 수를 가진 모델의 위치에서부터 least squares optimization을 통해 정확한 모델을 추정한다.</p>
<p> </p>
<h3 id="방법-2-Consensus-search-with-weights"><a href="#방법-2-Consensus-search-with-weights" class="headerlink" title="방법 2 - Consensus search with weights"></a>방법 2 - Consensus search with weights</h3><p>또 다른 방법은 방법1과 비슷하지만, threshold를 거는 것 대신에 모델로부터의 residual에 대한 distribution을 그리면 된다. residual이 큰 데이터일 수록 작은 가중치를 가지며 무시되고, residual이 작은 데이터일수록 더 높은 가중치를 가지며 inlier일 확률을 높히는 것이다. 이 방법에서는 실제로 inlier의 노이즈와 outlier 데이터도 계산에 포함되기 때문에, 올바른 distribution만 사용하면 정확한 값을 찾을 수 있다는 점이 장점이다.</p>
<p>하지만 그 올바른 distribution을 어떻게 찾을 것인가? 그 부분이 어렵다.</p>
<p>또, outlier가 강한 패턴을 가지고 있으면 어떻게 할 것인가? 그 부분도 해결되지 않았다.</p>
<img src="/20210716-outlier-0/Untitled%207.png" class="" title="Outlier가 나타날 때">

<p>Residual에 대한 distribution을 그려서 각각의 데이터에 대한 가중치로 이용한다.<br>높은 residual의 값, 즉 모델이 생각하는 outlier에는 낮은 가중치가,<br>낮은 residual의 값, 즉 모델이 생각하는 inlier에는 높은 가중치가 부여된다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>Robust estimation 시리즈</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>RANSAC</tag>
        <tag>Outlier rejection</tag>
        <tag>Robust estimation</tag>
        <tag>M-Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title>Robust estimation 시리즈 (1) - RANSAC이란?</title>
    <url>/20210716-outlier-1/</url>
    <content><![CDATA[<h2 id="논문-소개"><a href="#논문-소개" class="headerlink" title="논문 소개"></a>논문 소개</h2><p>RANSAC은 <strong>1981년에 Fischler와 Bolles가 제안한 Model fitting 방법론</strong>이다. RANSAC의 이름은 RANdom SAmple Consensus를 줄여서 만들어졌다. 논문이 처음 제안했던 방법도 컴퓨터 비전 분야에서의 사용 방법을 제안하였지만, 크게 주목을 받지 못하다가 2004년 Nister가 제안하는 이미지 프레임간 relative motion을 robust하게 찾아내는 5-point algorithm + RANSAC 프레임워크를 통해 주목을 받게 되었다.</p>
<p>그 후, 많은 양의 데이터로부터 outlier를 거르고 model fitting을 하는 방식으로 RANSAC이 많이 사용되고 있다. <strong>Fundamental matrix, Essential matrix, Homography, PnP, Rigid point cloud estimation</strong> 등등에서 사용된다.</p>
<p><a href="./Fischler_Bolles_1981_-_Random_Sample_Consensus_-_A_Paradigm_for_Model_Fitting_with_Applications_to_Image_Analysis_and_Automated_Cartography.pdf">오리지널 논문 링크: Fischler 1981 - Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography</a></p>
<p> </p>
<hr>
<h2 id="RANSAC-알고리즘-이해하기"><a href="#RANSAC-알고리즘-이해하기" class="headerlink" title="RANSAC 알고리즘 이해하기"></a>RANSAC 알고리즘 이해하기</h2><blockquote>
<p>RANSAC의 알고리즘을 사실 굉장히 간단하다.</p>
</blockquote>
<p> </p>
<h3 id="간단한-Linear-regression-예시"><a href="#간단한-Linear-regression-예시" class="headerlink" title="간단한 Linear regression 예시"></a>간단한 Linear regression 예시</h3><p>우선 간단하게 Linear regression으로 예시를 들어보자. Linear regression이 무엇인지 잘 모르겠다면, 어떤 점들이 흩뿌려져 있고, 그 점들의 중앙을 가장 잘 통과하는 라인이 어떤 라인인지 찾는 것이라고 보면 된다.</p>
<p>RANSAC에서는 우선 제일 먼저 <strong>Minimal set 크기의 무작위 데이터 샘플링과 함께 시작</strong>한다. Minimal set 데이터, 우리가 만들고 싶어하는 모델을 만드는데 필요한 최소한의 데이터 갯수를 의미한다. Linear regression에서는 ‘선’ 이라는 모델을 만들려고 하니까, ‘점’ 두개가 필요하겠다. 이번 예시에서는 점 하나는 무조건 그래프의 (0,0)으로 고정시켜서 문제를 간단하게 만들기로 한다.</p>
<p>무작위로 데이터를 뽑은 결과, 오른쪽 아래 있는 데이터가 뽑혔다. 이 데이터와 (0,0)에 고정된 위치를 이어서 ‘선’을 그렸다. 이때 우리는 <strong>이 데이터를 inlier라고 가정</strong>한다. 기존의 RANSAC 방법을 그대로 따르면 이 선 위에 올라가는 다른 데이터의 수를 고르면 된다. 하지만 로보틱스 기술 특성상, 모든 센서에는 노이즈가 포함되기 때문에, 이 선 위에 곧바로 올라가는 데이터는 거의 없을 것이다. 그러므로 <strong>어느정도 데이터에 여지를 줘서, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.005ex" height="1.645ex" role="img" focusable="false" viewBox="0 -717 444 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g></g></g></svg></mjx-container> 만큼의 거리 안에 들어오는 데이터의 수를 센다.</strong> 3개의 데이터가 이 공간에서 발견되었다.</p>
<p>그러면 노트에다가 <strong>‘현재까지 가장 잘 나온 모델은…  1. 샘플링 된 데이터 (i.e. 모델 파라미터로 쓰인 데이터), 2. inlier의 수’를 기록</strong>한다. </p>
<img src="/20210716-outlier-1/Untitled.png" class="" title="asdf">
<blockquote>
<p>Attempt 1 - 3개의 inlier가 발견됨.</p>
</blockquote>
<p>이 후, <strong>다른 데이터를 다시 무작위 샘플링</strong>하고, 아까와 같이 모델 (i.e. ‘선’)을 그린 다음에, <strong><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.005ex" height="1.645ex" role="img" focusable="false" viewBox="0 -717 444 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g></g></g></svg></mjx-container> 만큼의 공간 안에서 데이터의 수를 센다</strong>. 이때, <strong>새로 센 inlier의 수가 이전에 찾은 inlier의 수 보다 많으면, ‘현재까지 제일 잘 나온 모델’의 정보에서 이전의 기록을 삭제하고 새 데이터로 덮어씌운다</strong>.</p>
<img src="/20210716-outlier-1/Untitled%201.png" class="" title="asdfasdf">
<blockquote>
<p>Attempt 2 - 6개의 inlier가 발견됨. Attempt 1의 3개 inlier보다 많으므로, Attempt 2가 ‘현재까지 제일 잘 나온 모델’이 된다.</p>
</blockquote>
<p>이 작업을 계속해서 반복하다보면, ‘<strong>가장 많은 inlier를 가진 모델</strong>‘을 계산할 수 있다.</p>
<p>논문에서도 보여주듯이, <strong>수학적으로도 몇번의 샘플을 돌려야 가장 좋은 모델을 찾을 수 있는지에 대한 수식</strong>이 있다.</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex" xmlns="http://www.w3.org/2000/svg" width="22.106ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 9770.7 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(981.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2037.6, 0)"><g data-mml-node="mrow" transform="translate(1725.9, 710)"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1667, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2389.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3389.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(3892.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -710)"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778, 0)"></path></g><g data-mml-node="mo" transform="translate(1278, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="mrow" transform="translate(1444.7, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(389, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1111.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mo" transform="translate(2111.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(2500.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3222.7, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4222.9, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msup" transform="translate(4688.9, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(389, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5459.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><rect width="7493.2" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container> 는 샘플링의 횟수</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></svg></mjx-container> 는 우리가 고른 데이터가 inlier일 확률</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.054ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 466 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g></g></g></svg></mjx-container> 는 전체 데이터의 inlier : outlier 비율</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container> 는 minimal set을 고르기 위한 데이터의 갯수</li>
</ul>
<p>이 식은 두가지 방법으로 사용할 수 있다.</p>
<ol>
<li>Inlier를 얻어내고 싶은 확률을 설정해서, 샘플링의 횟수를 알아내기</li>
<li>샘플링의 횟수를 설정해서, Inlier를 얻어낼 확률을 설정하기.</li>
</ol>
<p>정확도를 위해서는 전자를, Real-time 환경에서의 속도 안정성을 위해서는 후자를 선택하면 된다.</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.699ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 751 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g></g></svg></mjx-container>를 대략 <strong>95% 또는 98%가 나올 수 있게 샘플링의 수를 맞춰두면</strong>, 왠만하면 실패하지 않고 잘 추정해내는 것을 볼 수 있다.</p>
<p> </p>
<h3 id="컴퓨터-비전-예시"><a href="#컴퓨터-비전-예시" class="headerlink" title="컴퓨터 비전 예시"></a>컴퓨터 비전 예시</h3><p>이제 컴퓨터 비전에 RANSAC을 적용해본 예시를 들어본다.</p>
<p><strong>Homography matrix</strong>를 구하기 위해서는, 두개의 이미지에서 공유하는 4쌍의 feature pair가 필요하다. 이 때 각각의 feature pair는 삼각측량을 했을 때 3D point를 구상해야하므로 총 4개의 3D point가 생기는데, 이 4개의 3D point 들은 하나의 plane 위에 있어야한다. 여기서 <strong>Homography matrix를 사용하면 하나의 이미지 위 4개의 feature들의 위치를 다른 이미지 위 4개의 feature들의 위치를 알 수 있다</strong>. 또, 여기서는 사용하지 않지만 이 homography matrix를 SVD 등을 통해 decompose하면 Rotation가 Translation을 얻을 수 있다.</p>
<p>Homography matrix를 구하기 위해서는, 계산에 들어가는 feature pair에 outlier가 없다는 것이 전제이다.</p>
<img src="/20210716-outlier-1/Untitled%202.png" class="" title="asdfsdf">
<blockquote>
<p>Homography 의 원리. 4쌍의 correspondence가 만드는 plane 위의 4개의 3D point.</p>
</blockquote>
<p>두개의 이미지 사이의 homography를 구해보기로 하자. 두개의 이미지를 준비했고, 양쪽 이미지에서 feature detection 및 descriptor 추출을 했다. FLANN을 사용해서 descriptor matching을 했고, 총 100개의 feature matching이 만들어졌다. 하지만, 이들 중 분명 몇개는 잘못된 feature matching이 있을 것이며, 이런 feature pair가 homography 계산에 들어가면 완전히 잘못된 결과가 나올 것이다.</p>
<img src="/20210716-outlier-1/Untitled%203.png" class="" title="aaasdf">
<blockquote>
<p>5-point algorithm을 위해 두 이미지에서 feature를 미리 뽑아놓음</p>
</blockquote>
<p>사실 RANSAC을 쓰지 않고도 homography 계산을 하는 방법은 있다. 100개의 피쳐매칭 중 어떤 피쳐매칭을 써야할지 모르겠을 때는, 존재하는 모든 경우의 수를 전부 계산한 후 가장 에러가 적은 방식을 고르는 것도 한가지 방법이다. 하지만 이 방법은 너무 오래 걸리기 때문에 실시간 컴퓨터 비전에서 돌리기에는 적절한 방법이 아니다.</p>
<img src="/20210716-outlier-1/Untitled%204.png" class="" title="asdfasdf">
<blockquote>
<p>존재하는 모든 경우의 수를 전부 계산하는 것도 하나의 방법이지만 너무 오래걸린다.</p>
</blockquote>
<p>그러므로, 우리는 RANSAC을 사용해 볼 것이다. <strong>우선, 랜덤하게 minimal set 데이터를 (i.e. feature pair 4개를) 고른다.</strong> 그리고, <strong>이 데이터로 모델 추정을 한다 (i.e. Homography 계산을 한다)</strong>.</p>
<img src="/20210716-outlier-1/Untitled%206.png" class="" title="asdfasdf">
<blockquote>
<p>Random sampling</p>
</blockquote>
<p>Homography matrix가 나왔으니, Image 1에 있는 feature들의 위치에 해당 homography matrix를 곱해주면, image 2에서 해당 feature들이 어디에서 보일지 예측할 수 있다. Image 1에 보이는 100개의 feature들의 위치에 homography matrix를 전부 곱해줘서, image 2에서 어디에 나올지 예상 위치를 찾는다.</p>
<img src="/20210716-outlier-1/Untitled%207.png" class="" title="asdjkhfl">
<blockquote>
<p>Image 1의 feature 위치에 Homography를 곱하면 Image 2의 feature 위치를 알 수 있다.</p>
</blockquote>
<p>그리고 그 <strong>예상되는 위치와 실제 feature matching이 된 위치의 차이를 (i.e. residual) 구한다</strong>. 이 때, 각각의 feature 위치마다의 에러 값을 residual <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.685ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 745 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>로 고려하며, SSE를 이용할 경우 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.598ex" height="2.244ex" role="img" focusable="false" viewBox="0 -833.9 1148.5 991.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mn" transform="translate(745, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> 을 계산한다. </p>
<img src="/20210716-outlier-1/Untitled%208.png" class="" title="asjdklf">

<p>또, <strong>설계자의 취향에 따라 특정 threshold 미만의 residual 값은 무시하는 방법</strong>도 있다. 이는 센서 noise 값을 고려해서 noise 제거와 outlier 제거를 명확히 구분하는 방법 중 하나이다. 이 residual 값들을 전부 더해서 SSE 값을 구하고, **’현재까지 가장 좋은 모델 값’**으로 저장한다.</p>
<img src="/20210716-outlier-1/Untitled%205.png" class="" title="asjdklf">

<p>‘현재까지 가장 좋은 모델 값’에 대한 업데이트가 끝나면, <strong>다시 랜덤하게 또 샘플링을 한 후, 위의 과정을 반복한다.</strong></p>
<img src="/20210716-outlier-1/Untitled%209.png" class="" title="akjsdlfasdf">
<blockquote>
<p>Random sampling</p>
</blockquote>
<p>반복하는 과정에서 좋은 모델이 나오면 SSE의 값이 적을 것이다. <strong>현재의 모델이 이전의 모델보다 SSE가 작을 경우, 이전의 모델은 잊어버리고 현재의 모델로 ‘현재까지 가장 좋은 모델 값’을 덮어씌운다.</strong> </p>

<blockquote>
<p>좋은 모델이 추정된 경우 (i.e. outlier가 없이 추정된 경우), residual이 작게 나오고 SSE가 작게 나온다.</p>
</blockquote>
<p><strong>RANSAC이 끝나는 조건</strong>은 여러가지 방법이 있지만, 많이 쓰이는 방법은 아래와 같다.</p>
<ol>
<li>정해놓은 iteration 수가 전부 돌았을 때 (e.g. 100번의 iteration을 돌라고 설계했고, 100번을 다 돌았을 때)</li>
<li>정해놓은 residual threshold보다 더 낮은 에러가 나왔을 때 (e.g. pixel RMSE가 2.0 미만이면 iteration을 중단)</li>
</ol>
<p>하지만 1번과 2번 방법 둘 다 장단점이 있다.</p>
<p>1번의 경우, 운이 좋아서 RANSAC 초반에 좋은 모델을 찾게 되어도 무조건 정해놓은 iteration이 다 돌 때 까지 계산을 돌려야한다. 즉, early-termination이 불가능하고, 쓸데없는 계산을 하게 된다.</p>
<p>2번의 경우, 운이 좋아서 RANSAC 초반에 좋은 모델을 찾게 되면 RANSAC을 빨리 종료할 수 있다. 하지만 outlier들끼리의 패턴 (i.e. local minima)에 빠지게 되면 헤어나올 방법이 없다.</p>
<p>어쨌든 끝나면서</p>
<ul>
<li><strong>Model과 Model을 이루는 inlier 데이터 (i.e. Outlier가 제거된 모델 + 데이터)</strong></li>
<li><strong>Sum of Squared Error (SSE) 값</strong></li>
</ul>
<p>을 가지고 끝난다.</p>
<p> </p>
<hr>
<h2 id="RANSAC의-코드"><a href="#RANSAC의-코드" class="headerlink" title="RANSAC의 코드"></a>RANSAC의 코드</h2><p>RANSAC의 알고리즘은 사실 이렇게 간단하지만, 의외로 RANSAC을 직접 짜서 쓰시는 분들을 주변에서 보기 힘들다. 이는 사실 코드를 구현하시는 분들 중에서 스크래치로 시작하시는 분들보다, 이미 잘 만들어진 라이브러리에서 가져와서 변형하시는 분들이 많은데, <strong>유명한 OpenCV나 PCL 등에는 딱 RANSAC() 이라는 함수가 없다</strong>. OpenCV에서 구현된 방식은 Fundamental Matrix 를 구하는 함수 내부에 RANSAC이 들어가있거나, solvePnPRANSAC() 같이 특정 목적을 가진 함수 내부에 구현이 되어있다.</p>
<p>그도 그럴만한게 RANSAC의 경우, model fitting 프레임워크로 봐야지, 실질적인 내부 계산은 우리가 어떤 계산을 하고 싶냐에 따라 달라지기 때문에, template class로 놔두는게 맞다고 본다. 하지만 OpenCV와 같은 오픈소스 라이브러리에서 template class로 구현하면 C나 Python, Java, javascript 등 해당 라이브러리가 지원하는 다른 언어에서의 함수 이름의 통일성이라던지가 어려워진다. 그리고 무엇보다 접근성이 많이 떨어지기 때문에, 지금까지 RANSAC을 자주 사용하는 함수 내부에 심어놓은 것으로 보인다.</p>
<p>하지만 이번 GSoC 2020을 통해서 RANSAC이 따로 떨어져 나올 수 있는 가능성이 보였다.</p>
<p>아래는 내가 RANSAC template class를 구현한다면 어떻게 만들지 간단하게 만들어본 디자인이다.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// A rough design of RANSAC template class</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Precision, <span class="keyword">typename</span> MODEL&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RANSAC</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="comment">// constructor</span></span><br><span class="line">	RANSAC(MODEL&amp; model)</span><br><span class="line">	{</span><br><span class="line">		mMinimalSet = MODEL.minimalSet();</span><br><span class="line">		mMaxIteration = <span class="number">100</span>; <span class="comment">// user-defined</span></span><br><span class="line">		mModel = model;</span><br><span class="line">		mResidualThreshold = <span class="keyword">static_cast</span>&lt;Precision&gt;(<span class="number">3</span>);</span><br><span class="line">		mModelThreshold = <span class="keyword">static_cast</span>&lt;Precision&gt;(<span class="number">2</span>);</span><br><span class="line">	}</span><br><span class="line"></span><br><span class="line">	<span class="comment">// run</span></span><br><span class="line">	<span class="function">MODEL <span class="title">run</span><span class="params">()</span></span></span><br><span class="line"><span class="function">	</span>{</span><br><span class="line">		<span class="built_in">std</span>::random_device randDev;</span><br><span class="line">		<span class="function"><span class="built_in">std</span>::mt19937 <span class="title">mtGenerator</span><span class="params">(randDev())</span></span>;</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">int32_t</span> iterNum = <span class="number">0</span>;		</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line">		{</span><br><span class="line">			<span class="comment">// Terminate when max iteration is reached</span></span><br><span class="line">			<span class="keyword">if</span> (iterNum == mMaxIteration)</span><br><span class="line">				<span class="keyword">return</span> mModel;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (iterate(mtGenerator));</span><br><span class="line">			{</span><br><span class="line">				<span class="keyword">if</span> (mModel.residual &lt; mModelTreshold)</span><br><span class="line">					<span class="keyword">return</span> mModel;</span><br><span class="line">			}</span><br><span class="line"></span><br><span class="line">			iterNum++;</span><br><span class="line">		}</span><br><span class="line">	}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	<span class="function"><span class="keyword">bool</span> <span class="title">iterate</span><span class="params">(<span class="built_in">std</span>::mt19937 randNum)</span></span></span><br><span class="line"><span class="function">	</span>{</span><br><span class="line">		<span class="comment">// Calculate residual based on random selection of data points. </span></span><br><span class="line">    <span class="comment">// Also somehow saves model params in model.</span></span><br><span class="line">		<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Precision&gt; residuals =	mModel.run(randNum); 		</span><br><span class="line">		</span><br><span class="line">		Precision sumResidual = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Residual thresholding - ignoring low residuals</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; res : residuals )</span><br><span class="line">		{</span><br><span class="line">			res = res &lt; mDataResidualThreshold? <span class="number">0</span> : res;</span><br><span class="line">			sumResidual += res;</span><br><span class="line">		}</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Update best model if residual is low</span></span><br><span class="line">		<span class="keyword">if</span>(sumResidual &lt; mBestModelRes)</span><br><span class="line">		{</span><br><span class="line">			mModel.saveResidualSum(sumResidual);</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">		}</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	}</span><br><span class="line">	</span><br><span class="line">	MODEL mModel;</span><br><span class="line">	<span class="keyword">int32_t</span> mMinimalSet = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int32_t</span> mMaxIteration = <span class="number">0</span>;</span><br><span class="line">	Precision mBestModelRes = <span class="number">0</span>;</span><br><span class="line">	Precision mDataResidualThreshold = <span class="number">0</span>;</span><br><span class="line">	Precision mModelThreshold = <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>Robust estimation 시리즈</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>RANSAC</tag>
        <tag>Outlier rejection</tag>
        <tag>Robust estimation</tag>
        <tag>M-Estimator</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2021 - Visual-Inertial SLAM and Spatial AI for Mobile Robotics (Prof. Stefan Leutenegger)</title>
    <url>/20210719-icra-vins-stefan-leutenegger/</url>
    <content><![CDATA[<h2 id="영상"><a href="#영상" class="headerlink" title="영상"></a>영상</h2><ul>
<li>원본 영상 (<span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS94bERidXc2c2thZw==">링크<i class="fa fa-external-link-alt"></i></span>)<ul>
<li>@2:23:50</li>
</ul>
</li>
<li>SLAM KR 발표 영상 (<span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9iN0RHNE4yM284Zw==">링크<i class="fa fa-external-link-alt"></i></span>)<ul>
<li>@56:00</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Stefan-Leutenegger-교수님"><a href="#Stefan-Leutenegger-교수님" class="headerlink" title="Stefan Leutenegger 교수님"></a>Stefan Leutenegger 교수님</h2><img src="/20210719-icra-vins-stefan-leutenegger/stefan.png" class="" title="stefan">

<ul>
<li>이력<ul>
<li>취리히 공대 학석박, Roland Siegwart 교수님 랩실에서 박사학위</li>
<li>2014년부터 영국 ICL에서 Lecturer</li>
<li>Andrew Davison 교수님과 함께 SLAMCore의 Cofounder</li>
</ul>
</li>
<li>주요 논문:<ul>
<li>BRISK</li>
<li>OKVIS</li>
<li>ElasticFusion</li>
<li>SemanticFusion</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Levels-in-Spatial-AI"><a href="#Levels-in-Spatial-AI" class="headerlink" title="Levels in Spatial AI"></a>Levels in Spatial AI</h2><table>
<thead>
<tr>
<th align="center">Level</th>
<th align="center">Task</th>
<th align="center">Technology</th>
<th align="center">Papers</th>
<th align="center">Outcome</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Level 4</td>
<td align="center">주위 물체들과의 상호작용을 이해</td>
<td align="center">Object-level/dynamic mapping</td>
<td align="center"><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9ndHVyYm9ObDlnZw==">MID-Fusion<i class="fa fa-external-link-alt"></i></span></td>
<td align="center">Advanced interaction</td>
</tr>
<tr>
<td align="center">Level 3</td>
<td align="center">주변에 어떤 물체들이 있는지 이해</td>
<td align="center">Semantic understanding</td>
<td align="center"><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9fYXFQOXJ1bWtnUQ==">SemanticFusion<i class="fa fa-external-link-alt"></i></span></td>
<td align="center">Basic interaction</td>
</tr>
<tr>
<td align="center">Level 2</td>
<td align="center">주위 빈 공간과 벽을 인식</td>
<td align="center">Dense mapping</td>
<td align="center"><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9YeVNyaFpwT0RZcw==">ElasticFusion<i class="fa fa-external-link-alt"></i></span></td>
<td align="center">Collision avoidance / motion planning</td>
</tr>
<tr>
<td align="center">Level 1</td>
<td align="center">자기 자신의 3D 위치를 인식</td>
<td align="center">SLAM (Sparse)</td>
<td align="center"><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9UYktFUEEyXy1tNA==">OKVIS<i class="fa fa-external-link-alt"></i></span>,OVKIS2</td>
<td align="center">Position control</td>
</tr>
</tbody></table>
<p> </p>
<hr>
<h2 id="SLAMCore"><a href="#SLAMCore" class="headerlink" title="SLAMCore"></a>SLAMCore</h2><img src="/20210719-icra-vins-stefan-leutenegger/slamcore.png" class="" title="level of performance">

<ul>
<li>예에 회사홍보</li>
<li>SLAMCore는 영국에 있는 로보틱스 비전을 위한 소프트웨어 회사이고, Spatial AI를 구현하기 위해 힘쓰고 있습니다~~~</li>
</ul>
<p> </p>
<hr>
<h2 id="Sparse-VINS-problem"><a href="#Sparse-VINS-problem" class="headerlink" title="Sparse VINS problem"></a>Sparse VINS problem</h2><h3 id="Visual-SLAM의-기본"><a href="#Visual-SLAM의-기본" class="headerlink" title="Visual SLAM의 기본"></a>Visual SLAM의 기본</h3><img src="/20210719-icra-vins-stefan-leutenegger/sparse_vins.png" class="" title="level of performance">

<ul>
<li>Visual SLAM의 기본적인 원리는 <strong>Camera pose와 3D landmark의 위치를 최적화</strong> 하는 것이다.<ul>
<li>모든 3D landmark에 대해서 camera pose에 대해 2D 이미지로 투영하였을 때, 투영된 landmark의 2D 위치와 해당 landmark가 관측된 feature의 위치의 차이 <strong>(i.e. 재투영 오차, reprojection error)의 합을 최소화</strong>하는 camera pose와 3D landmark의 위치를 찾는 것이다.</li>
</ul>
</li>
<li>수식 설명<ul>
<li>기호 설명<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>는 센서 값 (i.e. 뽑힌 피쳐의 위치)</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g></svg></mjx-container>는 투영 수식 (<a href="https://changh95.github.io/20210222-image-projection/">참조 링크</a>)</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>는 카메라 포즈</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>은 랜드마크의 위치를 의미한다.</li>
</ul>
</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="4.404ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1946.4 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(926.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(1648.4, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>은 3D landmark의 위치를 카메라 프레임에서 표현한 것</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.453ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3294.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(570, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(1315.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(2037.4, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2335.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></svg></mjx-container>는 카메라 프레임에서 표현된 3D landmark를 2D 이미지로 투영했을 때의 픽셀 위치</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.513ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5088.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(794.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1794.4, 0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2364.4, 0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(1315.2, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(2037.4, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2335.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></svg></mjx-container>$는 실제 이미지에서 뽑힌 visual keypoint의 위치와 2D 이미지로 투영된 3D landmark의 픽셀 위치 오차를 의미한다.</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="Visual-SLAM을-factor-graph로-표현하면"><a href="#Visual-SLAM을-factor-graph로-표현하면" class="headerlink" title="Visual SLAM을 factor graph로 표현하면?"></a>Visual SLAM을 factor graph로 표현하면?</h3><img src="/20210719-icra-vins-stefan-leutenegger/visual.png" class="" title="visual-only">

<ul>
<li>Visual SLAM을 factor graph로 표현하면 다음과 같은 그래프를 가진다.</li>
<li>각각의 pose들은 따로 떨어져있다.<ul>
<li>즉, <strong>camera pose끼리는 직접적인 constraint가 전혀 없다</strong>는 것이다.<ul>
<li>실제로 Visual SLAM은 frontend에서는 feature tracking을 위해 연속적인 이미지 시퀀스가 필요하지만, graph optimisation을 할 때는 연속적인 정보가 아니여도 된다.</li>
<li>또, 이 덕분에 모든 프레임을 사용하지 않아도 되며, 대부분의 경우 키프레임만 사용한다.</li>
</ul>
</li>
<li>이는, <strong>Visual SLAM에서는 temporal 정보가 없다</strong>는 것이다.<ul>
<li>(temporal = 시간이 흐름에 따라 변화하는 정보)</li>
</ul>
</li>
</ul>
</li>
<li>위 수식에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.14ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 504 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g></g></g></svg></mjx-container>는 멀티카메라 셋팅일 경우 카메라의 수를 뜻하고, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container>는 포즈의 수, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.432ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 633 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g></g></g></svg></mjx-container>는 모든 projection의 수를 뜻한다.<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="5.719ex" height="1.954ex" role="img" focusable="false" viewBox="0 -841.7 2527.8 863.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g><g data-mml-node="mi" transform="translate(1013.8, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(2061.8, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g></g></g></svg></mjx-container>는 에러 값을 제곱하는 least squares optimisation을 표현한 것이다.</li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="VINS를-factor-graph로-표현하면"><a href="#VINS를-factor-graph로-표현하면" class="headerlink" title="VINS를 factor graph로 표현하면?"></a>VINS를 factor graph로 표현하면?</h3><img src="/20210719-icra-vins-stefan-leutenegger/vins.png" class="" title="vins">

<ul>
<li>IMU 센서의 속도 200/400/800Hz 정도이며, 30Hz의 카메라의 속도보다 훨씬 빠르다.<ul>
<li>즉, 카메라 프레임들 사이마다 IMU 값을 얻게 된다.</li>
<li>IMU는 speed와 bias 값들을 state로 가지고 있는데, 이 state들은 추가 연산을 통해서 pose 값으로 변환이 가능하다<ul>
<li>선형 가속도 값을 2번 적분해서 translation 값으로 바꾸고, 각속도 값을 1번 적분해서 rotation 값으로 바꿀 수 있다.</li>
<li>이는 <strong>IMU measurement를 통해 camera pose 값들을 연속적으로 / temporal하게 이을 수 있게 된다</strong>.</li>
</ul>
</li>
<li><strong>VINS가 잘되는 이유는 이러한 temporal 정보까지 함께 최적화하기 때문</strong>이라고도 볼 수 있다.</li>
<li>IMU 센서 값은 종종 preintegration을 거친 형태를 가진다.</li>
</ul>
</li>
<li>IMU 센서의 특성을 고려했을 때 단순히 speed만 구하는게 아닌 센서의 빠른 bias 변화를 예측할 수 있어야한다.<ul>
<li>즉, bias 값까지 최적화 계산으로 풀어줘야하며, 최적화 해야하는 term과 constraint가 많아진다.</li>
<li>이 때문에 VINS는 Visual SLAM보다 기본적으로 계산량이 많다는 것을 알 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="OKVIS"><a href="#OKVIS" class="headerlink" title="OKVIS"></a>OKVIS</h2><ul>
<li>시간이 지날수록 최적화 해야하는 term들은 많이 쌓인다. <ul>
<li>실시간성을 유지하기 위해 늘어나는 계산량을 어떻게 관리할 것인지가 중요해진다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>ICRA</tag>
        <tag>VIO</tag>
        <tag>Imperial College London</tag>
        <tag>SLAMCore</tag>
        <tag>TUM</tag>
        <tag>Stefan Leutenegger</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 7월 SLAM 뉴스</title>
    <url>/20210728-2021-july-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> CodeMapping: Real-Time Dense Mapping for Sparse SLAM using Compact Scene Representations </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0ODQ3ODU=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><div class="video-container"><iframe src="https://www.youtube.com/embed/vuxcE1uBts0" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div></li>
<li>Scene representation을 담고 있는 code (scene representation을 auto-encoder를 사용해서 압축한 정보. Optimisation을 통해 파라미터을 smooth하게 조정 가능)를 사용하여 sparse-&gt;dense reconstruction을 가능하게 함.</li>
</ul>
</details>

<details>
  <summary> Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTQzODYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><div class="video-container"><iframe src="https://www.youtube.com/embed/G8PktlQ82uw" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div></li>
<li>기존의 Kimera 시스템에 multi-robot (i.e. collaborative SLAM)을 적용함.<ul>
<li>주요 관찰점은 loop closure, map update 등이 될듯.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Bootstrap Your Own Correspondences </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMDA2NzcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>저스틴 존슨이요…?!</li>
</ul>
</details>

<details>
  <summary> DPLVO: Direct Point-Line Monocular Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0ODQ3OTI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="LiDAR-Odometry"><a href="#LiDAR-Odometry" class="headerlink" title="LiDAR Odometry"></a>LiDAR Odometry</h2><details>
  <summary> FAST-LIO2: Fast Direct LiDAR-inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDY4MjkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> F-LOAM : Fast LiDAR Odometry and Mapping </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDA4MjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTE1MTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> GLO-SLAM: a slam system optimally combining GPS and LiDAR odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZW1lcmFsZC5jb20vaW5zaWdodC9jb250ZW50L2RvaS8xMC4xMTA4L0lSLTEyLTIwMjAtMDI3Mi9mdWxsL2h0bWw=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="Visual-SLAM-VINS"><a href="#Visual-SLAM-VINS" class="headerlink" title="Visual SLAM / VINS"></a>Visual SLAM / VINS</h2><details>
  <summary> A Comparison of Modern General-Purpose Visual SLAM Approaches </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDc1ODkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> TransformerFusion: Monocular RGB Scene Reconstruction using Transformers </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDIxOTEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> PLF-VINS: Real-Time Monocular Visual-Inertial SLAM With Point-Line Fusion and Parallel-Line Fusion </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NzgxOTU=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Architectures for SLAM and Augmented Reality Computing </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcmFtYXZyLmNvbS9PUmFtYVZSUHVibGljYXRpb25zL0ZQTDIwMjFfdmlwR1BVLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> OdoViz: A 3D Odometry Visualization and Processing Tool </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDc1NTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Dense point cloud map construction based on stereo VINS for mobile vehicles </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjQyNzE2MjEwMDE2NzI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Semantic and edge-based visual odometry by joint minimizing semantic and edge distance error </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAyNjI4ODU2MjEwMDE0NTE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving in Structural Scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDI1NTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTE4NTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Tutorial: Mobile Robotics, SLAM, Bayesian Filter, Keyframe Bundle Adjustment and ROS Applications </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ib29rcy5nb29nbGUuY28ua3IvYm9va3M/aWQ9UVFFNUVBQUFRQkFKJnBnPVBBMjI3JmxwZz1QQTIyNyZkcT1BK1R1dG9yaWFsOitNb2JpbGUrUm9ib3RpY3MsK1NMQU0sK0JheWVzaWFuK0ZpbHRlciwrS2V5ZnJhbWUrQnVuZGxlK0FkanVzdG1lbnQrYW5kK1JPUytBcHBsaWNhdGlvbnMmc291cmNlPWJsJm90cz1sTktBc3NPVUM4JnNpZz1BQ2ZVM1UxbV8yQ1JFaTdhbUxiV2JHY2FWZlI2Y25hMmxRJmhsPWtvJnNhPVgmdmVkPTJhaFVLRXdpaDc1alFpWVh5QWhWSU1kNEtIY2E4Q2Q4UTZBRXdEWG9FQ0FJUUF3I3Y9b25lcGFnZSZxPUElMjBUdXRvcmlhbCUzQSUyME1vYmlsZSUyMFJvYm90aWNzJTJDJTIwU0xBTSUyQyUyMEJheWVzaWFuJTIwRmlsdGVyJTJDJTIwS2V5ZnJhbWUlMjBCdW5kbGUlMjBBZGp1c3RtZW50JTIwYW5kJTIwUk9TJTIwQXBwbGljYXRpb25zJmY9ZmFsc2U=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Challenges of the Application of Front-Wheel Odometry for Vehicle Localization
</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0ODAyMjg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Moving Object Tracking for SLAM-based Augmented Reality </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL1JvZHJpZ28tU2lsdmEtMjAvcHVibGljYXRpb24vMzUyNjY0ODIxX01vdmluZ19PYmplY3RfVHJhY2tpbmdfZm9yX1NMQU0tYmFzZWRfQXVnbWVudGVkX1JlYWxpdHkvbGlua3MvNjBkMjE4M2E0NTg1MTU2NmQ1ODM3ZGVmL01vdmluZy1PYmplY3QtVHJhY2tpbmctZm9yLVNMQU0tYmFzZWQtQXVnbWVudGVkLVJlYWxpdHkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Geometry-Constrained Scale Estimation for Monocular Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Nzk3NzQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> DT-SLAM: Dynamic Thresholding Based Corner Point Extraction in SLAM System </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05NDY0MzQ3">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>


<details>
  <summary> RAM-VO: Less is more in Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDI5NzQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Image Enhancement using GANs for Monocular Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Njg4MzE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Dense point cloud map construction based on stereo VINS for mobile vehicles </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjQyNzE2MjEwMDE2NzI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> deep learning for VO or VSLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmthdGFzdHJvcy5jb20vYT9JRD0wMDYwMC0zMzc3MjE3MC1hNWZmLTQwMzEtOWE1NC1lNWNjMDRlZjA2OWM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Feature-Based SLAM: Why Simultaneous Localisation and Mapping? </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxNy9wMDA5LnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Application of machine learning in SLAM algorithms </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGVncnV5dGVyLmNvbS9kb2N1bWVudC9kb2kvMTAuMTUxNS85NzgzMTEwNzAyNTE0LTAwOS9odG1s">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>



<details>
  <summary> Dynamic RGB-D SLAM Based on Static Probability and Observation Number </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NTk2MDQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> DVT-SLAM: Deep-Learning Based Visible and Thermal Fusion SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDcvOTc4LTk4MS0xNi0zMTQyLTlfMzc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Global-Map-Registered Local Visual Odometry Using On-the-Fly Pose Graph Updates</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL01hc2FoaXJvLVlhbWFndWNoaS03L3B1YmxpY2F0aW9uLzM0Mzk4NDcwNl9HbG9iYWwtTWFwLVJlZ2lzdGVyZWRfTG9jYWxfVmlzdWFsX09kb21ldHJ5X1VzaW5nX09uLXRoZS1GbHlfUG9zZV9HcmFwaF9VcGRhdGVzL2xpbmtzLzYwNzU0MDI2OTI4NTFjYjRhOWQ4MmQ4YS9HbG9iYWwtTWFwLVJlZ2lzdGVyZWQtTG9jYWwtVmlzdWFsLU9kb21ldHJ5LVVzaW5nLU9uLXRoZS1GbHktUG9zZS1HcmFwaC1VcGRhdGVzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A survey on indoor 3D modeling and applications via RGB-D devices </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjE2MzEvRklURUUuMjAwMDA5Nw==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Robust and Accurate RGB-D Reconstruction With Line Feature Constraints </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Njg3MDY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Collaborative Visual Inertial SLAM for Multiple Smart Phones </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTIxODYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>


<p> </p>
<hr>
<h2 id="Implicit-representations"><a href="#Implicit-representations" class="headerlink" title="Implicit representations"></a>Implicit representations</h2><details>
  <summary> Correspondence-Free Point Cloud Registration with SO (3)-Equivariant Implicit Shape Representations </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMTAyOTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Advances in neural rendering </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9hYnMvMTAuMTE0NS8zNDUwNTA4LjM0NjQ1NzM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Animatable Neural Radiance Fields from Monocular RGB Video </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTM2MjkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="Backend-optimisation"><a href="#Backend-optimisation" class="headerlink" title="Backend (optimisation)"></a>Backend (optimisation)</h2><details>
  <summary> Consensus-Informed Optimization Over Mixtures for
Ambiguity-Aware Object SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDkyNjUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Continuous Integration over SO(3) for IMU Preintegration </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxNy9wMDc4LnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> ROBUST ESTIMATION IN ROBOT VISION AND PHOTOGRAMMETRY: A NEW
MODEL AND ITS APPLICATIONS </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93ZWIuYXJjaGl2ZS5vcmcvd2ViLzIwMjEwNjE4MDkyMjA2aWRfL2h0dHBzOi8vd3d3LmlzcHJzLWFubi1waG90b2dyYW1tLXJlbW90ZS1zZW5zLXNwYXRpYWwtaW5mLXNjaS5uZXQvVi0xLTIwMjEvMTM3LzIwMjEvaXNwcnMtYW5uYWxzLVYtMS0yMDIxLTEzNy0yMDIxLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Modular Multi-Sensor Fusion: A Collaborative State Estimation Perspective </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Nzk3NTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Fast and Memory Efficient Graph Optimization via ICM for Visual Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxNy9wMDkxLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Globally Optimal Consensus Maximization for Relative Pose Estimation With Known Gravity Direction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NDc5ODQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="Front-end-Local-global-features-closed-form-solutions"><a href="#Front-end-Local-global-features-closed-form-solutions" class="headerlink" title="Front-end (Local/global features, closed-form solutions)"></a>Front-end (Local/global features, closed-form solutions)</h2><details>
  <summary> HDPL: a hybrid descriptor for points and lines based on graph neural networks </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZW1lcmFsZC5jb20vaW5zaWdodC9jb250ZW50L2RvaS8xMC4xMTA4L0lSLTAyLTIwMjEtMDA0Mi9mdWxsL2h0bWw=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SSC: Semantic Scan Context for Large-Scale Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDAzODIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> ESA-VLAD: A Lightweight Network Based on Second-Order Attention and NetVLAD for Loop Closure Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NzI5NjY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A survey: which features are required for dynamic visual simultaneous localization and mapping? </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly92Y2liYS5zcHJpbmdlcm9wZW4uY29tL2FydGljbGVzLzEwLjExODYvczQyNDkyLTAyMS0wMDA4Ni13">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> What makes visual place recognition easy or hard? </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTI2NzEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL0Jpbmd4aV9MaXUyL3B1YmxpY2F0aW9uLzM1MzA3MTM3NV9BX0ZsZXhpYmxlX2FuZF9FZmZpY2llbnRfTG9vcF9DbG9zdXJlX0RldGVjdGlvbl9CYXNlZF9vbl9Nb3Rpb25fS25vd2xlZGdlL2xpbmtzLzYwZTY5ZWVlMGZiZjQ2MGRiOGVkZTY0ZS9BLUZsZXhpYmxlLWFuZC1FZmZpY2llbnQtTG9vcC1DbG9zdXJlLURldGVjdGlvbi1CYXNlZC1vbi1Nb3Rpb24tS25vd2xlZGdlLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Vector Semantic Representations as Descriptors for Visual Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxNy9wMDgzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Feature-Level Collaboration: Joint Unsupervised Learning of Optical Flow,
Stereo Depth and Camera Motion </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyMS9wYXBlcnMvQ2hpX0ZlYXR1cmUtTGV2ZWxfQ29sbGFib3JhdGlvbl9Kb2ludF9VbnN1cGVydmlzZWRfTGVhcm5pbmdfb2ZfT3B0aWNhbF9GbG93X1N0ZXJlb19EZXB0aF9DVlBSXzIwMjFfcGFwZXIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Relative scale estimation approach for monocular visual odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9yZXNlYXJjaC5sYXRpbnhpbmFpLm9yZy9wYXBlcnMvY3Zwci8yMDIxL3BkZi81OV9DYW1lcmFSZWFkeV81OS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Detection of loop closure in visual SLAM: a stacked assorted auto-encoder based approach </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExODAxLTAyMS0wMTU2LTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Towards real-time monocular depth estimation for mobile systems </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc3BpZWRpZ2l0YWxsaWJyYXJ5Lm9yZy9jb25mZXJlbmNlLXByb2NlZWRpbmdzLW9mLXNwaWUvMTE3ODUvMTE3ODUwSi9Ub3dhcmRzLXJlYWwtdGltZS1tb25vY3VsYXItZGVwdGgtZXN0aW1hdGlvbi1mb3ItbW9iaWxlLXN5c3RlbXMvMTAuMTExNy8xMi4yNTk2MDMxLnNob3J0P1NTTz0x">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTE0ODEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> InFlow: Robust outlier detection utilizing Normalizing Flows </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMTI4OTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Utilization of Semantic Planes: Improved Localization and Dense Semantic Map for Monocular SLAM in Urban Environment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NjI0Mzc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="3D-detection-6D-Pose-estimation"><a href="#3D-detection-6D-Pose-estimation" class="headerlink" title="3D detection / 6D Pose estimation"></a>3D detection / 6D Pose estimation</h2><details>
  <summary> Visual-Inertial-Semantic Scene Representation for 3D Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9jdnByXzIwMTcvcGFwZXJzL0RvbmdfVmlzdWFsLUluZXJ0aWFsLVNlbWFudGljX1NjZW5lX1JlcHJlc2VudGF0aW9uX0NWUFJfMjAxN19wYXBlci5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Monocular 3D Object Detection: An Extrinsic Parameter Free Approach </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudC9DVlBSMjAyMS9wYXBlcnMvWmhvdV9Nb25vY3VsYXJfM0RfT2JqZWN0X0RldGVjdGlvbl9Bbl9FeHRyaW5zaWNfUGFyYW1ldGVyX0ZyZWVfQXBwcm9hY2hfQ1ZQUl8yMDIxX3BhcGVyLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> 6D Object Pose Estimation using Keypoints and Part Affinity Fields </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDcuMDIwNTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="Hardware-acceleration"><a href="#Hardware-acceleration" class="headerlink" title="Hardware / acceleration"></a>Hardware / acceleration</h2><details>
  <summary> Multiple Master-Slave FPGA Architecture of a Stereo Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0OTIwOTU=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 6월 SLAM 뉴스</title>
    <url>/20210802-2021-june-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> Superquadric Object Representation for Optimization-based Semantic SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2gtY29sbGVjdGlvbi5ldGh6LmNoL2JpdHN0cmVhbS9oYW5kbGUvMjAuNTAwLjExODUwLzQ4NzUyNy8xL2FzbGRvY18yMDIxX2Z0c2Nob3BwX3N1cGVyX3F1YWRyaWNzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> RoadMap: A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMDI1MjcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> The 2021 Image Similarity Dataset and Challenge </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMDk2NzIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> TAG-Reg: Iterative Accurate Global Registration Algorithm  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MjgzMDM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> RISE: Real-Time Iteration Scheme for Estimation applied to Visual-Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL2NvcGxhbmQudWRlbC5lZHUvfmdodWFuZy9pY3JhMjEtdmlucy13b3Jrc2hvcC9wYXBlcnMvMDItRm9laG5fUklTRS1JQ1JBV1MtVklOUy5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Crowd-SLAM: Visual SLAM Towards Crowded Environments using Object Detection
 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwODQ2LTAyMS0wMTQxNC0x">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> DETReg: Unsupervised Pretraining with Region Priors for Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMDQ1NTAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> ORB-SLAM2S: A Fast ORB-SLAM2 System with Sparse Optical Flow Tracking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MzU5MTU=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<h2 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h2><details>
  <summary> A Comparison of Graph Optimization Approaches for Pose Estimation in SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9sYW1vci5mZXIuaHIvaW1hZ2VzLzUwMDM2NjA3LzIwMjEtYWp1cmljLWNvbXBhcmlzb24tbWlwcm8ucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><a href="https://changh95.github.io/20210607-solver-comp/?highlight=juric">리뷰 글</a><ul>
<li>SE-Sync 최고! GTSAM~Ceres 성능 또이또이! g2o는 쓰기 쉬움!</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Absolute Scale Estimation Approach for Monocular Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9yZXNlYXJjaC5sYXRpbnhpbmFpLm9yZy9wYXBlcnMvY3Zwci8yMDIxL3BkZi81OF9DYW1lcmFSZWFkeV81OC5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Relative scale estimation approach for monocular visual odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9yZXNlYXJjaC5sYXRpbnhpbmFpLm9yZy9wYXBlcnMvY3Zwci8yMDIxL3BkZi81OV9DYW1lcmFSZWFkeV81OS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Improving Inertial Navigation Accuracy with Bias Modeling </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDcvOTc4LTMtMDMwLTY0MTUxLTFfMTM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9sYW1vci5mZXIuaHIvaW1hZ2VzLzUwMDM2NjA3LzIwMjEtY3Zpc2ljLWtpdHRpY2FsLWVjbXIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> luvHarris: A Practical Corner Detector for Event-cameras </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMTE0NDMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A fast instance segmentation with one-stage multi-task deep neural network for autonomous driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwNDU3OTA2MjEwMDE5MjA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Dynamic RGB-D SLAM Based on Static Probability and Observation Number
 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NTk2MDQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDc1OTMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Pyramid Feature Attention Network for Monocular Depth Prediction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mjg0NDY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Scene Coordinate Regression Network With Global Context-Guided Spatial Feature Transformation for Visual Relocalization  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mzc2OTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> ATVIO: Attention Guided Visual-Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MTM5MTI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Globally Optimal Consensus Maximization for Relative Pose Estimation With Known Gravity Direction  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NDc5ODQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A nonlinear optimization-based monocular dense mapping system of visual-inertial odometry  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAyNjMyMjQxMjEwMDUxMjE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Robust Optical Flow Tracking Method Based On Prediction Model for Visual-Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mjk5Mjc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Assessment of Map Construction in vSLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mzg4ODQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Spatial semantic graph enhanced monocular SLAM System </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vbmxpbmVsaWJyYXJ5LndpbGV5LmNvbS9kb2kvYWJzLzEwLjEwMDIvY2F2LjIwMjU=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Scale-Adaptive ICP </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzE1MjQwNzAzMjEwMDAxODc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Robust registration algorithm based on rational quadratic kernel for point sets with outliers and noise </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMDQyLTAyMS0xMDg1MS14">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Robust Optical Flow Tracking Method Based On Prediction Model for Visual-Inertial Odometry  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mjk5Mjc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Fast Vision-inertial Odometer Based on Line Midpoint Descriptor  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5pamFjLm5ldC9jbi9hcnRpY2xlL2RvaS8xMC4xMDA3L3MxMTYzMy0wMjEtMTMwMy0y">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A Visual SLAM Image Mismatching Filter Algorithm Based on Progressive Aample Consensus
 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NDA1NjI=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Robust Sensor Fusion with Pairwise Dynamic Covariance Scaling
for Localization in Urban Areas </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pcmFwLmthaXN0LmFjLmtyL3B1YmxpY2F0aW9ucy95amtpbS0yMDIxLXVyLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>김아영 교수님!</li>
</ul>
</details>

<details>
  <summary> Single Image Depth Prediction with Wavelet Decomposition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDYuMDIwMjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> State of the Art in Vision-Based Localization Techniques for Autonomous Navigation Systems </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05NDM4NzA4">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Automatic Hyper-Parameter Tuning for Black-box LiDAR Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9zdGFmZi5haXN0LmdvLmpwL2sua29pZGUvYXNzZXRzL3BkZi9pY3JhMjAyMS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Radar Odometry Combining Probabilistic Estimation and Unsupervised Feature Learning </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMTQxNTIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> MAOMaps: A Photo-Realistic Benchmark For vSLAM and Map Merging Quality Assessment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMTQ5OTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Quantized Self-supervised Local Feature for Real-time Robot Indirect VSLAM  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0NDQ3Nzc=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDIxOTUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 5월 SLAM 뉴스</title>
    <url>/20210804-2021-may-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="학회"><a href="#학회" class="headerlink" title="학회"></a>학회</h2><ul>
<li>6월 초에 진행된 <a href="https://changh95.github.io/20210530-icra-2021/">ICRA 2021 튜토리얼 / 워크샵 / 논문 리스트</a></li>
<li>6월 중순에 진행된 <a href="https://changh95.github.io/20210619-cvpr-2021/">CVPR 2021 튜토리얼 / 워크샵 리스트</a></li>
</ul>
<p> </p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> Monocular Visual Odometry Using Template Matching and IMU </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MjcxOTA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Sebastian Scherer 교수님 랩실에서 나온 논문</li>
<li>지상을 바라보는 카메라로 VO를 할 때 template matching을 통해 Ackerman steering model에 대한 pose 변환을 잴 수 있으나, 강건하지 못하다.<ul>
<li>IMU를 사용해서 yaw 값을 치환해서 성능을 높인다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Super Odometry: IMU-centric LiDAR-Visual-Inertial Estimator for Challenging Environments</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMTQ5MzgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Sebastian Scherer 교수님 랩실에서 나온 논문</li>
<li>IMU를 중심으로 Camera + LiDAR를 퓨전해서 퓨어 비전 / 퓨어 라이다로는 어려운 공간에서 안정적으로 맵핑할 수 있는 기술</li>
</ul>
</details>

<details>
  <summary> Simultaneous Localization and Mapping: A Rapprochement of Filtering and Optimization-Based Approaches</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cyLmVlY3MuYmVya2VsZXkuZWR1L1B1YnMvVGVjaFJwdHMvMjAyMS9FRUNTLTIwMjEtNzYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>An invitiation to 3D vision의 저자인 Yi Ma와 Sastry가 마킹한 버클리 졸업논문.</li>
<li>Optimisation 방식과 EKF 방식을 비교 및 짬뽕</li>
</ul>
</details>

<details>
  <summary> Learning to Predict Repeatability of Interest Points</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDM1NzgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Niantic 팀과 Tat-jun Chin이 참여</li>
<li>시간에 따른 interest point의 repeatability를 평가하는 시스템을 만듬.<ul>
<li>하루의 시간동안 바뀌는 조명에서…</li>
<li>계절이 바뀌면서…</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and Reconstruction</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDc0NjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Roland Siegwart, Juan Nieto 교수님 참여</li>
<li>하나의 reconstruction volume으로 scene 전체와 모든 object를 표현하는 multi-object TSDF 기법을 소개함.<ul>
<li>기존의 TSDF 방식으로 다수의 object를 표현하기 위해서는 object의 수 만큼 reconstruction volume이 필요했어서 scalable하지 않았음.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Enhancing Photorealism Enhancement </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMDQ2MTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Vladlen Koltun + 인텔 자율주행 랩</li>
<li>가상 이미지를 실제처럼 보이게 바꿔주는 네트워크를 만듬.</li>
</ul>
</details>

<details>
  <summary> USACv20: robust essential, fundamental and homography matrix estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDUwNDQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>SAC계열의 고수들: Daniel Barath, Jiri Matas</li>
<li>RANSAC 계열 기술들을 리뷰하고, 가장 좋은것들만 골라서 USAC을 만듬.<ul>
<li>(근데 USAC 논문은 바뀐건 크게 없는거 같은데 왜 매번 나오는거지…?)</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Front-End for Dense Monocular SLAM using a Learned Outlier Mask Prior </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDA1NjIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>John Leonard 교수님 랩실</li>
<li>Depth from video를 수행하면서 부산물로 생기는 outlier mask까지 SLAM에 사용해서 depth 정보를 더 좋게 만들고, CNN-SLAM 파이프라인에 넣어서 Dense monocular SLAM의 frontend를 구현</li>
</ul>
</details>

<details>
  <summary> The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMTQ1NDAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Victor Prisacariu &amp; Niantic 팀</li>
<li>학습된 정보 뿐이 아닌 실시간으로 들어오는 정보까지 사용해서 더 좋은 성능을 내는 dense depth estimation 기법을 소개함.</li>
<li>해당 기법은 self-supervision으로만 학습됨.</li>
</ul>
</details>

<details>
  <summary> 드론 자율비행 기술 동향 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cua29yZWFzY2llbmNlLm9yLmtyL2FydGljbGUvSkFLTzIwMjE1MzE1MDY0NDEzNS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>ETRI</li>
</ul>
</details>

<details>
  <summary> Guest Editorial: Special Issue on Performance Evaluation in Computer Vision </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMjYzLTAyMS0wMTQ1NS14">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Torsten Sattler</li>
</ul>
</details>

<p> </p>
<hr>
<h2 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h2><details>
  <summary> MonoGRNet: A General Framework for Monocular 3D Object Detection</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDg3OTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMTA5NTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> PLC-VIO: Visual-Inertial Odometry Based on Point-Line Constraints </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MzE2Nzk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Scene Coordinate Regression Network With Global Context-Guided Spatial Feature Transformation for Visual Relocalization
</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0Mzc2OTk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>


<details>
  <summary> Adaptive depth estimation for pyramid multi-view stereo </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL2dyYXBodmlzaW9uLndodS5lZHUuY24vcGFwZXJzL2xpYW9qaWUyMDIxY2FnLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDgyNDgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> VS-Net: Voting with Segmentation for Visual Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMTA4ODYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDAxMTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Visual SLAM based on instance segmentation in dynamic scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pb3BzY2llbmNlLmlvcC5vcmcvYXJ0aWNsZS8xMC4xMDg4LzEzNjEtNjUwMS9hYmZjZWIvbWV0YQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Factor graph optimization for GNSS/INS integration: A comparison with the extended Kalman filter </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vbmxpbmVsaWJyYXJ5LndpbGV5LmNvbS9kb2kvYWJzLzEwLjEwMDIvbmF2aS40MjE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Monocular Semantic Mapping Based on 3D Cuboids Tracking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MDEwNzE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Adaptive Stereo Direct Visual Odometry with Real-Time Loop Closure Detection and Relocalization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MDE0Njk=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDUuMDk4OTkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Vehicle Odometry with Camera-Lidar-IMU Information Fusion and Factor-Graph Optimization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwODQ2LTAyMS0wMTMyOS14">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> RANSIC: Fast and Highly Robust Estimation for Rotation Search and Point Cloud Registration using Invariant Compatibility</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDkxMzMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Monocular Semantic Mapping Based on 3D Cuboids Tracking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MDEwNzE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A sky region segmentation method for outdoor visual-inertial SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MDg3NzE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Line Flow Based Simultaneous Localization and Mapping </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzOTM0NzQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDUzMjcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Semantic visual SLAM in dynamic environment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNTE0LTAyMS0wOTk3OS00">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Simultaneous Video Stabilization and Rolling Shutter Removal </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MTE3MDE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Semi-Direct Monocular SLAM With Three Levels of Parallel Optimizations </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzkzOTg4NjM=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Non-uniform Observability for FastMoving Horizon Estimation with application to the SLAM problem </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMTIzMjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Radar SLAM: A Robust SLAM System for All Weather Conditions </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDUzNDcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Not Only Look But Infer: Multiple Hypothesis Clustering of Data Association Inference for Semantic SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk0MTA2MTA=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> SE-Harris and eSUSAN: Asynchronous Event-Based Corner Detection Using Megapixel Resolution CeleX-V Camera </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZWVwYWkub3JnL3B1YmxpY2F0aW9uL3NlLWhhcnJpcy1hbmQtZXN1c2FuLWFzeW5jaHJvbm91cy1ldmVudC1iYXNlZC1jb3JuZXItZGV0ZWN0aW9uLXVzaW5nLW1lZ2FwaXhlbC1yZXNvbHV0aW9uLWNlbGV4LXYtY2FtZXJh">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> A scalable parallel preconditioned conjugate gradient method for bundle adjustment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNDg5LTAyMS0wMjM0OS04">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> Self-Supervised Multi-Frame Monocular Scene Flow </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMDIyMTY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> MonodepthPlus: self-supervised monocular depth estimation using soft-attention and learnable outlier-masking </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc3BpZWRpZ2l0YWxsaWJyYXJ5Lm9yZy9qb3VybmFscy9qb3VybmFsLW9mLWVsZWN0cm9uaWMtaW1hZ2luZy92b2x1bWUtMzAvaXNzdWUtMi8wMjMwMTcvTW9ub2RlcHRoUGx1cy0tc2VsZi1zdXBlcnZpc2VkLW1vbm9jdWxhci1kZXB0aC1lc3RpbWF0aW9uLXVzaW5nLXNvZnQtYXR0ZW50aW9uLzEwLjExMTcvMS5KRUkuMzAuMi4wMjMwMTcuc2hvcnQ/U1NPPTE=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAMOps를 위한 첫걸음 - Docker + CI</title>
    <url>/20210808-docker-for-slam/</url>
    <content><![CDATA[<blockquote>
<p>서문은 건너뛰고 바로 Docker 설치하실 분은 <code>3. Docker 설치방법</code>으로 넘어가세요!</p>
</blockquote>
<h2 id="Why-Docker"><a href="#Why-Docker" class="headerlink" title="Why Docker?"></a>Why Docker?</h2><p><code>???: "내 컴퓨터에서는 잘 되는데?"</code></p>
<p>이 말에는 다음과 같은 뜻이 숨겨져있습니다.</p>
<ul>
<li>“니 컴에서 안되는건 내 문제가 아닙니다”</li>
<li>“니 컴 문제는 니가 해결하십시오”</li>
</ul>
<img src="/20210808-docker-for-slam/ddocker.jpg" class="" title="ddocker">

<p> </p>
<p>보통 <strong>어떤 팀원이 새로운 코드를 팀 전체가 사용하는 코드에 푸쉬해놓고, 다른 팀원의 컴퓨터에서 빌드가 터질 때 변명처럼 나오는 말</strong>입니다.</p>
<p>정말 무책임하지 않나요?</p>
<p>다른 팀원은 지금 이 코드 업데이트 때문에 빌드도 안되서 일을 못하는데, 이 버그를 만든 장본인은 본인의 컴퓨터에는 문제가 없으므로 계속 일을 하겠다는거 말이죠.</p>
<p><strong>본인이 만든 버그는 본인이 책임지는게 맞는겁니다</strong>.</p>
<p> </p>
<p>근데 사실 왠만큼 인성이 비뚤어진 사람이거나, 아니면 우리 팀의 업무를 터트리려는 산업스파이가 아닌 이상, 본인도 어느정도 죄책감을 느낄겁니다.</p>
<p>다만 이 버그를 풀기 위해서는 <strong>다양한 컴퓨터 환경에서 복잡한 디펜던시 이슈를 풀어야하는데, 이 작업을 뚝딱 해결할 자신이 나지 않기 때문에 변명처럼 말하는거죠</strong>.</p>
<p> </p>
<p>이러한 문제를 단번에 해결해주는 것이 바로 <strong>Docker</strong>입니다.</p>
<p>Docker는 <strong>모든 팀원이 동일한 환경에서 빌드 &amp; 테스트</strong> 할 수 있게 해줍니다.</p>
<p>킹갓Docker를 사용하는 순간부터, 누가 다시 ‘내 컴퓨터에서는 잘 되는데?’ 라고 하면 다음과 같이 이야기해주시면 되겠습니다.</p>
<blockquote>
<p>A: “내 컴퓨터에서는 잘 되는데???”<br>B: “<strong>아! 그러면 너님의 컴퓨터를 제품으로 팔겠습니다!</strong>“<br>A: “?!”</p>
</blockquote>
<p> </p>
<hr>
<h2 id="SLAM만-해도-힘들어죽겠는데-Docker까지-해야하나…"><a href="#SLAM만-해도-힘들어죽겠는데-Docker까지-해야하나…" class="headerlink" title="SLAM만 해도 힘들어죽겠는데 Docker까지 해야하나…"></a>SLAM만 해도 힘들어죽겠는데 Docker까지 해야하나…</h2><p>로컬 환경에서 여러 SLAM 프로젝트를 진행하다보면 다양한 라이브러리들이 시스템에 설치되게 됩니다.</p>
<p>SLAM에서 많이 사용되는 OpenCV, PCL, Open3D 등등이 시스템에 설치되어있다면, 많은 오픈소스 프로젝트는 git clone으로 끌어와 바로 빌드할 수 있습니다.</p>
<p> </p>
<p>하지만 이러한 환경을 가지고 있을 때 다음과 같은 문제가 생깁니다.</p>
<ol>
<li>새로운 프로젝트를 시작할 때<ul>
<li>새 프로젝트의 디펜던시가 이미 설치되어있을 수 있다면?<ul>
<li>디펜던시 자동 설치 스크립트를 작성할 때, <strong>사전에 설치된 라이브러리에 대해 테스팅을 하지 않는 실수를 하기 쉬움</strong>.</li>
</ul>
</li>
<li>새 프로젝트의 디펜던시가 ‘다른 버전’으로 설치되어있을 수 있다면?<ul>
<li><strong>다른 버전을 사용하면서 API 충돌</strong>이 날 수 있음.</li>
<li>다른 버전을 사용하면서 API가 충돌이 나지 않는 경우는… 보통 성능개선 업데이트만 있는 경우인데, <strong>아무도 잘못된 버전을 사용한다는 것을 모른 채 계속 구린 성능의 버전을 쓸 수도 있음</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>옛날 프로젝트를 다시 볼 때<ul>
<li>OpenCV3 기반 프로젝트를 잠시 중단하고… OpenCV4 기반 프로젝트를 하다가… 다시 이 프로젝트로 돌아온다면?<ul>
<li>find_package(OpenCV REQUIRED)를 하면 <strong>어떤 버전의 라이브러리를 잡을지는 아무도 모름</strong></li>
<li>(확률적인 버그 잡이는 최악입니다…)</li>
<li>(진짜 마음만 같아서는 프로젝트마다 컴퓨터 한대씩 쓰고 싶다는 생각이 듭니다…)</li>
</ul>
</li>
</ul>
</li>
</ol>
<p> </p>
<p>위와 같은 문제를 해결하는건 정말 귀찮은 작업입니다.</p>
<ol>
<li>현재 시스템에 빌드되어있는 라이브러리들을 전부 정리해야합니다.<ul>
<li>설치된 라이브러리 이름, 버전명, 빌드 옵션, 빌드 컴파일러, 설치 경로…</li>
</ul>
</li>
<li>현재 시스템에 apt로 설치된 라이브러리들도 전부 정리해야합니다.<ul>
<li>컴파일러? 시스템 유틸?</li>
</ul>
</li>
<li>내가 진행한 프로젝트들마다 ‘각각’ 어떤 라이브러리를 어떤 옵션으로 사용하고 어떤 경로를 지정했는지 알아야합니다.</li>
<li>모든 프로젝트들을 클린 리눅스에서 테스트 해봐야합니다.</li>
<li>이제 이 환경을 모든 팀원들 컴퓨터에 그대로 깔아줘야합니다.</li>
</ol>
<p>1/2/3을 풀기 위해 이전에 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1L2NwcC1jdi1wcm9qZWN0LXRlbXBsYXRl">시스템 설치 디펜던시를 최소화하는 프로젝트(cpp-cv-project-template)<i class="fa fa-external-link-alt"></i></span>를 진행한 적이 있습니다.</p>
<p>하지만 이 방법도 <strong>설치해야하는 모든 시스템 라이브러리를 리스팅 하는 노가다</strong>를 엄청 많이 했어요.</p>
<p>디버깅을 하면서도 <strong>‘이게 맞나…? 다 한건가…?’ 싶다가도 버그가 터지면 ‘내가 그럼 그렇지’를 외치기를 몇번씩 반복</strong>했던 것 같습니다.</p>
<img src="/20210808-docker-for-slam/bug.jpg" class="" title="bug">

<p> </p>
<p><strong>근데 Docker를 쓰고 엄청 쉬워졌습니다.</strong></p>
<ol>
<li>디펜던시 빌드<ul>
<li>Docker로 Clean Ubuntu 20.04 이미지 생성</li>
<li>디버깅을 할 때마다 git에서 소스코드만 끌어와서 빌드<ul>
<li>버그가 나타나면 로그 남김</li>
</ul>
</li>
<li>코드를 수정하고 다시 디버깅을 시작하면, 5초만에 Clean Ubuntu 20.04가 다시 만들어짐.</li>
<li>다시 빌드를 시작</li>
<li>몇번 반복 후 완벽한 디펜던시 스크립트 작성 완료.</li>
</ul>
</li>
<li>SLAM 프로그램 빌드<ul>
<li>OpenCV랑 Ceres를 매번 빌드하기에는 너무 오래걸림 (30분 ~ 1시간)</li>
<li>그러니 Ubuntu 20.04에 OpenCV랑 Ceres를 미리 빌드해놓은 이미지를 생성</li>
<li>git에서 소스코드만 떙겨와서 빌드<ul>
<li><strong>1분만에 Clean Ubuntu 20.04에 OpenCV + Ceres + 나의 SLAM 프로그램이 빌드</strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>진짜 ‘<strong>아 왜 Docker를 지금 알게된거지? 인생 낭비했네</strong>‘ 생각이 들 정도였습니다.</p>
<p> </p>
<hr>
<h2 id="Docker-설치-방법"><a href="#Docker-설치-방법" class="headerlink" title="Docker 설치 방법"></a>Docker 설치 방법</h2><p>바로 Docker를 설치해보겠습니다.</p>
<ol>
<li>이전에 Docker를 설치하려고 한 경험이 있다면, Docker 버전 충돌을 피하기 위해 예전 Docker를 삭제합니다. (이전에 설치 경험이 없으면 이 단계는 건너뜁시다)</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove docker docker-engine docker.io containerd runc</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>apt에 Docker repository를 추가해줍니다. 이 과정을 거치면 apt install docker~~~ 로 쉽게 받을 수 있습니다.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg \</span><br><span class="line">    lsb-release</span><br><span class="line"></span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> \</span><br><span class="line"><span class="string">"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string"><span class="subst">$(lsb_release -cs)</span> stable"</span> | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Docker engine을 설치해줍니다.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Docker의 Hello world를 실행해서 잘 설치되었는지 확인합니다.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure>

<p>다음과 같은 화면이 나오면 성공입니다.</p>
<img src="/20210808-docker-for-slam/hello-docker.png" class="" title="hello-docker">

<ol start="5">
<li>(선택) Docker를 쓸 때는 항상 <code>sudo docker...</code>로 써야합니다. 매번 sudo 쳐주는것도 귀찮고, 아니면 CI 등 자동화 스크립트를 쓸 때 비밀번호 쳐주기 귀찮다면 sudo를 떼버립시다. 한번 로그아웃하고 다시 로그인해야지 적용됩니다.</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker</span><br><span class="line"></span><br><span class="line">sudo usermod -aG docker <span class="variable">$USER</span></span><br><span class="line"></span><br><span class="line">newgrp docker</span><br><span class="line"></span><br><span class="line">docker run hello-world</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="Clean-Ubuntu-설치-방법"><a href="#Clean-Ubuntu-설치-방법" class="headerlink" title="Clean Ubuntu 설치 방법"></a>Clean Ubuntu 설치 방법</h2><p>엄청 간단합니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run -dit ubuntu:latest</span><br><span class="line"></span><br><span class="line">sudo docker images</span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/clean_ubuntu.png" class="" title="clean_ubuntu">

<p> </p>
<hr>
<h2 id="Image-Container-사용법"><a href="#Image-Container-사용법" class="headerlink" title="Image / Container 사용법"></a>Image / Container 사용법</h2><p>기본적인 Docker 사용을 위해서는 image와 container의 개념을 이해해야합니다.</p>
<ul>
<li>Image<ul>
<li><strong>붕어빵을 찍어내는 빵판</strong>과 같습니다.</li>
<li>어떤 환경을 만들어낼지에 대한 instruction이 들어가있다고 보면 됩니다.</li>
</ul>
</li>
<li>Container<ul>
<li><strong>빵판에 찍혀나온 빵들</strong>과 같습니다.</li>
<li>Image에 명시된 instruction을 기반으로 만들어진 환경입니다.</li>
<li>하나의 image 기반으로 여러개의 container를 만들 수 있습니다.<ul>
<li>예를 들어, clean ubuntu 20.04의 이미지로, 100개의 clean ubuntu 20.04 환경을 만들어 낼 수 있습니다. 각각 다른 라이브러리를 깔아서 실험을 할 수 있겠죠.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Image-확인하기"><a href="#Image-확인하기" class="headerlink" title="Image 확인하기"></a>Image 확인하기</h3><p>현재 내 Docker에 설정이 된 image들은 다음과 같은 커맨드로 확인할 수 있습니다</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker images</span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/docker_images.png" class="" title="docker_images">

<h3 id="Container-확인하기"><a href="#Container-확인하기" class="headerlink" title="Container 확인하기"></a>Container 확인하기</h3><p>현재 내 Docker에서 만든 container들은 다음과 같은 커맨드로 확인할 수 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker ps -a</span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/docker_ps_a.png" class="" title="docker_ps_a">

<h3 id="Container-지우기"><a href="#Container-지우기" class="headerlink" title="Container 지우기"></a>Container 지우기</h3><p>우리가 앞으로 딱히 hello-world container를 더 쓸 것 같지 않습니다. 지워주도록 하겠습니다. 지울때는 container ID로 지워주거나, names로 지워주면 됩니다. <code>sad_faraday</code>나 <code>clever_meitner</code> 같은 것들이 name인데, 이는 우리가 container 생성 시 이름을 지어주지 않았기 때문에 docker가 임의로 지어준 것입니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NAMES 로 지우기</span></span><br><span class="line">sudo docker rm sad_farady</span><br><span class="line">sudo docker rm clever_meitner</span><br><span class="line"></span><br><span class="line"><span class="comment"># CONTAINER ID로 지우기</span></span><br><span class="line"></span><br><span class="line">sudo docker rm ceb8a255cab8</span><br><span class="line">sudo docker rm 425b90a1a354</span><br></pre></td></tr></table></figure>

<p>그러면 container가 지워진 걸 볼 수 있습니다</p>
<img src="/20210808-docker-for-slam/docker_ps_a_2.png" class="" title="docker_ps_a2">

<h3 id="Image-지우기"><a href="#Image-지우기" class="headerlink" title="Image 지우기"></a>Image 지우기</h3><p>hello-world 이미지도 지워보겠습니다.</p>
<p>Image를 지울때는 remove-image를 줄인 <code>rmi</code>를 사용합니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker rmi d1165f221234</span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/docker_rmi.png" class="" title="docker_rmi">

<h3 id="Container-생성하기-CLI"><a href="#Container-생성하기-CLI" class="headerlink" title="Container 생성하기 (CLI)"></a>Container 생성하기 (CLI)</h3><p>이제 image로부터 Container를 생성해보겠습니다.</p>
<p>가장 쉬운 방법은 아까의 <code>Clean Ubuntu 설치 방법</code> 섹션의 run 커맨드입니다. 여기에 <code>--name</code> flag를 주어서 container에 이름을 부여해줄 수 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run --name clean_ubuntu -dit ubuntu:latest</span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/docker_run_name.png" class="" title="docker_run_name">

<h3 id="Clean-Ubuntu-20-04-실행하기-i-e-Container-실행하기"><a href="#Clean-Ubuntu-20-04-실행하기-i-e-Container-실행하기" class="headerlink" title="Clean Ubuntu 20.04 실행하기 (i.e. Container 실행하기)"></a>Clean Ubuntu 20.04 실행하기 (i.e. Container 실행하기)</h3><p><code>sudo docker ps -a</code>로 container가 생성된 것을 확인할 수 있습니다. recursive_wing과 같이 Docker에서 임의로 정한 구린 이름 말고, 내가 직접 지정한 좋은 이름으로 생성되었습니다.</p>
<p>이제 생성된 container를 실행해서 실제로 Clean Ubuntu 환경으로 들어가보겠습니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NAME으로 접근하기</span></span><br><span class="line">sudo docker attach clean_ubuntu</span><br><span class="line"></span><br><span class="line"><span class="comment"># CONTAINER ID로 접근하기</span></span><br><span class="line">sudo docker attach 3cff7e522c6f</span><br></pre></td></tr></table></figure>

<p>아래 사진과 같이 Clean Ubuntu 20.04 환경에 root 권한으로 <code>/</code>에 들어왔습니다. <code>exit</code>을 사용해서 container를 빠져나올 수 있습니다.</p>
<img src="/20210808-docker-for-slam/docker_attach.png" class="" title="docker_attach">

<p>이제 여기서 <code>git clone ...</code>으로 소스코드를 땡겨오고 빌드를 하고 여러가지 실험을 할 수 있습니다. 실험이 끝나면 container를 삭제해주면 됩니다.</p>
<h3 id="Container-생성하기-Dockerfile"><a href="#Container-생성하기-Dockerfile" class="headerlink" title="Container 생성하기 (Dockerfile)"></a>Container 생성하기 (Dockerfile)</h3><p>근데 Clean Ubuntu이다보니 너무 clean 합니다.</p>
<p>git, cmake, build-essential, gcc, wget, curl 뭐 다 새로 깔아줘야하는데, <strong>실험을 위해 container를 만들 때 마다 매번 커맨드를 쳐주기 너무 귀찮습니다</strong>.</p>
<p>걱정하지마세요.</p>
<p><strong>이 반복적인 작업을 자동화하는 방법, Dockerfile</strong>이 있습니다.</p>
<p>우선 container에서 빠져나온 후, 원하는 위치에 Dockerfile을 만들고 (i.e. <code>touch Dockerfile</code>) 저장해봅시다.</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Ubuntu:latest는 20.04를 의미합니다.</span></span><br><span class="line"><span class="keyword">FROM</span> ubuntu:latest </span><br><span class="line"></span><br><span class="line"><span class="comment"># 작성자 이름입니다</span></span><br><span class="line"><span class="keyword">MAINTAINER</span> changh95</span><br><span class="line"></span><br><span class="line"><span class="comment"># apt로 패키지 받을 때 interative하게 사용하는 기능들을 끕니다. Docker에서 로그를 넘길 때 문제가 생길 수 있으므로 이 옵션은 필수입니다!</span></span><br><span class="line"><span class="keyword">ARG</span> DEBIAN_FRONTEND=noninteractive</span><br><span class="line"></span><br><span class="line"><span class="comment"># apt-get update와 기본적인 패키지들을 깔아줍니다.</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get -y update &amp;&amp;\</span></span><br><span class="line"><span class="bash">apt-get -y install build-essential cmake git sudo wget &amp;&amp;\</span></span><br><span class="line"><span class="bash">apt-get -y install python3 python3-pip &amp;&amp;\</span></span><br><span class="line"><span class="bash">apt-get autoclean</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 원하는 프로젝트의 소스코드를 clone해와서 빌드 스크립트를 실행합니다</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir cv-project &amp;&amp;\</span></span><br><span class="line"><span class="bash"><span class="built_in">cd</span> cv-project &amp;&amp;\</span></span><br><span class="line"><span class="bash">git <span class="built_in">clone</span> https://github.com/changh95/cpp-cv-project-template.git . &amp;&amp;\</span></span><br><span class="line"><span class="bash">git checkout <span class="string">"develop"</span> &amp;&amp;\</span></span><br><span class="line"><span class="bash">pip3 install pyyaml &amp;&amp;\</span></span><br><span class="line"><span class="bash">./setup.py</span></span><br></pre></td></tr></table></figure>

<p>그리고 <code>sudo docker build ...</code> 커맨드를 Dockerfile에 명시된 instruction을 따라 이미지를 생성합니다. 여러가지 flag도 사용하는데, 함께 설명합니다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker build --no-cache --force-rm -f Dockerfile -t cvproject:base .</span><br><span class="line"></span><br><span class="line"><span class="comment"># --no-cache 는 이전 빌드에서 생성된 캐시를 사용하지 않겠다는겁니다. 생성시간을 줄이기 위해서 캐시를 사용하는 경우도 있는데, 처음 사용하는 사람은 실수하기 쉬우니 일단 no cache 옵션을 씁시다.</span></span><br><span class="line"><span class="comment"># --force-rm 은 빌드하다가 터지면 이미지/컨테이너를 지워줍니다.</span></span><br><span class="line"><span class="comment"># -f 는 사용할 Dockerfile의 이름을 명시합니다</span></span><br><span class="line"><span class="comment"># -t 는 우리가 만드는 이미지의 이름 + 태그를 명시합니다</span></span><br><span class="line"><span class="comment"># . 는 Dockerfile의 경로를 명시합니다.</span></span><br></pre></td></tr></table></figure>

<img src="/20210808-docker-for-slam/dockerfile_build.png" class="" title="docker_attach">

<p> </p>
<hr>
<h2 id="고수처럼-CI-Docker-사용하기"><a href="#고수처럼-CI-Docker-사용하기" class="headerlink" title="고수처럼 CI + Docker 사용하기"></a>고수처럼 CI + Docker 사용하기</h2><p>우리는 이제 Docker의 기본적인 사용법을 알고있습니다.</p>
<p>Docker 이미지를 만들거나 Dockerfile을 만들어서 팀원들에게 공유하고, 모두가 같은 환경에서 코드를 짤 수 있게 되었죠.</p>
<p>하지만 아쉬운 점이라면… 로컬에서밖에 못쓴다는거죠.</p>
<p>그에 비해, 팀원들과의 협업은 GitHub나 GitLab 같은 온라인 Git 서비스에서 이뤄지는 경우가 대부분입니다.</p>
<p>GitHub에 Docker를 연결할 수 있는 방법이 있을까요?</p>
<img src="/20210808-docker-for-slam/building.png" class="" title="Docker CI">

<p> </p>
<p>여기서부터는 **CI (Continuous Integration)**을 개발한다고 볼 수 있습니다.</p>
<p>가장 간단한 CI 개념으로, <strong>Pull Request로 코드가 들어오면 자동으로 Docker build를 하고 성공/실패 결과를 표시</strong>할 수 있습니다.</p>
<p>간단하게 <strong>GitHub Actions</strong>를 사용해봅시다.</p>
<h3 id="GitHub-Actions-활성화하기"><a href="#GitHub-Actions-활성화하기" class="headerlink" title="GitHub Actions 활성화하기"></a>GitHub Actions 활성화하기</h3><p>GitHub 레포지토리에 들어가서, <code>Actions</code> 탭을 눌러 들어가면 아래와 같은 화면이 나옵니다.</p>
<img src="/20210808-docker-for-slam/ga.png" class="" title="github actions">

<p>‘set up a workflow yourself’를 누르면 <code>.github/workflows/main.yml</code>이라는 파일을 생성하며, 해당 파일이 가질 내용을 보여줍니다.</p>
<p>일단 이 파일을 저장해줍니다.</p>
<p>이제 우리는 1. <strong>MS Azure 클라우드 서버에서 빌드를 할지</strong>, 2. <strong>개인 장비에서 빌드를 할지 선택</strong>해야합니다.</p>
<p> </p>
<h3 id="개인장비로-빌드하기-Self-hosted-runners"><a href="#개인장비로-빌드하기-Self-hosted-runners" class="headerlink" title="개인장비로 빌드하기 (Self-hosted runners)"></a>개인장비로 빌드하기 (Self-hosted runners)</h3><p>Self-hosted runners 빌드 방법은 다음과 같은 경우에 좋습니다.</p>
<ul>
<li>많은 코어로 빌드를 빠르게 하고 싶을 때</li>
<li>다수의 코어 / 높은 메모리 사용량을 요구할 때<ul>
<li>e.g. SLAM 벤치마크…</li>
</ul>
</li>
<li>GitHub Actions 제한시간으로 부족할 때<ul>
<li>OpenCV + Ceres를 Debug/Release 모드로 빌드 할 때, GitHub Action의 무료제공 서버로는 제한시간 내 빌드할 수 없었습니다.</li>
</ul>
</li>
</ul>
<p>Self-hosted runner를 설정하는 방법은 이 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdpdGh1Yi5jb20vZW4vYWN0aW9ucy9ob3N0aW5nLXlvdXItb3duLXJ1bm5lcnMvYWRkaW5nLXNlbGYtaG9zdGVkLXJ1bm5lcnM=">링크<i class="fa fa-external-link-alt"></i></span>를 참조해주세요.</p>
<p>파일의 내용을 다음과 같이 바꿔줍니다. (cpp-cv-project-template는 바꿔도 됩니다!)</p>
<p>아래 스크립트는 main / development 브랜치에 push하거나 Pull request가 생기는 상황에서 GitHub Actions를 통해 Self-hosted runner에 Dockerfile 기반 빌드를 지시하는 커맨드입니다.</p>
<p>우리가 보았던 커맨드는 왠만하면 다 들어가있습니다.</p>
<p>추가로, Docker 빌드가 실패했을 경우 실패한 image / container를 삭제해주는 기능이 있습니다.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">CI</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">development</span></span><br><span class="line">  <span class="attr">pull_request:</span></span><br><span class="line">    <span class="attr">branches:</span> [<span class="string">development</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">self-hosted</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span> <span class="string">Docker</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">         <span class="string">echo</span> <span class="string">"=== Build start==="</span></span><br><span class="line">         <span class="string">echo</span> <span class="string">${{</span> <span class="string">github.head_ref</span> <span class="string">}}</span></span><br><span class="line">        </span><br><span class="line">         <span class="string">cd</span> <span class="string">Dockerfiles</span></span><br><span class="line">         <span class="string">docker</span> <span class="string">build</span> <span class="string">--force-rm</span> <span class="string">--no-cache</span> <span class="string">-f</span> <span class="string">build.dockerfile</span> <span class="string">-t</span> <span class="string">"cpp-cv-project-template:base"</span> <span class="string">--build-arg</span> <span class="string">BRANCH=${{</span> <span class="string">github.head_ref</span> <span class="string">}}</span> <span class="string">.</span> </span><br><span class="line">         <span class="string">echo</span> <span class="string">"=== Build finished==="</span></span><br><span class="line">         </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Clean</span> <span class="string">up</span> <span class="string">Docker</span> <span class="string">image</span> <span class="string">if</span> <span class="string">build</span> <span class="string">fails</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">failure()</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">         <span class="string">echo</span> <span class="string">"=== Remove failed image start==="</span></span><br><span class="line">         <span class="string">docker</span> <span class="string">rmi</span> <span class="string">-f</span> <span class="string">$(docker</span> <span class="string">images</span> <span class="string">-f</span> <span class="string">"dangling=true"</span> <span class="string">-q)</span></span><br><span class="line">         <span class="string">docker</span> <span class="string">images</span> </span><br><span class="line">         <span class="string">echo</span> <span class="string">"=== Removal finished==="</span></span><br></pre></td></tr></table></figure>

<p>레포지토리에서 branch protetion rule 까지 설정해주고 나면, <strong>빌드가 성공한 경우에만 merge가 가능하게 설정</strong>을 바꿀 수 있습니다.</p>
<p>제가 좋아하는 설정은 주로 1. <strong>최소 N명 이상의 코드 리뷰에서 approve</strong>를 받아야함, 2. <strong>모든 status check를 통과</strong>해야함 (e.g. 빌드, 유닛테스트, 린터/포맷터, 벤치마크)입니다.</p>
<p>아래는 가장 간단한 빌드 테스트에 대한 status check가 적용된 모습입니다.</p>
<img src="/20210808-docker-for-slam/status_check.png" class="" title="Status check">

<p>빌드가 진행되는 동안에는 다음과 같이 확인할 수 있습니다 (원래 빌드 진행 중 로그도 볼 수 있는데, 1시간동안 로그가 많이 쌓여서 로딩하는데 시간이 좀 걸리는 것 같습니다)</p>
<img src="/20210808-docker-for-slam/ga_2.png" class="" title="asdf">

<p> </p>
<h3 id="MS-Azure-클라우드-서버로-빌드하기"><a href="#MS-Azure-클라우드-서버로-빌드하기" class="headerlink" title="MS Azure 클라우드 서버로 빌드하기"></a>MS Azure 클라우드 서버로 빌드하기</h3><p>MS Azure 클라우드의 경우 2코어 7GB 램의 옵션을 가지고 있는데, Public repository의 경우 제한시간동안 공짜로 사용할 수 있습니다.</p>
<p>이 방법도 Self-hosted runner 방법과 크게 다르지 않으며, 대신 아래의 옵션들 중에 선택해서 넣으면 됩니다.</p>
<ul>
<li>Windows server: <code>windows-latest</code> (2019) 또는 <code>windows-2016</code></li>
<li>Ubuntu: <code>ubuntu-latest</code> (20.04) 또는 <code>ubuntu-18.04</code></li>
<li>Mac: <code>macos-11</code> (Big Sur) 또는 <code>macos-latest</code> (Catalina)</li>
</ul>
<p>근데 이 방법은 사실상 위의 환경들이 Docker로 만들어지는거라… Clean 환경이 필요한거라면 또 다시 Docker로 만들 필요는 굳이 없습니다.</p>
<p>Pre-built Docker image를 쓰고싶은 거라면, <span class="exturl" data-url="aHR0cHM6Ly9odWIuZG9ja2VyLmNvbS8=">DockerHub<i class="fa fa-external-link-alt"></i></span>에 올려놓고, GitHub Actions에서 Docker image를 긁어온다음에 프로그램만 빌드해서 실행하는 것도 좋습니다.</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Tightly-coupled vs Loosely-coupled 방식</title>
    <url>/20210816-tightly-loosely-coupled/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문"></a>예상 면접 질문</h2><ol>
<li>Tightly-coupled 방식과 Loosely-coupled 방식의 차이가 무엇일까요?</li>
<li>Tightly-coupled 방식은 언제 유리하고, loosely-coupled 방식은 언제 유리할까요?</li>
<li>VIO를 설계할 때 어떤 방식이 있을까요?</li>
<li>센서 퓨전을 하는 방식에는 어떤 방식이 있을까요?</li>
</ol>
<p> </p>
<hr>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><p>우선 Tightly-coupled 방식과 Loosely-coupled 방식은 모두 <strong>2개 이상의 센서 데이터를 받을 때 정보를 혼합하여 단일 상태치를 추정하기 위한 방법</strong>이다.</p>
<p>특히, 이종 센서간의 정보를 혼합할 때 사용하기 좋다 (e.g. Camera+IMU, LiDAR+IMU, Camera+LiDAR+IMU+GPS+Wheel)</p>
<p><strong>‘두 방법 중 어떤 방법이 더 좋다!’ 라고 단언할 수는 없으며</strong>, 실제로 둘 다 돌려보고 더 잘되는 방식을 써야한다.</p>
<p>아래 각각의 방식의 특징을 설명한다.</p>
<p> </p>
<hr>
<h3 id="Tightly-coupled-강결합-방식"><a href="#Tightly-coupled-강결합-방식" class="headerlink" title="Tightly-coupled (강결합) 방식"></a>Tightly-coupled (강결합) 방식</h3><img src="/20210816-tightly-loosely-coupled/vins.png" class="" title="vins">

<ul>
<li><strong>각각의 센서들에서 나온 관측치들을 한꺼번에 하나의 cost function을 통해 optimal state를 추정</strong>하는 방법<ul>
<li>e.g. Tightly-coupled VIO : 카메라 센서 관측치에 대해서는 reprojection error를 계산, IMU 센서에 대해서는 kinematic residual을 계산, 이후 이 값들을 합한 값이 최저치가 되는 motion을 추정.</li>
</ul>
</li>
<li><strong>Loosely-coupled 방식보다 더 정확하다는 인식</strong>이 있다.</li>
<li><strong>Loosely-coupled 방식보다 계산량이 더 많다는 인식</strong>이 있다.</li>
<li>Graph-based optimisation으로도 풀 수 있고, Filter-based approach로도 풀 수 있다.</li>
</ul>
<div class="tabs" id="graph-vs-filter"><ul class="nav-tabs"><li class="tab active"><a href="#graph-vs-filter-1">Graph-based optimisation</a></li><li class="tab"><a href="#graph-vs-filter-2">Filter-based approach</a></li></ul><div class="tab-content"><div class="tab-pane active" id="graph-vs-filter-1"><ul>
<li><strong>Graph에 다양한 센서들의 관측치가 포함</strong>된다.<ul>
<li>(위 이미지의 factor graph 참조. 카메라로 인한 variable과 IMU로 인한 variable이 둘 다 들어간다)</li>
<li>단일 센서로 추정할 때 보다 더 많은 variable과 constraint가 생긴다.<ul>
<li>이로 인해 얻는 장점: <strong>Filter 방식보다 동일 계산량 대비 더 정확한 결과</strong>를 얻을 수 있다 (이는 Graph-optimisation의 특징이기도 하다)</li>
<li>이로 인해 얻는 단점: Filter 방식보다 <strong>절대적인 계산량이 훨씬 높기 때문에 고성능 CPU를 요구</strong>한다.</li>
</ul>
</li>
</ul>
</li>
</ul></div><div class="tab-pane" id="graph-vs-filter-2"><ul>
<li><strong>Extended Kalman filter (EKF)</strong> 또는 <strong>Unscented Kalman filter (UKF)**와 같은 **filter 알고리즘의 인풋으로 다양한 센서의 관측치를 포함</strong>한다.<ul>
<li>이로 인해 얻는 장점: Graph-optimisation 방식에 비해 <strong>절대적인 계산량이 적기 때문에 저전력 CPU를 사용</strong>할 수 있다.</li>
<li>이로 인해 얻는 단점: EKF 방식은 선형화 오차로 graph-optimisation 방식에 비해 <strong>조금 부정확</strong>하다</li>
<li>이로 인해 얻는 단점2: EKF 방식은 관측치의 수가 많아질수록 연산량이 늘어나기 때문에 <strong>적은 수의 관측치만 이용</strong>할 수 있는데, 이는 <strong>상태추정을 불안정하게 만든다</strong> (i.e. robustness가 떨어진다).</li>
</ul>
</li>
</ul></div></div></div>

<p> </p>
<hr>
<h3 id="Loosely-coupled-약결합-방식"><a href="#Loosely-coupled-약결합-방식" class="headerlink" title="Loosely-coupled (약결합) 방식"></a>Loosely-coupled (약결합) 방식</h3><ul>
<li><strong>센서들마다 각각 state를 추정</strong>하고, <strong>여러개의 state 정보를 합성해서 optimal state를 추정</strong>하는 것.<ul>
<li>e.g. Loosely-coupled VIO = 카메라로 추정한 pose와 IMU로 추정한 pose를 EKF로 추정</li>
<li>이로 인해 얻는 장점: 센서마다 state 추정 모듈을 쉽게 만들수 있다.</li>
<li>이로 인해 얻는 장점2: <strong>센서 시스템을 쉽게 변경할 수 있다</strong> (i.e. 새 센서를 추가하기 쉽다 + 기존의 센서를 제거하기 쉽다)<ul>
<li>하나의 코어 센서퓨전 프레임워크를 만들고, 다양한 센서 구성을 가진 시스템 (e.g. 자율주행 소프트웨어 -&gt; 소형자동차,중형세단,버스 탑재)를 만들 때 유용하다</li>
</ul>
</li>
<li>이로 인해 얻는 장점3: <strong>특정 센서에 대해 가중치를 더 쉽게 줄 수 있다</strong>.<ul>
<li>어떤 센서가 잘 작동하지 않을 때 가중치를 낮추고, 다른 잘 작동하는 센서의 가중치를 높혀서 끊기지 않고 좋은 상태추정을 이어갈 수 있다 (i.e. robustness가 올라간다!)</li>
</ul>
</li>
<li>이로 인해 얻는 장점4: <strong>EKF를 사용할 때 많은 양의 관측치를 사용하지 않음으로 퓨전 프레임워크가 굉장히 가벼워진다</strong></li>
<li><strong>장점 정리: 엔지니어링 측면에서 자유도가 굉장히 높다!</strong></li>
<li>이로 인해 얻는 단점: <strong>특정 센서를 믿을 수 없을 때 어떻게 처리해야하는지 직접 설계해야한다</strong>.<ul>
<li>e.g. 갑자기 하얀 벽이 나타났을 때 카메라는 pose 추정을 할 수 없다. IMU는 pose 추정을 잘 한다. 두 pose 값의 가중치를 동적으로 바꿔야할까? 동적으로 바꾼다면 어떻게?</li>
<li>e.g. 저가형 IMU를 사용하기 때문에 종종 IMU의 값이 튄다. IMU 값이 튈 때는 가중치를 낮추고 싶은데, 어떻게 값이 튀는걸 감지할 수 있을까? 센서의 오작동으로 튀는 값과, 실제로 충격을 받아서 생기는 튀는 값을 어떻게 구분해야할까?</li>
<li>이러한 edge 케이스들은 굉장히 많을텐데 어떻게 설계할 것인가?</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Sensors</tag>
        <tag>Tightly-coupled</tag>
        <tag>Loosely-coupled</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual-SLAM vs VIO</title>
    <url>/20210817-vslam-vs-vio/</url>
    <content><![CDATA[<blockquote>
<p>모바일로 글을 보신다면 ‘데스크탑 버전으로 보기’를 누르시면 좀 더 보기 쉽습니다</p>
</blockquote>
<h2 id="예상-면접-질문"><a href="#예상-면접-질문" class="headerlink" title="예상 면접 질문"></a>예상 면접 질문</h2><ol>
<li>Visual-SLAM과 VIO의 차이점이 무엇인가요?</li>
<li>VIO는 왜 쓰나요? Visual-SLAM 만으로는 안되나요?</li>
<li>오픈소스로 공개된 VIO에는 어떤게 있을까요?</li>
</ol>
<p> </p>
<hr>
<h2 id="답"><a href="#답" class="headerlink" title="답"></a>답</h2><p>간단하게 이야기하면… 3D pose + 3D map point를 구하고 싶을 때,</p>
<ol>
<li><strong>카메라만 사용하면 Visual-SLAM</strong></li>
<li><strong>카메라 + IMU를 사용하면 Visual-inertial odometry (VIO)</strong></li>
</ol>
<p>Visual odometry가 아니라 Visual-SLAM이 되려면 global consistency가 유지되어야하는데 (i.e. 동네 한바퀴를 돌고와서 원점으로 왔을 때, 위치 정보와 scale이 정확하게 맞아떨어져야함), 이는 loop closure와 같은 기능으로 해결할 수 있어야한다. (Visual odometry만으로 위치정보와 scale이 정확하게 맞아떨어지면 그것도 SLAM이라고 할 수 있다. 하지만 실제로는 거의 불가능하기 때문에 학계에서는 잘 받아드려지지 않는 전제이다.)</p>
<p>Visual-SLAM과 VIO 모두 Monocular, Stereo, Multi-camera, RGB-D 형태를 가질 수 있다.</p>
<p> </p>
<hr>
<h2 id="Visual-SLAM"><a href="#Visual-SLAM" class="headerlink" title="Visual-SLAM"></a>Visual-SLAM</h2><ul>
<li>카메라만 사용해서 3D pose + 3D map point를 구하는 방법</li>
<li>최근 연구는 주로 graph-optimisation 기반의 SLAM 백엔드를 사용함<ul>
<li>Total reprojection error를 최소화하는 방향</li>
</ul>
</li>
<li>유명한 오픈소스 프로젝트<ul>
<li>Monocular: MonoSLAM, PTAM, ORB-SLAM, SVO, DSO</li>
<li>Stereo: ORB-SLAM2, Stereo-DSO, ProSLAM</li>
<li>RGB-D: KinectFusion, ElasticFusion, BundleFusion</li>
</ul>
</li>
</ul>
<div class="tabs" id="visual-slam-장점-vs-단점"><ul class="nav-tabs"><li class="tab active"><a href="#visual-slam-장점-vs-단점-1">장점</a></li><li class="tab"><a href="#visual-slam-장점-vs-단점-2">단점</a></li></ul><div class="tab-content"><div class="tab-pane active" id="visual-slam-장점-vs-단점-1"><ul>
<li>VIO 보다 적은 계산량</li>
<li>카메라만 있으면 작동할 수 있음</li>
</ul></div><div class="tab-pane" id="visual-slam-장점-vs-단점-2"><ul>
<li>카메라가 잘 작동하지 않는 상황에서는 Visual-SLAM도 잘 작동하지 못함<ul>
<li>e.g. 하얀 벽을 바라보며 feature가 전혀 뽑히지 않을 때</li>
<li>e.g. 반복되는 패턴이 연속적으로 나타나서 제대로 된 피쳐 매칭이 되지 않을 때</li>
<li>e.g. 너무 어둡거나 너무 밝아서 아무것도 보이지 않을 때</li>
</ul>
</li>
</ul></div></div></div>

<p> </p>
<hr>
<h2 id="Visual-inertial-odometry-VIO"><a href="#Visual-inertial-odometry-VIO" class="headerlink" title="Visual-inertial odometry (VIO)"></a>Visual-inertial odometry (VIO)</h2><ul>
<li>카메라와 IMU를 사용해서 3D pose + 3D map point를 구하는 방법</li>
<li>최근 graph-optimisation 기반 백엔드가 유행하기 시작했으나, 시중 제품 중에는 아직 filter 기반의 소프트웨어가 많음 (e.g. Google ARCore, Apple ARKit)<ul>
<li>IMU pre-integration 연구 이후 가능해짐<ul>
<li>이전에는 IMU 값을 보정 전/후로 state propagation을 두번 해야했지만, pre-integration 이후로 한번만 해도 되게 만들면서 동시에 수많은 센서 관측치를 단 하나의 관측치로 정리할 수 있게 됨.</li>
</ul>
</li>
<li>Graph-optimisation 기반의 방식이 발전하면서 loop closure 기능을 갖추게 됨<ul>
<li>초창기의 loop closure가 없는 VIO와 구분하기 위해 VINS (Visual-Inertial System)라고 표현하는 방법도 생김 (하지만 정식 단어는 아님)</li>
</ul>
</li>
</ul>
</li>
<li>유명한 오픈소스 프로젝트<ul>
<li>Monocular: MSCKF, OKVIS, ROVIO, VINS-Mono, ORB-SLAM3</li>
<li>Stereo: S-MSCKF, VI-DSO</li>
<li>RGB-D: ElasticFusion+IMU</li>
</ul>
</li>
</ul>
<div class="tabs" id="vio-장점-vs-단점"><ul class="nav-tabs"><li class="tab active"><a href="#vio-장점-vs-단점-1">장점</a></li><li class="tab"><a href="#vio-장점-vs-단점-2">단점</a></li></ul><div class="tab-content"><div class="tab-pane active" id="vio-장점-vs-단점-1"><ul>
<li>카메라와 IMU는 센서 특성상 서로의 단점을 보완함<ul>
<li>e.g. 어두운 곳에서 카메라가 아무것도 보지 못할 때, IMU 센서는 계속 pose 추정 가능 (VIO 설계에 따라 다름)</li>
</ul>
</li>
<li>Monocular의 경우 카메라만 사용했을 때는 얻어낼 수 없는 metric scale 정보를 IMU motion을 통해 추정 가능 (i.e. up-to-scale 맵이 아닌 실제 세상의 scale을 가질 수 있음)</li>
<li>더 많은 최적화 constraint로 얻어내는 높은 정확도</li>
<li>카메라 피쳐 트랙킹을 할 때 모션모델 가설 대신 IMU 센서로 얻은 모션 사용 가능<ul>
<li>실제 센서 값을 사용함으로써 탐색 범위를 축소 가능</li>
</ul>
</li>
</ul></div><div class="tab-pane" id="vio-장점-vs-단점-2"><ul>
<li>더 많은 최적화 constraint로 인한 많은 계산량<ul>
<li>고성능 CPU를 요구할 수 있음</li>
</ul>
</li>
<li>카메라-IMU 간의 정확한 캘리브레이션이 필요<ul>
<li>양산을 할 경우 추가 공정 필요</li>
</ul>
</li>
<li>두 센서 중 하나가 실패하면 전체가 실패함 (VIO 설계에 따라 다름)</li>
</ul></div></div></div>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>50가지 CV/SLAM 기술면접 질문들</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>VIO</tag>
      </tags>
  </entry>
  <entry>
    <title>cv-learn 도메인 부활! (그리고 사라진 방문자 카운트 ㅠㅠ)</title>
    <url>/20210827-new-domain-but-lost-all-view-counts/</url>
    <content><![CDATA[<h2 id="cv-learn-도메인-이전"><a href="#cv-learn-도메인-이전" class="headerlink" title="cv-learn 도메인 이전"></a>cv-learn 도메인 이전</h2><p>옛날의 cv-learn 블로그는 Notion 블로그 기반으로 운영했었다. 아마 이 당시의 블로그가 가장 cv-learn이 잘 알려지게 된 계기였지 않을까 싶다.</p>
<p>그러다가 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjAxMTI0LU5vdGlvbi1ibG9nLXRvLUdpdGh1Yi1ibG9nLw==">여러가지 이유<i class="fa fa-external-link-alt"></i></span>로 새롭게 GitHub 블로그를 시작하게 되었다.</p>
<p>물론 이전 블로그에서 새 블로그로 글을 옮기려고 했지만, 노션-&gt;Hexo로의 이동은 생각보다 큰 작업이였고, 짧은 시간동안 할 수 있는 작업이 아니였다.</p>
<p>그래서 글을 옮기고 또 새 글을 적는동안만큼은 노션 블로그를 유지하기로 했다. 일일 방문자의 양을 고려했을 때 그 편이 더 좋을 것 같았기 때문이다.</p>
<p>그리고 시간이 지나고, GitHub 블로그도 꽤 이제 블로그다운 구실을 갖추게 되었다.</p>
<h2 id="도메인-이전"><a href="#도메인-이전" class="headerlink" title="도메인 이전"></a>도메인 이전</h2><p>CloudFlare 서비스를 통해 도메인을 옮겼다.</p>
<p>생각처럼 쉬운 과정은 아니였지만, 그래도 1~2시간 만에 된거면 만족한다.</p>
<p>하지만 알아챈 것중에 정말 아쉬운 점은…</p>
<p>모든 글들에 대한 조회수 카운트, 블로그 전체 고유 방문자 수, 블로그 전체 사이트 조회 수가 리셋이 된거다.</p>
<p>아마 url에 고정되서 static website 캐싱 작업이 되었던 것 같은데… 새 도메인으로 옮기면 이 데이터가 이전이 안되는 것 같다.</p>
<p>그래서 새출발이 되어버렸다 아뿔싸~~~</p>
<h2 id="그래서"><a href="#그래서" class="headerlink" title="그래서"></a>그래서</h2><p>속상하지만 어쩌겠는가</p>
<p>난 웹개발자가 아니라서 이걸 고칠줄도 모르는걸… 고칠 시간도 없고</p>
<p>그냥 맨업 &amp; 새로 쌓아가기로 한다. (어흐흑)</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.3 Blog</category>
      </categories>
      <tags>
        <tag>cv-learn</tag>
        <tag>에바참치꽁치</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 8월 SLAM 뉴스</title>
    <url>/20210829-2021-august-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> ODAM: Object Detection, Association, and Mapping using Posed RGB Video </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMTAxNjUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Daniel DeTone, Richard Newcombe 참여 (Facebook Reality Labs)</li>
<li>딥러닝 기반 3D object detection을 수행, 이후 GNN을 이용해서 object-based map 생성. Object scale prior와 multiple view geometry 기반 최적화를 (i.e. SLAM) 사용해서 super-quadric 기반 object bounding volume을 최적화함.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Unified Representation of Geometric Primitives for Graph-SLAM Optimization Using Decomposed Quadrics</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDg5NTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Sebastian Scherer 교수님 랩실 연구</li>
<li>Points, lines, planes, ellipsoids, cylinders, cones 등을 하나의 representation으로 표현하는 ‘quadrics’를 사용한다.<ul>
<li>최근 Super-quadrics라던지 많이 사용되는 방식인데, related works와 ‘quadric basics’ 챕터가 굉장히 좋다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> DSP-SLAM: Object Oriented SLAM with Deep Shape Priors </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDk0ODEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9qaW5nd2Vud2FuZzk1LmdpdGh1Yi5pby9kc3Atc2xhbS8=">논문 소개 페이지<i class="fa fa-external-link-alt"></i></span><ul>
<li>CubeSLAM의 진화판?</li>
<li>기존의 ORB-SLAM2 프레임워크에서 추가로 Instance segmentation으로 object를 검출하고 deep shape embedding을 prior로 삼아서 object-level SLAM을 수행하는 것.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Semi-dense visual-inertial odometry and mapping for computationally constrained platforms </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNTE0LTAyMS0xMDAwMi16">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Vijay Kumar 교수님 랩실에서 나온 작품</li>
<li>초경량 ARM 보드에서 작동하는 Semi-dense Stereo VIO.<ul>
<li>VI-Stereo-DSO 보다 더 가볍게 돌아감</li>
<li>Semi-dense 방식이기 때문에 low textured area 에서도 잘 돌아감</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMTA4NjkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>딥러닝 기반 Dense Bundle Adjustment layer를 제안함<ul>
<li>이 layer는 RNN 계열로, camera pose와 pixelwise depth를 업데이트함</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3ByaW5jZXRvbi12bC9EUk9JRC1TTEFN">GitHub 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Comparison of modern open-source visual SLAM approaches</summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDE2NTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>오픈소스 Visual-SLAM을 다 비교해본다!</li>
<li>Related works가 굉장히 좋은 편이다. 이거만 읽어도 왠만한 트렌드 다 파악할듯</li>
<li>Euroc 데이터셋 우승자: Basalt, ORB-SLAM3</li>
<li>TUM-VI 데이터셋 우승자: VINS-Mono, ORB-SLAM3</li>
<li>KITTI 데이터셋 우승자: ORB-SLAM3, OpenVSLAM, ORB-SLAM2</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Multi-Hypothesis Approach to Pose Ambiguity in Object-Based SLAM</summary>

<ul>
<li>John Leonard 교수님 랩실 연구</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDEyMjUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Object의 pose를 계산할 때… 특정 view에서는 여러 pose에 대한 가능성이 제시될 때도 있으나, 네트워크는 하나의 아웃풋을 내도록 설계가 되어있다.<ul>
<li>잘못 추론한 경우 완전 꼬이게 된다</li>
</ul>
</li>
<li>이를 위해 여러개의 가능성을 모두 트랙킹하는 multiple pose hypothesis 기법을 제안한다.</li>
</ul>
</li>
</ul>
</details>

<p> </p>
<h2 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h2><details>
  <summary> PC-SD-VIO: A constant intensity semi-direct monocular visual-inertial odometry with online photometric calibration </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzA5MjE4ODkwMjEwMDE2Mjc=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>auto-exposure, camera response, vignetting 등으로 인해 야외에서 direct 기법은 잘 작동하지 않을때가 있음</li>
<li>Feature-based 방식과 (gain-adaptive KLT optical flow) Direct-방식 (gain-adaptive direct image aligment)를 이용해서 online photometric calibration과 backend optimization을 수행함<ul>
<li>Online photometric calibration이 되기 때문에 야외에서도 잘 되지 않을까!</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Accurate 6D object pose estimation and refinement in cluttered scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9lcHJpbnRzLndoaXRlcm9zZS5hYy51ay8xNzcwMTAv">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Object pose estimation을 할 수 있는 Contour alignment (CA) 기법을 소개함.<ul>
<li>기존의 PnP 방식보다 더 잘된다고…</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Linear RGB-D SLAM for Structured Environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1MjE3NDI=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>권인소 교수님 참여</li>
<li>Manhattan world 에서 효율적으로 작동하는 SLAM을 제안함<ul>
<li>Manhattan world에서는 camera rotation과 structural regularity를 추정하는 것을 따로 떼어놓을 수 있는데, camera rotation을 먼저 구하고 linear kalman filter를 통해 camera translation과 global planar map을 효율적으로 (i.e. linear complexity)하게 풀 수 있음</li>
<li>L-SLAM 이라고 칭함</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Robust Visual-Lidar Simultaneous Localization and Mapping System for UAV </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1MDExNjc=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Visual + LiDAR 시스템을 통해서 point cloud로부터 더 안정적으로 line / plane feature를 추출</li>
<li>이미지 + 포인트 클라우드 데이터를 혼합해서 사용하는 direct odometry를 사용해서 위치 추정</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> SM3D: Simultaneous Monocular Mapping and 3D Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1MDYzMDI=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>딥러닝 기반 Depth estimation을 통해 Pseudo-LiDAR point cloud를 생성하고, 이로부터 3D detector를 사용해서 3D detection + Localization을 수행함</li>
<li>기존의 3D detector만 사용하는 것 보다 더 정확함</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Visual Simultaneous Localization and Mapping (SLAM) Based on Blurred Image Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwODQ2LTAyMS0wMTQ1Ni01">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>VSLAM을 구동할 때 blur된 이미지는 정확도를 저하시키는 요인 중 하나이다.</li>
<li>Blurred image가 있어도 잘 되는 VSLAM을 제안한다.<ul>
<li>Haar wavelet transform을 통해 blur를 제거한다</li>
<li>Correlation-weighted pose optimization을 통해 좋은 결과를 뽑아 내는 백엔드를 제안한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Continual Learning for Image-Based Camera Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDkxMTIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>기존의 딥러닝 기반 Visual localization 네트워크 (i.e. Direct pose regression)을 학습할 때는 data distribution을 크게 고려하지 않는 것 같다.<ul>
<li>이로 인해 catastrophic forgetting 등의 문제가 나타난다.</li>
</ul>
</li>
<li>Continual learning (i.e. incremental training) 학습 방식을 제안한다.<ul>
<li>고정된 버퍼에서 반복적으로 이미지를 보여줌으로써 좋은 baseline을 생성한다</li>
<li>Buff-CS라는 coverage score 기반의 샘플링 기법을 소개한다</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> SBAS: Salient Bundle Adjustment for Visual SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1MTQ1NzA=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Geometric + Semantic 정보를 기반으로 추론한 saliency map을 이용한 VSLAM을 제안한다.<ul>
<li>Saliency prediction이 feature point의 weight 값으로 들어가는 Salient bundle adjustment (SBA)를 제안한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Majorization Minimization Methods for Distributed Pose Graph Optimization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDAwODMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>여러 robust loss kernel과 함께 잘 작동하는 Majorization minimization method for distributed pose graph optimization (MM-PGO) 방법을 소개한다.<ul>
<li>Majorization minimization이 뭐야;;</li>
</ul>
</li>
<li>Multi-Robot PGO 환경에서 사용하는 기법인 것 같음.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Blitz-SLAM: A semantic SLAM in dynamic environments </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAwMzEzMjAzMjEwMDQwNjQ=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>RGB-D 데이터에서 Semantic mask와 뎁스 정보를 이용해서 움직이는 객체를 제외하고 SLAM을 진행하는 방법</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Rethinking Training Objective For Self-Supervised Monocular Depth Estimation: Semantic Cues To Rescue </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1MDY3NDQ=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>(IEEE지만 논문 공개되어있음)</li>
<li>기존의 Self-supervised learning 기반의 Monocular depth estimation 학습을 할 때, semantic 정보까지 함께 넣음으로써 좀 더 정확한 depth 추론을 할 수 있게 함<ul>
<li>기존의 depth estimation은 depth 네트워크와 pose 네트워크를 이용한 후 view synthesis 워핑을 통해 photommetric loss를 기반으로 학습하였음.</li>
<li>여기에 semantic 정보까지 넣음으로써 ambiguity가 생기는 부분들을 semantic 정보로 풀어냄</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Incorporating Learnt Local and Global Embeddings into Monocular Visual SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDIwMjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>딥러닝을 사용한 Visual SLAM의 정석?<ul>
<li>SuperPoint 기반 Local feature extraction</li>
<li>Model uncertainty + direct 방식을 이용한 feature tracking</li>
<li>NetVLAD 기반 global feature extraction</li>
<li>ORB-SLAM 파이프라인 + Bundle adjustment 최적화</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A dual-mode automatic switching feature points matching algorithm fusing IMU data </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL2Ficy9waWkvUzAyNjMyMjQxMjEwMDk2ODQ=">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Feature matching을 할 때 brute-force 매칭과 같은 방식은 너무 오래걸린다</li>
<li>이 논문에서는 IMU를 함께 사용함으로써 global matching &lt;-&gt; local matching 모드를 자동전환하는 방식을 제안한다.<ul>
<li>Global 방식은 brute-force와 같이 모든 local feature를 비교하는 방식이다.</li>
<li>Local 방식은 IMU pre-integration 데이터로 추정한 pose를 통해 다음 이미지 위 가능성이 높은 소수의 feature들과의 매칭을 하는 방식이다.</li>
<li>IMU가 실패하는 것을 감지하기 위해 Support vector machine (SVM) 방식으로 돌아가는 디텍터 알고리즘을 만든다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>2021년 9월 SLAM 뉴스</title>
    <url>/20210910-2021-september-slam-news/</url>
    <content><![CDATA[<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> DnD: Dense Depth Estimation in Crowded Dynamic Indoor Scenes </summary>

<ul>
<li>NAVER LABS 연구</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDU2MTUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>사람들이 많이 돌아다니는 환경에서도 정확하게 depth를 추정할 수 있는 네트워크를 제안<ul>
<li>3D reconstruction으로 미리 생성해둔 3D 모델로부터 추출한 sparse depth map과 RGB 이미지를 사용</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Comprehensive Review of Coverage Path Planning in Robotics Using Classical and Heuristic Algorithms </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj05NTIzNzQz">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Coverage path planning에 대한 서베이 논문</li>
<li>Classical -&gt; Heuristic -&gt; DL 방법까지!</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> The Hilti SLAM Challenge Dataset </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDkuMTEzMTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>짱짱맨 데이터셋은 언제나 환영합니다~~~~</li>
<li>IROS 2021의 SLAM 워크샵 챌린지 데이터셋입니다</li>
</ul>
</details>


<p> </p>
<h2 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h2><details>
  <summary> A stereo matching algorithm based on the improved PSMNet </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9qb3VybmFscy5wbG9zLm9yZy9wbG9zb25lL2FydGljbGU/aWQ9MTAuMTM3MS9qb3VybmFsLnBvbmUuMDI1MTY1Nw==">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>기존의 PSMNet (Pyramid Stereo Matching Network)를 개선해서 weak texture가 있는 곳에서도 잘 작동하게 만들었다.<ul>
<li>ResNeXt를 사용해서 feature를 뽑고, ASPP (Atrous Spatial Pyramid Pooling)을 사용해서 multiscale spatial feature 정보를 뽑는다.</li>
<li>Feature fusion 모델을 사용해서 여러 스케일의 feature 정보를 퓨전하고 matching cost volume을 생성한다.</li>
<li>Stacked encoding / decoding 구조를 사용하는 개선된 3D CNN를 통해 다양한 parallax 환경에서도 correspondence를 찾는다.</li>
<li>마지막으로, regression을 통해 disparity map을 생성한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Category-Level 6D Object Pose Estimation via
Cascaded Relation and Recurrent Reconstruction Networks </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDg3NTUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Cascaded relation 네트워크와 recurrent reconstruction 네트워크를 사용해서 정확한 category-level 6D pose estimation을 한다.<ul>
<li>Cascaded relation 네트워크는 RGB 이미지 &lt;-&gt; 3D point cloud &lt;-&gt; 카테고리의 shape prior를 학습한다.</li>
<li>Recurrent reconstruction 네트워크는 coarse-to-fne 방식으로 correspondence를 추정하고, 이후 iterative하게 reconstruction을 수행하는 네트워크이다.</li>
<li>6D pose는 추정된 3D point cloud와 reconstruction된 3D model의 dense correspondence를 구함으로써 추정할 수 있다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93YW5namlhemUuY24vcHJvamVjdHMvNkRQb3NlRXN0aW1hdGlvbi5odG1s">코드 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Semantic Reinforced Attention Learning for Visual Place Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDg0NDMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>Place recognition을 할 때 모든 픽셀의 정보가 중요한 것은 아니다.</li>
<li>중요한 semantic 정보를 추출하는 Semantic reinforced attention learning network (SRALNet)을 제안한다.<ul>
<li>Semantic prior를 이용해서 local attention에 weight를 더 줄 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDc3MzYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>UnRectDepthNet을 사용해서 dense depth map을 추론</li>
<li>ORB-SLAM2 프레임워크를 사용해서 Feature extraction + RGBD Camera tracking + Keyframe 추출 </li>
<li>추정된 모션 정보를 기반으로 dense surfel map 생성</li>
<li>RGB-D pose refinement 수행 (Joint photometric + geometric alignment)</li>
<li>추정된 모션 정보를 기반으로 dense surfel map 생성<ul>
<li>Joint photometric + geometric alignment (synthetic + live RGB-D 이미지)</li>
</ul>
</li>
<li>Loop closure는 surface-to-surface constraint를 사용해서 deformation graph 생성 후 최적화</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Is Pseudo-Lidar needed for Monocular 3D Object detection? </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDguMDY0MTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span><ul>
<li>DD3D라는 End-to-end single stage monocular 3D object detector를 제안함.<ul>
<li>Pseudo-LiDAR와 같은 기법을 통해 depth pre-training을 정보를 얻어낼 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Benchmark Comparison of Visual Place Recognition Techniques for Resource-Constrained Embedded Platforms </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDkuMTEwMDIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>최신 Visual Place Recognition 기법들을 임베디드-&gt;데탑 보드까지 컴퓨팅 파워/메모리 순서대로 돌려보면서 성능을 비교한 논문<ul>
<li>비교한 보드들은 라즈베리파이3 (ARM), ODROID (ARM), UP (x86_64 아키텍처), 노트북 (하이엔드 노트북), 데스크탑</li>
<li>비교한 기술들은 HOG, CoHOG, HybridNet, CALC, RegionVLAD</li>
</ul>
</li>
<li>전반적인 트렌드: 임베디드 보드의 정확도와 데스크탑의 정확도는 그렇게 크게 차이나지 않았다</li>
<li>좋은 보드: 라즈베리파이 3 (UP, ODROID와 비교해서)<ul>
<li>파워를 제일 적게 먹고, 정확도도 데탑과 비슷, 속도도 괜찮은 편</li>
</ul>
</li>
<li>임베디드 보드 트렌드: descriptor size 때문에 메모리를 많이 먹는건 어쩔 수 없음… swap space를 추가하기 전까지는 성능이 안나옴</li>
<li>알고리즘 트렌드: 파이와 ODROID에서는 CALC가 제일 전력을 많이 소비하고 RegionVLAD가 제일 적게 소비함. UP에서는 CoHOG가 전력을 제일 많이 소비하는데 아무래도 x86_64 아키텍처에서는 모든 코어를 다 돌리느라 그런 듯.</li>
<li>저자의 의견: 여러 플랫폼에 올라가는 SDK를 만들 때에는 사용하는 하드웨어에 적절한 알고리즘을 고를 수 있는 로직을 넣어두는게 좋을 것 같다. 새로운 보드가 나올 때에는 RAM의 크기가 커졌는지 보는게 리얼타임을 구현하는데에 중요할 것이다 (라즈베리파이4가 그런면에서 굉장히 좋다). </li>
</ul>
</details>

<details>
  <summary> RGB-D DSO: Direct Sparse Odometry with RGB-D Cameras for Indoor Scenes </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk1NDY1MzQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>기존의 DSO 모듈에 RGB-D 카메라로부터 얻은 depth 값을 이용해서 occlusion removal과 depth refinement를 추가해서 성능을 더 높인 논문<ul>
<li>Occlusion removal을 함으로써 energy function 최적화 할 때 생길 수 있는 문제를 회피함</li>
<li>Depth refinement를 함으로써 keyframe에 골고루 depth 값이 분포할 수 있도록 함</li>
</ul>
</li>
<li>Monocular에서는 불안정하게 얻는 depth 값을 그냥 센서 값으로 대체함으로써 훨씬 안정적으로 만든 것 (조금 뻔한?)</li>
</ul>
</details>

<details>
  <summary> Semantic Segmentation in the Task of Long-Term Visual Localization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDcvOTc4LTMtMDMwLTg3NzI1LTVfMw==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2N2Zy9IaWVyYXJjaGljYWwtTG9jYWxpemF0aW9u">Hierarchical Localization (hloc)<i class="fa fa-external-link-alt"></i></span> 을 실행할 때 fine-tuning으로 성능이 더 올라갈 수 있는 것을 확인</li>
<li>그리고, 이미지 속 어떤 부분이 localization에 전혀 도움이 안되는 부분인지 semantic segmentation 학습을 통해 분석 가능 (데이터셋 분석용으로 좋을듯?)</li>
</ul>
</details>

<details>
  <summary> Infrastructure Node-based Vehicle Localization for Autonomous Driving </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDkuMTA0NTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>

<details>
  <summary> </summary>

<ul>
<li><a href="">논문 링크</a></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>(ROS 삽질기 - 0) Docker 환경에서 ROS 돌리기</title>
    <url>/20210912-ros-tutorials-1/</url>
    <content><![CDATA[<h2 id="왜-Docker에서-ROS를-돌리는가"><a href="#왜-Docker에서-ROS를-돌리는가" class="headerlink" title="왜 Docker에서 ROS를 돌리는가?"></a>왜 Docker에서 ROS를 돌리는가?</h2><ul>
<li>(적어도 내가 알기로는) ROS는 시스템에 설치되어야하는 소프트웨어이다.</li>
<li>SLAM을 공부하면서 여러 오픈소스 SLAM들을 돌려보게 되는데, 이 <strong>SLAM 알고리즘들이 공개된 시기에 따라 ROS 디펜던시의 버전이 다르다</strong><ul>
<li>Ubuntu 14.04 + ROS Indigo</li>
<li>Ubuntu 16.04 + ROS Kinetic</li>
<li>Ubuntu 18.04 + ROS Melodic</li>
<li>Ubuntu 20.04 + ROS Noetic</li>
</ul>
</li>
<li>당연히 ROS 버전들끼리 충돌이 난다.<ul>
<li>추가로, OpenCV와 Eigen과 같은 라이브러리들의 버전들도 충돌날것이다.</li>
<li>이럴 때는 <strong>각각의 알고리즘마다 독립된 가상환경을 만들어두면 좋다</strong></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Docker-ROS를-쓸-때-고려해야하는-점"><a href="#Docker-ROS를-쓸-때-고려해야하는-점" class="headerlink" title="Docker + ROS를 쓸 때 고려해야하는 점"></a>Docker + ROS를 쓸 때 고려해야하는 점</h2><ul>
<li>Docker를 통해 개발을 할 때는 <strong>원격 코드 접근</strong>이 필요하다<ul>
<li>이는 SSH나 VSCode의 Docker extension을 사용해서 디버깅을 할 수 있다. </li>
</ul>
</li>
<li>ROS를 사용할 때 <strong>GUI로 결과를 보는 경우</strong>가 많다<ul>
<li>VNC, X11, WayLand 등으로 시각화된 결과물을 보여줄 수 있다.</li>
</ul>
</li>
<li>rosrun 기반으로 돌릴 때는 <strong>여러 터미널</strong>을 열 수 있어야한다.<ul>
<li>tmux, terminator 등으로 하나의 컨테이너에 여러 bash 터미널을 접근해야한다.<ul>
<li>(아니면 그냥 터미널 많이 열어도 되구…)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><blockquote>
<p>tmux를 좀 더 쉽게 쓰고 싶으면 필자가 이전에 적은 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjEwMzI2LXRtdXgv">tmux 글<i class="fa fa-external-link-alt"></i></span>을 참조하면 좋다.</p>
</blockquote>
<h3 id="tmux-설치"><a href="#tmux-설치" class="headerlink" title="tmux 설치"></a>tmux 설치</h3><p>tmux는 여러개의 터미널을 하나의 윈도우에 띄울 수 있는 유용한 툴이다.</p>
<p>아래의 커맨드를 사용해서 tmux를 설치한다</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install tmux</span><br></pre></td></tr></table></figure>

<h3 id="tmux-사용해서-터미널-쪼개기"><a href="#tmux-사용해서-터미널-쪼개기" class="headerlink" title="tmux 사용해서 터미널 쪼개기"></a>tmux 사용해서 터미널 쪼개기</h3><p>설치가 끝나면 터미널에서 <code>tmux</code> 커맨드를 입력하여 tmux를 실행한다.<br>이후 <code>ctrl + b</code>를 누르고 <code>"</code>를 누르면 하나의 터미널이 위/아래로 쪼개지고, <code>ctrl + b</code>를 누른 후 <code>%</code>를 누르면 하나의 터미널이 좌/우로 쪼개진다.</p>
<p>터미널을 옮겨다니려면 <code>ctrl + b</code>를 누르고 화살표 키를 눌러서 움직일 수 있다.</p>
<p>위/아래와 좌/우를 한번씩 쪼개면 아래의 사진처럼 큰 윈도우가 4개로 쪼개진다.</p>
<img src="/20210912-ros-tutorials-1/tmux.png" class="" title="tmux">

<p> </p>
<hr>
<h2 id="X11-포워딩-환경-준비하기"><a href="#X11-포워딩-환경-준비하기" class="headerlink" title="X11 포워딩 환경 준비하기"></a>X11 포워딩 환경 준비하기</h2><p>필자의 PC 환경은 Nvidia GPU를 사용하는 PC로, GPU 드라이버는 기본 Ubuntu 20.04가 잡아준 것을 사용하고있다.</p>
<p>우선 호스트 PC 환경에서 다음과 같은 커맨드를 사용해서 Ubuntu GUI 패키지를 설치해준다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install xorg xrdp xserver-xorg mesa-utils xauth gdm3</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="Docker-설치"><a href="#Docker-설치" class="headerlink" title="Docker 설치"></a>Docker 설치</h2><p>Docker 설치에 관한 부분은 필자가 이전에 적은 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjEwODA4LWRvY2tlci1mb3Itc2xhbS8=">Docker 글<i class="fa fa-external-link-alt"></i></span>을 참조하면 좋다.</p>
<p> </p>
<hr>
<h2 id="OSRF-ROS-Docker-이미지"><a href="#OSRF-ROS-Docker-이미지" class="headerlink" title="OSRF ROS Docker 이미지"></a>OSRF ROS Docker 이미지</h2><h3 id="이미지-땡겨오기"><a href="#이미지-땡겨오기" class="headerlink" title="이미지 땡겨오기"></a>이미지 땡겨오기</h3><p>이번 글에서는 우선 빠르게 결과를 보기 위해, Open Source Robotics Foundation (이하 OSRF)에서 제작한 ROS docker 이미지를 사용한다.</p>
<p>Base ubuntu 이미지에 ROS를 직접 설치할 수도 있지만, 이 방식은 다음 글에서 설명하도록 한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &lt;DISTRO&gt;는 원하는대로 바꿀 수 있다. indigo / kinetic / melodic / noetic 등등...</span></span><br><span class="line">docker run -it \</span><br><span class="line">    --env=<span class="string">"DISPLAY"</span> \</span><br><span class="line">    --env=<span class="string">"QT_X11_NO_MITSHM=1"</span> \</span><br><span class="line">    --volume=<span class="string">"/tmp/.X11-unix:/tmp/.X11-unix:rw"</span> \</span><br><span class="line">    osrf/ros:&lt;DISTRO&gt;-desktop-full</span><br><span class="line"><span class="built_in">export</span> containerId=$(docker ps -l -q)</span><br></pre></td></tr></table></figure>

<h3 id="ROS-환경-활성화-roscore-실행"><a href="#ROS-환경-활성화-roscore-실행" class="headerlink" title="ROS 환경 활성화 + roscore 실행"></a>ROS 환경 활성화 + roscore 실행</h3><p>이 커맨드를 입력하고나면 DockerHub로부터 해당 Distro의 ROS가 포함된 Docker 이미지를 다운받고 컨테이너를 생성한 후 실행한다.</p>
<p>실행된 컨테이너에서 <code>source /opt/ros/&lt;DISTRO&gt;/setup.bash</code>를 통해 ROS 환경을 읽어준 후, <code>printenv | grep ROS</code>를 사용해서 정확하게 환경이 읽혔는지 확인한다.</p>
<p>ROS 환경이 잘 읽힌게 확인이 되면 <code>roscore</code> 커맨드를 통해 ros 서버를 실행시킨다.</p>
<p> </p>
<hr>
<h2 id="TurtleSim을-통해-GUI-앱-렌더링-확인"><a href="#TurtleSim을-통해-GUI-앱-렌더링-확인" class="headerlink" title="TurtleSim을 통해 GUI 앱 렌더링 확인"></a>TurtleSim을 통해 GUI 앱 렌더링 확인</h2><h3 id="X11-포워딩-열어주기"><a href="#X11-포워딩-열어주기" class="headerlink" title="X11 포워딩 열어주기"></a>X11 포워딩 열어주기</h3><p>tmux 기능을 이용해서 새로운 터미널로 옮긴다 (i.e.<code>ctrl + b</code> + <code>-&gt;</code>).</p>
<p>Host PC에서 X11 포워딩을 열어주기 위해 <code>xhost +loal:root</code> 커맨드를 입력해서 포워딩을 열어준다.</p>
<h3 id="새-터미널-열기"><a href="#새-터미널-열기" class="headerlink" title="새 터미널 열기"></a>새 터미널 열기</h3><p><code>docker ps -a</code>를 통해 ROS Docker 컨테이너의 이름을 알아낸다. 이 이름을 <container_name>이라고 하겠다.</container_name></p>
<p><code>docker exec -it &lt;CONTAINER_NAME&gt; bash</code> 커맨드를 통해 해당 컨테이너에서 터미널을 열 수 있다.</p>
<p>우선 가장 먼저 <code>source /opt/ros/&lt;DISTRO&gt;/setup.bash</code>를 통해 ROS 환경을 읽어주고, <code>printenv | grep ROS</code>를 사용해서 정확하게 환경이 읽혔는지 확인한다.</p>
<h3 id="TurtleSim-설치하기"><a href="#TurtleSim-설치하기" class="headerlink" title="TurtleSim 설치하기"></a>TurtleSim 설치하기</h3><p>TurtleSim을 바로 돌리면 좋겠지만, 이 컨테이너에는 아직 turtlesim이 없으므로 아래 커맨드를 통해 직접 설치해줘야한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install ros-&lt;DISTRO&gt;-ros-tutorials</span><br></pre></td></tr></table></figure>

<p>이후, <code>rosrun turtlesim turtlesim_node</code> 커맨드를 통해 Turtlesim을 실행시키면 GUI 앱이 렌더링 될 것이다.</p>
<p>다시 한번 tmux 기능을 이용해서 새로운 터미널로 옮긴다.</p>
<p><code>docker exec -it &lt;CONTAINER_NAME&gt; bash</code> 커맨드를 이용해 새 터미널을 열어주고, 다시 <code>source /opt/ros/&lt;DISTRO&gt;/setup.bash</code> 커맨드를 이용해서 ROS 환경을 읽어준다.</p>
<p>이후 <code>rosrun turtlesim teleop_key</code> 커맨드를 이용해서 TurtleSim의 거북이를 움직일 수 있는 node를 실행시킨다.</p>
<p>이제 방향키를 움직여서 거북이가 움직이는 것을 볼 수 있다.</p>
<h3 id="결과-확인"><a href="#결과-확인" class="headerlink" title="결과 확인"></a>결과 확인</h3><img src="/20210912-ros-tutorials-1/ros.png" class="" title="ros">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Best Practices (1) - Unit test를 쓰시죠!</title>
    <url>/20210925-cpp-best-practices-automated-tests/</url>
    <content><![CDATA[<h2 id="기존의-개발-방법"><a href="#기존의-개발-방법" class="headerlink" title="기존의 개발 방법"></a>기존의 개발 방법</h2><p>보통 우리가 코드를 개발하는 방법은 다음과 같습니다.</p>
<ul>
<li>생각한다.</li>
<li>코드로 옮긴다.</li>
<li>주어진 환경에서 작동하는지 확인한다.</li>
<li>될때까지 수정한다.</li>
</ul>
<p>내가 짠 코드가 작동하는걸 보고 신나서 스크린샷 찍은 다음, GitHub Pull Request에 ‘잘 돌아갑니다~~’ 하고 스크린샷을 첨부합니다.</p>
<p>근데 이 코드가 정말 좋은 코드일까요?</p>
<p><strong>코드가 딱 정해진 상황에서만 의도했던대로 작동해야하지, 그 외의 상황에서도 작동한다면 그건 버그입니다</strong>.</p>
<p>내가 딱 원하는 상황에서만 돌아가는 코드라고 확신할 수 있나요?</p>
<p>비슷하지만 조금 다른 환경에서는 작동하지 않을거라는 확신이 있나요?</p>
<img src="/20210925-cpp-best-practices-automated-tests/code.jpg" class="" title="code">

<p>&nbsp;</p>
<hr>
<h2 id="자동-테스트-프레임워크"><a href="#자동-테스트-프레임워크" class="headerlink" title="자동 테스트 프레임워크"></a>자동 테스트 프레임워크</h2><blockquote>
<p>‘내 코드가 제대로 작동하는 코드인가?’에 대한 답변</p>
</blockquote>
<p>C++ 코드를 테스트하기 위한 프레임워크들이 있습니다.</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NhdGNob3JnL0NhdGNoMg==">Catch2<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29ucXRhbS9kb2N0ZXN0">doctest<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS9nb29nbGV0ZXN0">Google Test<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuYm9vc3Qub3JnL2RvYy9saWJzLzFfNzVfMC9saWJzL3Rlc3QvZG9jL2h0bWwvaW5kZXguaHRtbA==">Boost.Test<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>테스트 프레임워크를 처음 써보는거라면 <strong>제가 만든</strong> <a href="https://github.com/changh95/gtest_sample"><strong>Google Test 프레임워크를 사용한 데모</strong></a>를 써보시는걸 강력 추천합니다.</p>
<p>프레임워크는 아무거나 써도 상관이 없습니다만, 하나에 묶여서 다른 프레임워크를 사용하는데에 거부감이 느껴지면 안됩니다.</p>
<p>코드 퀄리티를 높이기 위해 테스트 프레임워크는 <strong>필수</strong>라고 보셔야합니다.</p>
<p>CppCast에서 Oleg Rabaev는 이렇게 말했습니다.</p>
<ul>
<li>당신의 코드가 테스트하기 어려운 코드라면, 그 코드는 제대로 설계가 된 코드가 아닐것이다.</li>
<li>당신의 코드가 테스트하기 쉬운 코드라면, 그 코드는 제대로 설계가 되었다는 신호를 보여준다.</li>
<li><strong>당신의 코드가 제대로 설계가 되었다면, 그 코드는 테스트하기 쉬울 것이다</strong>.</li>
</ul>
<img src="/20210925-cpp-best-practices-automated-tests/result.png" class="" title="result">

<p>&nbsp;</p>
<hr>
<h2 id="실전"><a href="#실전" class="headerlink" title="실전"></a>실전</h2><p>현재 작성중인 코드에 자동 테스트 프레임워크를 적용해보세요.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>C++ Best Practices 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Modern C++</tag>
        <tag>cpp</tag>
        <tag>TDD</tag>
        <tag>C++ Best Practices</tag>
        <tag>Unit test</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Best Practices (0) - 시작하면서...</title>
    <url>/20210925-cpp-best-practices/</url>
    <content><![CDATA[<h2 id="시리즈를-시작하면서"><a href="#시리즈를-시작하면서" class="headerlink" title="시리즈를 시작하면서"></a>시리즈를 시작하면서</h2><p>제 주변에는 기업/대학원에서 연구를 하시는 분들이 많이 계십니다.</p>
<p>공부도 많이 하시면서 state-of-the-art 알고리즘을 개발하시는 분들이 많으신데, 굉장히 의외인 점을 하나 발견했습니다.</p>
<blockquote>
<p>Software engineering에 약하다.</p>
</blockquote>
<p>왜 그런걸까요?</p>
<p>개인적으로 느끼기에는 많은 분들이 대학교-&gt;대학원-&gt;연구소 테크트리를 타면서, 기업에서 중요시하는 소프트웨어 엔지니어링을 겪지 못하면서 생기는 문제인 것 같습니다.</p>
<p>특히나 컴퓨터 공학을 전공하지 않아도 진학할 수 있지만, C++ 코딩을 많이 해야하는 분야에서 이러한 모습이 많이 보입니다 (e.g. 로보틱스 분야 -&gt; 전기전자공학 졸업생)</p>
<p>Software engineering을 공부해도 좋지만, 대부분의 사람들은 시간이 없습니다.</p>
<img src="/20210925-cpp-best-practices/cpp.jpg" class="" title="cpp">

<p>&nbsp;</p>
<p>그러므로 이번 시리즈에서는 알짜배기 느낌으로 C++ 언어로 프로그래밍을 할 때 권장되는 기법에 대해 소개합니다.</p>
<p>이 기법들을 사용했을 때 기대할 수 있는 점들은…</p>
<ul>
<li><strong>초보가 하기 쉬운 실수들을 피할 수 있습니다</strong></li>
<li><strong>코드에서 버그를 찾기 쉬워집니다</strong></li>
<li><strong>좋은 성능을 유지하면서 코드 퀄리티를 높일 수 있습니다.</strong></li>
</ul>
<p>이 시리즈를 적을 때 <span class="exturl" data-url="aHR0cHM6Ly9sZWFucHViLmNvbS9jcHBiZXN0cHJhY3RpY2Vz">Jason Turner의 C++ Best Practices<i class="fa fa-external-link-alt"></i></span> 책의 목차를 많이 참고했습니다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="C-코딩의-특징"><a href="#C-코딩의-특징" class="headerlink" title="C++ 코딩의 특징"></a>C++ 코딩의 특징</h2><p>여러분이 C++로 코드를 짜는 이유는 단 하나 때문일겁니다.</p>
<p><strong><em>성능</em></strong> 때문이지요.</p>
<p>C++은 굉장히 low-level한 수준까지 코드를 짤 수 있게 해주지만, 그만큼 고려해야할 부분이 많고 실수하기 쉽습니다.</p>
<p>다른 언어보다 더 좋은 성능을 얻기 위해 C++을 썼지만, 더 위험할수도 있다는 것이지요 (하이리스크, 하이리턴이라고도 볼 수 있습니다).</p>
<p>우리의 C++ 코드에서 실수가 있을 때, 실제 사용처에서는 이러한 경우가 나타납니다.</p>
<ul>
<li>게임분야 : 버그가 나타납니다. 게임성이 떨어질수도 있고, 게임이 꺼질수도 있고, 해킹을 당할수도 있습니다.</li>
<li>금융분야 : 잘못된 결정을 내려 돈을 잃습니다. 내 결정으로 인해 시장의 다른 결정들도 영향을 받을 수도 있습니다.</li>
<li>로보틱스 : 당신의 로봇/드론이 사고를 겪어 더 움직일 수 없습니다.</li>
<li>자율주행/우주항공 : 당신의 자동차/로켓이 사고를 겪어 폐기해야합니다. 인명사고가 있을 수 있습니다.</li>
</ul>
<p>이러한 C++의 특성을 고려했을 때, 코드 퀄리티를 높이고 좋은 코드를 적어야하는건 중요합니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>C++ Best Practices 시리즈</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Modern C++</tag>
        <tag>cpp</tag>
        <tag>C++ Best Practices</tag>
      </tags>
  </entry>
  <entry>
    <title>ICCV 2021 튜토리얼 / 워크샵 리스트</title>
    <url>/20211010-ICCV2021/</url>
    <content><![CDATA[<h2 id="Workshops-Tutorials"><a href="#Workshops-Tutorials" class="headerlink" title="Workshops / Tutorials"></a>Workshops / Tutorials</h2><h3 id="월요일"><a href="#월요일" class="headerlink" title="월요일"></a>월요일</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbWxhZC1pY2N2MjAyMQ==">Map-Based Localization for Autonomous Driving<i class="fa fa-external-link-alt"></i></span><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1wMDAtS09JbkdtYyZhYl9jaGFubmVsPWN2cHJ0dW0=">Youtube 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>10월 11일 월요일 Full Day</li>
<li>이건 꼭 봐야지~~</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9uZXVyYWwtYXJjaGl0ZWN0dXJlLXBwZi5naXRodWIuaW8v">Neural Architectures: Past, Present and Future<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 11일 월요일 Full Day</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ob2xpc3RpYy0zZC5naXRodWIuaW8vaWNjdjIxLw==">Holistic Structures for 3D Vision<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 11일 월요일 Half Day Morning</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3VuaXRuLml0LzNkb2Rp">Workshop on 3D Object Detection from Images<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 11일 월요일 Half Day Morning</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cua2ktZGVsdGFsZWFybmluZy5kZS9ldmVudC9lcmN2YWQyMDIx">Workshop on “Embedded and Real-World Computer Vision in Autonomous Driving”<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 11일 월요일 Half Day Morning</li>
</ul>
</li>
</ul>
<h3 id="토요일"><a href="#토요일" class="headerlink" title="토요일"></a>토요일</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly94ci5jb3JuZWxsLmVkdS93b3Jrc2hvcC8yMDIx">Fifth Workshop on Computer Vision for AR/VR<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Full Day</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ob2xpc3RpYy12aWRlby11bmRlcnN0YW5kaW5nLmdpdGh1Yi5pby90dXRvcmlhbHMvaWNjdjIwMjEuaHRtbA==">Second Tutorial on Large Scale Holistic Video Understanding<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Full Day</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvZGxnYy13b3Jrc2hvcC1pY2N2MjAyMS8=">Deep Learning for Geometric Computing<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Full Day</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9yZXNlYXJjaC5tYXBpbGxhcnkuY29tL01ldHJvcG9saXNUdXRvcmlhbA==">Benchmarking City-Scale Semantic 3D Map Making with Mapillary Metropolis<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Half Day Afternoon 튜토리얼</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvcGVyY2VwdGlvbmF0bWFnaWNsZWFwLw==">Building Digital Twins for Large Scale Augmented Reality<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Half Day Afternoon 튜토리얼</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvdHJhZGljdg==">1st Workshop on Traditional Computer Vision in the Age of Deep Learning (TradiCV)<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Half Day Afternoon</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zc2xhZDIwMjEuZ2l0aHViLmlvL2luZGV4Lmh0bWw=">Self-supervised Learning for Next-Generation Industry-level Autonomous Driving<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 16일 토요일 Half Day Afternoon</li>
</ul>
</li>
</ul>
<h3 id="일요일"><a href="#일요일" class="headerlink" title="일요일"></a>일요일</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hdnZpc2lvbi54eXovaWNjdjIxLw==">2nd AVVision Workshop<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 17일 일요일 Full Day</li>
<li>이건 꼭 봐야지~~</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvdGFnLWN2LTIwMjE=">Topology, Algebra and Geometry in Computer Vision<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 17일 일요일 Full Day</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbHN2cHIyMDIxL2hvbWU=">Large-Scale Visual Localization<i class="fa fa-external-link-alt"></i></span><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1SYVZQaUlHaGRXayZhYl9jaGFubmVsPUdpb3Jnb3NUb2xpYXM=">Youtube 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>10월 17일 일요일 Half Day Morning 튜토리얼</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmV0cm9jYXVzYWwuYWkvY3Y0ZmY=">ICCV Workshop on Computer Vision for the Factory Floor<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 17일 일요일 Half Day Morning</li>
</ul>
</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbHR2bDIwMjEv">Long-Term Visual Localization under Changing Conditions<i class="fa fa-external-link-alt"></i></span><ul>
<li>10월 17일 일요일 Half Day Afternoon</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>ICCV</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>LiDAR SLAM 논문 리스트</title>
    <url>/20211201-lidar-slam/</url>
    <content><![CDATA[<h2 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h2><p>이 글의 출처는 <span class="exturl" data-url="aHR0cHM6Ly93d3cubm90aW9uLnNvL0xpREFSLVNMQU0tMjk1MTQ3MDQwNjgzNDgzZDhjOGM1ODc3NmE0N2Y5Yzc=">Giseop Kim 님의 Notion 글<i class="fa fa-external-link-alt"></i></span>입니다. 그냥 다 그대로 가져온 수준이에요(…)</p>
<p>기섭님 <span class="exturl" data-url="aHR0cHM6Ly9naXNiaS1raW0uZ2l0aHViLmlvLw==">블로그<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vY2hhbm5lbC9VQ3JtVk1KM0tFRmJERDlFdG5BbURUNmc=">유투브<i class="fa fa-external-link-alt"></i></span>에도 좋은 슬램 관련 글과 영상이 있습니다.</p>
<h2 id="Paper-List"><a href="#Paper-List" class="headerlink" title="Paper List"></a>Paper List</h2><ul>
<li>1987 PAMI <span class="exturl" data-url="aHR0cHM6Ly93d3cuZWNlLnF1ZWVuc3UuY2EvcGVvcGxlL1MtRC1CbG9zdGVpbi9wYXBlcnMvUEFNSS0zRExTLTE5ODcucGRm">Least-squares fitting of two 3D point sets<i class="fa fa-external-link-alt"></i></span><ul>
<li>SVD-based closed form of registration; a basic of basic of the scan matching  </li>
</ul>
</li>
<li>92 PAMI <span class="exturl" data-url="aHR0cDovL2dyYXBoaWNzLnN0YW5mb3JkLmVkdS9jb3Vyc2VzL2NzMTY0LTA5LXNwcmluZy9IYW5kb3V0cy9wYXBlcl9pY3AucGRm">A method for registration of 3-D shapes<i class="fa fa-external-link-alt"></i></span><ul>
<li>ICP - a basic of the scan matching</li>
</ul>
</li>
<li>97 AR <span class="exturl" data-url="aHR0cDovL2NpdGVzZWVyeC5pc3QucHN1LmVkdS92aWV3ZG9jL2Rvd25sb2FkP2RvaT0xMC4xLjEuMzAuODg4OCZyZXA9cmVwMSZ0eXBlPXBkZg==">Globally Consistent Range Scan Alignment for Environment Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>mostly called as “Lu and Milios”; considered as the first work of a scan matching and pose graph optimization-based SLAM</li>
</ul>
</li>
<li>03 IROS <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEyNDkyODU=">The Normal Distributions Transform: A New Approach to Laser Scan Matching<i class="fa fa-external-link-alt"></i></span><ul>
<li>NDT registration</li>
</ul>
</li>
<li>06 IJRR <span class="exturl" data-url="aHR0cHM6Ly93d3cuY2MuZ2F0ZWNoLmVkdS9+ZGVsbGFlcnQvcHViL0RlbGxhZXJ0MDZpanJyLnBkZg==">Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing<i class="fa fa-external-link-alt"></i></span><ul>
<li>from probabilistic nature to least square formulation of SLAM for smoothing (i.e., modification of past poses)</li>
</ul>
</li>
<li>07 JFR <span class="exturl" data-url="aHR0cDovLzEzMC4yNDMuMTA1LjQ5L1Jlc2VhcmNoL21yby9wdWJsaWNhdGlvbnMvMjAwNy9NYWdudXNzb25fZXRhbF8yMDA3LUpGUi0zRF9TY2FuX1JlZ2lzdHJhdGlvbl9mb3JfQXV0b25vbW91c19NaW5pbmdfVmVoaWNsZXMucGRm">Scan registration for autonomous mining vehicles using 3D-NDT<i class="fa fa-external-link-alt"></i></span><ul>
<li>3D version of NDT registration</li>
</ul>
</li>
<li>08 TRO <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy9wdWIvS2Flc3MwOHRyby5wZGY=">iSAM: Incremental Smoothing and Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>incremental SAM and an open source library</li>
</ul>
</li>
<li>09 ICRA <span class="exturl" data-url="aHR0cHM6Ly9hcHJpbC5lZWNzLnVtaWNoLmVkdS9wZGZzL29sc29uMjAwOWljcmEucGRm">Real-Time Correlative Scan Matching<i class="fa fa-external-link-alt"></i></span><ul>
<li>prof. Olson; later affects  to Cartographer, etc</li>
</ul>
</li>
<li>09 ICRA <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3ZsLmlpcy51LXRva3lvLmFjLmpwL2NsYXNzMjAxNi8yMDE2dy9wYXBlcnMvNi4zRGRhdGFQcm9jZXNzaW5nL1J1c3VfRlBGSF9JQ1JBMjAwOS5wZGY=">Fast Point Feature Histograms (FPFH) for 3D Registration<i class="fa fa-external-link-alt"></i></span><ul>
<li>FPFH (the most famous 3D local descriptor) registration</li>
</ul>
</li>
<li>09 RSS <span class="exturl" data-url="aHR0cHM6Ly93d3cucm9ib3RzLm94LmFjLnVrL35hdnNlZ2FsL3Jlc291cmNlcy9wYXBlcnMvR2VuZXJhbGl6ZWRfSUNQLnBkZg==">Generalized-ICP<i class="fa fa-external-link-alt"></i></span><ul>
<li>uncertainty-embedded ICP (probabilistic perspective)</li>
</ul>
</li>
<li>10 ITSM <span class="exturl" data-url="aHR0cDovL3d3dzIuaW5mb3JtYXRpay51bmktZnJlaWJ1cmcuZGUvfnN0YWNobmlzL3BkZi9ncmlzZXR0aTEwdGl0c21hZy5wZGY=">A Tutorial on Graph-Based SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>Grisetti’s must-read tutorial</li>
</ul>
</li>
<li>11 IV <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzU5NDAzOTY=">Velodyne SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>an early work of modern 3D scanning LiDAR-based motion estimation</li>
</ul>
</li>
<li>12 TRO <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzYyMjA5MDA=">Zebedee: Design of a Spring-Mounted 3-D Range Sensor with Application to Mobile Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>mobile mapping system and IMU fusion</li>
</ul>
</li>
<li>12 RAM <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzYyOTkxNjY=">Tutorial: Point Cloud Library: Three-Dimensional Object Recognition and 6 DOF Pose Estimation<i class="fa fa-external-link-alt"></i></span><ul>
<li>PCL tutorial, but not much delved into the SLAM perspective.</li>
</ul>
</li>
<li>12 IJRR <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MuY211LmVkdS9+a2Flc3MvcHViL0thZXNzMTJpanJyLnBkZg==">iSAM2: Incremental smoothing and mapping using the Bayes tree<i class="fa fa-external-link-alt"></i></span><ul>
<li>in GTSAM 4.0, iSAM2 (not iSAM1) is currently a de-facto default factor graph optimizer.</li>
</ul>
</li>
<li>13 ICRA <span class="exturl" data-url="aHR0cHM6Ly92aXNpb24uaW4udHVtLmRlL19tZWRpYS9zcGV6aWFsL2JpYi9rZXJsMTNpY3JhLnBkZg==">Robust Odometry Estimation for RGB-D Cameras<i class="fa fa-external-link-alt"></i></span><ul>
<li>DVO; this is not an actually LiDAR thing, but to understand the effectiveness of direct alignment rather ICP</li>
</ul>
</li>
<li>13 IROS <span class="exturl" data-url="aHR0cHM6Ly9qc3R1cm0uZGUvcHVibGljYXRpb25zL2RhdGEva2VybDEzaXJvcy5wZGY=">Dense Visual SLAM for RGB-D Cameras<i class="fa fa-external-link-alt"></i></span><ul>
<li>a SLAM version (i.e., including loop closures) of the DVO; studying RGB-D SLAMs is also worthy for LiDAR guys because they frequently considers the both a photometric error and a geometric error</li>
</ul>
</li>
<li>13 AR <span class="exturl" data-url="aHR0cHM6Ly9oYWwuYXJjaGl2ZXMtb3V2ZXJ0ZXMuZnIvaGFsLTAxMTQzNDU0L2RvY3VtZW50">Challenging data sets for point cloud registration algorithms<i class="fa fa-external-link-alt"></i></span><ul>
<li>a.k.a the open library: Libpointmatcher*</li>
</ul>
</li>
<li>14 RSS <span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS9wdWJfZmlsZXMvMjAxNC83L0ppX0xpZGFyTWFwcGluZ19SU1MyMDE0X3Y4LnBkZg==">LOAM: Lidar Odometry and Mapping in Real-time<i class="fa fa-external-link-alt"></i></span><ul>
<li>THE LOAM; (surface and corner) feature matching for frame-to-frame registration and frame-to-map refinement</li>
</ul>
</li>
<li>15 ICRA <span class="exturl" data-url="aHR0cHM6Ly9mcmMucmkuY211LmVkdS9+emhhbmdqaS9wdWJsaWNhdGlvbnMvSUNSQV8yMDE1LnBkZg==">Visual-lidar Odometry and Mapping: Low-drift, Robust, and Fast<i class="fa fa-external-link-alt"></i></span><ul>
<li>Visual + LOAM</li>
</ul>
</li>
<li>15 ICRA <span class="exturl" data-url="aHR0cHM6Ly93d3cuY2MuZ2F0ZWNoLmVkdS9+ZGVsbGFlcnQvcHViL0NhcmxvbmUxNWljcmEucGRm">Initialization Techniques for 3D SLAM: a Survey on Rotation Estimation and its Use in Pose Graph Optimization<i class="fa fa-external-link-alt"></i></span><ul>
<li>LiDAR SLAM usually have more opportunity to think of the better pose graph optimization because it directly measures the depth (rather easier front-end than visual domain).</li>
</ul>
</li>
<li>15 RAM <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWlzLnVuaS1ib25uLmRlL3BhcGVycy9SQU1fMjAxNV9Ib2x6X1BDTF9SZWdpc3RyYXRpb25fVHV0b3JpYWwucGRm">Registration with the Point Cloud Library: A Modular Framework for Aligning in 3-D<i class="fa fa-external-link-alt"></i></span><ul>
<li>PCL tutorial for registration</li>
</ul>
</li>
<li>15 IROS [NICP: Dense normal based point cloud registration<ul>
<li>as already in the title, dense normal](<span class="exturl" data-url="aHR0cDovL2phY29wb3NlcmFmaW4uY29tL3dwLWNvbnRlbnQvdXBsb2Fkcy9zZXJhZmluMTVpcm9zLnBkZg==">http://jacoposerafin.com/wp-content/uploads/serafin15iros.pdf<i class="fa fa-external-link-alt"></i></span>)</li>
</ul>
</li>
<li>16 ICRA <span class="exturl" data-url="aHR0cHM6Ly9zdGF0aWMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL21lZGlhL3Jlc2VhcmNoLmdvb2dsZS5jb20vZW4vL3B1YnMvYXJjaGl2ZS80NTQ2Ni5wZGY=">Real-time loop closure in 2D LIDAR SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>the Google Cartographer’s paper</li>
</ul>
</li>
<li>16 SSRR <span class="exturl" data-url="aHR0cHM6Ly9oYWwuYXJjaGl2ZXMtb3V2ZXJ0ZXMuZnIvaGFsLTAxNTIyMjQ4L2RvY3VtZW50">ICP-based pose-graph SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>an almost standard framework of scan matching- and pose-graph-based LiDAR SLAM</li>
</ul>
</li>
<li>16 IROS <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzc3NTkwNjA=">M2DP: A novel 3D point cloud descriptor and its application in loop closure detection<i class="fa fa-external-link-alt"></i></span><ul>
<li>place descriptor using a single LiDAR scan</li>
</ul>
</li>
<li>16 BookChapter <span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9yZWZlcmVuY2V3b3JrZW50cnkvMTAuMTAwNyUyRjk3OC0zLTU0MC0zMDMwMS01XzM3">World modeling<i class="fa fa-external-link-alt"></i></span><ul>
<li>various representations of a surrounding environment; selecting a proper representation of the environment is important and it determines the state estimation ways.</li>
</ul>
</li>
<li>18 RSS <span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxNC9wMTYucGRm">Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments<i class="fa fa-external-link-alt"></i></span><ul>
<li>a.k.a SuMa; projective view rendering, and open source</li>
</ul>
</li>
<li>18 ICRA <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDIuMDg2MzM=">IMLS-SLAM: scan-to-model matching based on 3D data<i class="fa fa-external-link-alt"></i></span><ul>
<li>sophisticated feature selections, not real-time but for the accuracy, this is considered as a SOTA</li>
</ul>
</li>
<li>18 ICRA <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MTEuMDE2OTE=">Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>(continuous-time) non-rigid map deformation (see 15 RSS ElasticFusion also)</li>
</ul>
</li>
<li>18 ICRA <span class="exturl" data-url="aHR0cDovL2Fpcy51bmktYm9ubi5kZS9wYXBlcnMvSUNSQV8yMDE4X0Ryb2VzY2hlbC5wZGY=">Efficient Continuous-time SLAM for 3D Lidar-based Online Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>a hierarchical continuous-time LiDAR SLAM</li>
</ul>
</li>
<li>18 IROS <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg1OTQyOTk=">LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain<i class="fa fa-external-link-alt"></i></span><ul>
<li>range image-based fast feature selection for LOAM; and open source</li>
</ul>
</li>
<li>18 <span class="exturl" data-url="aHR0cDovL3VkZWwuZWR1L355dXlhbmcvZG93bmxvYWRzL2dlbmV2YV9pcm9zMjAxOC5wZGY=">IROS LIPS: LiDAR-Inertial 3D Plane SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>leveraging plane for LINS system</li>
</ul>
</li>
<li>18 IROS <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg1OTM5NTM=">Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map<i class="fa fa-external-link-alt"></i></span><ul>
<li>A visibility-based place descriptor for fast and robust place recognition, and open source</li>
</ul>
</li>
<li>19 A-LOAM (code only)<ul>
<li>a well-implemented LOAM algorithm (the original LOAM author closed the official code), and open source</li>
</ul>
</li>
<li>19 ICRA <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDQuMDY5OTM=">Tightly Coupled 3D Lidar Inertial Odometry and Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>a.k.a lio-mapping; imu tight fusion but practically slow, and <em>open source</em></li>
</ul>
</li>
<li>19 IV <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDQuMTI2Njc=">DeLiO: Decoupled LiDAR Odometry<i class="fa fa-external-link-alt"></i></span><ul>
<li>rotation and translation are decoupled</li>
</ul>
</li>
<li>19 IJRR <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDkuMTI4Mzc=">SegMap: Segment-based mapping and localization using data-driven descriptors<i class="fa fa-external-link-alt"></i></span><ul>
<li>deep segment feature learning for LiDAR place recognition</li>
</ul>
</li>
<li>19 IROS <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMTEzMjA=">SuMa++: Efficient LiDAR-based Semantic SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>merging semantic information into SuMa</li>
</ul>
</li>
<li>20 AR <span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNTE0LTAxOS0wOTg4MS0w">DVL-SLAM: sparse depth enhanced direct visual-LiDAR SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>enhanced visual SLAM by LiDAR data</li>
</ul>
</li>
<li>20 RSS <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDUuMTEzNDQ=">OverlapNet: Loop Closing for LiDAR-based SLAM<i class="fa fa-external-link-alt"></i></span><ul>
<li>learning two scan’s overlap and integrated it into the modern probabilistic SLAM system.</li>
</ul>
</li>
<li>20 IROS <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDcuMDAyNTg=">LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping<i class="fa fa-external-link-alt"></i></span><ul>
<li>IMU fusion (tightly) of LOAM, and open source.</li>
</ul>
</li>
<li>20 IROS <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkzNDE1NDk=">SpoxelNet: Spherical Voxel-based Deep Place Recognition for 3D Point Clouds of Crowded Indoor Spaces<i class="fa fa-external-link-alt"></i></span><ul>
<li>Deep LiDAR feature learning for place recognition and robust to occlusions</li>
</ul>
</li>
<li>20 IROS <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDguMTE0NTk=">Semantic Graph Based Place Recognition for 3D Point Clouds<i class="fa fa-external-link-alt"></i></span><ul>
<li>Summarizing a place with a single semantic graph. The matching part is also deep (SegMap didn’t).</li>
</ul>
</li>
<li>20 IROS <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDkuMDcyNjc=">A Fast and Robust Place Recognition Approach for Stereo Visual Odometry Using LiDAR Descriptors<i class="fa fa-external-link-alt"></i></span><ul>
<li>LiDAR descriptors are also good for stereo-camera-based place recognition</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>LiDAR</tag>
        <tag>LiDAR SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Rust 설치 + Hello World + Hello Cargo</title>
    <url>/20211207-rust-start-0/</url>
    <content><![CDATA[<h2 id="Why-Rust"><a href="#Why-Rust" class="headerlink" title="Why Rust?"></a>Why Rust?</h2><ul>
<li>기존의 C/C++ 언어가 지니는 메모리 관리의 어려움을 언어 차원에서 해소<ul>
<li>Windows OS에서 발견된 버그의 70%가 메모리 관련 버그임</li>
</ul>
</li>
<li>최신 멀티 코어 프로세스를 활용한 동시성에 대한 지원</li>
<li>비용 없는 추상화</li>
</ul>
<p> </p>
<hr>
<h2 id="Rust-설치-amp-설정"><a href="#Rust-설치-amp-설정" class="headerlink" title="Rust 설치 & 설정"></a>Rust 설치 &amp; 설정</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl https://sh.rustup.rs -sSf | sh</span><br><span class="line">$ <span class="built_in">source</span> <span class="variable">$HOME</span>/.cargo/env <span class="comment"># or add the following to ~/.bashrc `export PATH="$HOME/.cargo/bin:$PATH"</span></span><br><span class="line"></span><br><span class="line">$ rustc --version <span class="comment"># to check Rust version</span></span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><figure class="highlight rust"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>(){</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">"Hello, world!"</span>);</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<ul>
<li>Rust에서 들여쓰기는 탭이 아니라 공백문자 4개를 이용한다.</li>
<li><code>println!</code>은 Rust macro이다. 함수가 아니라 매크로를 호출하는것이다.</li>
<li>각 구문은 <code>;</code>로 끝난다.</li>
</ul>
<p> </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rustc main.rs</span><br><span class="line">$ ./main</span><br><span class="line">Hello, world!</span><br></pre></td></tr></table></figure>

<ul>
<li>처음에 위 코드를 돌렸을 때 <code>error: linker 'cc' not found</code> 라는 에러가 나타났다.<ul>
<li><code>sudo apt-get install gcc clang</code>을 함으로써 해결되었다. 내 경우는 cc가 clang과 연결되어있어서 clang을 설치해줬더니 해결되었다. 대부분의 경우 gcc와 연결이 되어있을테니 gcc를 설치하면 해결될 것이다.</li>
</ul>
</li>
<li>Rust는 ahead-of-time compilation을 하기 때문에, 생성된 바이너리는 Rust를 설치하지 않고도 사용할 수 있다.<ul>
<li>이러한 점은 Python, javascript, ruby와는 다른 점이다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Hello-Cargo"><a href="#Hello-Cargo" class="headerlink" title="Hello Cargo"></a>Hello Cargo</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cargo new hello_cargo_project</span><br><span class="line">$ <span class="built_in">cd</span> hello_cargo_project</span><br></pre></td></tr></table></figure>

<ul>
<li><code>cargo</code>라는 Rust 언어 전용 패키지 매니저를 사용해서 프로젝트를 생성한다.</li>
<li>위와 같은 커맨드로 프로젝트를 생성하면 다음과 같은 내용들이 있다.<ul>
<li>src 폴더 (내부에는 main.rs 파일이 있음)</li>
<li>Cargo.toml 파일<ul>
<li>프로젝트 config 파일</li>
<li>패키지 이름, 버전, 작성자, 디펜던시 정보 등이 적혀있음.</li>
<li>Rust에서 패키지는 <code>crate</code>라고 부름.</li>
</ul>
</li>
<li>.gitignore 및 .git 폴더</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cargo build <span class="comment"># Build only (Debug by default)</span></span><br><span class="line">$ cargo build --release <span class="comment"># Build Release mode</span></span><br><span class="line">$ ./target/debug/hello_cargo_project</span><br><span class="line">$ cargo run <span class="comment"># Build + Run</span></span><br><span class="line">$ cargo check <span class="comment"># Only check if code can be compiled without error (i.e. Does not produce binary program)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>위와 같은 커맨드를 이용해서 빌드 / 빌드 + 실행 / 컴파일 검사를 할 수 있다.</li>
<li><code>--release</code> 옵션을 이용해서 코드 실행에 최적화된 release 모드 빌드를 할 수 있다.</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.4 Rust</category>
      </categories>
      <tags>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title>2019.06.26 Universal Robotics 세미나</title>
    <url>/20211230-190626-UR-seminar/</url>
    <content><![CDATA[<h2 id="Universal-Robotics-회사는"><a href="#Universal-Robotics-회사는" class="headerlink" title="Universal Robotics 회사는?"></a>Universal Robotics 회사는?</h2><p><a href="(https://www.universal-robots.com/)"><strong>Universal Robotics</strong></a>는 산업용 / 연구용 로봇 플랫폼을 제작하는 기업이다. UR에서 제작하는 로봇은 생산성 증대를 위한 협동형 로봇 (cobot) 플랫폼에 중점을 두고 있다.</p>
<p><strong>Cobot</strong>은 사람과 로봇의 협동을 통해 더욱 자연스럽고 효율적인 작업을 유도하는데에 목표를 두는데, 이를 위해서는 섬세한 기동 및 안전에 유의해야한다.</p>
<p>UR의 파트너사들은 UR 플랫폼과 함께 사용할 수 있는 부품 (e.g. end-effector, vision sensor), 또는 UR 플랫폼을 이용한 솔루션 개발에 중점을 두고 있다.</p>
<img src="/20211230-190626-UR-seminar/Untitled.png" class="" title="UR robot">

<p> </p>
<hr>
<h2 id="첫번째-발표-Universal-Robotics"><a href="#첫번째-발표-Universal-Robotics" class="headerlink" title="첫번째 발표: Universal Robotics"></a>첫번째 발표: Universal Robotics</h2><h3 id="현재-제조업-생태계의-문제"><a href="#현재-제조업-생태계의-문제" class="headerlink" title="현재 제조업 생태계의 문제"></a>현재 제조업 생태계의 문제</h3><ul>
<li>불안정한 정치/경제적 상황이 야기하는 문제<ul>
<li>국내 투자 감소 및 국내 기업의 해외진출 선호</li>
</ul>
</li>
<li>생산가능 인구의 저하<ul>
<li>고령화, 저출산</li>
</ul>
</li>
</ul>
<img src="/20211230-190626-UR-seminar/20190625_154227.jpg" class="" title="국내투자 감소 및 해외진출 선호">

<p> </p>
<h3 id="업체가-바라보는-현재-업계-상황-자동화만이-정답인가"><a href="#업체가-바라보는-현재-업계-상황-자동화만이-정답인가" class="headerlink" title="업체가 바라보는 현재 업계 상황 (자동화만이 정답인가?)"></a>업체가 바라보는 현재 업계 상황 (자동화만이 정답인가?)</h3><ul>
<li>시장이 제조업에게 요구하는 사항들:<ul>
<li>다품종 소량생산</li>
<li>원가절감+품질향상</li>
<li>생산성 증대</li>
<li>운용자금 및 원가상승</li>
</ul>
</li>
</ul>
<p>위와 같은 요구사항을 맞추기 위해 많은 기업들이 <strong>‘자동화’전략</strong>을 선택한다. 자동화를 통해 생산성 증대와 가격효율성 증대를 얻기 위함이다. </p>
<p>미래에 대해 대비하기위해 여러가지 변화가 요구되는데, <strong>생산성 향상을 위해 무인화만이 정말 정답일까?</strong></p>
<p>정답은 ‘<strong>NO</strong>‘이다. Tesla의 case-study를 보았을 때, 공장 전체를 자동화하려는 시도를 했지만 실패하고 결국 인력을 다시 고용해야했다. (Elon Musk - “사람의 가치를 과소평가하였다. 테슬라 공장을 완전 자동화하려고 했던 것은 실수였다”)</p>
<p> </p>
<h2 id="Cobot으로-이-문제를-풀-수-있을까"><a href="#Cobot으로-이-문제를-풀-수-있을까" class="headerlink" title="Cobot으로 이 문제를 풀 수 있을까?"></a>Cobot으로 이 문제를 풀 수 있을까?</h2><ul>
<li>MIT 실험 / 모라벡의 역설 (생산성 향상)<ul>
<li>사람이 가진 능력:  창의력, 적응력, 증흥성</li>
<li>로봇이 가진 능력: 반복성,정화성, 참을성</li>
<li>둘이 협업할때 85% 더효율적이다. 통계자료가 있음.</li>
</ul>
</li>
</ul>
<img src="/20211230-190626-UR-seminar/20190625_154834.jpg" class="" title="Cobot과 협업할 때 80% 더 생산적">

<ul>
<li>UR은 Cobot을 통해 다음과 같은 효과를 추구한다.<ul>
<li>제조현장에 유연성을 가져오자! </li>
<li>Universal Robot은 Human Empowerment를 추구한다! </li>
<li>로봇을 사람의 생상성 향상을 위한 도구로 만들자!</li>
</ul>
</li>
<li>유연성 + 생산력 증대를 위한 제조현장의 변화:<ul>
<li>고정형 설비에서→ 이동 설치형 설비로</li>
<li>사람과 로봇을 분리하던 방법에서 → 사람과 로봇이 <strong>협업하는 공간으로</strong></li>
<li>단순반복작업에서 → 다양한 작업들로</li>
<li>장기 투자회수 방식에서 → <strong>단기 투자회수 방식으로</strong></li>
</ul>
</li>
</ul>
<p> </p>
<img src="/20211230-190626-UR-seminar/20190625_142707.jpg" class="" title="Cobot 시장">

<h3 id="협동로봇-시장은-점점-크고-있다"><a href="#협동로봇-시장은-점점-크고-있다" class="headerlink" title="협동로봇 시장은 점점 크고 있다"></a>협동로봇 시장은 점점 크고 있다</h3><p>앞으로도 계속 클 것이다.</p>
<img src="/20211230-190626-UR-seminar/20190625_141515.jpg" class="" title="Cobot 시장">

<p> </p>
<hr>
<h2 id="두번째-발표-Universal-Robotics"><a href="#두번째-발표-Universal-Robotics" class="headerlink" title="두번째 발표: Universal Robotics+"></a>두번째 발표: Universal Robotics+</h2><h3 id="UR-란"><a href="#UR-란" class="headerlink" title="UR+란?"></a>UR+란?</h3><ul>
<li>엔드이펙터, 비전, 악세서리, 소프트웨어 회사들과 협업해서 만드는 솔루션</li>
<li>기존 UR 로봇의 빠른설치+사용과 설계고민 해결이라는 장점을 추가된 솔루션으로 더욱 이끌어냄.</li>
</ul>
<h3 id="실사용-케이스"><a href="#실사용-케이스" class="headerlink" title="실사용 케이스"></a>실사용 케이스</h3><ul>
<li>사용 케이스:<ul>
<li>현대고주파열처리</li>
<li>Paradigm electronics</li>
<li>Continental</li>
<li>Orkla foods</li>
<li>x-ray, 용접, 샌딩, 컷팅, 빈픽킹,</li>
</ul>
</li>
<li>Digital Twin:<ul>
<li>기존 UR 로봇과 OPC 통신이 가능 (RocketFarm)</li>
<li>로봇이 움직이면, 3D 툴에서 로봇도 같이 움직임. 반대로, 3D 툴에서 움직이면, 로봇도 같이 움직임. <ul>
<li>스마트 팩토리 컨셉에 비교적 쉽게 접근 가능.</li>
<li>티치 펜던트 없이, ROS 인터페이스로도 사용 가능함. 3D 툴과 ROS를 연동시킬 수 있을지도?</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="세번째-발표-Onrobot"><a href="#세번째-발표-Onrobot" class="headerlink" title="세번째 발표: Onrobot"></a>세번째 발표: Onrobot</h2><h3 id="Onrobot이-판매하는-그리퍼-제품들"><a href="#Onrobot이-판매하는-그리퍼-제품들" class="headerlink" title="Onrobot이 판매하는 그리퍼 제품들"></a>Onrobot이 판매하는 그리퍼 제품들</h3><img src="/20211230-190626-UR-seminar/grippers.png" class="" title="Onrobot grippers">

<h3 id="어떤-작업들에서-사용할-수-있을까"><a href="#어떤-작업들에서-사용할-수-있을까" class="headerlink" title="어떤 작업들에서 사용할 수 있을까?"></a>어떤 작업들에서 사용할 수 있을까?</h3><ul>
<li>포장/팔레트 작업</li>
<li>픽앤플레이스 (컨베이어에서 내리기)</li>
<li>머신텐딩 (기계 다루기)</li>
<li>조립</li>
<li>표면마감 (디버링 그라인딩 폴리싱)</li>
<li>품질시험 및 검사 (정밀도 일관성)</li>
</ul>
<p> </p>
<hr>
<h2 id="네번째-발표-Pickit"><a href="#네번째-발표-Pickit" class="headerlink" title="네번째 발표: Pickit"></a>네번째 발표: Pickit</h2><h3 id="제조업에서의-비전-솔루션이란"><a href="#제조업에서의-비전-솔루션이란" class="headerlink" title="제조업에서의 비전 솔루션이란?"></a>제조업에서의 비전 솔루션이란?</h3><ul>
<li>6자유도로 움직이는 로봇은 산업기술에 필수다!<ul>
<li>근데 그 로봇들에 눈이 달려있지 않아서 우리가 원하는 작업을 완벽하게 구현하기 어렵다.</li>
</ul>
</li>
<li>픽킷은 3D비전을 이용해서 로봇에 눈을 달아준다.<ul>
<li>2D 비전만으로는 Bin-picking과 같이 어려운 작업은 할 수 없다.</li>
</ul>
</li>
</ul>
<img src="/20211230-190626-UR-seminar/20190625_171435.jpg" class="" title="bin-picking">

<ul>
<li>비전 기반 로봇 자동화 시스템이 창출하는 세가지 가치<ul>
<li>유연성 (Pickit의 경우 그리퍼 5개 + 3D 비전. 비전이 있기 때문에 5개의 그리퍼를 자유롭게 사용 가능.)</li>
<li>생산성</li>
<li>효율성</li>
</ul>
</li>
</ul>
<img src="/20211230-190626-UR-seminar/20190625_171028.jpg" class="" title="cycle-time / flexibility trade off">

<h3 id="Bin-picking-자동화-솔루션"><a href="#Bin-picking-자동화-솔루션" class="headerlink" title="Bin-picking 자동화 솔루션"></a>Bin-picking 자동화 솔루션</h3><ul>
<li>노동력의 38%가 빈에서 기계로 물체를 이동시키는 머신로딩 작업에 사용된다.</li>
<li>기계가 멈춤으로 인해 발생하는 손실이 사람의 인건비보다 비싸다.<ul>
<li>하지만 사람이 하는 머신로딩 작업의 기회비용은 얼마나 될까?</li>
</ul>
</li>
<li>셋업하기 쉽고, 사용하기 쉽고, 잘 작동하는 시스템이 있으면 완전 대체가 가능하다.</li>
</ul>
<img src="/20211230-190626-UR-seminar/20190625_171622.jpg" class="" title="Bin-picking automation">

<h3 id="시연-영상"><a href="#시연-영상" class="headerlink" title="시연 영상"></a>시연 영상</h3><blockquote>
<p>Pickit사의 구조광 비전 센서를 이용한 Bin-picking 자동화 솔루션 시연 장면.</p>
</blockquote>
<img src="/20211230-190626-UR-seminar/2.gif" class="" title="Bin-picking automation">

<blockquote>
<p>Pickit 사의 Bin-picking 자동화 솔루션의 소프트웨어 소개 장면</p>
</blockquote>
<img src="/20211230-190626-UR-seminar/3.gif" class="" title="Bin-picking automation">

<img src="/20211230-190626-UR-seminar/20190625_171904.jpg" class="" title="Bin-picking automation">

<p> </p>
<hr>
<h2 id="그-외-현장-데모-파트너사"><a href="#그-외-현장-데모-파트너사" class="headerlink" title="그 외 현장 데모 파트너사"></a>그 외 현장 데모 파트너사</h2><h3 id="ROBOTIS"><a href="#ROBOTIS" class="headerlink" title="ROBOTIS"></a>ROBOTIS</h3><ul>
<li>국산 그리퍼<img src="/20211230-190626-UR-seminar/1.gif" class="" title="Robotics gripper">

</li>
</ul>
<h3 id="SKF"><a href="#SKF" class="headerlink" title="SKF"></a>SKF</h3><ul>
<li>Linearised stage for UR robot<div class="video-container"><iframe src="https://www.youtube.com/embed/VytTqE_ODmg" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

</li>
</ul>
<h3 id="Syscon-Engineering"><a href="#Syscon-Engineering" class="headerlink" title="Syscon Engineering"></a>Syscon Engineering</h3><ul>
<li>UR 로봇이 탑재 가능한 + 팔렛트 이송 자율주행 AMR Autocon System<div class="video-container"><iframe src="https://www.youtube.com/embed/Hak8-Ugo4wI" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

</li>
</ul>
<h3 id="Pickit"><a href="#Pickit" class="headerlink" title="Pickit"></a>Pickit</h3><ul>
<li>빈픽킹용 3D 비전<div class="video-container"><iframe src="https://www.youtube.com/embed/vir5D6Y_OcI" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

</li>
</ul>
<h3 id="Onrobot"><a href="#Onrobot" class="headerlink" title="Onrobot"></a>Onrobot</h3><ul>
<li>여러가지 종류의 그리퍼<div class="video-container"><iframe src="https://www.youtube.com/embed/_kSW_yB6WZ0" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div></li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.4 Robotics</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>Seminar</tag>
        <tag>Cobot</tag>
        <tag>Universal Robotics</tag>
      </tags>
  </entry>
  <entry>
    <title>최신 딥러닝 / 로보틱스 뉴스를 따라잡는 방법</title>
    <url>/20211231-deep-learning-ai-news/</url>
    <content><![CDATA[<h2 id="커뮤니티에-기대서-뉴스를-보는건-이제-그만"><a href="#커뮤니티에-기대서-뉴스를-보는건-이제-그만" class="headerlink" title="커뮤니티에 기대서 뉴스를 보는건 이제 그만!"></a>커뮤니티에 기대서 뉴스를 보는건 이제 그만!</h2><hr>
<p>AI 와 Robotics 뉴스는 매일같이 쏟아져 나온다.</p>
<p>딥러닝/로보틱스를 처음 알게 된 3년 전 쯤엔 뉴스를 받아보기 위해 종종 관심있는 연구 그룹의 블로그에 들어가서 뉴스를 확인하곤 했다.</p>
<p>그러다가 어느 순간부터는 페이스북 TensorFlow KR이나 AI Korea 등의 커뮤니티를 통해 뉴스를 접하게 되었다. 위 커뮤니티에는 빠르게 뉴스를 전달해주시는 분들도 계셨고, 관리자가 만든 뉴스 봇도 있었기 때문이다. 페이스북을 통해서 뉴스를 받아보는것에 익숙해지다보니, 점차 뉴스를 보기 위해 페이스북에 접속하게 되었다.</p>
<p>파이썬도 모르던 상태에서 뉴스와 추천링크를 통해 CNN과 RNN의 기초를 이해하게 되었고, YOLO와 RCNN 계열의 차이도 알게 되었다. 이 부분에 대해서는 정말로 감사하게 생각한다.</p>
<p>그러다가 복합적인 이유로 페이스북에서 뉴스를 읽는 것을 그만두게 되었다. 가장 큰 이유는 페이스북의 열기가 식으면서 홍보성 소식만이 난무하게 되어 더 이상 내게 도움이 된다고 판단하지 못하게 된 것이다. 예전에는 적어도 다양한 분야의 뉴스가 나왔기 때문에, 조금의 노력을 통해 내가 보고싶은 분야의 뉴스를 걸러볼 수 있었지만, 이제는 정보의 총량 자체가 줄어 내가 정말로 보고싶은 분야의 뉴스는 보기가 어렵다는 점도 있었다.</p>
<p>그래서 내가 보고싶은 분야의 뉴스를 골라보는 방법을 강구했다. </p>
<p>이번 글에서는 이 방법들을 공유한다.</p>
<p> </p>
<hr>
<h2 id="Google-Scholar"><a href="#Google-Scholar" class="headerlink" title="Google Scholar"></a>Google Scholar</h2><p><strong>진리의 Google scholar</strong>이다.</p>
<p>이미 익숙한 분야에서 최신 논문 소식을 받는데에 가장 좋은 방법이며, 막 입문하는 분야에서는 조금 비추하는 방법이다.</p>
<p>Google scholar를 통해 내가 관심있어하는 저자들의 소식을 가장 빠르고 정확하게 받을 수 있다.</p>
<p>Google scholar는 1. 저자들이 새로운 논문을 등록했을 때, 2. 다른 사람들이 저자의 논문을 참조하였을 때, 3. 다른 사람들이 저자의 연구 분야와 비슷한 논문을 등록했을 때의 정보를 취합하여 정기적으로 업데이트를 이메일로 보내준다. (아래 사진은 3일치 논문 업데이트이다)</p>
<img src="/20211231-deep-learning-ai-news/scholar_0.png" class="" title="Google scholar email">

<p>각각의 이메일에는 1개부터 N개 까지 논문 소식이 공유된다.</p>
<p>딥러닝 분야의 경우 ArXiv에 올라오는 경우가 많아 링크를 타고 바로 pdf를 볼 수 있지만, 로보틱스의 경우 IEEE에 연결되는 경우가 많아 바로 액세스가 어려울 때도 있다 (학교에 있다면 문제없다).</p>
<p>이 방법의 단점이라면, 아이디어를 선점하기 위해 완성되지 않는 논문이 올라오거나, 학회 제출 전 영어가 다듬어지지 않은 논문이 올라올 경우 굉장히 헷갈리게 된다. 그래서 개인적으로 공신력 있는 저자가 아닌 경우에는 abstract만 빠르게 읽는다.</p>
<img src="/20211231-deep-learning-ai-news/scholar_1.png" class="" title="email detail">

<p>3~5일에 한번씩 업데이트 되기 때문에, 출퇴근하면서 읽기 딱이다 ㅎㅎ</p>
<p> </p>
<hr>
<h2 id="Feedly-앱"><a href="#Feedly-앱" class="headerlink" title="Feedly 앱"></a>Feedly 앱</h2><p>Feedly 앱을 통해 여러 뉴스 웹사이트와 블로그 RSS 피드를 가져와서 한 앱에서 한눈에 쉽게 볼 수 있다. 아이패드 또는 기타 태블릿을 사용하는 것을 추천한다.</p>
<p>이 방식의 장점은, 구글에서 내가 기억하는 랩실의 이름을을 하나씩 기억하고 찾는 것 보다, 아이패드에서 앱을 한번 누르는 것만으로 쉽게 많은 정보를 빠르게 접할 수 있기 때문이다 (정보를 얻는데에 들어가는 수고를 덜어준다).</p>
<p>또 다른 좋은 점은 기사 / 블로그 글의 썸네일까지 함께 제공하기 때문에, 시각적으로도 좀 더 끌리는 것 같다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled.png" class="" title="feedly">

<p>AI 관련으로는 가장 SOTA를 많이 뽑아내는 Google AI Blog, OpenAI, BAIR, Nvidia, FAIR, DeepMind, Microsoft Research, Fast.ai와 몇가지 상업적인 AI 뉴스를 구독했다. 아직 AI 분야는 내가 핸즈온으로 도전하고 있지 않았기 때문에, 논문보다는 기사 제목만 훑는 것 만으로 어느정도 연구 트렌드 변화와 산업 변화를 확인할 수 있다.</p>
<p>최신 논문을 빨리 훑어보려고 Arxiv 피드도 구독했었다. 그러나 지금은 구독 취소를 했는데, 매일 내 분야도 아닌 논문이 300개씩 추가가 되서이다.</p>
<p>Robotics 분야의 뉴스는 사실 특정 기술의 내용보다는 제품군이 좀 더 많이 나오는 것 같다. 그나마 조금 깊게 들어가는 IEEE Spectrum을 보는게 조금 좋은 것 같다. 단순히 로보틱스 뿐만이 아니라 주변 기술 (automation, energy) 등등 피드도 구독해놨다. </p>
<p>기본적으로 나는 이런 피드들을 읽을 때 모든 글을 읽겠다는 생각은 하고있지 않다. 나는 내가 구독한 소스의 수가 많다고 생각하고 있고, 또 너무 많은 정보를 소화하려고 하다보면 정보의 바다에 휩쓸리기 쉽고, 개인 스케줄 등으로 혹시나 딜레이가 생기게 될 경우 스트레스를 받는 편이다.</p>
<p>실제로 사용할 때는 아래처럼 보인다. 썸네일이 안보이는 이유는 아마 로딩중이라서 그런 것 같다. 아침에 출근하면서 지하철에서 글 하나, 점심시간에 하나, 퇴근하면서 하나 정도 보면 딱 괜찮은 것 같다.</p>
<img src="/20211231-deep-learning-ai-news/feedly.png" class="" title="email detail">

<p> </p>
<hr>
<h2 id="Paperswithcode"><a href="#Paperswithcode" class="headerlink" title="Paperswithcode"></a>Paperswithcode</h2><img src="/20211231-deep-learning-ai-news/Untitled%202.png" class="" title="paperswithcode">

<p>딥러닝을 업으로 삼으신 지인으로부터, SOTA 랭킹에 대해 알고싶다면 Paperswithcode를 보라고 추천을 받았다.</p>
<p>종종 논문 검색을 위해 이 페이지를 이용했지만, 아직 딥러닝에 대한 지식이 부족해서 애용하는 정도는 아니다. 그래도 가끔 들어가서 현재 SOTA가 무엇인지 확인하고, 딥러닝을 업으로 삼으시는 분들과 이야기 할 때 도움이 된다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnN3aXRoY29kZS5jb20v">Papers With Code : the latest in machine learning<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h2 id="Youtube"><a href="#Youtube" class="headerlink" title="Youtube"></a>Youtube</h2><p>Paperswithcode는 어떤 기술이 가장 좋은 벤치마크를 내는지 알 수 있지만, 사실 이러한 벤치마크 정보는 이미 해당 분야를 업으로 삼으신 분들께 도움이 되는 것 같다.</p>
<p>아직 나처럼 딥러닝 꼬꼬마는 Youtube 영상들을 통해서 눈으로 보고 흥미를 느끼는게 좀 더 도움이 될 것이라고 생각한다.</p>
<h3 id="Two-Minute-Paper"><a href="#Two-Minute-Paper" class="headerlink" title="Two Minute Paper"></a>Two Minute Paper</h3><p>Two Minute Paper는 딥러닝이 적용되는 다양한 분야의 기술들을 짧게 설명해준다.</p>
<p>오전에 지하철 타고 출근하면서 보기 딱 좋은 채널이다. 다루는 분야가 상당히 많은데, 기존의 detection / segmentation과 같은 컴퓨터 비전, BERT, GPT-3 같은 NLP 분야, 강화학습 분야…를 넘어서 종종 물리학 시뮬레이션과 그래픽스, 모션 추정 등 다양한 분야를 섭렵하는게 넓고 얕은 지식을 쌓기에 좋다.</p>
<img src="/20211231-deep-learning-ai-news/tmp.png" class="" title="tmp">

<h3 id="Karol-Majek"><a href="#Karol-Majek" class="headerlink" title="Karol Majek"></a>Karol Majek</h3><p>Karol Majek 채널은 실제로 컴퓨터 비전 분야의 detection / segmentation / feature extraction 코드를 돌리면서 나오는 결과를 보여준다. </p>
<p>이런 영상이 좋은 이유는, 논문들의 홍보 영상들을 보다보면 항상 좋은 결과만 나오기 때문이다. 이런 영상들 중, 어떤 기술이 실제로 좋고 어떤 영상이 cherry pick인지 알 수 없는데, Majek의 영상들을 보면 각각의 딥러닝 모델들의 실제 퍼포먼스가 어느정도 되는지 알 수 있다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%204.png" class="" title="email detail">

<h3 id="Lex-Fridman"><a href="#Lex-Fridman" class="headerlink" title="Lex Fridman"></a>Lex Fridman</h3><p>여러 분야의 전문가들을 데려와서 그들의 전문 분야에 대한 인사이트나 사고방식에 대해 팟캐스트 인터뷰를 진행한다.</p>
<p>초창기에는 AI와 자율주행 쪽 연사들이 많이 나왔다. 이전에 Elon Musk가 나왔을 때 부터 봤는데, 얼마 지나지 않아 George Hotz가 나오는 것을 보고 완전 팬이 되었다. 이 둘은 사이도 좋지 않고 기술적으로도 경쟁하는 관계라고 들었는데, 그만큼 Lex는 중립을 지키면서 팟캐스트를 운영한다는 것을 보여주는 것 같다. 요즘은 화이자 CEO도 데려오고, 투자가도 데려오고, 운동선수도 데려오는걸 보면 정말 다양한 분야에서 자명한 사람들을 다 데려온다.</p>
<p>자주 나오는 질문 중에 ‘우리가 지금 살아가는 이 세상이, 사실 시뮬레이션 일 것 같은가?’ 라는 질문이 있다. 처음엔 이게 무슨 공상과학 질문인가 싶었는데, 은근히 여러 전문가들마다 색다른 견해를 보여주는게 재밌다.</p>
<p>단점이라면 너무 길어서 일주일동안 한 영상을 쪼개보거나 주말에 몰아봐야한다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%205.png" class="" title="email detail">

<h3 id="Yannic-Kilcher"><a href="#Yannic-Kilcher" class="headerlink" title="Yannic Kilcher"></a>Yannic Kilcher</h3><p>복잡한 개념을 설명하는데에 도가 튼 것 같다.</p>
<p>사실 아직 딥러닝의 기초도 탄탄히 잡은 편도 아니라 영상을 봐도 30% 정도밖에 못 집어먹기 때문에 자주 찾아보지는 않는다. 다만, DETR이라던지, DELF 와 같이 Attention이 컴퓨터 비전에 적용이 되는 것을 보고 Attention을 이해하기 위해 이 채널을 찾게 되었는데, 기술에 대한 설명을 정말로 깔끔하게 하는 것을 보고 이 채널에 대해 좋은 인상을 가지고 있다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%206.png" class="" title="yannic">

<h3 id="Henry-AI-Labs"><a href="#Henry-AI-Labs" class="headerlink" title="Henry AI Labs"></a>Henry AI Labs</h3><p>Two Minute Paper와 비슷한 느낌의 채널이다. 채널의 OpenAI와 Facebook의 연구를 특히 좋아하는 것 같은데, 평소에 나오는 영상들은 사실 내 취향은 아니다. 하지만 학회가 진행될 때 마다 특정 기업의 연구 결과와 트렌드를 한번에 정리해주는게 굉장히 도움이 많이 된다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%207.png" class="" title="yannic">

<h3 id="Gadget-Seoul"><a href="#Gadget-Seoul" class="headerlink" title="Gadget Seoul"></a>Gadget Seoul</h3><p>(드디어 처음으로 나오는 한국인 채널 ㅋㅋ)</p>
<p>이 정도의 인사이트를 무료로 볼 수 있다고?</p>
<p>가젯서울 채널은 정확히 이야기하면 딥러닝 채널은 아니다. 반도체, 배터리, 데이터센터 관련된 뉴스가 주를 이루는데, 지난 1년간 엔비디아가 만들어내는 뉴스, 인텔의 자율주행 뉴스 등등으로 인해 딥러닝 프로세서 기업들의 트렌드를 알기 굉장히 좋다. 종종 가젯서울 채널주의 의견과 인사이트가 영상을 통해 공유되는데, 반도체 시장에 대한 굉장히 깊은 이해를 통해 오는 인사이트라고 느껴지고, 개인적으로 이정도 급의 인사이트를 무료로 들을 수 있다는 것에 대해 감사하게 생각한다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%208.png" class="" title="yannic">

<h3 id="딥러닝-논문읽기-모임-PR12"><a href="#딥러닝-논문읽기-모임-PR12" class="headerlink" title="딥러닝 논문읽기 모임 + PR12"></a>딥러닝 논문읽기 모임 + PR12</h3><p>그리고 국내에서 유명한 두 딥러닝 논문 읽기 채널들이다. PR12는 워낙 유명해서 말할 것도 없고, 딥러닝 논문 읽기 모임도 업로드한 영상이 많이 쌓이면서 인지도가 많이 쌓이고 있다.</p>
<img src="/20211231-deep-learning-ai-news/Untitled%2011.png" class="" title="yannic">

<img src="/20211231-deep-learning-ai-news/Untitled%2012.png" class="" title="yannic">

<p> </p>
<hr>
<h2 id="조금-늦어도-괜찮다면…"><a href="#조금-늦어도-괜찮다면…" class="headerlink" title="조금 늦어도 괜찮다면…"></a>조금 늦어도 괜찮다면…</h2><p>딥러닝 학회에서 공개되어있는 Tutorial들을 보는 것들도 최신 트렌드를 파악하는데에 아주 큰 도움이 된다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>논문 리뷰</category>
        <category>1.4 Robotics</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>GDG 광주 2019 - 앙상블 / AutoML (발표자 - 이종훈)</title>
    <url>/20211231-gdg-gwangju-2019-ensemble-automl/</url>
    <content><![CDATA[<h2 id="원본-발표-자료"><a href="#원본-발표-자료" class="headerlink" title="원본 발표 자료"></a>원본 발표 자료</h2><p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMWJ0cTZlQi1Gakp3WU1rTUR2UG9NY2ZGb1BidENUb08zX0ppRjJDR3g1UVkvcHJlc2VudD9zbGlkZT1pZC5nNWZiNzEzOTJmZl8yXzE5">[DLSS]08_AutoML<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h2 id="Ensemble-AutoML이란"><a href="#Ensemble-AutoML이란" class="headerlink" title="Ensemble / AutoML이란?"></a>Ensemble / AutoML이란?</h2><p>Ensemble - 여러 모델을 함께 사용한다.<br>AutoML - 여러 모델을 만들어서 최적의 모델을 자동으로 찾음.</p>
<p>공통점:</p>
<ul>
<li>성능이 좋다</li>
<li>현업과 대회에서 많이 사용된다</li>
<li>여러 모델을 만든다.</li>
</ul>
<p>많은 사람들이 Ensemble이나 AutoML을 안써봤는데… 왜일까?</p>
<p>의심가는 이유:</p>
<ul>
<li>GPU가 엄청 많이 필요할텐데… 그게 없어서…</li>
<li>유명한 논문을 벤치마킹만 해도 잘 된다…? 굳이…? 왜 써야할지 몰라서…?</li>
</ul>
<h3 id="왜-써야-하는가"><a href="#왜-써야-하는가" class="headerlink" title="왜 써야 하는가?"></a>왜 써야 하는가?</h3><ul>
<li>앙상블을 올바르게 쓰면 성능이 더 좋아서!<ul>
<li>여기서 얘기하는 성능은 Accuracy뿐만이 아니다.</li>
<li>Efficiency, Speed, Cost 등 전부 중요하다. 이걸 포괄적으로 봐서 ‘성능’ (performance)이라고 한다.</li>
</ul>
</li>
</ul>
<p>하나의 모델이 모든 경우에 대한 Rule을 만들 수 있는가? 이걸 할 수 있는 모든 경우의 데이터가 있는가?</p>
<p>그렇게 하기 어렵기 때문에, 경우의 범위 또는 단위를 정해야하며, 그리고 처음 보는 경우에는 대처해야한다. 단일모델의 경우, 그 모델이 대처하지 못하는 경우를 마주하게 될 경우</p>
<ul>
<li>오류가 나거나</li>
<li>그냥 아무거나 뱉거나</li>
<li>모르겠다고 얘기하거나</li>
<li>(거의 없지만) 알아서 상황에 맞는 정답을 찾는다.</li>
</ul>
<p>위의 케이스들을 딥러닝의 관점으로 보면 오버피팅(low bias high variance) / 언더피팅(high bias low variance)으로 볼 수 있다.</p>
<p>최적의 해 (low bias / low variance)를 구하기 위해서 (언더피팅도 아니고 오버피팅도 아닌), Ensemble 기법들에 대해 알아본다.</p>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/48673F8D-40E7-4E01-B2E9-3749BFD4BFCB.jpeg" class="" title="ml_sdfdl">

<p> </p>
<hr>
<h2 id="Ensemble-기법들-머신러닝"><a href="#Ensemble-기법들-머신러닝" class="headerlink" title="Ensemble 기법들 (머신러닝)"></a>Ensemble 기법들 (머신러닝)</h2><h3 id="Voting"><a href="#Voting" class="headerlink" title="Voting"></a>Voting</h3><ul>
<li>분류에서는 voting, 회귀에서는 averaging이 기본.</li>
<li>여러 모델들이 개, 개, 개, 개, 고양이, 고양이를 내뱉는다면, 개를 아웃풋으로 내는 방식. 물론 이에 대해서 여러가지 방식이 있다 (softmax probability output으로 한다던지…)</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/84C6017B-121D-497E-A577-4542E8EA8197.jpeg" class="" title="ml_sdfdl">

<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><ul>
<li>Bagging = Bootstrap aggregating - 학습 데이터를 달리하여 여러 모델을 생성</li>
<li>Variance를 낮춤으로써 overfitting을 방지 (일반화)</li>
<li>딥러닝에서 잘 안 씀…</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/6BA94029-7121-4711-AE3D-8E8262E83750.jpeg" class="" title="ml_sdfdl">
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/77B3AAA8-C654-4999-B518-ACD09A3959D5.jpeg" class="" title="ml_sdfdl">

<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><ul>
<li>머신러닝 쪽에서 상당히 성능이 좋다고 함… 테이블과 같이 정형화된 데이터에서.</li>
<li>Bagging을 기본으로 하지만, 순차적으로 학습을 진행. 이 때, 이전 모델의 오답에 가중치를 부여 (boosting).</li>
<li>오답을 잘 걸러냄! 근데… 이미지 등 과 같이 노이즈가 많이 섞인, outlier가 있는 데이터라면…? 그 outlier에 가중치를 많이 주기 때문에 에러가 나타남. 즉, 정확도가 높지만 outlier나 noise에 취약! (고로 딥러닝에 적합하지 않음)</li>
<li>Adaboost, XGBoost, LightGBM, CatBoost</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/AB58E03D-868D-4C3F-88A1-1AA171F9D03F.jpeg" class="" title="ml_sdfdl">

<h3 id="Stacking-amp-Blending"><a href="#Stacking-amp-Blending" class="headerlink" title="Stacking & Blending"></a>Stacking &amp; Blending</h3><ul>
<li>Meta modelling (Meta Learning)!</li>
<li>각 모델이 도출한 결과들을 또 다른 모델의 입력으로 사용. 보통 3층 4층 정도 쌓음.</li>
<li>성능 향상이 크지만, 너무 무거운 경우가 많아 실제 사용이 어려움.</li>
<li>StackNet</li>
<li>리얼타임으로 안 도니까 기업들에서는 보통 안 쓰는 편이고… 쓴다면 좀 어려운 문제를 꼭 풀어야할때?</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/E955FB9B-B1A4-4EF8-BA71-3933638827DB.jpeg" class="" title="ml_sdfdl">

<p> </p>
<h2 id="Ensemble-기법들-딥러닝"><a href="#Ensemble-기법들-딥러닝" class="headerlink" title="Ensemble 기법들 (딥러닝)"></a>Ensemble 기법들 (딥러닝)</h2><h3 id="데이터를-다양하게-하는-방법"><a href="#데이터를-다양하게-하는-방법" class="headerlink" title="데이터를 다양하게 하는 방법"></a>데이터를 다양하게 하는 방법</h3><ul>
<li>K-fold cross-validation ensemble</li>
<li>Bootstrap aggregation ensemble (Bagging)</li>
<li>BgN-Score and BsN-Score<ul>
<li>…이렇게 있다고 하는데 학습에 시간이 많이 걸리고 무거워서 쓰기 어려움.</li>
<li>또, Boosting방법이 overfitting을 부추김.</li>
</ul>
</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/7B04D521-A6D0-457F-A59C-EB9468709930.jpeg" class="" title="ml_sdfdl">

<h3 id="모델을-다양하게-하는-경우"><a href="#모델을-다양하게-하는-경우" class="headerlink" title="모델을 다양하게 하는 경우"></a>모델을 다양하게 하는 경우</h3><ul>
<li>딥러닝에서는 모델 아키텍처를 어설프게 바꿔서 앙상블을 하는거보다, 차라리 initial weight기법이나 optimisation기법을 다르게 해서 앙상블을 하는게 훨씬 좋음! (힌튼 교수 연구)</li>
</ul>
<ol>
<li>“Snapshot Ensemble: Train 1, Get M for Free” 논문!<ul>
<li>일부러 로컬 미니마에 빠트린 여러개의 경우 (여러개의 모델 파라미터)를 앙상블한다.</li>
</ul>
</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/4899BF51-668B-4584-A84C-D911E27DE8AF.jpeg" class="" title="ml_sdfdl">

<ol start="2">
<li>“SGDR: Stochastic Gradient Descent with Warm Restarts” 논문</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/5BC7BAA5-0AA4-4B69-A514-4AFD94FA1B10.jpeg" class="" title="ml_sdfdl">

<ol start="3">
<li>“Loss Surfaces, Mode Connectivity and Fast Ensembling of DNNs.” 논문</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/2BDA7064-3F4C-4D73-AB2C-D771D02F292B.jpeg" class="" title="ml_sdfdl">

<h3 id="모델-조합을-다양하게-하는-경우"><a href="#모델-조합을-다양하게-하는-경우" class="headerlink" title="모델 조합을 다양하게 하는 경우"></a>모델 조합을 다양하게 하는 경우</h3><ol>
<li>Knowledge Distillation - 힌튼 교수가 만든 기법인데, 모델에서 좋은 정보를 추출해서 다른 모델에 심자는 기법. 현재는 모델 경량화에 사용되고 있지만, 원래는 앙상블을 위해 만든 것이다.</li>
</ol>
<ul>
<li>하나의 모델에서 좋은 정보를 추출해서 여러개의 훨씬 작은 모델들에 이식하고, 그 작은 모델들로 앙상블을 한다.</li>
</ul>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/1549E85F-EDF5-43C2-A8A8-4A7130F1506E.jpeg" class="" title="ml_sdfdl">

<ol start="2">
<li>Weight의 평균값을 취함으로써 더 넓은 Optima와 Better Generalization을 얻을 수 있다.</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/2F968751-0739-4379-B08A-473C7676332D.jpeg" class="" title="ml_sdfdl">   

<h3 id="Specializing-models"><a href="#Specializing-models" class="headerlink" title="Specializing models"></a>Specializing models</h3><ol>
<li>A novel Image Classification Method with CNN-XGBoost Model<ul>
<li>한때 유행했던 방법. 성능은 좋았는데, 너무 특정 케이스에만 적용됨.</li>
</ul>
</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/128AE4DE-4EB6-4CAD-BE2A-87CEE66F8E63.jpeg" class="" title="ml_sdfdl">

<ol start="2">
<li>Confidence multiple choice learning<ul>
<li>특정 클래스만 찾는 모델 (e.g. 강아지만 찾는다던가… 고양이만 찾는다던가…)을 여러개를 학습을 시키고, 그 모델들을 앙상블함.</li>
</ul>
</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/95D6EB0E-A8B2-4342-BD17-4BF71576F45D.jpeg" class="" title="ml_sdfdl">

<ol start="3">
<li>Learning to specialize with knowledge distillation for Visual Question Answering<ul>
<li>위의 방법들에 knowledge distillation을 적용함.</li>
</ul>
</li>
</ol>
<img src="/20211231-gdg-gwangju-2019-ensemble-automl/98C4D0E5-214D-4E71-B474-15FC2349C071.jpeg" class="" title="ml_sdfdl">

<p> </p>
<hr>
<h2 id="Ensemble-정리"><a href="#Ensemble-정리" class="headerlink" title="Ensemble 정리"></a>Ensemble 정리</h2><ul>
<li>무조건 좋은 방법은 존재하지 않는다!</li>
<li>내 모델이 무엇을 요구하는지 알아라!<ul>
<li>학습은 느려도 되는데 인퍼런스는 빨라야하는가?</li>
<li>인퍼런스는 좀 느려도 되는데 학습이 빨라야하는가?</li>
</ul>
</li>
<li>분석/설명이 어렵다는 얘기도 있는데, 이 또한 분석하기에 딸 ㅏ오히려 훨씬 데이터와 문제에 대해 해석하기 유리한 부분이 많다.</li>
<li>Kaggle에서 winning solution의 대부분이 ensemble을 사용했기 때문에 해당 코드들을 보면서 공부하고 연습하자.</li>
</ul>
<p> </p>
<hr>
<h2 id="AutoML"><a href="#AutoML" class="headerlink" title="AutoML"></a>AutoML</h2><p>목적: Data Cleaning 부터 prediction까지 모두 자동화 하는 것이 목표.</p>
<p>딥러닝을 하기 위한 최적의 모델은 무엇인가??</p>
<p>2017년 Google에서 아이디어를 키노트 발표했다. 2년 전부터 얘네는 이걸 하고있다는거잖아! ㅠㅠㅋㅋ 구글 클라우드에서 API를 통해서 사용할 수 있다.</p>
<p>AutoML의 성능은 진짜 좋다…?</p>
<p>AutoML을 하려면 어떤 점을 고려해야할까?</p>
<ul>
<li>Search Space<ul>
<li>선행지식을 사용하면 search space가 간소화되지만, 인간에 의한 bias가 발생</li>
</ul>
</li>
<li>Search Strategy<ul>
<li>Exploration-exploitation trade-off 이슈</li>
</ul>
</li>
<li>Performance Estimation Strategy(Which objective function + How to calculate)<ul>
<li>Cost를 줄이는 방향으로 발전중</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="AutoML-논문"><a href="#AutoML-논문" class="headerlink" title="AutoML 논문"></a>AutoML 논문</h2><p>AutoML의 타입:</p>
<ul>
<li>RL기반<ul>
<li>NASRL, TNAS, ENAS, MNAS</li>
</ul>
</li>
<li>Evolutionary Algorithms (EA):<ul>
<li>TPOP, AmeobaNet</li>
</ul>
</li>
<li>Bayesian Optimization (BO)<ul>
<li>AutoKeras</li>
</ul>
</li>
<li>Sequential Model-Based Optimization (SMBO)<ul>
<li>PNAS</li>
</ul>
</li>
<li>Gradient Descent (GD)<ul>
<li>DARTS</li>
</ul>
</li>
<li>NASRL 2016<ul>
<li>NAS를 널리 알린 논문</li>
<li>강화학습 기반의 아키텍처 최적화.</li>
<li>근데 너무 오래 걸림! 800개의 GPU를 28일동안 해서 한게 CIFAR-10…<ul>
<li>왜일까? 규칙성이 없이 너무 랜덤하게 서치함.</li>
</ul>
</li>
</ul>
</li>
<li>NASNet 2017<ul>
<li>2016년 연구에서 search space를 줄임으로써 속도 개선.</li>
<li>전체 네트워크를 다 디자인하는게 아니라, 셀 구조를 찾아서 쌓는 방식을 넣으면 어떨까?</li>
</ul>
</li>
<li>NASRL에 transfer learning 개념을 도입해서 빠르게 학습할 수 있는 방법을 제안함.<ul>
<li>NASRL은 28일 걸렸는데, 이건 4일 걸림! 800GPU로 4일 트레이닝도 엄청 긴데 ㅋㅋㅋ 여윽씨 구글스러운 연구</li>
</ul>
</li>
<li>AdaNet 2017<ul>
<li>깊이도 중요하지만, 두껍게 만드는것도 좋은 방법인 것 같아서, 앙상블 기법을 사용해봄.</li>
<li>아직도 GPU가 많이 필요함.</li>
</ul>
</li>
<li>ENAS 2018<ul>
<li>아키텍처 서치에 필요한 시간을 획기적으로 줄임!</li>
<li>GTX 1080Ti 1대로 16시간만에 아키텍처를 찾을 수 있음. 다만 NAS보다 성능이 약간 떨어짐.</li>
<li>기존 방법의 문제점은, child model이 학습한 weight들을 완전히 버려버리기 때문. 얘네들이 weight를 공유해서 만들어보게하면 어떨까?</li>
<li>Directed Acyclic Graph (DAG)구조를 써서 효율적으로 만듬. 이게 현재 구글 클라우드 AutoML에 들어간 거라고 함.</li>
</ul>
</li>
<li>DARTS 2018<ul>
<li>딥마인드와 CMU에서 개발.</li>
<li>기존의 RNN구조를 사용하지 않음!</li>
<li>ENAS구조를 따옴…? 기존의 딥러닝에서 학습하는 방법들을 적용할 수 있기 때문에, 장래가 제일 좋은 방법으로 보임.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="AutoML의-문제…"><a href="#AutoML의-문제…" class="headerlink" title="AutoML의 문제…?"></a>AutoML의 문제…?</h2><p>정확도를 높인 모델을 찾는건 이제 잘 한다!</p>
<p>근데… efficiency를 찾는 모델을 찾진 않잖아? 속도가 빠른 모델을 찾을때는?</p>
<p>목적에 따른 차이:</p>
<ul>
<li>Cost는 줄이고 speed를 높히기 위함 (모바일, 임베디드의 경우 quality를 손해봐서라도 이를 원할 때가 있음)</li>
<li>Cost와 speed가 어느정도 정해져 있는 상태에서 성능을 올리기 위함 (한정된 자원에서 모델을 키워야 할 때)</li>
<li>빠른 학습을 위함 (같은 리소스에서 더 많은 데이터를 학습하고 평가하기 위한 경우)</li>
</ul>
<p>그래서 Accuracy와 efficiency를 동시에 찾는 AutoML을 필요로 함!</p>
<p>그래서 나온게:</p>
<ul>
<li>MNAS 2018<ul>
<li>MobileNet, ShuffleNet을 만든 팀과, ENAS를 만든 팀이 합작</li>
<li>Reward에 validation accuracy 뿐만이 아니라 latency (inference speed)도 포함함.<ul>
<li>디텍션, segmentation등</li>
</ul>
</li>
</ul>
</li>
<li>EfficientNet 2019<ul>
<li>MNAS의 저자들이 작성</li>
<li>Accuracy와 efficiency를 둘 다 잡기 위한 model scaling 방법을 제안.</li>
<li>대부분 Model Scaling을 위해 width, depth, image resolution을 늘려왔는데, 많은 경우 나머지 요소를 고정하고 하나의 요소에 대해 집중적으로 scaling을 함. EfficientNet에서는 이것을 균형있게 조정하려고 함.<ul>
<li>성능이 빼앰 완전 좋다</li>
</ul>
</li>
</ul>
</li>
<li>MixNet 2019<ul>
<li>EfficientNet이 Macro-architecture에 대해 다뤘다면, 여기서는 micro-architecture에 대해 다룸.</li>
<li>Depthwise convolution과 group kernel의 개념을 적절히 잘 조합.<ul>
<li>최근에는 5x5를 3x3 두개로 쪼개기보다… 5x5로 하고 5x1과 1x5로 하는 depthwise separation을 하는게 더 좋다…? 으에엥</li>
</ul>
</li>
<li>적절히 잘 섞으니까 성능이 더 좋아지더라! 7x7이나 9x9까지 키운것도 있음. 또, 모바일에서 아주 좋은 효과를 보임. 2019년 8월기준 현재 SOTA.</li>
</ul>
</li>
<li>Randomly Wired Neural networks 2019 (페이스북)<ul>
<li>NAS같은 network generator를 자동으로 만들자! (네트워크를 만드는 네트워크를 만드는 네트워크)</li>
<li>뇌과학쪽에서 예전에 많이 나왔던 개념…?</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="AutoML-정리"><a href="#AutoML-정리" class="headerlink" title="AutoML 정리"></a>AutoML 정리</h2><ul>
<li>AutoML은 생각보다 멀리 있지 않다. 종훈님 말씀으로는 내년부터 실제로 사람들이 쓰지 않을까?</li>
<li>AutoML 자체를 연구하지 않더라도 이를 보고 공부하고 영감을 받을 수 있다.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>Ensemble</tag>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title>GDG 광주 2019 - 머신러닝 vs 딥러닝 개념 잡기 (발표자 - 이용이)</title>
    <url>/20211231-ml-vs-dl/</url>
    <content><![CDATA[<h2 id="원본-발표자료"><a href="#원본-발표자료" class="headerlink" title="원본 발표자료"></a>원본 발표자료</h2><p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMXYyOVNnN2xGaDFlZlVuV1B1dnJxTE0zX2U2T283d09CamZ1OFVtNHNrTkEvcHJlc2VudD9zbGlkZT1pZC5nNWY4NWY2ZWU2Y18yXzY=">[DLSS]01_DL<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h2 id="머신러닝이란"><a href="#머신러닝이란" class="headerlink" title="머신러닝이란?"></a>머신러닝이란?</h2><p>ML - 데이터 기반으로 알고리즘을 추론하는 법<br>DL -  ANN을 이용한 방법인데, 기존 ML의 방법을 아득히 뛰어넘음.</p>
<img src="/20211231-ml-vs-dl/1.png" class="" title="ml_dl">

<h3 id="개발-방법의-패러다임-변화"><a href="#개발-방법의-패러다임-변화" class="headerlink" title="개발 방법의 패러다임 변화"></a>개발 방법의 패러다임 변화</h3><p>이전엔 Explicit programming으로 프로그램을 만들었다.</p>
<ul>
<li>개발자의 사전 지식을 가지고 알고리즘을 설계했다 (룰-베이스)</li>
<li>룰이 너무 많은 경우에 (e.g. 경우의 수가 너무 많은 경우에는) 사람이 전부 정의하기 쉽지 않음.<ul>
<li>바둑? 자율주행?</li>
</ul>
</li>
</ul>
<img src="/20211231-ml-vs-dl/2.png" class="" title="ml_sdfdl">

<p>이제는 ML기법을 사용해서 프로그램을 만들 수 있다.</p>
<ul>
<li>많은 데이터를 이용해서 룰을 뽑아냄.<ul>
<li>많은 데이터 → 더 정확한 시스템</li>
<li>간단하지만 universal rule</li>
<li>새로운 상황 (데이터)가 정의되면 업데이트 하기가 쉬움.<ul>
<li>새로운 데이터로 다시 학습을 시켜주면 됨.</li>
</ul>
</li>
<li>복잡한 상황에서 오히려 사람이 기계로부터 통찰력을 얻게 됨.</li>
</ul>
</li>
</ul>
<img src="/20211231-ml-vs-dl/3.png" class="" title="ml_sdfdl">

<p> </p>
<hr>
<h2 id="머신러닝의-종류"><a href="#머신러닝의-종류" class="headerlink" title="머신러닝의 종류"></a>머신러닝의 종류</h2><ul>
<li>Unsupervised learning<ul>
<li>정답이 정해져있지 않은 데이터</li>
<li>데이터들 간의 관계에 기반해 숨겨진 패턴이나 형태, 구조를 도출.<ul>
<li>이를 통해 데이터를 분류하거나 가공하는데 사용.</li>
</ul>
</li>
</ul>
</li>
<li>Supervised learning<ul>
<li>정답이 있는 데이터셋이 있는 경우, 정답을 모르는 새로운 데이터의 특성을 예측할 수 있음.</li>
</ul>
</li>
<li>Reinforcement learning<ul>
<li>데이터가 전혀 없는데도 사용 가능. 이런 경우에는 동적인 환경에서 (직접 부딪쳐보면서) 데이터를 수집하면서 학습을 진행함.</li>
<li>Reward / Penalty 시스템을 통해서 직접 학습을 함.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Supervised-learning에-집중해보면…"><a href="#Supervised-learning에-집중해보면…" class="headerlink" title="Supervised learning에 집중해보면…"></a>Supervised learning에 집중해보면…</h2><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p>Input에 대응되는 continous output을 찾는 문제</p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>Input에 대응되는 discrete output을 찾는 문제</p>
<p> </p>
<hr>
<h2 id="ML의-파이프라인"><a href="#ML의-파이프라인" class="headerlink" title="ML의 파이프라인"></a>ML의 파이프라인</h2><p>가설→ Cost function → 최적화 (optimisation)</p>
<h3 id="가설"><a href="#가설" class="headerlink" title="가설"></a>가설</h3><ul>
<li>이러이러한 문제는 이러이러한 데이터가 있으면 풀릴것이다~</li>
</ul>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><ul>
<li>적절한 norm을 골라야한다.</li>
</ul>
<h3 id="최적화"><a href="#최적화" class="headerlink" title="최적화"></a>최적화</h3><ul>
<li>문제</li>
</ul>
<img src="/20211231-ml-vs-dl/4.png" class="" title="ml_sdfdl">

<p> </p>
<hr>
<h2 id="Classification-예제"><a href="#Classification-예제" class="headerlink" title="Classification 예제"></a>Classification 예제</h2><h3 id="가설-1"><a href="#가설-1" class="headerlink" title="가설"></a>가설</h3><p>Binary classification은 0과 1사이의 값만 내보내는 함수 - step function.</p>
<p>하지만 step function은 미분이 불가능하기 때문에, 결과 값을 확률로 나타내면서 미분이 가능한 함수 - sigmoid function를 사용한다.</p>
<p>기존에 probit regression이라는 함수도 가능하지만, 미분이 굉장히 복잡해서, sigmoid가 훨씬 쉬워서 사용한다.</p>
<p>Multiclass classification은 softmax 함수를 사용한다.</p>
<h3 id="Cost-function-1"><a href="#Cost-function-1" class="headerlink" title="Cost function"></a>Cost function</h3><p>Sigmoid function에 적합한 loss function을 정의.</p>
<p>Softmax function의 경우에는 cross-entropy 를 loss function으로 사용.</p>
<h3 id="문제들"><a href="#문제들" class="headerlink" title="문제들"></a>문제들</h3><img src="/20211231-ml-vs-dl/5.png" class="" title="ml_sdfdl">

<h3 id="Evaluation-techniques"><a href="#Evaluation-techniques" class="headerlink" title="Evaluation techniques"></a>Evaluation techniques</h3><ul>
<li>Confusion matrix</li>
</ul>
<img src="/20211231-ml-vs-dl/6.png" class="" title="Confusion matrix">

<ul>
<li>Evaluation metrics<ul>
<li>Accuracy - 모든 데이터에 대하여 정답을 제대로 예측한 비율</li>
<li>Precision - 정답이라고 예측한 데이터중에 진짜로 정답의 데이터 비율</li>
<li>Recall - 맞춰야 하는 정답 데이터에 대하여 실제로 정답을 맞춘 데이터의 비율</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><h3 id="ANN"><a href="#ANN" class="headerlink" title="ANN"></a>ANN</h3><p>뉴런이 신호를 보내는건 생화학적 신호가 특정 threshold를 넘으면 전기적 신호를 보낸다.</p>
<p>이와 비슷하게 Activation function을 만들었고, 여러 노드 (뉴런)을 이어서 multi-layer perceptron을 만들었다.</p>
<p>Multilayer perceptron을 최적화하는데 너무 많은 파라미터가 있다.</p>
<p>20년간의 AI Winter가 있다가, 힌튼 교수에 의해 cost function을 미분해서 최적화를 하는 backpropagation이라는 방법이 나타난다.</p>
<p>이제 뉴럴넷에서 최적화를 가능하게 되었다.</p>
<p>그런데, 뉴럴넷을 많이 쌓으면 쌓을수록 backpropagation을 할 때 vanishing gradient 문제로 최적화가 어려워졌다. Chain rule에 의한 sigmoid function의 미분 계수는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="10.224ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 4519.1 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(777.8, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(1833.6, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2963.3, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mn" transform="translate(4019.1, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>에 존재했고, 이 미분 계수는 곱해질수록 0에 수렴했기 때문에 업데이트가 어려워지는 문제가 vanishing gradient 문제이다. 이 때문에 2번째 AI Winter가 나타났다.(1986-2006)</p>
<p>그러면 미분 계수가 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="10.224ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 4519.1 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(777.8, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(1833.6, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2963.3, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mn" transform="translate(4019.1, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>이 아니면 되는게 아닌가? ReLU라는 activation function을 써봤더니, gradient vanishing / exploding이 생기지 않았다. Dying ReLU라는 문제도 있었는데, 이를 고치기 위해 Leaky ReLU라는 activation function도 나타났다.</p>
<img src="/20211231-ml-vs-dl/7.png" class="" title="Activation function">

<p>또 다른 소프트웨어 적 발전으로는, 초기 값을 설정하는게 중요하다는 점이 부각되었다.</p>
<ul>
<li>초기값이 optimal value와 멀리 떨어져있으면 학습시간이 길어지고 이는 overfitting을 야기한다. 그렇다고 초기값이 0으로 사용할 경우, 포워드 프로파게이션에 의해 전달되는 값이 사라지므로, 학습이 불가능해졌다.</li>
<li>이에 대해서 여러가지 초기값 initialization방법들이 나타났다. 현재 사용되는 두가지 방법으로는 Xavier Initialization (glorot 2010)[sigmoid나 tanh에 적합한 초기값 설정법],와 He initialization (He 2015)[ReLU에 적합한 초기값 설정법이 있다.</li>
</ul>
<p>이 외로 여러가지 소프트웨어적인 딥러닝 기법들이 나타났다.</p>
<ul>
<li>오버피팅 피하기<ul>
<li>Batch normalization (Loffe2015)[활성화 값을 강제로 퍼뜨려서 gradient vanishing/exploding 해결]</li>
<li>Dropout (Srivastava2014)</li>
<li>Ensemble</li>
<li>Early Stopping</li>
</ul>
</li>
<li>최적화<ul>
<li>SGD</li>
<li>Momentum (1999) [로컬 미니마에서 나오는 방법]</li>
<li>AdaGrad (Duchi 2011)</li>
<li>Adam (Kingma 2014)</li>
</ul>
</li>
</ul>
<p>이외에 딥러닝을 발전시키는데에는 효율적인 병렬처리를 가능하게 한 GPU의 발전과, 많은 양의 데이터로 오버피팅을 피하게 해줄 수 있는 빅데이터가 나타났다.</p>
<p>이제는 최적화된 뉴럴넷을 자동으로 찾을 수 있는 AutoML이라는 기법도 나타났다. </p>
<p>Learning Transferable Architectures for Scalable Image Recognition (Zoph 2017) [AutoML]</p>
<p> </p>
<hr>
<h2 id="발표자님께서-말하시는-딥러닝을-할-때의-팁"><a href="#발표자님께서-말하시는-딥러닝을-할-때의-팁" class="headerlink" title="발표자님께서 말하시는 딥러닝을 할 때의 팁"></a>발표자님께서 말하시는 딥러닝을 할 때의 팁</h2><ol>
<li>데이터셋 준비</li>
<li>Deep NN 설계<ul>
<li>네트워크 스트럭쳐 설계</li>
<li>액티베이션 펑션 고르기</li>
</ul>
</li>
<li>Initilization 방법<ul>
<li>Xavier, He</li>
</ul>
</li>
<li>Optimisation<ul>
<li>SGD, Momentum, Adam</li>
</ul>
</li>
<li>Overfitting 방지<ul>
<li>Dropout, Early Stopping</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>팔로우 하기 좋은 SLAM 연구자 링크</title>
    <url>/20211231-slam-researcher-follow/</url>
    <content><![CDATA[<h2 id="해외"><a href="#해외" class="headerlink" title="해외"></a>해외</h2><table>
<thead>
<tr>
<th>Name</th>
<th>Tags</th>
<th>Google Scholar</th>
</tr>
</thead>
<tbody><tr>
<td>Andrew Davison</td>
<td>MonoSLAM, Spatial AI, SLAM with deep representations, SLAM with graph processors</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9QTBhZTFhZ0FBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=A0ae1agAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Stefan Leutenegger</td>
<td>BRISK, ElasticFusion, SemanticFusion, OKVIS</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jaC9jaXRhdGlvbnM/dXNlcj1TbUdRNDhnQUFBQUomYW1wO2hsPWRl">https://scholar.google.ch/citations?user=SmGQ48gAAAAJ&amp;hl=de<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Daniel Cremers</td>
<td>Direct SLAM, DSO, Variational methods</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9Y1hRY2lNRUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=cXQciMEAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Davide Scaramuzza</td>
<td>SVO, VIO, Event-based Camera, Agile drone, Active perception</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9U0M5d1Yya0FBQUFKJmFtcDtobD1pdA==">https://scholar.google.com/citations?user=SC9wV2kAAAAJ&amp;hl=it<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Sebastian Thrun</td>
<td>Localization, Probabilistic Robotics, Robot Mapping</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9N0szNGQ3Y0FBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=7K34d7cAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Vijay Kumar</td>
<td>UAV, Visual-Inertial Odometry</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9RlVPRUJEVUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=FUOEBDUAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Marc Pollefeys</td>
<td>3D Reconstruction</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1ZWUgwQmpFQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=YYH0BjEAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Luca Carlone</td>
<td>Hierarchical semantic SLAM, Visual-Inertial Odometry</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9VTRrS1JkTUFBQUFKJmFtcDtobD1pdA==">https://scholar.google.com/citations?user=U4kKRdMAAAAJ&amp;hl=it<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>John Leonard</td>
<td>Beacon, Sonar, iSAM2</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9V1BlN3ZXd0FBQUFKJmFtcDtobD1rbw==">https://scholar.google.com/citations?user=WPe7vWwAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Sebastian Scherer</td>
<td>Deep SLAM frontend, End-to-End deep SLAM, Sensor-fusion</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9Z3hvUGZJWUFBQUFKJmFtcDtobD1kZQ==">https://scholar.google.com/citations?user=gxoPfIYAAAAJ&amp;hl=de<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Guoquan (Paul) Huang</td>
<td>Visual-Inertial Odometry</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9dHJNVXlaSUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=trMUyZIAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Cyrill Stachniss</td>
<td>LiDAR SLAM, OctoMap</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9OHZpYjJsQUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=8vib2lAAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Frank Dellaert</td>
<td>Factor graphs, GT-SAM, iSAM, iSAM2</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9WnhYQmFzd0FBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=ZxXBaswAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Dieter Fox</td>
<td>Localization, Monte Carlo Localization, Probabilistic Robotics, RGB-D SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9RHFYc2JQQUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=DqXsbPAAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Andreas Geiger</td>
<td>Autonomous Driving Perception, Implicit neural representation, KITTI, Scene-flow</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9U3JWbnJQY0FBQUFKJmFtcDtobD1rbw==">https://scholar.google.com/citations?user=SrVnrPcAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Victor Prisacariu</td>
<td>InfiniTAM v3, Deep SLAM frontend</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby51ay9jaXRhdGlvbnM/dXNlcj1HbVdBLUxvQUFBQUomYW1wO2hsPWVu">https://scholar.google.co.uk/citations?user=GmWA-LoAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Vladlen Koltun</td>
<td>View synthesis, Transformer, perception, control</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5uei9jaXRhdGlvbnM/dXNlcj1rZzRiQ3BnQUFBQUomYW1wO2hsPWVu">https://scholar.google.co.nz/citations?user=kg4bCpgAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Vincent Lepetit</td>
<td>DAISY, EPnP, FERNS, object pose-estimation</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9aDBhNXEzUUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=h0a5q3QAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Tat-jun Chin</td>
<td>Geometric Vision, Pose estimation, Robust estimation</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jYS9jaXRhdGlvbnM/dXNlcj1XeXFHRjEwQUFBQUomYW1wO2hsPWVu">https://scholar.google.ca/citations?user=WyqGF10AAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Jacob Engel</td>
<td>LSD-SLAM, DSO</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9bmRPTVpYTUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=ndOMZXMAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Christian Forster</td>
<td>SVO, VIO, AR navigation</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9ejdJQWdJd0FBQUFKJmFtcDtobD1rbw==">https://scholar.google.com/citations?user=z7IAgIwAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Ronald Clark</td>
<td>Information priors for SLAM, scene understanding</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby51ay9jaXRhdGlvbnM/dXNlcj0wYnpxbzBZQUFBQUomYW1wO2hsPWVu">https://scholar.google.co.uk/citations?user=0bzqo0YAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Rui Wang</td>
<td>Direct SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5kZS9jaXRhdGlvbnM/dXNlcj1idU4zeXc4QUFBQUomYW1wO2hsPWVu">https://scholar.google.de/citations?user=buN3yw8AAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Jürgen Sturm</td>
<td>Dense SLAM, RGB-D SLAM, ARCore</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9dzdOYVREMEFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=w7NaTD0AAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Tomasz Malisiewicz</td>
<td>Superpoint, SuperGlue, Augmented Reality</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9UkNUZVRWMEFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=RCTeTV0AAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Paul-Edouard Sarlin</td>
<td>Superglue, hloc, Back to the Feature, pixloc, pixel-perfect sfm</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jaC9jaXRhdGlvbnM/dXNlcj0yZkFVZ2ZBQUFBQUomYW1wO2hsPWVu">https://scholar.google.ch/citations?user=2fAUgfAAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Girgio Grisetti</td>
<td>GraphSLAM, g2o</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5pdC9jaXRhdGlvbnM/dXNlcj15RC1TRkc0QUFBQUomYW1wO2hsPWVu">https://scholar.google.it/citations?user=yD-SFG4AAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Eric Brachmann</td>
<td>6-D Pose Estimation, Camera Pose Regression, DSAC, Localization</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5kZS9jaXRhdGlvbnM/dXNlcj1jQUlzaHNZQUFBQUomYW1wO2hsPWRl">https://scholar.google.de/citations?user=cAIshsYAAAAJ&amp;hl=de<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Torsten Sattler</td>
<td>BAD-SLAM, Image Retrieval, Large-Scale Relocalization, Localization, SCRAMSAC</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9anp4Nl9aSUFBQUFKJmFtcDtobD1kZQ==">https://scholar.google.com/citations?user=jzx6_ZIAAAAJ&amp;hl=de<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Zuzana Kukelova</td>
<td>Grobner Basis, Minimal solvers</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jYS9jaXRhdGlvbnM/dXNlcj1NNGEzVnlZQUFBQUomYW1wO2hsPWVu">https://scholar.google.ca/citations?user=M4a3VyYAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Kenji Koide</td>
<td>LiDAR odometry</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5pdC9jaXRhdGlvbnM/dXNlcj00bUlnQldZQUFBQUomYW1wO2hsPWVu">https://scholar.google.it/citations?user=4mIgBWYAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Vladyslav Usenko</td>
<td>Direct sparse odometry, VIO</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5kZS9jaXRhdGlvbnM/dXNlcj1BUFROS2pvQUFBQUomYW1wO2hsPWVu">https://scholar.google.de/citations?user=APTNKjoAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Nan Yang</td>
<td>Deep Direct SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5kZS9jaXRhdGlvbnM/dXNlcj1wVWoyZmZ3QUFBQUomYW1wO2hsPWVu">https://scholar.google.de/citations?user=pUj2ffwAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Berta Bescos</td>
<td>SLAM in dynamic scenes, Image inpainting SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9OGtvVnBId0FBQUFKJmFtcDtobD1pdA==">https://scholar.google.com/citations?user=8koVpHwAAAAJ&amp;hl=it<i class="fa fa-external-link-alt"></i></span></td>
</tr>
</tbody></table>
<h2 id="국내"><a href="#국내" class="headerlink" title="국내"></a>국내</h2><table>
<thead>
<tr>
<th>Name</th>
<th>Tags</th>
<th>Google Scholar</th>
</tr>
</thead>
<tbody><tr>
<td>Ayoung Kim</td>
<td>Navigation, Perception, Visual-SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9N3l2ZXVmZ0FBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=7yveufgAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Inso Kweon</td>
<td>Deep Learning</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1YQThFT2xFQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=XA8EOlEAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Jongwoo Lim</td>
<td>Omnidirectional SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9NV91d3NHQUFBQUFKJmFtcDtobD1rbw==">https://scholar.google.com/citations?user=5_uwsGAAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Hyun Myung</td>
<td>State estimation</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1OcldmSjFnQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=NrWfJ1gAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Kyungdon Joo</td>
<td>SLAM in structured environments</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1oTnhJUHpNQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=hNxIPzMAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>SangJun Lee</td>
<td>Visual localization, Deep SLAM, Point and Line SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9VnpCQjFTb0FBQUFKJmFtcDtobD1rbw==">https://scholar.google.com/citations?user=VzBB1SoAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>EungChang Lee</td>
<td>VIO, UAV</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1MMDJiMzhvQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=L02b38oAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Hyungtae Lim</td>
<td>LiDAR applications</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vY2l0YXRpb25zP3VzZXI9UzFBM25iSUFBQUFKJmFtcDtobD1lbg==">https://scholar.google.com/citations?user=S1A3nbIAAAAJ&amp;hl=en<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Giseop Kim</td>
<td>Scan Context, LiDAR SLAM, Radar SLAM</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj05bUtPTFg4QUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=9mKOLX8AAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Younggun Cho</td>
<td>SLAM in urban environment</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1XNU1PS1dJQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=W5MOKWIAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Jinyong Jeong</td>
<td>SLAM in urban environment</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1fUkRSUld3QUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=_RDRRWwAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Young-Sik Shin</td>
<td>SLAM in urban environment</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1nR2ZCUmF3QUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=gGfBRawAAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
<tr>
<td>Sungho Yoon</td>
<td>Line descriptor for visual localization</td>
<td><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jby5rci9jaXRhdGlvbnM/dXNlcj1QVXZCejgwQUFBQUomYW1wO2hsPWtv">https://scholar.google.co.kr/citations?user=PUvBz80AAAAJ&amp;hl=ko<i class="fa fa-external-link-alt"></i></span></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual-SLAM 키워드 리스트</title>
    <url>/20211231-visual-slam-keyword/</url>
    <content><![CDATA[<h2 id="Camera-Model-Projective-Geometry"><a href="#Camera-Model-Projective-Geometry" class="headerlink" title="Camera Model + Projective Geometry"></a>Camera Model + Projective Geometry</h2><ul>
<li>Digital Image의 특성<ul>
<li>해상도, 컬러 채널, 대비</li>
<li>Point, Line, Edge, Blob</li>
</ul>
</li>
<li>카메라 / 렌즈 하드웨어<ul>
<li>Sensor Resolution, Aperture, ISO, Exposure, Shutter Speed</li>
<li>Noise model, convolution, filtering, Fourier analysis</li>
</ul>
</li>
<li>영상처리<ul>
<li>rgb2gray, image resize, template matching</li>
</ul>
</li>
<li>Pinhole 카메라 모델, 왜곡, 캘리브레이션<ul>
<li>Pinhole 카메라 모델의 영상 투영</li>
<li>Radial / Tangential 왜곡 모델</li>
<li>Zhang 카메라 캘리브레이션</li>
</ul>
</li>
<li>Projective Geometry<ul>
<li>Vanishing point</li>
<li>Euclidean / Homogeneous coordinates</li>
<li>Euler Angle, Axis-angle, Quaternion, SO(3), SE(3)</li>
<li>Coordinate transformation</li>
</ul>
</li>
</ul>
<h2 id="Feature-Detection-Descriptor-Matching"><a href="#Feature-Detection-Descriptor-Matching" class="headerlink" title="Feature Detection / Descriptor / Matching"></a>Feature Detection / Descriptor / Matching</h2><ul>
<li>Feature Detector<ul>
<li>Properties of corner, Scale space</li>
<li>Harris / Shi-Tomasi</li>
<li>SIFT / SURF</li>
<li>FAST / oFAST (ORB)</li>
</ul>
</li>
<li>Feature Descriptor<ul>
<li>SIFT</li>
<li>BRIEF / rBRIEF (ORB)</li>
</ul>
</li>
<li>Deep local feature<ul>
<li>SuperPoint</li>
<li>D2Net</li>
</ul>
</li>
<li>Feature Matching<ul>
<li>Brute-Force matching</li>
<li>FLANN - Nearest Neighbour</li>
<li>LSH / Multi-probe LSH</li>
<li>HBST</li>
</ul>
</li>
<li>Optical flow, KLT Tracker<ul>
<li>Horn &amp; Schunck L2 / LK / L1 Reg Optical flow</li>
<li>KLT Tracker</li>
</ul>
</li>
<li>Global feature<ul>
<li>Bag-of-visual-words</li>
<li>VLAD</li>
<li>NetVLAD</li>
</ul>
</li>
</ul>
<h2 id="Multiview-geometry"><a href="#Multiview-geometry" class="headerlink" title="Multiview geometry"></a>Multiview geometry</h2><ul>
<li>Epipolar geometry (2D-2D)<ul>
<li>Essential / Fundamental Matrix (5/8-point)</li>
<li>Singular Value Decomposition (SVD)</li>
<li>Homography matrix (4-point)</li>
</ul>
</li>
<li>Multiple view (2D-3D)<ul>
<li>Perspective-n-Points Problem (PnP)</li>
<li>Triangulation / Disparity</li>
</ul>
</li>
<li>Transformation between point clouds (3D-3D)<ul>
<li>Iterative Closet Points Problem (ICP)</li>
</ul>
</li>
<li>Outlier Rejection<ul>
<li>RANSAC</li>
<li>Robust Estimator / Maximum consensus problem</li>
<li>Convex relaxation</li>
</ul>
</li>
<li>Bundle adjustment / Structure from Motion<ul>
<li>Numerical Methods and optimisation</li>
<li>Least squares problem</li>
<li>Gauss-Newton, Levenberg-Marquardt algorithm</li>
<li>Non-linear solver libraries / factor graphs</li>
<li>Motion-only bundle adjustment / Full BA</li>
</ul>
</li>
</ul>
<h2 id="Simultaneous-Localization-and-Mapping"><a href="#Simultaneous-Localization-and-Mapping" class="headerlink" title="Simultaneous Localization and Mapping"></a>Simultaneous Localization and Mapping</h2><ul>
<li>Introduction to SLAM<ul>
<li>Definition of SLAM</li>
<li>Difference between SLAM, SfM, Odometry, Path planning, Image stitching</li>
<li>Indirect (feature-based) vs Direct-based SLAM</li>
</ul>
</li>
<li>Loop closure / Bag of Visual Words<ul>
<li>Descriptor database formulation / query</li>
<li>kd-tree</li>
</ul>
</li>
<li>Mapping<ul>
<li>Stereo matching</li>
<li>OctoMap</li>
<li>TSDF</li>
<li>Voxel Hashing</li>
</ul>
</li>
<li>Sensor fusion / Tracking<ul>
<li>KF, EKF, UKF</li>
<li>Factor graph optimisation</li>
<li>VINS estimator</li>
</ul>
</li>
<li>Rolling Shutter compensation</li>
</ul>
<h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><ul>
<li>Recognition<ul>
<li>HOG, SIFT, GIST</li>
<li>Viola-Jones, Cascades</li>
<li>AdaBoost, kNN, SVM, Random Forest</li>
</ul>
</li>
<li>Principle component analysis<ul>
<li>PCA, ICA, CCA</li>
</ul>
</li>
<li>Estimation<ul>
<li>EM Algorithm → ML, MAP</li>
</ul>
</li>
<li>Clustering<ul>
<li>K-means, Mean-shift, Pedro-clustering</li>
</ul>
</li>
</ul>
<h2 id="Artificial-Neural-Network"><a href="#Artificial-Neural-Network" class="headerlink" title="Artificial Neural Network"></a>Artificial Neural Network</h2><ul>
<li>Neural Network<ul>
<li>Backpropagation, Multi-layer perceptron, SoftMax</li>
<li>Optimization, Stochastic Gradient Descent SGD), ADAM</li>
<li>Batch Norm, Dropout, Momentum</li>
<li>Ensemble, Transfer Learning, Data augmentation</li>
</ul>
</li>
<li>Convolutional Neural Network<ul>
<li>Convolution, Pooling</li>
<li>AlexNet, VGG, GoogLeNet, ResNet, ResNeXT, EfficientNet</li>
<li>Sparse Convolutional Neural Network</li>
</ul>
</li>
<li>Recurrent Neural Network<ul>
<li>RNN, LSTM, GRU, Attention, Transformer</li>
<li>Soft Attention</li>
</ul>
</li>
<li>Detection<ul>
<li>YOLO, MobileNet-SSD, RCNN,</li>
</ul>
</li>
<li>Segmentation<ul>
<li>U-Net</li>
</ul>
</li>
<li>Pose Estimation</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>Visual-SLAM 공부 로드맵</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>2022년에 하고 싶은 것들 로드맵</title>
    <url>/20220102-what-i-want-to-do-2022/</url>
    <content><![CDATA[<h2 id="2022년의-목표"><a href="#2022년의-목표" class="headerlink" title="2022년의 목표"></a>2022년의 목표</h2><p>2022년 목표는 3가지로 크게 나눌 수 있다.</p>
<ul>
<li>내 주요 기술 분야에서 남들과 다른 경쟁력을 갖추기</li>
<li>SLAM이 아닌 돈 많이 벌 수 있는 기술 도전해보기</li>
</ul>
<p> </p>
<h3 id="내-주요-기술-분야에서-남들과-다른-경쟁력을-갖추기"><a href="#내-주요-기술-분야에서-남들과-다른-경쟁력을-갖추기" class="headerlink" title="내 주요 기술 분야에서 남들과 다른 경쟁력을 갖추기"></a>내 주요 기술 분야에서 남들과 다른 경쟁력을 갖추기</h3><p>메인 기술로 가져가는 SLAM 기술분야에서 아주 강력한 kick이 될 수 있는 기술을 가지고 싶다.</p>
<p>해당 기술에는 다음과 같은 후보군이 있다.</p>
<ol>
<li>딥러닝 + SLAM을 통한 Semantic SLAM</li>
<li>SIMD 프로그래밍을 통한 알고리즘 가속.</li>
</ol>
<p>1번의 경우 이미 회사에서 하고 있는 업무이지만 딥러닝 아키텍처 선정 및 학습도 내가 직접 해보고싶다.<br>2번의 경우 기존의 방식보다 몇배는 더 효율적이게 알고리즘을 실행함으로써 그만큼 내가 짜는 알고리즘의 가치도 높힐 수 있지 않을까 생각한다.</p>
<p> </p>
<h3 id="SLAM이-아닌-돈-많이-벌-수-있는-기술-도전해보기"><a href="#SLAM이-아닌-돈-많이-벌-수-있는-기술-도전해보기" class="headerlink" title="SLAM이 아닌 돈 많이 벌 수 있는 기술 도전해보기"></a>SLAM이 아닌 돈 많이 벌 수 있는 기술 도전해보기</h3><p>최근 <strong>메타버스 시장</strong>이 급격하게 떠오르고 있다. 시장이 떠오르고 있다는 것은 돈이 많이 풀리고 있다는 것이고, 또 커지는 시장파이로부터 이득을 얻을 수 있다는 이야기다. 메타버스 시장은 내가 몸을 담궜던 AR 기술과 연관이 높은데, 특히나 내가 가진 SLAM / 3D recon 기술과 좋은 시너지를 보인다.</p>
<p> </p>
<p>하지만 <strong>혼자서는 아무리 기술을 잘 한들 절대 큰 돈을 벌 수 없다</strong>. 큰 기업들에서 몇백명의 월드클래스 엔지니어들을 통해 이미 최고급 수준의 기술을 보유하고 있기 때문에, 나 혼자서는 아무리 24시간 내내 코딩을 한들 큰 경쟁력을 가지기가 어렵다. 성공하려면, 테크자이언트를 뛰어넘을 기발한 아이디어를 보유한 스타트업이 최소 몇십명의 엔지니어를 갈아넣어 생사를 건 베팅을 해야한다. 하지만 성공적인 엑시트의 경우 차익이 엄청나기 때문일까, 최근 굉장히 많은 기술 스타트업이 생겨났다.</p>
<p> </p>
<p>이러한 스타트업 생태계에서 내가 취한 스탠스는 SLAM 기술 세미나를 진행하여 업계 인지도를 높여 최고의 환경을 가진 스타트업에 취직하는 것이였다. 그리고 이 방법은 성공적이였다고 생각한다. 목표를 달성하고 둘러다보니, <strong>아직 다른 방법으로 이 시장에서는 얻어낼 수 있는 것이 많다</strong>고 생각한다.</p>
<p> </p>
<p><strong>기술 스타트업이 필요로 하는 것들을 제공해주며, 대가로 돈을 벌 수 있지 않을까?</strong> 가장 뻔한 방법은 교육이나 컨설팅이겠지만, 이러한 방법은 현 회사도 내켜하지 않을 것이다. 무엇보다 내 시간/노력을 돈과 1:1로 교환하는 것이기 때문에 좋은 방법은 아닌 것 같다 (인터넷 강의는 제외한다. 이 방법은 1:N이라서 좋은듯!)</p>
<p> </p>
<p><strong>가장 좋은건 기술 자체를 판매하는 것이라고 생각한다 (회사가 허용하는 한!)</strong>. 개인적으로 해보고싶은 것은 오픈소스 알고리즘들 보다 더 좋은 알고리즘을 짜서, 데모로 만들어 홍보를 하고 직접 가격에 팔아보는 것이다. 스타트업은 초반에 인지도가 높지 않을 때 엔지니어를 구인하는 것이 어려워 제품 개발에 박차를 가하기가 어려운데, 오픈소스보다 좋은 알고리즘을 적당한 가격에 구할 수 잇으면 적은 인력으로도 빠르게 좋은 제품을 만들지 않을까? ㅎㅎ</p>
<p> </p>
<p>다른 생각으로는, <strong>요즘 블록체인 기술을 통해 완전히 새로운 시장이 열리고 있다</strong>. 완전히 새로운 시작은 누구나 초짜이지 않을까? 아이디어와 운만 좋으면 성공할 수 있지 않을까? 라는 저렴한 생각을 해본다. 마침 좋은 아이디어도 생각나기도 했고… 나도 해보고 싶다는 생각이 들었다 ㅎㅎ </p>
<p> </p>
<hr>
<h2 id="그래서-뭐할건데"><a href="#그래서-뭐할건데" class="headerlink" title="그래서 뭐할건데?"></a>그래서 뭐할건데?</h2><table>
<thead>
<tr>
<th>Tech</th>
<th>Purpose</th>
<th>Expectation</th>
</tr>
</thead>
<tbody><tr>
<td>TensorFlow/PyTorch)</td>
<td>혼자서 Semantic SLAM을 시작할 수 있는 단계가 된다. 풀스택 SLAM 엔지니어가 되기 위한 큰 한걸음!</td>
<td>기본적인 딥러닝 컴퓨터 비전 스택 (Object detection, Instance segmentation, GAN)에 사용되는 모델들을 공부하고, 직접 학습을 진행해보고, 인퍼런스 모델까지 만들어보기.</td>
</tr>
<tr>
<td>ARM Neon SIMD</td>
<td>회사 업무와 개인 프로젝트에도 효율적인 프로그래밍을 할 수 있게 된다. 풀스택 SLAM 엔지니어가 되기 위한 큰 한걸음 2!</td>
<td>ARM CPU에서 빠르게 연산할 수 있는 SIMD 프로그래밍을 공부하여, 훨씬 더 좋은 성능의 영상처리/SLAM 알고리즘을 짜보기.</td>
</tr>
<tr>
<td>—</td>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>Dart/Flutter</td>
<td>내가 만든 영상처리/SLAM 기술들의 모바일 성능을 확인할 수 있는 데모 앱을 만들 수 있게 된다. 이게 잘되면 데모 앱으로 홍보를 해서 기술 모듈을 팔아볼 수 있게 된다.</td>
<td>Flutter 프레임워크를 이용하여 내가 만든 SLAM 모듈을 빠르게 안드로이드/아이폰 앱으로 탑재하기</td>
</tr>
<tr>
<td>Swift</td>
<td>내가 만든 영상처리/SLAM 기술들을 아이폰 포팅할 수 있게 된다.</td>
<td>Flutter를 사용해보다가 내가 원하는 기능이 안나오면 아이폰에만 집중해서 데모 앱으로 만들어보고 싶다</td>
</tr>
<tr>
<td>Javascript/Typescript + Electron + React + Next.js</td>
<td>웹/앱의 기본을 익혀 언제든지 아이디어를 제품으로 만들어보거나 사업을 도전해볼 수 있는 상태가 된다.</td>
<td>내가 생각하고 있는 아이디어를 프로토타입 앱으로 만들어본다.</td>
</tr>
<tr>
<td>Blockchain/NFT/Solidity</td>
<td>블록체인 생태계를 이해하고, NFT의 가능성을 확인해본다. 내 취향이라고 생각되면 직접 코딩을 해볼 수 있는 단계가 된다.</td>
<td>NFT를 발행해보고, 간단한 블록체인 앱을 만들어본다.</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Rust</tag>
        <tag>Web3</tag>
        <tag>NFT</tag>
        <tag>Blockchain</tag>
        <tag>App dev</tag>
        <tag>React</tag>
        <tag>Next.js</tag>
        <tag>Javascript</tag>
        <tag>Swift</tag>
        <tag>Dart</tag>
        <tag>Flutter</tag>
        <tag>SIMD</tag>
      </tags>
  </entry>
  <entry>
    <title>libc++ vs libstdc++</title>
    <url>/20220105-libstdc-libc/</url>
    <content><![CDATA[<h2 id="libc-vs-libstdc"><a href="#libc-vs-libstdc" class="headerlink" title="libc++ vs libstdc++"></a>libc++ vs libstdc++</h2><p>libc++는 Clang 프로젝트에서 나온 C++ native 라이브러리이며, libstdc++는 GCC 프로젝트에서 나온 C++ native 라이브러리이다.</p>
<p>각각 다른 프로젝트에서 나왔기 때문에 C++의 구현 방식이 다르다.</p>
<p>아무래도 GCC는 GNU/Linux를 메인 타겟으로 삼고, Clang은 MacOS를 메인 타겟으로 삼기 때문에, 각각의 라이브러리들은 메인 타겟이 아닌 환경에서는 호환성이 부족한 부분도 있다. C++14까지는 왠만한 기능들은 서로 다 지원이 되지만, C++20과 같이 막 릴리즈된 기능들은 호환성이 부족할 확률이 높다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9nY2MuZ251Lm9yZy9vbmxpbmVkb2NzL2xpYnN0ZGMrKy9tYW51YWwvc3RhdHVzLmh0bWwjc3RhdHVzLmlzby4yMDE3">libstdc++의 C++17 구현 상황<i class="fa fa-external-link-alt"></i></span> 과 <span class="exturl" data-url="aHR0cHM6Ly9saWJjeHgubGx2bS5vcmcvU3RhdHVzL0N4eDE3Lmh0bWw=">libc++의 C++17 구현 상황<i class="fa fa-external-link-alt"></i></span>을 보면 은근히 C++17도 구현이 다 안되어있다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>libc++</tag>
        <tag>libstdc++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 프레임워크 - Boost vs Abseil</title>
    <url>/20220106-boost-vs-abseil/</url>
    <content><![CDATA[<h2 id="딜레마"><a href="#딜레마" class="headerlink" title="딜레마"></a>딜레마</h2><p>고객사의 요청으로 C++11로 코드를 짜달라는 요청을 받았다.</p>
<p>C++17의 다양한 유틸리티 기능에 익숙한 나로써는 C++11은 조금 불편하지 않을까, 라는 생각이 들었다.</p>
<p>C++11 빌드임에도 C++17, 또는 그 후의 기능을 사용할 수 있는 방법이 없을까?</p>
<p> </p>
<p>예전에 들은 바로는, Boost와 같은 C++ 프레임워크에서 사용하던 좋은 기능들이 STL에 추가되기도 한다고 들었다.</p>
<p>이러한 프레임워크들을 사용하면 C++11 빌드임에도 그 이후 스탠다드의 기능들을 사용할 수 있을 것이라고 생각이 든다.</p>
<p> </p>
<h2 id="Boost"><a href="#Boost" class="headerlink" title="Boost"></a>Boost</h2><p><span class="exturl" data-url="aHR0cHM6Ly90aGVib29zdGNwcGxpYnJhcmllcy5jb20v">Boost<i class="fa fa-external-link-alt"></i></span>가 지향하는 방향은 **’추후 C++ Standard에 포함될 기능’**을 만드는 것이다. </p>
<p>그렇기에 우리가 기대할 수 있는 특징은 2가지가 있다.</p>
<ul>
<li>기존의 STL과의 호환성이 높다</li>
<li>하드웨어 호환성에 초점을 높게 두지는 않았을 것이다.</li>
<li>실험적인 기능들이 많이 있을 것이다.<ul>
<li>추후 C++ standard에 포함될 것들을 미리 써볼 수 있다.</li>
<li>종종 Boost 릴리즈 버전이 올라가면서 사라지는 기능들도 있을 것이다<ul>
<li>하지만 이 기능은, 우리가 Boost 버전을 고정하면 문제가 되지 않을 것이다. (i.e. 짧은 수명을 가진 프로젝트에는 문제가 되지 않는다)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Boost가 가진 기능들은 다음과 같다.</p>
<img src="/20220106-boost-vs-abseil/boost.png" class="" title="Boost">

<p> </p>
<h2 id="Abseil"><a href="#Abseil" class="headerlink" title="Abseil"></a>Abseil</h2><p><span class="exturl" data-url="aHR0cHM6Ly9hYnNlaWwuaW8v">Abseil<i class="fa fa-external-link-alt"></i></span>은 Google이 만든 C++ 프레임워크로써, <strong>호환성</strong>에 중점을 둔다.</p>
<p>Abseil이 제공하는 <span class="exturl" data-url="aHR0cHM6Ly9hYnNlaWwuaW8vYWJvdXQvY29tcGF0aWJpbGl0eQ==">하드웨어 호환성 리스트<i class="fa fa-external-link-alt"></i></span>가 있는데, Abseil 버전이 올라가면서 왠만한 경우에 Abseil API는 지속적으로 호환된다.</p>
<p>C++ 스탠다드가 지원하는 기능들은 <del>전부</del> 지원한다고 보면 되고, 그 외의 추가적인 개선점이나 기능들을 사용할 수 있다.</p>
<ul>
<li>Update:<ul>
<li>컴파일러 단계에서 지원하는 기능들이 지원되지 않는다<ul>
<li>e.g. C++17 structured-binding</li>
</ul>
</li>
<li>특정 기능들이 지원되지 않는다<ul>
<li>e.g. std::filesystem</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20220106-boost-vs-abseil/abseil.png" class="" title="abseil">

<p> </p>
<h2 id="그래서-Boost-vs-Abseil"><a href="#그래서-Boost-vs-Abseil" class="headerlink" title="그래서, Boost vs Abseil?"></a>그래서, Boost vs Abseil?</h2><ul>
<li>Boost를 써도 될 때<ul>
<li>C++의 가장 최신 기능을 사용하고싶다면</li>
<li>Long-term life를 고려하지 않는다면</li>
</ul>
</li>
<li>Abseil을 써도 될 때<ul>
<li>호환성이 높은 프레임워크를 쓰고 싶다면 </li>
<li>Long-term life가 중요하다면</li>
<li>STL 보다 좀 더 좋은 성능 + 적당히 최신 기능을 사용하고 싶다면</li>
</ul>
</li>
<li>둘 다 쓰면 안될 때<ul>
<li>이미 컴파일된 코드를 오래오래 쓰고 싶다면</li>
</ul>
</li>
</ul>
<h2 id="참고자료"><a href="#참고자료" class="headerlink" title="참고자료"></a>참고자료</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ncm91cHMuZ29vZ2xlLmNvbS9nL2Fic2VpbC1pby9jL3ZDUTNMMTBZRmRNP3BsaT0x">https://groups.google.com/g/abseil-io/c/vCQ3L10YFdM?pli=1<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Boost</tag>
        <tag>Abseil</tag>
      </tags>
  </entry>
  <entry>
    <title>IROS 2019 VINS Workshop 정리</title>
    <url>/20220107-2019-iros-vins-workshop/</url>
    <content><![CDATA[<h2 id="Presentations"><a href="#Presentations" class="headerlink" title="Presentations"></a>Presentations</h2><ul>
<li>Stergios Roumeliotis (UMN): A Short Tutorial on VINS (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL3N0ZXJnaW9zLXJvdW1lbGlvdGlzLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Davide Scaramuzza (Zurich): Visual Inertial SLAM: Current Status and the Road Ahead (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL2RhdmlkZS1zY2FyYW11enphLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Laurent Kneip (Shanghai Tech): Dimensionality reduction in visual-inertial SLAM (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL2xhdXJlbnQta25laXAucGRm">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Maurice Fallon (Oxford): VILENS - the Challenge of Visual Navigation on Quadruped Robots (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL21hdXJpY2UtZmFsbG9uLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Luca Carlone (MIT): Chasing a Chimera: from VIN to real-time high-level understanding (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL2x1Y2EtY2FybG9uZS5wZGY=">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Guofeng Zhang (ZJU): Robust VI-SLAM and HD-Map Reconstruction for Location-based Augmented Reality (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL2d1b2ZlbmctemhhbmcucGRm">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Giuseppe Loianno (NYU): Challenges and Opportunities for Visual Inertial Navigation of Aerial Robots (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL2dpdXNlcHBlLWxvaWFubm8ucGRm">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Paloma Sodhi / Michael Kaess (CMU): Robust Multi-Stereo Visual-Inertial Odometry (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL21pY2hhZWwta2Flc3MucGRm">slides<i class="fa fa-external-link-alt"></i></span>)</li>
<li>Ross Hartley (Amazon): Contact-aided Invariant Extended Kalman Filtering for Legged Robot State Estimation (<span class="exturl" data-url="aHR0cHM6Ly91ZGVsLmVkdS9+Z2h1YW5nL2lyb3MxOS12aW5zLXdvcmtzaG9wL3Jvc3MtaGFydGxleS5wZGY=">slides<i class="fa fa-external-link-alt"></i></span>)</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>VINS</tag>
        <tag>VIO</tag>
        <tag>IROS</tag>
        <tag>Workshop</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 초고수가 되는 길 (Herb Sutter의 Gotw)</title>
    <url>/20220107-gotw/</url>
    <content><![CDATA[<h2 id="C-고수가-되는-길"><a href="#C-고수가-되는-길" class="headerlink" title="C++ 고수가 되는 길"></a>C++ 고수가 되는 길</h2><p>Gotw는 Herb Sutter가 작성한 C++의 고급 문제들을 모아놓은 블로그이다.<br>2013년부터 꾸준하게 추가되고 있으며, C++에 대한 깊은 이해를 요구한다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9oZXJic3V0dGVyLmNvbS9nb3R3Lw==">https://herbsutter.com/gotw/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220107-gotw/gotw.png" class="" title="gotw">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Herb Sutter</tag>
        <tag>gotw</tag>
      </tags>
  </entry>
  <entry>
    <title>Leaving Google (원저자 - Jay Conrod)</title>
    <url>/20220107-leaving-google-jayconrod/</url>
    <content><![CDATA[<h2 id="시작하기-전"><a href="#시작하기-전" class="headerlink" title="시작하기 전"></a>시작하기 전</h2><p>Google에서 2015년부터 일한 사람이 Google에서의 후기와 SW Engineer 포지션에 대한 꿀팁을 작성한 글이다.<br><span class="exturl" data-url="aHR0cHM6Ly9qYXljb25yb2QuY29tL3Bvc3RzLzEyMi9sZWF2aW5nLWdvb2dsZQ==">블로그 글<i class="fa fa-external-link-alt"></i></span>로 공유해주신 내용을 정리했다.</p>
<ul>
<li>Google Docs 팀에서 일하다가</li>
<li>Bazel 팀으로 이동했다.</li>
</ul>
<p> </p>
<hr>
<h2 id="Docs-팀에서의-교훈"><a href="#Docs-팀에서의-교훈" class="headerlink" title="Docs 팀에서의 교훈"></a>Docs 팀에서의 교훈</h2><blockquote>
<p>TLDR;</p>
<ol>
<li>팀에는 멘토가 있어야한다.</li>
<li>매니지먼트가 날 서포트 할 수 있어야한다.</li>
<li>조직의 목표와 내 업무적 가치관이 잘 맞는지 확인하자.</li>
</ol>
</blockquote>
<h3 id="멘토"><a href="#멘토" class="headerlink" title="멘토"></a>멘토</h3><blockquote>
<p><strong>새로운 팀에 참여할 때, 해당 팀에 멘토가 있는지 꼭 확인하자</strong></p>
</blockquote>
<p>Jay는 신설 Docs팀에 합류하였다. 정확히 얘기하자면, Docs팀은 원래 Google Drive 팀에 소속되어있다가 막 분리된 제품 팀이였는데, 새로 만든 조직이였기 때문에 팀 인원이 막 입사한 사람들로 이뤄져있었다.</p>
<p>그러다보니 팀의 인프라 구조를 아는 사람도 없었고, 누구한테 어떤 질문을 던져야하는지도 잘 모르는 상황이 되었다.</p>
<p>이 부분에 대해 Jay가 조언하는 바는, <strong>‘새로운 팀에 참여할 때, 해당 팀에 멘토가 있는지 꼭 확인하자’</strong> 이다. 누군가는 프로젝트 아키텍처, 역사, 조직의 소통 및 정치(?) 구조를 설명해 줄 수 있어야한다. 내가 새로운 팀에 참여하면 꼭 이런 것들을 알려줄 수 있는 사람을 찾자.</p>
<h3 id="매니지먼트"><a href="#매니지먼트" class="headerlink" title="매니지먼트"></a>매니지먼트</h3><blockquote>
<p><strong>‘승진하기 위해서는 매니지먼트의 서포트가 필수적이다’</strong></p>
</blockquote>
<p>Docs 팀의 매니저는 바쁜 사람이였다. 25개의 리포트를 한번에 처리해야했고, 팀원과의 미팅도 3주에 한번 (그것도 겨우 30분)만 했다. 이런 형태가 지속되다보니, 몇달이 지나도 매니저는 Jay의 프로젝트 이름을 제대로 기억하지도 못했다. 이런 상황에서 승진은 꿈꿀 수도 없다.</p>
<p>심지어 Jay는 이전 직장인 Qualcomm에서 더 높은 직책이였는데, Google로 옮길 때는 하나 낮은 직책으로 가도 금방 실력을 보여서 승진할 수 있을 것이라고 생각했다. 아무리 실력이 좋아도, 매니지먼트가 관심을 줄 수 없는 상황이라면 승진은 어렵다. 2년동안 Docs에서 일하다가 못참겠어서 다른 팀으로 이동하고, 그 팀에서는 1년만에 승진했다고 한다.</p>
<h3 id="조직-나의-목표"><a href="#조직-나의-목표" class="headerlink" title="조직/나의 목표"></a>조직/나의 목표</h3><blockquote>
<p><strong>‘조직과 나의 목표가 맞아야한다’</strong></p>
</blockquote>
<p>조직의 목표에 내 목표를 억지로 맞추자는 것이 아니다.</p>
<p>Docs팀 PM의 목표는 고객에게 새로운 피쳐를 지속적으로 빠르게 전달해주는 것이였다. 그에 반해, Jay의 목표는 간단명료하고 버그가 없는 잘 도는 소프트웨어를 만드는 것을 중요시하였다. 둘 중 무엇이 맞고 무엇이 틀렸다는 것이 아니다 (둘 다 중요하다). 하지만 개발 방법론에 있어서 이 둘은 상충되기도 한다.</p>
<p>조직의 목표과 내 목표가 다르다면, 조직이 요구하는 업무를 수행하면서 나는 절대로 행복할 수 없을 것이다.</p>
<p> </p>
<hr>
<h2 id="Go-팀에서의-교훈"><a href="#Go-팀에서의-교훈" class="headerlink" title="Go 팀에서의 교훈"></a>Go 팀에서의 교훈</h2><blockquote>
<p>TLDR;</p>
<ol>
<li>많이 아는 것도 좋지만 빠르게 공부할 줄 아는 것도 굉장히 중요하다.</li>
<li>개개인 단위에서 집중할 수 있는만큼만 중장기 프로젝트를 운용하자. 제품을 책임지는건 팀이다. 내가 아니고.</li>
<li>대규모 프로젝트에 참여하는건 종종 ‘지루할 수도’ 있다.</li>
</ol>
</blockquote>
<h3 id="공부"><a href="#공부" class="headerlink" title="공부"></a>공부</h3><p>Jay는 2017년에 Go 팀으로 이동하며, Bazel 빌드 시스템에 Go 지원 기능을 추가하는 업무를 담당하게 되었다. Jay는 Go나 Bazel을 잘 쓸 줄 몰랐다 (본인 말로는 책 하나 읽은 정도라고 한다). Jay의 팀원들 중에는 팀에 참여하고나서 Go를 처음 배운 사람들도 있다고 한다.</p>
<p>Jay는 ‘N년의 경력’ 같은건 중요하지 않다고 한다. 어떤 것이든 빠르게 배워서 적용할 수 있으면, 경력 따위는 중요하지 않다고 한다. <code>'Smart people can pick up anything quickly, and diverse perspectives are valuable'</code>.</p>
<h3 id="업무로드-amp-번아웃"><a href="#업무로드-amp-번아웃" class="headerlink" title="업무로드 & 번아웃"></a>업무로드 &amp; 번아웃</h3><p>Go 팀의 단점을 꼽자면, 팀원 모두가 엄청나게 일을 많이 하고 있었다는 것이다. 물론 팀원들은 모두 생산성 넘치고 똑똑한 사람들이였지만, 업무로드가 믿기지 않을 정도로 많이 걸려있었다. 프로젝트는 유지보수 업무가 끝나지 않았기 때문에 지속적으로 끌고가면서, 계속 추가적인 프로젝트가 들어왔다. Jay는 나중에는 6개의 프로젝트를 진행해야했다.</p>
<p>동시에 너무 많은 프로젝트를 진행하면 정신도 여러 곳으로 분산될 뿐만이 아니라 시간을 제대로 할애하지 못한다. 6개의 프로젝트를 진행할 때에는 이슈 트랙킹과 코드리뷰만 해도 하루가 끝나기도 했다.</p>
<h3 id="대규모-프로젝트에서는-똑똑하게-일해야한다-그게-항상-재밌고-활기찬건-아니다"><a href="#대규모-프로젝트에서는-똑똑하게-일해야한다-그게-항상-재밌고-활기찬건-아니다" class="headerlink" title="대규모 프로젝트에서는 똑똑하게 일해야한다 (그게 항상 재밌고 활기찬건 아니다)"></a>대규모 프로젝트에서는 똑똑하게 일해야한다 (그게 항상 재밌고 활기찬건 아니다)</h3><p>Bazel 팀에서 일 할 때에는, 동료들이 특정 버그나 이슈가 생겼을 때 바로바로 해결해주고 고맙다는 이야기를 많이 들었다. 굉장히 뿌듯했고, 하나씩 쳐내간다는 성취감이 아주 좋았다. 하지만 이런 액션은 사실 끝도없이 계속 나타난다. 도큐먼트가 생기기 전 까지는 어쩔 수 없이 반복하게 된다.</p>
<p>Module reference documentation에 대해 작업을 한 적이 있다. 약 80장 정도 되는 리포트에 Module system의 작동원리를 적었고, 빌드 중 나타나는 이슈들에 대한 매뉴얼을 적었다. 도큐먼트를 적는거는 지루하지만, 결과적으로 더 많은 사람들이 빌드를 하는데에 생기는 이슈들을 피할 수 있었다.</p>
<p> </p>
<hr>
<h2 id="구글에서-일을-할-수-있던-이유"><a href="#구글에서-일을-할-수-있던-이유" class="headerlink" title="구글에서 일을 할 수 있던 이유"></a>구글에서 일을 할 수 있던 이유</h2><p>좋은 동료들이 있어서!</p>
<p>Docs 팀에서는 밤새 게임을 같이 하기도 했다. 바에 같이 놀러가기도 했다.</p>
<p>Go 팀에서는 요리를 같이하거나 디너 파티를 하기도 했다. GopherCon에서 같이 마가리타를 마시면서 당구를 친건 정말 재밌었다.</p>
<p>밝은 분위기를 만들어주는 팀원들에 감사한다.</p>
<p>원격근무로 전환 시 이런 좋은 관계들을 가지지 못할 것 같아서 아쉽다. 이러한 관계는 직접 만나서만 만들 수 있기 때문이다.</p>
<p> </p>
<hr>
<h2 id="구글을-떠난-이유"><a href="#구글을-떠난-이유" class="headerlink" title="구글을 떠난 이유"></a>구글을 떠난 이유</h2><p>Google에 있는 사람들은 다 조금씩 ‘Google이 직장 중에 제일 좋다’라는 생각을 가지고 다닌다. 딱, 퇴사할 때 까지만.</p>
<p>Google 보다 좋은 곳들은 사실 많다.</p>
<p>Google을 떠나게 된 이유는 다음과 같다.</p>
<h3 id="번아웃"><a href="#번아웃" class="headerlink" title="번아웃"></a>번아웃</h3><p>지쳤다. 이전에 트윗을 올린게 화제가 된 적이 있다 (모두가 피곤해서, 미팅에서 아무도 얘기할 거리를 가져오지 않아서 미팅이 계속 취소된다는 트윗. 미팅을 진행해도 별 이야기 없이 엄청 금방 끝난다는 트윗.).</p>
<p>지쳤고, 일이 몰렸고, 코로나 때문에 엄청 불편하게 되었다. 잠을 잘 못자게 되었고, 두통이 자주 찾아오고 허리가 아팠다. ‘Brain fog’ 때문에 집중하는것도 어려웠고, 결정을 내리기도 어려웠다. 말을 하던 도중에 단어와 이름을 자주 잊어버렸고, 대화의 내용을 잊어버렸다. 회사 업무 외의 다른 일을 할 에너지도 없었다.</p>
<h3 id="커리어-amp-보상"><a href="#커리어-amp-보상" class="headerlink" title="커리어 & 보상"></a>커리어 &amp; 보상</h3><p>이렇게 힘들때는 보통 좀 쉬고 오면 괜찮아진다. Google은 3달은 무급휴가로 쉴 수 있다. 그럼에도 Google을 나온 이유는 다음과 같다.</p>
<p>Google에서 원격근무 포지션으로 전환 시 연봉의 10%를 차감한다고 했다. 이 방침이 결정되기 전에 Jay는 San Diego로 이사를 갔다. 물론 Jay는 원격근무 방침이 생길 것을 알고 있었고, 해당 방침으로 인해 연봉 조건이 안 좋아질 경우 퇴사를 할 각오도 다져놨다. 동일한 팀에서 동일한 일을 하는데 연봉 10% 삭감은 Jay의 입장에서는 받아드릴 수 없는 부분이였다. 특히나, 회사가 돈이 없거나 그런 것도 아니였다. Google Cloud는 2021년 Q1에 전년도에 비해 53%의 매출이 더 났기 때문이다.</p>
<p>또 다른 이유로는, Go 팀에서 하던 작업으로는 L6로 승진을 할 것 같지 않았기 때문이다. Google은 Jay가 개발한 go command 나 module로 작업을 하지 않는다. 단순히 오픈소스의 용도로만 만든 것이다. Google에서 L5 이상으로 승진을 하기 위해서는 비즈니스에 직접적으로 수익을 가져올 수 있는 작업을 해야하기 때문에, 대부분의 Go 팀 인원들은 L5에 있었다. 더 높은 직책으로 가기 위해서는 Google 내 새로운 포지션을 찾거나 이직을 해야했다.</p>
<p> </p>
<hr>
<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><p>쉬고 오신답니다 ㅎㅎ</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Career</tag>
      </tags>
  </entry>
  <entry>
    <title>MacOS 꿀팁 블로그 레퍼런스</title>
    <url>/20220107-macos/</url>
    <content><![CDATA[<h2 id="Subicura-블로그"><a href="#Subicura-블로그" class="headerlink" title="Subicura 블로그"></a>Subicura 블로그</h2><p><span class="exturl" data-url="aHR0cHM6Ly9zdWJpY3VyYS5jb20vbWFjLw==">https://subicura.com/mac/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220107-macos/mac.png" class="" title="Mac">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>MacOS</tag>
        <tag>Apple</tag>
        <tag>MacBook</tag>
      </tags>
  </entry>
  <entry>
    <title>MLOps for all 레퍼런스</title>
    <url>/20220107-mlops-for-all/</url>
    <content><![CDATA[<h2 id="MLOps-for-all"><a href="#MLOps-for-all" class="headerlink" title="MLOps-for-all"></a>MLOps-for-all</h2><p>마키나락스에서 만든 MLOps 튜토리얼 </p>
<p><span class="exturl" data-url="aHR0cHM6Ly9tbG9wcy1mb3ItYWxsLmdpdGh1Yi5pby8=">https://mlops-for-all.github.io/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220107-mlops-for-all/mlops" class="mlops.png">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>MLOps</tag>
        <tag>MakinaRocks</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>모던 C++ 치트시트</title>
    <url>/20220107-modern-cpp-features-cheatsheet/</url>
    <content><![CDATA[<h2 id="Cheatsheet"><a href="#Cheatsheet" class="headerlink" title="Cheatsheet"></a>Cheatsheet</h2><p>C++11/14/17/20 마다 어떤 변화가 있는지 잘 기억이 안날 때, 바로 찾아보기 좋은 웹사이트이다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0FudGhvbnlDYWxhbmRyYS9tb2Rlcm4tY3BwLWZlYXR1cmVz">https://github.com/AnthonyCalandra/modern-cpp-features<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220107-modern-cpp-features-cheatsheet/modern-cpp.png" class="" title="Modern C++">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Modern C++</tag>
        <tag>Cheatsheet</tag>
      </tags>
  </entry>
  <entry>
    <title>3D Rotation converter 툴</title>
    <url>/20220108-3d-rotation-converter/</url>
    <content><![CDATA[<h2 id="3D-Rotation-converter-툴"><a href="#3D-Rotation-converter-툴" class="headerlink" title="3D Rotation converter 툴"></a>3D Rotation converter 툴</h2><p>3D Rotation converter는 Andre Gaschler가 만든 온라인 계산기이다.</p>
<p>Rotation matrix &lt;-&gt; Quaternion &lt;-&gt; Axis-angle &lt;-&gt; Euler angle 변환 계산을 할 때 굉장히 편하다.</p>
<p>Euler angle의 순서 및 방향을 확인하기 위해 (e.g. XYZ, ZYX) 간단한 값들을 넣어서 확인해볼 수 있다.</p>
<p>또, Eigen 기반으로 rotation conversion 관련 함수를 짤 때 굉장히 유용하게 쓸 수 있다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5kcmUtZ2FzY2hsZXIuY29tL3JvdGF0aW9uY29udmVydGVyLw==">https://www.andre-gaschler.com/rotationconverter/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220108-3d-rotation-converter/rotConvert.png" class="" title="rotation-converter">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>3D Rotation converter</tag>
        <tag>Geometry</tag>
      </tags>
  </entry>
  <entry>
    <title>(0) - Introduction to blockchain (비트코인, 이더리움, 오라클, 블록체인, DAO)</title>
    <url>/20220108-blockchain-coding/</url>
    <content><![CDATA[<h2 id="영상-링크"><a href="#영상-링크" class="headerlink" title="영상 링크"></a>영상 링크</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/M576WGiDBdQ" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<hr>
<h2 id="비트코인"><a href="#비트코인" class="headerlink" title="비트코인"></a>비트코인</h2><ul>
<li>비트코인은 블록체인을 사용한다<ul>
<li>Satoshi Nakamoto가 만들었다<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9iaXRjb2luLm9yZy9iaXRjb2luLnBkZg==">BitCoin whitepaper 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li><strong>탈중앙화된 네트워크에서 거래</strong>를 할 수 있다</li>
<li>Cryptography로 작동한다</li>
<li>‘디지털 금’이라고도 불린다<ul>
<li>실제 금과 비슷하게 <strong>한정된 수량</strong>만이 존재하며, 거래를 할 수 있기 떄문이다</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="이더리움"><a href="#이더리움" class="headerlink" title="이더리움"></a>이더리움</h2><ul>
<li>이더리움도 블록체인을 사용한다<ul>
<li>Vitalik Buterin이 만들었다 (2015)<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ldGhlcmV1bS5vcmcvZW4vd2hpdGVwYXBlci8=">Ethereum whitepaper 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>비트코인과 동일한 인프라를 가지고 있지만, 다른 추가된 기능이 있다.<ul>
<li><strong>비트코인의 구조에 smart contract를 추가할 수 있게 만든 점</strong>이다.<ul>
<li>이를 통해 **탈중앙화된 앱 (i.e. dApp)**이나 **탈중앙화 된 조직 (i.e. DAO)**를 만들 수 있다.<ul>
<li>이 방식은 1994년에 Nick Sazbo가 만든 아이디어이다</li>
</ul>
</li>
<li>깊게 따지고 보면 비트코인에도 이 기능이 있지만, 이더리움 만큼 완성된 기능은 아니다.<ul>
<li>비트코인은 Asset이라는 점에 중점을 둔 기능이고, <strong>이더리움은 Asset + 앱개발 utility에 중점</strong>을 둔 것이라고 보면 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Oracles-amp-Hybrid-smart-contract"><a href="#Oracles-amp-Hybrid-smart-contract" class="headerlink" title="Oracles & Hybrid smart contract"></a>Oracles &amp; Hybrid smart contract</h2><blockquote>
<p>Blockchain oracle과 oracle problem에 대한 문서 (<span class="exturl" data-url="aHR0cHM6Ly9lbmN5Y2xvcGVkaWEucHViLzM3MTg=">링크<i class="fa fa-external-link-alt"></i></span>)</p>
</blockquote>
<ul>
<li><strong>Oracle</strong>은 블록체인 <strong>바깥에 있는 실제 세상의 정보를 체인으로 가져오는 시스템</strong>이다.<ul>
<li>Smart contract 시스템이 실제 세상의 정보를 사용하는 경우에는 (e.g. 날씨, 주식 가격, 뉴스), 외부 정보를 끌어오는 시스템이 필요하다.<ul>
<li>이러한 외부 정보는 탈중앙화된 시스템에서 온 것이 아니다.</li>
</ul>
</li>
<li>블록체인은 탈중앙화 된 계산 방식을 사용하여 신뢰를 구축한다<ul>
<li>체인 내부의 정보로만 계산하면 안전하다.</li>
<li>체인 외부의 정보 (i.e. Oracle 정보)는 탈중앙화되어있지 않기 때문에 위험하다<ul>
<li>이러한 외부 정보는 ‘근원’을 믿을 수 없다. 해킹당하거나 한다면?<ul>
<li>i.e. <strong>Oracle problem</strong></li>
</ul>
</li>
<li>체인 외부의 정보도 탈중앙화 할 수 있는가?<ul>
<li>탈중앙화된 체인 로직과 탈중앙화 되지 않은 외부정보 사이에 layer를 둠으로써 이 둘을 연결할 수 있다.<ul>
<li>i.e. <strong>Hybrid smart contract</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Features-of-blockchain"><a href="#Features-of-blockchain" class="headerlink" title="Features of blockchain"></a>Features of blockchain</h2><h3 id="Decentralized"><a href="#Decentralized" class="headerlink" title="Decentralized"></a>Decentralized</h3><ul>
<li>Decentralized<ul>
<li>단일 객체가 데이터를 통제하지 않는다</li>
<li>대신, 수많은 node operator가 소프트웨어를 통해 연결되어있다</li>
</ul>
</li>
<li>Benefits<ul>
<li>단일 객체가 우리의 결정에 개입하지 못한다<ul>
<li>은행은 우리 계좌를 마음대로 정지시킬 수 있다</li>
<li>주식 거래 앱이 특정 주식을 매수/매도 하는데에 제약을 걸면? (e.g. Robinhood Gamestop 사태)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Transparency-amp-Flexibility"><a href="#Transparency-amp-Flexibility" class="headerlink" title="Transparency & Flexibility"></a>Transparency &amp; Flexibility</h3><ul>
<li>Transparency<ul>
<li>모두가 똑같은 규칙을 지킨다</li>
<li>모든 거래내역을 확인할 수 있다<ul>
<li>내가 모르는 어떤 불공정 거래가 나타날 수 없다</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Speed-amp-Efficiency"><a href="#Speed-amp-Efficiency" class="headerlink" title="Speed & Efficiency"></a>Speed &amp; Efficiency</h3><ul>
<li>Speed<ul>
<li>은행 업무처리는 느리다<ul>
<li>주식 매수/매도 신청을 넣고 적용되는데 까지 꽤 걸린다</li>
</ul>
</li>
<li>체인에서는 곧바로 된다</li>
</ul>
</li>
</ul>
<h3 id="Security-amp-Immutability"><a href="#Security-amp-Immutability" class="headerlink" title="Security & Immutability"></a>Security &amp; Immutability</h3><ul>
<li>Immutabiliity<ul>
<li>블록체인 데이터를 corrupt 시키는 것은 어렵다</li>
<li>모든 node가 꺼지기 전 까지 체인은 살아있다</li>
</ul>
</li>
</ul>
<h3 id="Removal-of-counterparty-risk"><a href="#Removal-of-counterparty-risk" class="headerlink" title="Removal of counterparty risk"></a>Removal of counterparty risk</h3><ul>
<li>Counterparty risk<ul>
<li>중앙화된 객체의 목적은 보통 우리의 목적과 다르다<ul>
<li>e.g. 보험사와 나의 관계<ul>
<li>나는 보험사에게 매달 100달러씩 내며, 사고 시 보험사는 내 사고에 대한 비용을 지불한다</li>
<li>내 목적은 안전이지만, 보험사의 목적은 수익창출이다</li>
<li>사고가 나더라도 보험사는 계약 내의 허점을 찾아 지출을 최소한으로 줄이려고 할 것이다</li>
<li>이 때, 계약의 내용은 보험사가 만들기 때문에 (i.e. 보험사 상품), 나는 항상 불리한 위치에 서게 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Trust-minimized-agreement"><a href="#Trust-minimized-agreement" class="headerlink" title="Trust minimized agreement"></a>Trust minimized agreement</h3><ul>
<li>Trust<ul>
<li>기반이 없는 신뢰 (e.g. 브랜드 이름값)에 기대는 것이 아닌, 수식적으로 사전정의된 계약을 사용할 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="DAO"><a href="#DAO" class="headerlink" title="DAO"></a>DAO</h2><ul>
<li>DAO<ul>
<li>고객에게 블록체인 기반 서비스를 제공하여 신뢰를 구축하는 (i.e. trustless) 기업</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Solidity, Blockchain, and Smart Contract 강의 정리</category>
      </categories>
      <tags>
        <tag>Blockchain</tag>
        <tag>Smart contract</tag>
        <tag>Bitcoin</tag>
        <tag>Ethereum</tag>
        <tag>DAO</tag>
        <tag>Oracle</tag>
        <tag>DeFi</tag>
        <tag>Solidity</tag>
      </tags>
  </entry>
  <entry>
    <title>(1) Metamask 지갑 설정 + 테스트서버 Faucet + Gas비</title>
    <url>/20220108-blockchain-coding2/</url>
    <content><![CDATA[<h2 id="Metamask-지갑-셋업"><a href="#Metamask-지갑-셋업" class="headerlink" title="Metamask 지갑 셋업"></a>Metamask 지갑 셋업</h2><p><a href="https://metamask.io/index.html"><strong>Metamask</strong></a>는 가장 유명한 지갑 시스템 중 하나이다.</p>
<p>Metamask를 다운받자. 웹 브라우저 익스텐션 형태로 깔린다.</p>
<img src="/20220108-blockchain-coding2/metamask_download.png" class="" title="metamask">

<p>다운로드가 끝나면 새로운 지갑을 생성하자.</p>
<p>비밀번호를 설정하면 Secret backup phrase (i.e. seed phrase)가 생성된다.</p>
<img src="/20220108-blockchain-coding2/seed.png" class="" title="seed_phrase">

<p>이 <strong>Seed phrase는 무조건 백업을 하고 절대로 남에게 알려주면 안된다</strong>. 이 seed phrase가 있으면 계정에 들어있는 모든 자금에 접근할 수 있기 때문이다. 돈 뺏기기 싫으면 꼭 백업하고 꼭꼭 숨겨두자.</p>
<p>하나의 seed phrase로 여러개의 지갑을 생성할 수 있다. 이 뜻은, seed phrase가 털리면 내 모든 지갑에 있는 코인이 털릴 수 있다는 것이다.</p>
<p>설정이 끝나면 메인 페이지로 이동한다.</p>
<p> </p>
<h2 id="메타마스크-인터페이스"><a href="#메타마스크-인터페이스" class="headerlink" title="메타마스크 인터페이스"></a>메타마스크 인터페이스</h2><p>메인 페이지에는 다음과 같은 모습이 보인다.</p>
<img src="/20220108-blockchain-coding2/main.png" class="" title="main">

<h3 id="Account-정보"><a href="#Account-정보" class="headerlink" title="Account 정보"></a>Account 정보</h3><p>가장 상위단에는 account 정보가 있다. 이 account 정보에는 우리 지갑의 <strong>public address</strong>가 적혀있는데, 이 address 정보는 <span class="exturl" data-url="aHR0cHM6Ly9ldGhlcnNjYW4uaW8v">Etherscan<i class="fa fa-external-link-alt"></i></span>과 같은 서비스를 통해서 조회할 수 있다.</p>
<p>Account 칸 옆에 3개의 점이 찍혀있는 것을 볼 수 있는데, ‘Account details’에 들어가서 <strong>private key</strong>를 받을 수도 있다. Private key는 해당 account에 접근 권한을 의미한다.</p>
<h3 id="네트워크-설정"><a href="#네트워크-설정" class="headerlink" title="네트워크 설정"></a>네트워크 설정</h3><p>페이지 우 상단에는 네트워크 설정을 할 수 있다.</p>
<p>이더리움을 사게 되면 <strong>Ethereum mainnet 네트워크</strong>에서 작업을 하게 된다. 현재 서비스되고 있는 DeFi들이 이 네트워크 위에서 돌아간다.</p>
<p>우리가 코드를 작업하면서 테스트 해보기 위한 네트워크도 존재한다. Ropsten Test network, Rinkeby Test network, Goerli Test network, Kovan Test network와 같이 Ethereum 네트워크와 비슷한 구조를 가지고 있는 <strong>테스트 네트워크</strong>도 있다. 이러한 네트워크 위에서는 실제 돈을 사용하지 않아도 된다.</p>
<p> </p>
<h2 id="Rinkeby-faucet"><a href="#Rinkeby-faucet" class="headerlink" title="Rinkeby faucet"></a>Rinkeby faucet</h2><p>이 코스에서는 블록체인 개발을 할 때 <span class="exturl" data-url="aHR0cHM6Ly9mYXVjZXQucmlua2VieS5pby8=">Rinkeby faucet<i class="fa fa-external-link-alt"></i></span>을 이용할 것이다. 추후에 Kovan network도 사용할 것이다.</p>
<p>Faucet은 개발/테스트 목적을 위한 ethereum을 무료로 제공하는 것을 뜻한다.</p>
<img src="/20220108-blockchain-coding2/rinkeby.png" class="" title="rinkeby">

<p>무료 eth를 받기 위해서는 페이스북/트위터를 통해 소셜 네트워크 포스트를 하나 올려야한다.</p>
<p>트위터를 사용할 때는 다음과 같은 화면이 나온다. 여기서 0x00000000000…이라고 된 부분을 내 지갑의 public address로 바꾼다.</p>
<img src="/20220108-blockchain-coding2/rinkeby2.png" class="" title="rinkeby">

<p>이제 트윗의 링크를 복사해서 rinkeby 웹사이트에 입력하자. 이후, 3일간 18.75 eth를 받는 옵션을 선택한다.</p>
<p>(필자는 Rinkeby faucet에서 받으려고 했지만, Rinkeby 네트워크 이슈로 Kovan test network에서 받았다.)</p>
<p> </p>
<hr>
<h2 id="Transaction-amp-Gas"><a href="#Transaction-amp-Gas" class="headerlink" title="Transaction & Gas"></a>Transaction &amp; Gas</h2><p>Metamask를 통해 ETH를 받은 것을 확인할 수 있다. 이후, 사용중인 testnet의 Etherscan에 들어가서 누가 보냈는지도 확인할 수 있다. Etherscan과 같은 서비스는 <strong>Block Explorer</strong>라고 하며 거래내역을 볼 수 있게 만든 것이다.</p>
<img src="/20220108-blockchain-coding2/kovan_etherscan.png" class="" title="kovan">

<p>Transaction hash (txn hash)는 해당 거래에 대한 유일 hash이다. 보낸 사람과 받는 사람의 account address도 확인할 수 있다.</p>
<p>Transaction fee와 관련해서 Gas에 대해서도 확인할 수 있다. <strong>Gas</strong>는 계산을 하는데에 필요한 컴퓨팅 자원을 뜻하며, <strong>거래를 하기 위해 많은 계산을 할 수록 더 많은 gas비를 내야한다</strong>. 이 gas비는 계산을 수행한 node-operator에게 간다.</p>
<p>이는 즉 체인 내부에 어떠한 변화를 주게 될 경우, 무조건 gas비를 내게 된다는 것이다. 거래의 입장에서는 ETH를 1000개의 address에 보내는 것 보다 1개의 address에 보내는 것이 더 저렴하다. 앱 개발의 입장에서 봤을 때는 또 다른 시나리오를 고려해야한다.</p>
<img src="/20220108-blockchain-coding2/gas.png" class="" title="gas">

<p>이제 거래를 직접 해보자. 거래를 하면 Gas를 내게 될 것이다.</p>
<p>Gas비는 ETH보다 작은 Gwei라는 단위를 가진다. ETH와 Gwei의 단위 환산을 위해서는 <span class="exturl" data-url="aHR0cHM6Ly9ldGgtY29udmVydGVyLmNvbS8=">Ethereum Unit converter<i class="fa fa-external-link-alt"></i></span>와 같은 기능을 사용하면 좋다.</p>
<p><strong>Gas Price</strong>는 gas의 단위마다의 가격을 의미한다. <strong>Gas Limit</strong>은 거래에 사용되는 최대 gas의 양을 의미한다.</p>
<p>Transaction fee는 Gas Used * Gas price 이다. 예를 들어 21,000gas @ 1 GWEI per gas라면 21,000 GWEI가 거래 수수료인것이다.</p>
<img src="/20220108-blockchain-coding2/transaction.png" class="" title="transaction">

<p>거래를 진행할 때 Gas비를 올리거나 낮출 수 있다. Gas비를 수수료처럼 생각하면 무조건 낮은게 좋아보이는데, 올리는 옵션은 대체 왜 있는걸까?</p>
<p>위 질문에 대한 해답은 다음과 같다. 블록체인에서 Node operator들은 계산을 해주고 gas비를 받는데, 아무래도 동일한 계산에 대해 더 높은 수수료를 지급하는 계산을 하고 싶을 것이다. 낮은 수수료의 계산은 손해라고 생각할 것이며, priority가 낮아질 수 밖에 없다. 내가 내 거래의 gas비를 높게 책정하면 아무래도 node operator들이 이 gas비를 받기 위해 내 계산을 먼저 해줄 확률이 높다.</p>
<p>체인 내부에서 gas비의 시세를 확인하려면 <span class="exturl" data-url="aHR0cHM6Ly9ldGhnYXNzdGF0aW9uLmluZm8v">ETH Gas Station<i class="fa fa-external-link-alt"></i></span>과 같은 서비스를 사용하면 좋다. 이 웹사이트는 1. 당장 거래를 원할 때 (2분), 2. 비교적 빠른 편으로 거래를 원할 때 (5분), 3. 천천히 거래를 진행해도 될 때 (30분) 의 평균 gas비를 알려준다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Solidity, Blockchain, and Smart Contract 강의 정리</category>
      </categories>
      <tags>
        <tag>Blockchain</tag>
        <tag>Smart contract</tag>
        <tag>Bitcoin</tag>
        <tag>Ethereum</tag>
        <tag>DAO</tag>
        <tag>Oracle</tag>
        <tag>DeFi</tag>
        <tag>Solidity</tag>
        <tag>Metamask</tag>
        <tag>Gas</tag>
      </tags>
  </entry>
  <entry>
    <title>카메라 캘리브레이션 체커보드 패턴 생성기 레퍼런스 (calib.io)</title>
    <url>/20220108-calibration-pattern-generator/</url>
    <content><![CDATA[<h2 id="Calib-io"><a href="#Calib-io" class="headerlink" title="Calib.io"></a>Calib.io</h2><p>Calib.io은 카메라 캘리브레이션에 사용되는 패턴을 판매한다.<br>고맙게도 패턴 제네레이터도 웹에 공유해주었다.</p>
<p>PDF 파일로 만든 다음에 프린트 후 사용해도 된다.</p>
<p>퀄리티가 마음에 안들때는, calib.io에 주문해서 좋은 퀄리티에 메탈 프린팅으로 받을 수 있다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9jYWxpYi5pby9wYWdlcy9jYW1lcmEtY2FsaWJyYXRpb24tcGF0dGVybi1nZW5lcmF0b3I=">https://calib.io/pages/camera-calibration-pattern-generator<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220108-calibration-pattern-generator/pattern.png" class="" title="pattern">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual SLAM</tag>
        <tag>Camera calibration</tag>
        <tag>calib.io</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 개발에 유용한 라이브러리</title>
    <url>/20220108-cpp-libraries/</url>
    <content><![CDATA[<p>적는중</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Boost</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR 2014 Visual SLAM Tutorial 자료들</title>
    <url>/20220108-cvpr14-visual-slam-tutorial/</url>
    <content><![CDATA[<h2 id="발표"><a href="#발표" class="headerlink" title="발표"></a>발표</h2><h3 id="Session-1-Visual-Odometry"><a href="#Session-1-Visual-Odometry" class="headerlink" title="Session 1: Visual Odometry"></a>Session 1: Visual Odometry</h3><ul>
<li>Visual odometry (Stephan Weiss) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUExMS1WaXN1YWxPZG9tZXRyeS5wZGY=">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Stereo VO (Chris Beall) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUExMi1TdGVyZW9WTy5wZGY=">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Bundle adjustment (Frank Dellaert) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUExMy1CdW5kbGVBZGp1c3RtZW50LnBkZg==">handout<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUExMy1CdW5kbGVBZGp1c3RtZW50LnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Session-2-Visual-SLAM"><a href="#Session-2-Visual-SLAM" class="headerlink" title="Session 2: Visual SLAM"></a>Session 2: Visual SLAM</h2><ul>
<li>Scale ambiguity, inertial (Stephan Weiss) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUEyMS1EZWFsaW5nV2l0aFNjYWxlLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Efficient inference (Michael Kaess) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUEyMi1JbmNyZW1lbnRhbC5wZGY=">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>VSLAM on phones with demo and loop closing (Frank Dellaert) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LUEyMy1Mb29wQ2xvc2luZy5wZGY=">slides<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Session-3-Advanced-Topics"><a href="#Session-3-Advanced-Topics" class="headerlink" title="Session 3: Advanced Topics"></a>Session 3: Advanced Topics</h2><ul>
<li>Lowering computational cost (Michael Kaess) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAxMS1MYXJnZVNjYWxlRWZmaWNpZW5jeS5wZGY=">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Dense VO (Richard Newcombe) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAxMi1EZW5zZVZPLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Hybrids: PTAM, SVO (Stephan Weiss) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAxMy1IeWJyaWRzLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>Dense mapping: KinectFusion and DTAM (Richard Newcombe) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAxNC1HcmVlZHlEZW5zZVNMQU0ucGRm">slides<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="Session-4-Dense-SLAM"><a href="#Session-4-Dense-SLAM" class="headerlink" title="Session 4: Dense SLAM"></a>Session 4: Dense SLAM</h2><ul>
<li>Kintinuous (Michael Kaess) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAyMS1LaW50aW51b3VzLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
<li>SLAM++ (Richard Newcombe) <span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQvbWVkaWEvVlNMQU0tVHV0b3JpYWwtQ1ZQUjE0LVAyMi1TTEFNJTJCJTJCLnBkZg==">slides<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<h2 id="웹사이트"><a href="#웹사이트" class="headerlink" title="웹사이트"></a>웹사이트</h2><p><span class="exturl" data-url="aHR0cDovL3d3dy5jcy5jbXUuZWR1L35rYWVzcy92c2xhbV9jdnByMTQv">http://www.cs.cmu.edu/~kaess/vslam_cvpr14/<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual SLAM</tag>
        <tag>CVPR</tag>
        <tag>Tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title>폴더 지우기 - os.rmdir, os.system, shutil.rmtree</title>
    <url>/20220108-delete-folder/</url>
    <content><![CDATA[<h2 id="폴더를-지워야-할-때"><a href="#폴더를-지워야-할-때" class="headerlink" title="폴더를 지워야 할 때"></a>폴더를 지워야 할 때</h2><p>자동화 스크립트를 적다보면 폴더를 생성하고 지워야할 때가 있다.</p>
<p>예를 들어,</p>
<ul>
<li>특정 버전의 라이브러리 소스코드를 인터넷에서 땡겨온다</li>
<li>해당 소스코드 기반으로 라이브러리를 빌드 한다</li>
<li>빌드 후, 원본 소스코드를 포함하는 폴더를 삭제한다.</li>
</ul>
<h2 id="방법-1-os-system"><a href="#방법-1-os-system" class="headerlink" title="방법 1: os.system"></a>방법 1: os.system</h2><p>작업중인 OS에서 지원하는 폴더 삭제 명령을 파이썬 명령어를 통해 입력하는 방법이다.</p>
<p>가장 간단한 방법이다.</p>
<p>Ubuntu Linux 등에서 관리자 권한으로 폴더를 지울 때는 sudo를 입력해야할 때가 있는데, 이럴 때는 사전에 sudo 권한을 켜두면 좋다. Docker에서는 자동으로 관리자 권한이라 좀 더 편하긴 하다.</p>
<p>아래 코드에는 내가 좋아하는 패스워드 모듈을 함께 첨부했다. 이 모듈은 argparse를 통해 스크립트 실행 시 커맨드라인에 패스워드를 입력해서 sudo 권한을 매번 자동으로 입력하게 해주는 모듈이다. (물론 보안 상 엄청 안 좋은 방법이지만, 가장 쉬운 방법 중 하나이다)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Password</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, password</span>):</span></span><br><span class="line">        self.data = password</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sudo</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.data == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"sudo "</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"echo "</span> + self.data + <span class="string">" | sudo -S "</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">            description=<span class="string">'sample script'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, metavar=<span class="string">'\b'</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">""</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'Provide your Linux password to avoid manually typing in your password for every auto internal \'sudo\' command usage. This will leave traces of your password in your shell history. If you are concerned about security, do not use this option.'</span>)        </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">global</span> pw</span><br><span class="line">    pw = Password(args.password)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ... some code</span></span><br><span class="line"></span><br><span class="line">    folder_path = <span class="string">"~/directory/to/some/path"</span></span><br><span class="line">    os.system(pw.sudo() + <span class="string">"rm -rf "</span> + folder_path)</span><br></pre></td></tr></table></figure>

<h2 id="방법-2-shutil-rmtree"><a href="#방법-2-shutil-rmtree" class="headerlink" title="방법 2: shutil.rmtree"></a>방법 2: shutil.rmtree</h2><p>크로스플랫폼 지원이 되는 방법이다.</p>
<p>사실 위 방법보다 이게 좀 더 좋은듯?</p>
<p><code>ignore_errors</code> 옵션을 넣으면 폴더가 없을 때도 삭제할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line">shutil.rmtree(<span class="string">'~/folder'</span>, ignore_errors=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="방법-3-os-rmdir"><a href="#방법-3-os-rmdir" class="headerlink" title="방법 3: os.rmdir"></a>방법 3: os.rmdir</h2><p>아무 내용이 없는 빈 폴더를 지울 때만 사용 가능하다.</p>
<p>개인적으로 아직 실제로 써본 적은 없는듯</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.3 Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Eigen Cheat sheet 레퍼런스</title>
    <url>/20220108-eigen-cheatsheet/</url>
    <content><![CDATA[<h2 id="치트시트"><a href="#치트시트" class="headerlink" title="치트시트"></a>치트시트</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// A simple quickref for Eigen. Add anything that's missing.</span></span><br><span class="line"><span class="comment">// Main author: Keir Mierle</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Eigen/Dense&gt;</span></span></span><br><span class="line"></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, <span class="number">3</span>&gt; A;               <span class="comment">// Fixed rows and cols. Same as Matrix3d.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, Dynamic&gt; B;         <span class="comment">// Fixed rows, dynamic cols.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, Dynamic, Dynamic&gt; C;   <span class="comment">// Full dynamic. Same as MatrixXd.</span></span><br><span class="line">Matrix&lt;<span class="keyword">double</span>, <span class="number">3</span>, <span class="number">3</span>, RowMajor&gt; E;     <span class="comment">// Row major; default is column-major.</span></span><br><span class="line">Matrix3f P, Q, R;                     <span class="comment">// 3x3 float matrix.</span></span><br><span class="line">Vector3f x, y, z;                     <span class="comment">// 3x1 float matrix.</span></span><br><span class="line">RowVector3f a, b, c;                  <span class="comment">// 1x3 float matrix.</span></span><br><span class="line">VectorXd v;                           <span class="comment">// Dynamic column vector of doubles</span></span><br><span class="line"><span class="keyword">double</span> s;                            </span><br><span class="line"></span><br><span class="line"><span class="comment">// Basic usage</span></span><br><span class="line"><span class="comment">// Eigen          // Matlab           // comments</span></span><br><span class="line">x.size()          <span class="comment">// length(x)        // vector size</span></span><br><span class="line">C.rows()          <span class="comment">// size(C,1)        // number of rows</span></span><br><span class="line">C.cols()          <span class="comment">// size(C,2)        // number of columns</span></span><br><span class="line">x(i)              <span class="comment">// x(i+1)           // Matlab is 1-based</span></span><br><span class="line">C(i,j)            <span class="comment">// C(i+1,j+1)       //</span></span><br><span class="line"></span><br><span class="line">A.resize(<span class="number">4</span>, <span class="number">4</span>);   <span class="comment">// Runtime error if assertions are on.</span></span><br><span class="line">B.resize(<span class="number">4</span>, <span class="number">9</span>);   <span class="comment">// Runtime error if assertions are on.</span></span><br><span class="line">A.resize(<span class="number">3</span>, <span class="number">3</span>);   <span class="comment">// Ok; size didn't change.</span></span><br><span class="line">B.resize(<span class="number">3</span>, <span class="number">9</span>);   <span class="comment">// Ok; only dynamic cols changed.</span></span><br><span class="line">                  </span><br><span class="line">A &lt;&lt; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,     <span class="comment">// Initialize A. The elements can also be</span></span><br><span class="line">     <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>,     <span class="comment">// matrices, which are stacked along cols</span></span><br><span class="line">     <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>;     <span class="comment">// and then the rows are stacked.</span></span><br><span class="line">B &lt;&lt; A, A, A;     <span class="comment">// B is three horizontally stacked A's.</span></span><br><span class="line">A.fill(<span class="number">10</span>);       <span class="comment">// Fill A with all 10's.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Eigen                                    // Matlab</span></span><br><span class="line">MatrixXd::Identity(rows,cols)               <span class="comment">// eye(rows,cols)</span></span><br><span class="line">C.setIdentity(rows,cols)                    <span class="comment">// C = eye(rows,cols)</span></span><br><span class="line">MatrixXd::Zero(rows,cols)                   <span class="comment">// zeros(rows,cols)</span></span><br><span class="line">C.setZero(rows,cols)                        <span class="comment">// C = zeros(rows,cols)</span></span><br><span class="line">MatrixXd::Ones(rows,cols)                   <span class="comment">// ones(rows,cols)</span></span><br><span class="line">C.setOnes(rows,cols)                        <span class="comment">// C = ones(rows,cols)</span></span><br><span class="line">MatrixXd::Random(rows,cols)                 <span class="comment">// rand(rows,cols)*2-1            // MatrixXd::Random returns uniform random numbers in (-1, 1).</span></span><br><span class="line">C.setRandom(rows,cols)                      <span class="comment">// C = rand(rows,cols)*2-1</span></span><br><span class="line">VectorXd::LinSpaced(size,low,high)          <span class="comment">// linspace(low,high,size)'</span></span><br><span class="line">v.setLinSpaced(size,low,high)               <span class="comment">// v = linspace(low,high,size)'</span></span><br><span class="line">VectorXi::LinSpaced(((hi-low)/step)+<span class="number">1</span>,      <span class="comment">// low:step:hi</span></span><br><span class="line">                    low,low+step*(size<span class="number">-1</span>))  <span class="comment">//</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix slicing and blocks. All expressions listed here are read/write.</span></span><br><span class="line"><span class="comment">// Templated size versions are faster. Note that Matlab is 1-based (a size N</span></span><br><span class="line"><span class="comment">// vector is x(1)...x(N)).</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">x.head(n)                          <span class="comment">// x(1:n)</span></span><br><span class="line">x.head&lt;n&gt;()                        <span class="comment">// x(1:n)</span></span><br><span class="line">x.tail(n)                          <span class="comment">// x(end - n + 1: end)</span></span><br><span class="line">x.tail&lt;n&gt;()                        <span class="comment">// x(end - n + 1: end)</span></span><br><span class="line">x.segment(i, n)                    <span class="comment">// x(i+1 : i+n)</span></span><br><span class="line">x.segment&lt;n&gt;(i)                    <span class="comment">// x(i+1 : i+n)</span></span><br><span class="line">P.block(i, j, rows, cols)          <span class="comment">// P(i+1 : i+rows, j+1 : j+cols)</span></span><br><span class="line">P.block&lt;rows, cols&gt;(i, j)          <span class="comment">// P(i+1 : i+rows, j+1 : j+cols)</span></span><br><span class="line">P.row(i)                           <span class="comment">// P(i+1, :)</span></span><br><span class="line">P.col(j)                           <span class="comment">// P(:, j+1)</span></span><br><span class="line">P.leftCols&lt;cols&gt;()                 <span class="comment">// P(:, 1:cols)</span></span><br><span class="line">P.leftCols(cols)                   <span class="comment">// P(:, 1:cols)</span></span><br><span class="line">P.middleCols&lt;cols&gt;(j)              <span class="comment">// P(:, j+1:j+cols)</span></span><br><span class="line">P.middleCols(j, cols)              <span class="comment">// P(:, j+1:j+cols)</span></span><br><span class="line">P.rightCols&lt;cols&gt;()                <span class="comment">// P(:, end-cols+1:end)</span></span><br><span class="line">P.rightCols(cols)                  <span class="comment">// P(:, end-cols+1:end)</span></span><br><span class="line">P.topRows&lt;rows&gt;()                  <span class="comment">// P(1:rows, :)</span></span><br><span class="line">P.topRows(rows)                    <span class="comment">// P(1:rows, :)</span></span><br><span class="line">P.middleRows&lt;rows&gt;(i)              <span class="comment">// P(i+1:i+rows, :)</span></span><br><span class="line">P.middleRows(i, rows)              <span class="comment">// P(i+1:i+rows, :)</span></span><br><span class="line">P.bottomRows&lt;rows&gt;()               <span class="comment">// P(end-rows+1:end, :)</span></span><br><span class="line">P.bottomRows(rows)                 <span class="comment">// P(end-rows+1:end, :)</span></span><br><span class="line">P.topLeftCorner(rows, cols)        <span class="comment">// P(1:rows, 1:cols)</span></span><br><span class="line">P.topRightCorner(rows, cols)       <span class="comment">// P(1:rows, end-cols+1:end)</span></span><br><span class="line">P.bottomLeftCorner(rows, cols)     <span class="comment">// P(end-rows+1:end, 1:cols)</span></span><br><span class="line">P.bottomRightCorner(rows, cols)    <span class="comment">// P(end-rows+1:end, end-cols+1:end)</span></span><br><span class="line">P.topLeftCorner&lt;rows,cols&gt;()       <span class="comment">// P(1:rows, 1:cols)</span></span><br><span class="line">P.topRightCorner&lt;rows,cols&gt;()      <span class="comment">// P(1:rows, end-cols+1:end)</span></span><br><span class="line">P.bottomLeftCorner&lt;rows,cols&gt;()    <span class="comment">// P(end-rows+1:end, 1:cols)</span></span><br><span class="line">P.bottomRightCorner&lt;rows,cols&gt;()   <span class="comment">// P(end-rows+1:end, end-cols+1:end)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Of particular note is Eigen's swap function which is highly optimized.</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">R.row(i) = P.col(j);               <span class="comment">// R(i, :) = P(:, j)</span></span><br><span class="line">R.col(j1).swap(mat1.col(j2));      <span class="comment">// R(:, [j1 j2]) = R(:, [j2, j1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Views, transpose, etc;</span></span><br><span class="line"><span class="comment">// Eigen                           // Matlab</span></span><br><span class="line">R.adjoint()                        <span class="comment">// R'</span></span><br><span class="line">R.transpose()                      <span class="comment">// R.' or conj(R')       // Read-write</span></span><br><span class="line">R.diagonal()                       <span class="comment">// diag(R)               // Read-write</span></span><br><span class="line">x.asDiagonal()                     <span class="comment">// diag(x)</span></span><br><span class="line">R.transpose().colwise().reverse()  <span class="comment">// rot90(R)              // Read-write</span></span><br><span class="line">R.rowwise().reverse()              <span class="comment">// fliplr(R)</span></span><br><span class="line">R.colwise().reverse()              <span class="comment">// flipud(R)</span></span><br><span class="line">R.replicate(i,j)                   <span class="comment">// repmat(P,i,j)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// All the same as Matlab, but matlab doesn't have *= style operators.</span></span><br><span class="line"><span class="comment">// Matrix-vector.  Matrix-matrix.   Matrix-scalar.</span></span><br><span class="line">y  = M*x;          R  = P*Q;        R  = P*s;</span><br><span class="line">a  = b*M;          R  = P - Q;      R  = s*P;</span><br><span class="line">a *= M;            R  = P + Q;      R  = P/s;</span><br><span class="line">                   R *= Q;          R  = s*P;</span><br><span class="line">                   R += Q;          R *= s;</span><br><span class="line">                   R -= Q;          R /= s;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Vectorized operations on each element independently</span></span><br><span class="line"><span class="comment">// Eigen                       // Matlab</span></span><br><span class="line">R = P.cwiseProduct(Q);         <span class="comment">// R = P .* Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() * s.<span class="built_in">array</span>();     <span class="comment">// R = P .* s</span></span><br><span class="line">R = P.cwiseQuotient(Q);        <span class="comment">// R = P ./ Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() / Q.<span class="built_in">array</span>();     <span class="comment">// R = P ./ Q</span></span><br><span class="line">R = P.<span class="built_in">array</span>() + s.<span class="built_in">array</span>();     <span class="comment">// R = P + s</span></span><br><span class="line">R = P.<span class="built_in">array</span>() - s.<span class="built_in">array</span>();     <span class="comment">// R = P - s</span></span><br><span class="line">R.<span class="built_in">array</span>() += s;                <span class="comment">// R = R + s</span></span><br><span class="line">R.<span class="built_in">array</span>() -= s;                <span class="comment">// R = R - s</span></span><br><span class="line">R.<span class="built_in">array</span>() &lt; Q.<span class="built_in">array</span>();         <span class="comment">// R &lt; Q</span></span><br><span class="line">R.<span class="built_in">array</span>() &lt;= Q.<span class="built_in">array</span>();        <span class="comment">// R &lt;= Q</span></span><br><span class="line">R.cwiseInverse();              <span class="comment">// 1 ./ P</span></span><br><span class="line">R.<span class="built_in">array</span>().inverse();           <span class="comment">// 1 ./ P</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">sin</span>()                <span class="comment">// sin(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">cos</span>()                <span class="comment">// cos(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">pow</span>(s)               <span class="comment">// P .^ s</span></span><br><span class="line">R.<span class="built_in">array</span>().square()             <span class="comment">// P .^ 2</span></span><br><span class="line">R.<span class="built_in">array</span>().cube()               <span class="comment">// P .^ 3</span></span><br><span class="line">R.cwiseSqrt()                  <span class="comment">// sqrt(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">sqrt</span>()               <span class="comment">// sqrt(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">exp</span>()                <span class="comment">// exp(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">log</span>()                <span class="comment">// log(P)</span></span><br><span class="line">R.cwiseMax(P)                  <span class="comment">// max(R, P)</span></span><br><span class="line">R.<span class="built_in">array</span>().max(P.<span class="built_in">array</span>())       <span class="comment">// max(R, P)</span></span><br><span class="line">R.cwiseMin(P)                  <span class="comment">// min(R, P)</span></span><br><span class="line">R.<span class="built_in">array</span>().min(P.<span class="built_in">array</span>())       <span class="comment">// min(R, P)</span></span><br><span class="line">R.cwiseAbs()                   <span class="comment">// abs(P)</span></span><br><span class="line">R.<span class="built_in">array</span>().<span class="built_in">abs</span>()                <span class="comment">// abs(P)</span></span><br><span class="line">R.cwiseAbs2()                  <span class="comment">// abs(P.^2)</span></span><br><span class="line">R.<span class="built_in">array</span>().abs2()               <span class="comment">// abs(P.^2)</span></span><br><span class="line">(R.<span class="built_in">array</span>() &lt; s).select(P,Q );  <span class="comment">// (R &lt; s ? P : Q)</span></span><br><span class="line">R = (Q.<span class="built_in">array</span>()==<span class="number">0</span>).select(P,A) <span class="comment">// R(Q==0) = P(Q==0)</span></span><br><span class="line">R = P.unaryExpr(ptr_fun(func)) <span class="comment">// R = arrayfun(func, P)   // with: scalar func(const scalar &amp;x);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Reductions.</span></span><br><span class="line"><span class="keyword">int</span> r, c;</span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">R.minCoeff()              <span class="comment">// min(R(:))</span></span><br><span class="line">R.maxCoeff()              <span class="comment">// max(R(:))</span></span><br><span class="line">s = R.minCoeff(&amp;r, &amp;c)    <span class="comment">// [s, i] = min(R(:)); [r, c] = ind2sub(size(R), i);</span></span><br><span class="line">s = R.maxCoeff(&amp;r, &amp;c)    <span class="comment">// [s, i] = max(R(:)); [r, c] = ind2sub(size(R), i);</span></span><br><span class="line">R.sum()                   <span class="comment">// sum(R(:))</span></span><br><span class="line">R.colwise().sum()         <span class="comment">// sum(R)</span></span><br><span class="line">R.rowwise().sum()         <span class="comment">// sum(R, 2) or sum(R')'</span></span><br><span class="line">R.prod()                  <span class="comment">// prod(R(:))</span></span><br><span class="line">R.colwise().prod()        <span class="comment">// prod(R)</span></span><br><span class="line">R.rowwise().prod()        <span class="comment">// prod(R, 2) or prod(R')'</span></span><br><span class="line">R.trace()                 <span class="comment">// trace(R)</span></span><br><span class="line">R.all()                   <span class="comment">// all(R(:))</span></span><br><span class="line">R.colwise().all()         <span class="comment">// all(R)</span></span><br><span class="line">R.rowwise().all()         <span class="comment">// all(R, 2)</span></span><br><span class="line">R.any()                   <span class="comment">// any(R(:))</span></span><br><span class="line">R.colwise().any()         <span class="comment">// any(R)</span></span><br><span class="line">R.rowwise().any()         <span class="comment">// any(R, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Dot products, norms, etc.</span></span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">x.norm()                  <span class="comment">// norm(x).    Note that norm(R) doesn't work in Eigen.</span></span><br><span class="line">x.squaredNorm()           <span class="comment">// dot(x, x)   Note the equivalence is not true for complex</span></span><br><span class="line">x.dot(y)                  <span class="comment">// dot(x, y)</span></span><br><span class="line">x.cross(y)                <span class="comment">// cross(x, y) Requires #include &lt;Eigen/Geometry&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//// Type conversion</span></span><br><span class="line"><span class="comment">// Eigen                  // Matlab</span></span><br><span class="line">A.cast&lt;<span class="keyword">double</span>&gt;();         <span class="comment">// double(A)</span></span><br><span class="line">A.cast&lt;<span class="keyword">float</span>&gt;();          <span class="comment">// single(A)</span></span><br><span class="line">A.cast&lt;<span class="keyword">int</span>&gt;();            <span class="comment">// int32(A)</span></span><br><span class="line">A.real();                 <span class="comment">// real(A)</span></span><br><span class="line">A.imag();                 <span class="comment">// imag(A)</span></span><br><span class="line"><span class="comment">// if the original type equals destination type, no work is done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Note that for most operations Eigen requires all operands to have the same type:</span></span><br><span class="line">MatrixXf F = MatrixXf::Zero(<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line">A += F;                <span class="comment">// illegal in Eigen. In Matlab A = A+F is allowed</span></span><br><span class="line">A += F.cast&lt;<span class="keyword">double</span>&gt;(); <span class="comment">// F converted to double and then added (generally, conversion happens on-the-fly)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Eigen can map existing memory into Eigen matrices.</span></span><br><span class="line"><span class="keyword">float</span> <span class="built_in">array</span>[<span class="number">3</span>];</span><br><span class="line">Vector3f::Map(<span class="built_in">array</span>).fill(<span class="number">10</span>);            <span class="comment">// create a temporary Map over array and sets entries to 10</span></span><br><span class="line"><span class="keyword">int</span> data[<span class="number">4</span>] = {<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>};</span><br><span class="line"><span class="function">Matrix2i <span class="title">mat2x2</span><span class="params">(data)</span></span>;                    <span class="comment">// copies data into mat2x2</span></span><br><span class="line">Matrix2i::Map(data) = <span class="number">2</span>*mat2x2;           <span class="comment">// overwrite elements of data with 2*mat2x2</span></span><br><span class="line">MatrixXi::Map(data, <span class="number">2</span>, <span class="number">2</span>) += mat2x2;      <span class="comment">// adds mat2x2 to elements of data (alternative syntax if size is not know at compile time)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Solve Ax = b. Result stored in x. Matlab: x = A \ b.</span></span><br><span class="line">x = A.ldlt().solve(b));  <span class="comment">// A sym. p.s.d.    #include &lt;Eigen/Cholesky&gt;</span></span><br><span class="line">x = A.llt() .solve(b));  <span class="comment">// A sym. p.d.      #include &lt;Eigen/Cholesky&gt;</span></span><br><span class="line">x = A.lu()  .solve(b));  <span class="comment">// Stable and fast. #include &lt;Eigen/LU&gt;</span></span><br><span class="line">x = A.qr()  .solve(b));  <span class="comment">// No pivoting.     #include &lt;Eigen/QR&gt;</span></span><br><span class="line">x = A.svd() .solve(b));  <span class="comment">// Stable, slowest. #include &lt;Eigen/SVD&gt;</span></span><br><span class="line"><span class="comment">// .ldlt() -&gt; .matrixL() and .matrixD()</span></span><br><span class="line"><span class="comment">// .llt()  -&gt; .matrixL()</span></span><br><span class="line"><span class="comment">// .lu()   -&gt; .matrixL() and .matrixU()</span></span><br><span class="line"><span class="comment">// .qr()   -&gt; .matrixQ() and .matrixR()</span></span><br><span class="line"><span class="comment">// .svd()  -&gt; .matrixU(), .singularValues(), and .matrixV()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Eigenvalue problems</span></span><br><span class="line"><span class="comment">// Eigen                          // Matlab</span></span><br><span class="line">A.eigenvalues();                  <span class="comment">// eig(A);</span></span><br><span class="line"><span class="function">EigenSolver&lt;Matrix3d&gt; <span class="title">eig</span><span class="params">(A)</span></span>;     <span class="comment">// [vec val] = eig(A)</span></span><br><span class="line">eig.eigenvalues();                <span class="comment">// diag(val)</span></span><br><span class="line">eig.eigenvectors();               <span class="comment">// vec</span></span><br><span class="line"><span class="comment">// For self-adjoint matrices use SelfAdjointEigenSolver&lt;&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="원본-링크"><a href="#원본-링크" class="headerlink" title="원본 링크"></a>원본 링크</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXN0LmdpdGh1Yi5jb20vZ29jYXJsb3MvYzkxMjM3YjAyYzEyMGM2MzE5NjEyZTQyZmExOTZkNzc=">https://gist.github.com/gocarlos/c91237b02c120c6319612e42fa196d77<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>4. Maths</category>
      </categories>
      <tags>
        <tag>Eigen</tag>
      </tags>
  </entry>
  <entry>
    <title>점프투파이썬 Ebook 레퍼런스</title>
    <url>/20220108-jump-to-python-ebook/</url>
    <content><![CDATA[<h2 id="링크"><a href="#링크" class="headerlink" title="링크"></a>링크</h2><p>위키북스에 무료공개된 점프 투 파이썬 책<br>파이썬 공부할 떄 레퍼런스 삼기 매우 좋음! 바이블!</p>
<p><span class="exturl" data-url="aHR0cHM6Ly93aWtpZG9jcy5uZXQvYm9vay8x">https://wikidocs.net/book/1<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<img src="/20220108-jump-to-python-ebook/j2p.png" class="" title="j2p">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.3 Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>점프투파이썬</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM 개발에 유용한 라이브러리</title>
    <url>/20220108-slam-libraries/</url>
    <content><![CDATA[<blockquote>
<p>C++ 개발에 유용한 라이브러리들은 <a href="www.cv-learn.com/20220108-slam-libraries/">여기</a>를 봐주세요!</p>
</blockquote>
<blockquote>
<p>라이브러리마다 <strong>3줄 요약 + 링크</strong>를 남깁니다.</p>
</blockquote>
<h2 id="영상처리"><a href="#영상처리" class="headerlink" title="영상처리"></a>영상처리</h2><h3 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h3><blockquote>
<p>제일 많이 사용되는 영상처리 라이브러리!<br>왠만한 플랫폼에 적용 가능 + 왠만한 영상 알고리즘 성능의 베이스라인<br>개인적으로 생각하는 SLAM 개발자의 필수 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9vcGVuY3Yub3JnLw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW5jdi9vcGVuY3Y=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="EmguCV"><a href="#EmguCV" class="headerlink" title="EmguCV"></a>EmguCV</h3><blockquote>
<p>OpenCV의 C# 버전<br>C# 유니티를 쓰면서 필요하다면 쓸 수 있겠지만, 개인적으로는 C++ 모듈로 개발한 다음에 export하는 것을 추천<br>블로그 글 순서 상 여기에 적었지만, 사실 그렇게 추천하는 라이브러리는 아님</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VtZ3Vjdi9lbWd1Y3Y=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="FastCV"><a href="#FastCV" class="headerlink" title="FastCV"></a>FastCV</h3><blockquote>
<p>ARM 코어 + Snapdragon Mobile CPU에서 가속이 잘 되도록 만든 컴퓨터 비전 라이브러리<br>모바일 전용 영상처리가 필요할 때 좋을 듯</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIucXVhbGNvbW0uY29tL3NvZnR3YXJlL2Zhc3Rjdi1zZGs=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIucXVhbGNvbW0uY29tL3NpdGVzL2RlZmF1bHQvZmlsZXMvZG9jcy9mYXN0Y3YvYXBpL2luZGV4Lmh0bWw=">API doc<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="OpenGV"><a href="#OpenGV" class="headerlink" title="OpenGV"></a>OpenGV</h3><blockquote>
<p>3D geometric vision을 위한 minimal / non-minimal solver 알고리즘들을 모아놓은 라이브러리<br>EPnP, P3P와 같은 알고리즘 구현체가 상당히 좋음</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9sYXVyZW50a25laXAuZ2l0aHViLmlvL29wZW5ndi8=">웹사이트 / API doc<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9mdXJnYWxlcC5naXRodWIuaW8vYmliL2tuZWlwX2ljcmExNC5wZGY=">논문: Kneip 2014 - OpenGV: A Unified and Generalized Approach to Real-Time Calibrated Geometric Vision<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="PCL"><a href="#PCL" class="headerlink" title="PCL"></a>PCL</h3><blockquote>
<p>포인트 클라우드 프로세싱 라이브러리<br>LiDAR SLAM, RGB-D SLAM을 한다면 필수로 사용하는 라이브러리<br>OpenCV에 더불어 사용할 수 있으면 매우 도움이 됨</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9wb2ludGNsb3Vkcy5vcmcv">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BvaW50Q2xvdWRMaWJyYXJ5L3BjbA==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Open3D"><a href="#Open3D" class="headerlink" title="Open3D"></a>Open3D</h3><blockquote>
<p>PCL을 위협하는 최신 라이브러리<br>하지만 아직 첫번째 메이저 릴리즈가 진행되지 않았음 (i.e. 조금 불안정할수도?)<br>딥러닝 기능들과도 밀접한 연관을 가지고 있기 때문에, 최신 연구를 한다면 강력하게 추천</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5vcGVuM2Qub3JnLw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lzbC1vcmcvT3BlbjNE">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="GML-Camera-Calibration-Toolbox"><a href="#GML-Camera-Calibration-Toolbox" class="headerlink" title="GML Camera Calibration Toolbox"></a>GML Camera Calibration Toolbox</h3><blockquote>
<p>쓰기 쉬운 카메라 캘리브레이션 앱<br>Camera coefficient를 4개까지 계산할 수 있다.<br>스마트폰 카메라처럼 화각이 넓은 카메라를 쓴다면 OpenCV 카메라 캘리브레이션을 추천한다</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL2dyYXBoaWNzLmNzLm1zdS5ydS9lbi9ub2RlLzkwOQ==">웹사이트 (요즘 좀좀 링크가 다운되는듯?)<i class="fa fa-external-link-alt"></i></span></li>
<li><a href="./GML_CameraCalibrationInstall_0.75.exe.zip">Windows용 파일 다운로드</a></li>
</ul>
</li>
</ul>
<h3 id="Android-camera-calibration"><a href="#Android-camera-calibration" class="headerlink" title="Android camera calibration"></a>Android camera calibration</h3><blockquote>
<p>안드로이드 모바일에서 카메라 캘리브레이션 하는 앱<br>OpenCV3와 Camera2 API를 이용함</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JwbmcvYW5kcm9pZC1jYW1lcmEtY2FsaWJyYXRpb24=">GitHub 링크<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="COLMAP"><a href="#COLMAP" class="headerlink" title="COLMAP"></a>COLMAP</h3><blockquote>
<p>Structure-from-Motion / Multi-View Stereo 라이브러리<br>3D reconstruction 또는 Visual localization을 연구하려면 필수</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9jb2xtYXAuZ2l0aHViLmlvLw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvbG1hcC9jb2xtYXA=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="FFmpeg"><a href="#FFmpeg" class="headerlink" title="FFmpeg"></a>FFmpeg</h3><blockquote>
<p>동영상 처리 라이브러리<br>OpenCV에 포함시켜 빌드 할 수 있음 (이 방법이 더 쓰기 쉬움…)</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZmZtcGVnLm9yZy8=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="OpenMVS"><a href="#OpenMVS" class="headerlink" title="OpenMVS"></a>OpenMVS</h3><blockquote>
<p>COLMAP이나 OpenMVG가 하지 못하는 full surface reconstruction을 수행하는 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NkY3NlYWNhdmUvb3Blbk1WUw==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="OpenMVG"><a href="#OpenMVG" class="headerlink" title="OpenMVG"></a>OpenMVG</h3><blockquote>
<p>COLMAP 과 비슷한 용도의 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW5NVkcvb3Blbk1WRw==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="OpenSfM"><a href="#OpenSfM" class="headerlink" title="OpenSfM"></a>OpenSfM</h3><blockquote>
<p>Mapillary에서 만든 Python 기반 Structure-from-Motion 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hcGlsbGFyeS9PcGVuU2ZN">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Meshroom"><a href="#Meshroom" class="headerlink" title="Meshroom"></a>Meshroom</h3><blockquote>
<p>Point cloud의 시각화하여 후처리 및= 분석을 할 수 있는 프로그램.</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hbGljZXZpc2lvbi5vcmcvI21lc2hyb29t">웹사이트<i class="fa fa-external-link-alt"></i></span> </li>
</ul>
</li>
</ul>
<h3 id="ARM-Compute-Library"><a href="#ARM-Compute-Library" class="headerlink" title="ARM Compute Library"></a>ARM Compute Library</h3><blockquote>
<p>ARM CPU/GPU코어에서 빠른 딥러닝 연산을 위한 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0FSTS1zb2Z0d2FyZS9Db21wdXRlTGlicmFyeQ==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="vilib"><a href="#vilib" class="headerlink" title="vilib"></a>vilib</h3><blockquote>
<p>VIO 프론트엔드를 CUDA 가속 시킨 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3V6aC1ycGcvdmlsaWI=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="REMODE"><a href="#REMODE" class="headerlink" title="REMODE"></a>REMODE</h3><blockquote>
<p>Probabilistic dense map을 생성해서 sparse -&gt; dense point cloud를 만드는 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3V6aC1ycGcvcnBnX29wZW5fcmVtb2Rl">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Deprecated…"><a href="#Deprecated…" class="headerlink" title="Deprecated…"></a>Deprecated…</h3><ul>
<li>CVD</li>
<li>Metaio SDK</li>
</ul>
<p> </p>
<hr>
<h2 id="수학-최적화"><a href="#수학-최적화" class="headerlink" title="수학 / 최적화"></a>수학 / 최적화</h2><h3 id="Eigen"><a href="#Eigen" class="headerlink" title="Eigen"></a>Eigen</h3><blockquote>
<p>가장 많이 사용되는 선형대수 라이브러리<br>3D geometric vision을 한다면 거의 필수적으로 사용하는 라이브러리<br>로우레벨 알고리즘을 직접 구현한다면 거의 필수적으로 사용하는 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9laWdlbi50dXhmYW1pbHkub3JnL2luZGV4LnBocD90aXRsZT1NYWluX1BhZ2U=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li>[GitLab]</li>
<li><a href="www.cv-learn.com/20220108-eigen-cheatsheet/">Cheat sheet</a></li>
</ul>
</li>
</ul>
<h3 id="BLAS-ATLAS-LAPACK"><a href="#BLAS-ATLAS-LAPACK" class="headerlink" title="BLAS + ATLAS + LAPACK"></a>BLAS + ATLAS + LAPACK</h3><blockquote>
<p>많은 선형대수 라이브러리의 백엔드로 사용되는 라이브러리<br>… 정말로 로우레벨로 내려갈게 아니라면 사용할 일 없는 라이브러리. 지식 채움 용도로만 사용하는 것을 추천.<br>Fotran으로 작성되어있고, 왠만하면 C래퍼로 된 라이브러리를 사용하는편.</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5uZXRsaWIub3JnL2JsYXMv">BLAS 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5uZXRsaWIub3JnL2xhcGFjay8=">LAPACK 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Ceres-solver"><a href="#Ceres-solver" class="headerlink" title="Ceres-solver"></a>Ceres-solver</h3><blockquote>
<p>비선형 최적화 라이브러리<br>g2o, GTSAM과 더불어 가장 많이 사용되는 SLAM 백엔드 라이브러리<br>커스텀 cost function을 짤 수 있는게 가장 큰 장점! 초심자부터 고수까지 쓰기 좋음.</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL2NlcmVzLXNvbHZlci5vcmcvaW5kZXguaHRtbA==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NlcmVzLXNvbHZlci9jZXJlcy1zb2x2ZXIv">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="g2o"><a href="#g2o" class="headerlink" title="g2o"></a>g2o</h3><blockquote>
<p>Graph optimization 라이브러리<br>SLAM 백엔드로 사용하기에 난이도가 낮지만, 성능이 다른 라이브러리에 비해 낮은 편</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1JhaW5lckt1ZW1tZXJsZS9nMm8=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="GTSAM"><a href="#GTSAM" class="headerlink" title="GTSAM"></a>GTSAM</h3><blockquote>
<p>Factor graph 기반 로보틱스 센서퓨전 라이브러리<br>SLAM 백엔드로 쓰기에 아주 좋은 라이브러리 - 특히나 IMU를 사용한다면 Pre-integration 알고리즘이 이미 들어가있음</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ndHNhbS5vcmcv">웹페이지<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JvcmdsYWIvZ3RzYW0=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="SE-Sync"><a href="#SE-Sync" class="headerlink" title="SE-Sync"></a>SE-Sync</h3><blockquote>
<p>비교적 최근에 공개된 SLAM 백엔드 라이브러리<br>Global minimum을 찾을 수 있는 최적화 알고리즘을 사용한다고 함</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RhdmlkLW0tcm9zZW4vU0UtU3luYw==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Sophus"><a href="#Sophus" class="headerlink" title="Sophus"></a>Sophus</h3><blockquote>
<p>최적화를 위한 Lie algebra / Lie group 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0cmFzZGF0L1NvcGh1cw==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Manif"><a href="#Manif" class="headerlink" title="Manif"></a>Manif</h3><blockquote>
<p>비교적 최근에 나온 최적화를 위한 Lie 이론 알고리즘 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FydGl2aXMvbWFuaWY=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="딥러닝"><a href="#딥러닝" class="headerlink" title="딥러닝"></a>딥러닝</h2><h3 id="TensorFlow-PyTorch-MxNet"><a href="#TensorFlow-PyTorch-MxNet" class="headerlink" title="TensorFlow / PyTorch / MxNet"></a>TensorFlow / PyTorch / MxNet</h3><blockquote>
<p>딥러닝 필수 라이브러리<br>C++ API도 있음</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvP2hsPWtv">TensorFlow 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy8=">PyTorch 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9teG5ldC5hcGFjaGUub3JnL3ZlcnNpb25zLzEuOS4wLw==">mxnet 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Detectron-2"><a href="#Detectron-2" class="headerlink" title="Detectron 2"></a>Detectron 2</h3><blockquote>
<p>General object detection, segmentation, pose estimation 및 기타 딥러닝 기반 비전 태스크 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZGV0ZWN0cm9uMg==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="DarkNet"><a href="#DarkNet" class="headerlink" title="DarkNet"></a>DarkNet</h3><blockquote>
<p>C와 CUDA로 작성되어있는 딥러닝 라이브러리<br>대표적인 알고리즘으로는 YOLO가 있음</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9wanJlZGRpZS5jb20vZGFya25ldC8=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3BqcmVkZGllL2RhcmtuZXQ=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="DLib"><a href="#DLib" class="headerlink" title="DLib"></a>DLib</h3><blockquote>
<p>C++ 머신러닝 라이브러리<br>SVM와 같은 고전 머신러닝 알고리즘 사용 가능<br>Face landmark detection과 같은 딥러닝 기능도 사용 가능</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Rhdmlza2luZy9kbGli">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="DBoW2-DBoW3-FBoW"><a href="#DBoW2-DBoW3-FBoW" class="headerlink" title="DBoW2 / DBoW3 / FBoW"></a>DBoW2 / DBoW3 / FBoW</h3><blockquote>
<p>DBoW2 - ORB-SLAM에서 Loop closure detection을 위해 사용하는 라이브러리<br>DBoW3 - DBoW2를 개선시켰다고 주장하는 라이브러리<br>FBoW - DBoW3에 하드웨어 가속을 적용한 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RvcmlhbjNkL0RCb1cy">DBoW2 GitHub<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jtc2FsaW5hcy9EQm93Mw==">DBoW3 GitHub<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jtc2FsaW5hcy9mYm93">FBoW GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="SNPE-SDK"><a href="#SNPE-SDK" class="headerlink" title="SNPE SDK"></a>SNPE SDK</h3><blockquote>
<p>Qualcomm Snapdragon 칩에서 빠르게 딥러닝 기능을 작동시킬 수 있는 라이브러리 - Snapdragon CPU, Adreno GPU, Hexagon DSP<br>Caffe, Caffe2, ONNX, TensorFlow 모델을 Snapdragon에서 가속시킬 수 있는 형태로 변환 가능</p>
</blockquote>
<ul>
<li><p>Snapdragon Neural Processing Engine</p>
</li>
<li><p>링크</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIucXVhbGNvbW0uY29tL3NvZnR3YXJlL3F1YWxjb21tLW5ldXJhbC1wcm9jZXNzaW5nLXNkaw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIucXVhbGNvbW0uY29tL3NpdGVzL2RlZmF1bHQvZmlsZXMvZG9jcy9zbnBlL292ZXJ2aWV3Lmh0bWw=">Docs<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="시뮬레이션"><a href="#시뮬레이션" class="headerlink" title="시뮬레이션"></a>시뮬레이션</h2><h3 id="CARLA"><a href="#CARLA" class="headerlink" title="CARLA"></a>CARLA</h3><blockquote>
<p>가장 유명한 자율주행 시나리오 시뮬레이터</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL2NhcmxhLm9yZy8=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NhcmxhLXNpbXVsYXRvci9jYXJsYQ==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Habitat-Sim-Replica"><a href="#Habitat-Sim-Replica" class="headerlink" title="Habitat-Sim + Replica"></a>Habitat-Sim + Replica</h3><blockquote>
<p>인도어 환경에서 다양한 비전/언어 태스크를 수행할 수 있도록 만들어진 시뮬레이터</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9haWhhYml0YXQub3JnLw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvaGFiaXRhdC1zaW0=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="AirSim"><a href="#AirSim" class="headerlink" title="AirSim"></a>AirSim</h3><ul>
<li><p>Unity / Unreal Engine의 플러그인으로써, 게임 엔진 속 세상에 자동차/드론을 사용하는 시뮬레이터</p>
</li>
<li><p>링크</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9taWNyb3NvZnQuZ2l0aHViLmlvL0FpclNpbS8=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21pY3Jvc29mdC9BaXJTaW0=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="NVIDIA-Omniverse-Isaac-Sim"><a href="#NVIDIA-Omniverse-Isaac-Sim" class="headerlink" title="NVIDIA Omniverse / Isaac Sim"></a>NVIDIA Omniverse / Isaac Sim</h3><blockquote>
<p>최근 NVIDIA에서 공개한 Omniverse 그래픽 엔진 + ISAAC 로보틱스 시뮬레이터</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9pc2FhYy1zaW0=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Gazebo"><a href="#Gazebo" class="headerlink" title="Gazebo"></a>Gazebo</h3><blockquote>
<p>ROS에서 많이 사용하는 시뮬레이터<br>최신 시뮬레이터들에 비해 그래픽은 많이 부족한 편</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL2dhemVib3NpbS5vcmcv">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29zcmYvZ2F6ZWJv">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="그래픽스-렌더링"><a href="#그래픽스-렌더링" class="headerlink" title="그래픽스 / 렌더링"></a>그래픽스 / 렌더링</h2><h3 id="OpenGL"><a href="#OpenGL" class="headerlink" title="OpenGL"></a>OpenGL</h3><blockquote>
<p>그래픽스 렌더링을 위해 가장 많이 사용되는 API<br>수많은 엔진들이 OpenGL 기반으로 되어있음</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9sZWFybm9wZW5nbC5jb20v">OpenGL 공부 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Vulkan"><a href="#Vulkan" class="headerlink" title="Vulkan"></a>Vulkan</h3><blockquote>
<p>크로스 컴파일이 가능한 그래픽스 API<br>OpenGL에 비해 훨씬 어렵지만, 그만큼 컨트롤 할 수 있는 부분도 많다고 한다.</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Nhc2NoYVdpbGxlbXMvVnVsa2Fu">Vulkan 공부 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Metal"><a href="#Metal" class="headerlink" title="Metal"></a>Metal</h3><blockquote>
<p>Apple 제품 및 맥북에서 사용 가능한 그래픽스 API</p>
</blockquote>
<h3 id="Qt"><a href="#Qt" class="headerlink" title="Qt"></a>Qt</h3><blockquote>
<p>크로스 컴파일이 가능한 GUI 앱 개발 프레임워크<br>OpenGL의 난이도에 크로스 컴파일 기능이 들어간 정도</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucXQuaW8v">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Filament"><a href="#Filament" class="headerlink" title="Filament"></a>Filament</h3><blockquote>
<p>크로스 컴파일이 가능한 물리 렌더링 엔진<br>OpenGL을 할 수 있다면 이 라이브러리를 적극 추천함</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS9maWxhbWVudA==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Pangolin"><a href="#Pangolin" class="headerlink" title="Pangolin"></a>Pangolin</h3><blockquote>
<p>오픈소스 SLAM에서 많이 사용하는 3D 시각화 라이브러리<br>성능이 그렇게 좋은 편은 아니지만, OpenCV나 OpenGL을 알면 쉽게 쓸 수 있도록 되어있다</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0ZXZlbmxvdmVncm92ZS9QYW5nb2xpbg==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="rviz"><a href="#rviz" class="headerlink" title="rviz"></a>rviz</h3><blockquote>
<p>ROS에서 많이 사용되는 3D 시각화 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ydml6">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jvcy12aXN1YWxpemF0aW9uL3J2aXo=">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="Ogre-GODOT-Hazel-VTK-ITK"><a href="#Ogre-GODOT-Hazel-VTK-ITK" class="headerlink" title="Ogre, GODOT, Hazel, VTK, ITK"></a>Ogre, GODOT, Hazel, VTK, ITK</h3><blockquote>
<p>가벼운 3D 게임 / 렌더링 엔진</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cub2dyZTNkLm9yZy8=">Ogre 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9nb2RvdGVuZ2luZS5vcmcv">GODOT 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1RoZUNoZXJuby9IYXplbA==">Hazel 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly92dGsub3JnLw==">VTK 웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="통신"><a href="#통신" class="headerlink" title="통신"></a>통신</h2><h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><blockquote>
<p>고성능 RPC 프레임워크</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ncnBjLmlvLw==">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="rpcLib"><a href="#rpcLib" class="headerlink" title="rpcLib"></a>rpcLib</h3><blockquote>
<p>클라이언트~서버 간의 rpc 통신을 위한 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3JwY2xpYi9ycGNsaWI=">웹사이트<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<h3 id="lcm"><a href="#lcm" class="headerlink" title="lcm"></a>lcm</h3><blockquote>
<p>가벼운 메세지 통신 및 data marshalling 라이브러리</p>
</blockquote>
<ul>
<li>링크<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xjbS1wcm9qL2xjbQ==">GitHub<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>PCL</tag>
        <tag>SLAM</tag>
        <tag>Visual SLAM</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Tutorial</tag>
        <tag>Eigen</tag>
        <tag>OpenCV</tag>
        <tag>Open3D</tag>
        <tag>COLMAP</tag>
      </tags>
  </entry>
  <entry>
    <title>(2) Blockchain visual demo (Anders Brown)</title>
    <url>/20220109-blockchain-coding-3/</url>
    <content><![CDATA[<h2 id="Visual-demo-by-Anders-Brown"><a href="#Visual-demo-by-Anders-Brown" class="headerlink" title="Visual demo by Anders Brown"></a>Visual demo by Anders Brown</h2><p><span class="exturl" data-url="aHR0cHM6Ly9hbmRlcnNicm93bndvcnRoLmNvbS9ibG9ja2NoYWluLw==">https://andersbrownworth.com/blockchain/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h2 id="SHA256"><a href="#SHA256" class="headerlink" title="SHA256"></a>SHA256</h2><ul>
<li>Hash<ul>
<li>“A unique fixed length string, meant to identify a piece of data. They are created by placing said data into a ‘hash function’”</li>
<li><strong>입력 데이터에 대해 유일한 hash가 생성</strong>된다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h2><ul>
<li>Block에는 block number, nonce, data가 있다.<ul>
<li>이제 Hash는 block number, nonce, data를 모두 함께 섞어서 만들어낸다.</li>
<li>Block number는 blockchain에 올라갈 때 block의 순서이다</li>
<li><strong>Nonce</strong>는 <strong>Hash의 첫 앞 4자리를 0000으로 만드는 숫자 값</strong>이다.<ul>
<li>Data가 들어왔을 때 hash의 앞 4자리가 0000이 될 확률은 굉장히 낮다.</li>
<li>Nonce가 몇이 될지 우리는 모른다. <strong>0부터 시작해서 1씩 더해가며, hash의 첫 앞 4자리가 0000이 될 때 까지 계산</strong>한다.<ul>
<li>이 과정을 <strong>채굴</strong> - <strong>Mining</strong>이라고 한다.</li>
<li>채굴 작업의 계산량은 많다.</li>
</ul>
</li>
</ul>
</li>
<li>이렇게 채굴을 함으로써 nonce를 찾는 작업을 <strong>Proof of work</strong> (PoW)라고 하기도 한다.</li>
</ul>
</li>
</ul>
<blockquote>
<p>hash의 첫 앞자리가 0000이 된다는 것은 여기서의 예제이다.</p>
</blockquote>
<p> </p>
<hr>
<h2 id="Blockchain"><a href="#Blockchain" class="headerlink" title="Blockchain"></a>Blockchain</h2><ul>
<li>Blockchain은 block이 연결되어있다.<ul>
<li>각각의 Block은 이제 <strong>previous block의 hash도 함께</strong> 데이터로 가진다.</li>
<li>Blockchain의 첫 block은 <strong>Genesis block</strong>이라고 하며, 이 block은 prev hash를 가지지 않는다.</li>
<li><strong>중간 block의 데이터를 수정하면, 해당 block에서부터 chain의 끝 block까지 mining을 해야한다</strong>.<ul>
<li>왜냐하면… 우선 해당 block의 hash가 0000으로 시작하지 않을 것이고, 이를 위해 새롭게 nonce를 채굴해야한다.</li>
<li>hash가 0000이 되는 nonce를 채굴하면, 그 다음 block의 prev hash 데이터가 바뀌어 hash 데이터가 0000으로 시작하지 않게 된다.</li>
<li>이 block의 nonce도 채굴하고… 또 다음 block의 prev hash가 바뀌어서 또 nonce를 채굴하고…</li>
</ul>
</li>
<li>채굴 계산은 계산량이 굉장히 많다.<ul>
<li>중간에 있는 block 값을 바꾸면 다수의 block을 새로 채굴해야하기 때문에 계산량이 굉장히 많아질 것이다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Distributed-blockchain"><a href="#Distributed-blockchain" class="headerlink" title="Distributed blockchain"></a>Distributed blockchain</h2><ul>
<li>다수의 사람이 동일한 blockchain 정보를 가지고 있다고 해보자. <ul>
<li>이 때, 한명이 중간 block의 데이터를 바꿔 chain의 끝까지 새롭게 채굴했다고 해보자.</li>
<li>그래도 다른 사람들의 blockchain은 바뀌지 않았다.<ul>
<li>여러 blockchain들을 비교했을 때 <strong>다수결의 blockchain이 더 신빙성 있다고 판단</strong>할 수 있다.</li>
<li>이러한 구조를 통해 임의의 유저가 blockchain 내의 정보를 해킹하기 어렵게 만든다.<ul>
<li>해킹을 하기 위해서는 blockchain을 가진 모든 사람의 최소 50%는 해킹을 다 해야하기 때문이다.</li>
<li>종종, 특정 node에서 block 정보를 해킹함으로써 다른 정보를 가지고 있을 때는 다수의 consensus를 유지하기 위해 해당 node를 제거해버리기도 한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Tokens"><a href="#Tokens" class="headerlink" title="Tokens"></a>Tokens</h2><ul>
<li>Blockchain에 거래 내역에 대한 데이터를 담는다고 해보자.<ul>
<li><strong>거래 내역에 대한 데이터도 hash로 변환</strong>될 수 있다.</li>
<li>거래 내역을 조작하려고 하면 해당 block의 hash가 변경되며, 이후 모든 block들을 새롭게 채굴해야한다.</li>
</ul>
</li>
<li>보내는 사람과 받는 사람의 정보는 public key를 통해 만들어진다.<ul>
<li>이 public key는 private key 정보에 Elliptic Curve Digital Signature Algorithm (ECDSA) 알고리즘을 통해 생성된다.<ul>
<li>Private key는 seed phrase와 account address를 섞어서 만들어진다.</li>
</ul>
</li>
<li>담고 싶은 정보와 private key를 함께 데이터에 넣어서 ECDSA 알고리즘에 넣으면 message signature가 나온다.<ul>
<li>타인은 public key가 있으면 해당 message signature가 진짜인지 아닌지 구분할 수 있다.<ul>
<li>즉, 누구든지 이 거래가 진짜인지 아닌지 확인할 수 있다는 것이다.</li>
</ul>
</li>
<li>타인은 해당 message signature로부터 private key를 추론할 수 없다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Consensus"><a href="#Consensus" class="headerlink" title="Consensus"></a>Consensus</h2><blockquote>
<p>Consensus에 대한 심도 깊은 <span class="exturl" data-url="aHR0cHM6Ly9ldGhlcmV1bS5vcmcvZW4vZGV2ZWxvcGVycy9kb2NzL2NvbnNlbnN1cy1tZWNoYW5pc21zLw==">문서<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<ul>
<li>탈중앙화 된 시스템에서 모든 node들이 동의할 수 있는 단 하나의 blockchain의 state를 정하는 작업을 consensus라고 한다.<ul>
<li>네트워크의 51%가 동의하는 blockchain이 진짜라고 믿는 방식이다.</li>
<li>두가지 중요한 consensus 메커니즘이 있다 : Chain selection, Sybil resistance.<ul>
<li><strong>Chain selection</strong><ul>
<li><a href="https://blockonomi.com/nakamoto-consensus/"><strong>Nakamoto consensus</strong></a><ul>
<li>진짜 블록체인과 가짜 블록체인을 가려내는 방식</li>
<li>가장 긴 체인을 쓰는 방식을 사용함<ul>
<li>체인이 길 수록 proof of work를 하는데에 시간이 더 오래 걸리기 때문 (i.e. 데이터를 쉽게 바꾸지 못함)</li>
</ul>
</li>
<li>비트코인과 이더리움이 사용하는 방식.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Sybil resistance</strong><ul>
<li>여기에 두가지 방법이 있다 - proof of work, proof of stake</li>
<li><a href="https://ethereum.org/en/developers/docs/consensus-mechanisms/pow/"><strong>Proof of work</strong></a><ul>
<li>POW: 모든 node가 채굴을 목적으로 경쟁을 하며, 가장 빠르게 pow를 끝내는 쪽이 돈을 받는 방식.</li>
<li>51%의 네트워크를 장악하기 위해서는, 이론적으로 51%의 네트워크보다 더 빠르게 계산할 수 있어야한다.<ul>
<li>버는 돈 보다 장비 사는 돈이 더 많을 수도…</li>
</ul>
</li>
<li>비트코인과 이더리움이 사용하는 방식.</li>
</ul>
</li>
<li><a href="https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/"><strong>Proof of stake</strong></a><ul>
<li>Proof of work는 에너지를 많이 소비하기 때문에 환경에 좋지 않다.<ul>
<li>Proof of stake가 proof of work에 들어가는 에너지의 1%만 소비할 것으로 예상된다.</li>
</ul>
</li>
<li>Proof of stake는 node들이 각자 ‘나는 나쁘게 굴지 않겠습니다’라는 의미로 코인을 올려놓는다고 한다.<ul>
<li>나쁘게 굴면 이 코인을 몰수한다.</li>
<li>착한 node들에게 랜덤하게 채굴 작업이 부여된다.</li>
</ul>
</li>
<li>51%의 네트워크를 장악하기 위해서는, 이론적으로 네트워크에 올려놓은 코인들의 51%보다 많이 가지고 있어야한다는 것이다.<ul>
<li>이 부분에 대해서 중앙화/탈중앙화에 대한 토론이 오가고있다.</li>
</ul>
</li>
<li>Avalanche, Solana, Polygon, Polkadot 등이 사용하는 방식</li>
<li>이더리움도 ETH 2.0로 업그레이드 할 때 이 방식을 사용할 것이라고 한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="Proof-of-work-amp-Block-award"><a href="#Proof-of-work-amp-Block-award" class="headerlink" title="Proof of work & Block award"></a>Proof of work &amp; Block award</h2><ul>
<li>Proof of work는 굉장히 많은 에너지를 소비한다 (i.e. 연산량이 많다)<ul>
<li>특정 block에 대한 pow를 먼저 끝내는 node가 돈을 받게 된다.<ul>
<li>이 때, 두가지 방법으로 돈을 받는다 : Transaction fee, Block award.<ul>
<li>Transaction fee는 Gas비와 동일하다.</li>
<li><a href="https://www.investopedia.com/terms/b/block-reward.asp"><strong>Block award</strong></a>는 <strong>블록체인 자체에서 주는 돈</strong>으로써, Pow에 대한 보상을 주는 것이다.<ul>
<li>비트코인 블록체인은 block award로 비트코인을, 이더리움 블록체인은 block award로 이더리움을 준다.</li>
<li>블록체인이 제공할 수 있는 block award는 한정되어있기 때문에, 시간이 지날수록 block award가 줄어든다.<ul>
<li>비트코인의 반감기는 4년 정도이다.</li>
</ul>
</li>
<li>Block award를 통해 시간이 지날수록 더 많은 돈이 시장에 풀릴 수 있도록 하는 시스템이다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h2 id="해킹"><a href="#해킹" class="headerlink" title="해킹"></a>해킹</h2><ul>
<li><strong>Sybil attack</strong><ul>
<li>유저가 다수의 계정을 생성해서 블록체인의 consensus를 통제하려는 시도</li>
<li>해당 유저는 모든 계정에서 각각 pow를 해야한다. (엄청나게 많은 계산량!)</li>
</ul>
</li>
<li><strong>51% attack</strong><ul>
<li>블록체인 consensus는 51% 이상이 동의하는 가장 긴 블록체인을 사용한다.</li>
<li>유저 또는 유저들이 가장 긴 블록체인을 가지고 있고 51% 이상의 네트워크를 점유하고 있다면, 모든 네트워크가 해당 유저가 가진 블록체인을 따르게 할 수 있다.</li>
</ul>
</li>
<li><strong>네트워크가 클수록 블록체인이 더 안전하다</strong>.</li>
</ul>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Solidity, Blockchain, and Smart Contract 강의 정리</category>
      </categories>
      <tags>
        <tag>Blockchain</tag>
        <tag>Smart contract</tag>
        <tag>Bitcoin</tag>
        <tag>Ethereum</tag>
        <tag>DAO</tag>
        <tag>DeFi</tag>
        <tag>Solidity</tag>
      </tags>
  </entry>
  <entry>
    <title>Solidity programming</title>
    <url>/20220109-blockchain-coding-4/</url>
    <content><![CDATA[<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">pragma solidity ^<span class="number">0.6</span><span class="number">.0</span>; <span class="comment">// Any versions of 0.6.X will do.</span></span><br><span class="line"></span><br><span class="line">contract SimpleStorage{</span><br><span class="line"></span><br><span class="line">    uint256 public favouriteNumber; <span class="comment">// This value will be initialised to 0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//----Other types of variables----//</span></span><br><span class="line">    <span class="comment">// bool favouriteBool = true;</span></span><br><span class="line">    <span class="comment">// string favouriteString = "String";</span></span><br><span class="line">    <span class="comment">// int256 favouriteInt = -5;</span></span><br><span class="line">    <span class="comment">// address favouriteAddress = 0xBEB40F5Fd51b8E499Ec25Eae84227b65ea84563E;</span></span><br><span class="line">    <span class="comment">// bytes32 favouriteBytes = "cat"; // Strings can be converted to bytes</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- Visibility --- //</span></span><br><span class="line">    <span class="comment">// external - Can only be called by something outside of the contract</span></span><br><span class="line">    <span class="comment">// public - Anyone can call this value / function</span></span><br><span class="line">    <span class="comment">// internal - Can only be called by something inside of the contact, or any contracts deriving from it</span></span><br><span class="line">    <span class="comment">// private - Can only be called by within the contract, and NOT by any contacts deriving from it</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">store</span>(<span class="params">uint256 _favouriteNumber</span>) <span class="title">public</span> </span>{</span><br><span class="line">        favouriteNumber = _favouriteNumber;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- view, pure --- //</span></span><br><span class="line">    <span class="comment">// `view`, `pure` do not cost any transactions.</span></span><br><span class="line">    <span class="comment">// They only access the variables, and they do not make any state changes</span></span><br><span class="line">    <span class="comment">// `view` lets you read a variable number</span></span><br><span class="line">    <span class="comment">// `pure` function lets you do maths operation </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">retrieve</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">view</span> <span class="title">returns</span>(<span class="params">uint256</span>)</span>{</span><br><span class="line">        <span class="keyword">return</span> favouriteNumber;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- Struct --- //</span></span><br><span class="line">    <span class="comment">// Make your new data structure using struct!</span></span><br><span class="line"></span><br><span class="line">    struct People {</span><br><span class="line">        uint256 favouriteNumber;</span><br><span class="line">        string name;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    People public person = People({<span class="attr">favouriteNumber</span>: <span class="number">777</span>, <span class="attr">name</span>: <span class="string">"Hyunggi"</span>});</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- Arrays --- //</span></span><br><span class="line">    <span class="comment">// People[] public people_dynamic_array;</span></span><br><span class="line">    <span class="comment">// People[2] public people_static_array;</span></span><br><span class="line">    People[] public people;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- mapping --- //</span></span><br><span class="line">    <span class="comment">// mapping - very similar to Python Dictionary. Hash table.</span></span><br><span class="line">    mapping(<span class="function"><span class="params">string</span> =&gt;</span> uint256) public nameToFavouriteNumber;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- memory vs storage --- //</span></span><br><span class="line">    <span class="comment">// memory - Data will only be wstored during the execution of the function</span></span><br><span class="line">    <span class="comment">// storage - Data will persist even after the execution</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">addPerson</span>(<span class="params">string memory _name, uint256 _favouriteNumber</span>) <span class="title">public</span></span>{</span><br><span class="line">        people.push(People(_favouriteNumber, _name));</span><br><span class="line">        nameToFavouriteNumber[_name] = _favouriteNumber;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Solidity, Blockchain, and Smart Contract 강의 정리</category>
      </categories>
      <tags>
        <tag>Blockchain</tag>
        <tag>Smart contract</tag>
        <tag>Bitcoin</tag>
        <tag>Ethereum</tag>
        <tag>DAO</tag>
        <tag>DeFi</tag>
        <tag>Solidity</tag>
      </tags>
  </entry>
  <entry>
    <title>CppCon 2021 - Misra Parallelism Safety-critical Guidelines for C++11, 17, Then C++20,</title>
    <url>/20220110-misra-c-c-17/</url>
    <content><![CDATA[<h2 id="영상"><a href="#영상" class="headerlink" title="영상"></a>영상</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/hVv7Nc3f4Jo" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<hr>
<h2 id="MISRA와-C-Core-guidelines의-현재-상태"><a href="#MISRA와-C-Core-guidelines의-현재-상태" class="headerlink" title="MISRA와 C++ Core guidelines의 현재 상태"></a>MISRA와 C++ Core guidelines의 현재 상태</h2><p><strong>안전한 프로그래밍</strong>이 필요한 곳들은 어디인가? 과거에는 군사용 프로그램들이였지만, 최근에는 로보틱스와 자율주행 프로그램들에서도 필요하게 되었다. 윤리적으로, 기능상의 안전으로, 인공지능의 안전으로… 등등 여러가지 ‘안전한 프로그램’을 만드는 시도가 있다. 이 중에서는 ‘<strong>안전한 C++ 프로그램을 만들기</strong>‘도 포함된다.</p>
<p>이러한 움직임의 최상단에는 <strong>United Nations of Economics Commissions for Europe (UNECE)**가 있는데, 이 기관에서 정하는 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZG52LmNvLmtyL3NlcnZpY2VzL3BhZ2UtMTk3MTM0">WP 29 사이버 보안 규제<i class="fa fa-external-link-alt"></i></span>를 통해 아랫단에 위치한 RISC-V, ISO, MISRA 기구들에서 안전 기능을 추가하게 한다. 각각의 기구들은 담당하는 기술 분야에서 사용하는 API들에 적용되는 안전 기능 및 요구사항들을 개발한다. 이러한 요구사항들은 **오토체커와 같은 도구를 사용해 컴퓨터가 확인</strong>할 수도 있고, <strong>사람이 직접 확인해야 하는 부분들도 있다</strong>.</p>
<img src="/20220110-misra-c-c-17/safety-critical-api.png" class="" title="safety-critical-api">

<p>C++ 업계에서 가장 많이 사용되는 안전한 가이드라인은 2개이다.</p>
<ul>
<li><strong>JSF</strong>: Joint Strike Fighter Air Vehicle C++ Standards for the System Developments and Demonstration Program 2005</li>
<li><strong>MISRA C++</strong>: 2008 Guidelines for the use of the C++ language in critical systems, The Motor Industry Software Reliability Association, 2008.</li>
</ul>
<p>위 두개의 가이드라인의 특징이라면, 너무 오래전에 만들어져서 모던 C++이 적용되어있지 않다는 것이다. 하지만 사용하고 있는 시스템들이 많아서 그런지 많이들 사용한다. 특히, <strong>MISRA C++**는 자동차 시스템 개발을 목적으로 만들어졌음에도, **현재 많은 ‘안전한 프로그래밍’이 필요한 분야 (e.g. 드론, 수술로봇) 등에서도 사용</strong>된다.</p>
<p>물론 다른 가이드라인들도 있다. C++11,14,17,20을 타겟팅하는 가이드라인들도 있으나, 이러한 가이드라인 중 완벽하게 ‘안전한 프로그래밍’을 지향하는 가이드라인은 없다. <strong>AUTOSAR와 같이 C++14에서 안전한 프로래밍을 만들려고 했던 시도</strong>도 있으나, <strong>대부분의 아이디어는 MISRA에서 시작한 아이디어</strong>들이다. 이 중 가장 유명한 AUTOSAR는 개발이 중단되었고, 현재까지 만들어진 부분은 MISRA의 새로운 모던 가이드라인 개발 파이프라인에 녹아들게 되었다. 아래 이미지는 AUTOSAR와 비교했을 때, MISRA와 C++ core guideline이 얼마나 비슷한지 보여주는 표이다. MISRA와 AUTOSAR는 거의 비슷하다.</p>
<img src="/20220110-misra-c-c-17/pedigree.png" class="" title="pedigree">

<h3 id="C-Coding-guideline"><a href="#C-Coding-guideline" class="headerlink" title="C++ Coding guideline"></a>C++ Coding guideline</h3><p>Safety와는 별개로, 가장 유명한 C++ 가이드라인은 C++ 코딩 가이드라인이다.</p>
<p>Bjarne Stroustroup의 토크를 보면, C++ CG가 추구하는 방향은 1. 버그가 없는 코드를 작성하는 방법, 2. 좋은 성능을 보여주는 방법, 3. 보기 좋은 코드를 작성하는 방법 이 잘 아우러지는 가이드라인을 만드는 것이다.</p>
<p>이러한 코드를 만들기 위해 C++ CG는 Meta rule (i.e. 확인은 할 수 없지만 가이드를 주는 룰)과 checkable rules (i.e. 달성/미달성을 확인할 수 있는 룰)을 함께 사용한다. 여기서 checkable rules는 보통 GSL 라이브러리나 CLion 내부에 들어있는 static analysis를 통해 손쉽게 확인할 수 있다.</p>
<h3 id="MISRA-guideline"><a href="#MISRA-guideline" class="headerlink" title="MISRA guideline"></a>MISRA guideline</h3><p>MISRA는 Safety에 초점을 둔 가이드라인이다. </p>
<p>MISRA는 safety를 구현하기 위해 대다수의 룰들을 checkable rules로 만들었고 또 강제성을 가지게 만들었다. 실제로 228개의 MISRA 룰들 중 219는 강제성을 가진다.</p>
<p>MISRA가 코드를 보는 방향은, ‘코드는 아무래도 어쩔 수 없이 버그를 내포한다’ 라는 점이다. 대신 이 버그가 절대로 유저에게 악영향을 끼쳐서는 안되게 룰을 빡빡하게 가져간다 (i.e. Do no harm).</p>
<h3 id="MISRA와-C-CG에게-부족한-점들"><a href="#MISRA와-C-CG에게-부족한-점들" class="headerlink" title="MISRA와 C++ CG에게 부족한 점들"></a>MISRA와 C++ CG에게 부족한 점들</h3><p>두 가이드라인은 꽤나 성숙함에도 불구하고, 안전한 프로그래밍을 위해서 최신 트렌드를 제대로 반영하지 못한다는 점에 있다. 리스트를 만들자면 다음과 같다.</p>
<ul>
<li><strong>Sequential code에 맞춰져있다</strong></li>
<li><strong>병렬 프로그래밍</strong>에 대해 룰의 수가 너무 적다<ul>
<li>오염된 데이터에 대해서 병렬 프로그래밍을 할 때에 대해서는?</li>
<li>고장난 하드웨어에서 작동할 때에는?</li>
</ul>
</li>
<li><strong>동시성/이벤트 프로그래밍</strong>에 대해 룰의 수는 더 적다</li>
<li><strong>이종 프로그래밍</strong>에 대한 부분은 아예 없다.<ul>
<li>딥러닝에서 필수적이지 않을까?</li>
</ul>
</li>
</ul>
<p>업계에서는 이미 병렬/동시성/이벤트/이종 프로그래밍을 활발하게 사용하고 있다. 하지만 이에 대한 안전 가이드라인은 업계의 속도를 쫒아오지 못하고 있다. 발표자는 위 내용들에 대한 코딩 가이드라인을 만드는데에 참여하면서 safety도 함께 고려하고 있다고 한다.</p>
<p> </p>
<hr>
<h2 id="Safety를-위한-룰"><a href="#Safety를-위한-룰" class="headerlink" title="Safety를 위한 룰"></a>Safety를 위한 룰</h2><p>룰을 만들 때 5가지 종류의 룰이 나올 수 있다.</p>
<ol>
<li>사람이 체크하기 좋은 룰</li>
<li>툴이 체크하기 좋은 룰</li>
<li>사람과 툴이 둘 다 체크하기 좋은 룰</li>
<li>사람과 툴이 둘 다 체크하기 어려운 룰</li>
<li>사람만 체크할 수 있는 룰</li>
</ol>
<p>사람이 체크하기 좋은 룰들은 굉장히 간단한 편이다. 이러한 룰들은 코드 리뷰를 거쳐서 확인할 수 있게 하고, 프로그래머들이 숙지할 수 있도록 만들어져있다.</p>
<p>툴이 체크하기 좋은 룰들은 static scope, 또는 dynamic scope에서 작동한다. Static scope에 대한 분석은 쉽지만, dynamic scope에 대한 분석은 전체 프로그램의 분석이 필요할 수도 있기 때문에 좀 더 어렵다.</p>
<p>사람과 툴이 둘 다 체크하기 좋은 케이스는 대체적으로 쉬운 편이지만, 사람과 툴이 둘 다 체크하기 어려운 케이스는 dynamic scope이면서 전체 프로그램 분석이 필요하기도 하고, 코드 자체의 목적이 쉽게 보이지 않는 경우가 많다.</p>
<p>이 때, CppCG와 MISRA의 특징을 고려했을 때, 사람이 체크하기 좋은 룰들은 CppCG에 많이 있고, 툴이 체크하기 좋은 룰들은 MISRA에 분포해있다. 이 둘을 섞으면 좋은 방식을 만들 수 있지 않을까? </p>
<p>사람만 체크할 수 있는 룰은 meta rules가 있겠다.</p>
<p> </p>
<hr>
<h2 id="MISRA-Next"><a href="#MISRA-Next" class="headerlink" title="MISRA Next"></a>MISRA Next</h2><p>C++11/14/17/20으로 넘어오면서 수많은 병렬/동시성 프로그램이 기능들이 생겼다. 각각의 기능들마다 당연히 safety 가이드라인이 생겨야한다.</p>
<img src="/20220110-misra-c-c-17/parallel.png" class="" title="parallel">

<p>위와 같은 기능들을 고려한 가이드라인을 만드는건 사실 새로운 가이드라인을 발명하는거와는 거리가 멀다. 이미 업계에서 많이 사용하고 있는 방법들이 있는데, 거기에 안전을 고려해서 문서화를 할 뿐이다.</p>
<p>하지만 이러한 문서를 만든다고해도, 실제로 업계에서 이 가이드라인에 익숙해지는데에도 시간이 걸리고 피드백을 받아 적절한 수준의 safety를 만드는 데에는 시간이 걸린다.</p>
<p>우선 가장 먼저 만들 <strong>MISRA Next</strong>는 <strong>MISRA 2008 + AUTOSAR</strong>가 될 확률이 매우 높으며, <strong>C++17을 타겟팅할 예정</strong>이다. C++20은 들어가지 않으며, C++17의 기능도 몇개 빼먹을 수도 있다. 하지만 이 가이드라인을 만드는것도 쉽지 않은데, 인력도 부족하고 아직 안전 가이드라인에 대한 경험이 부족하기 때문이다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>MISRA C</tag>
        <tag>MISRA C++</tag>
        <tag>CppCon</tag>
      </tags>
  </entry>
  <entry>
    <title>Solidity 공부하기 좋은 예제</title>
    <url>/20220110-solidity-tutorial-website/</url>
    <content><![CDATA[<h2 id="출처"><a href="#출처" class="headerlink" title="출처"></a>출처</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FkYW1iYXJkL2xlYXJueGlueW1pbnV0ZXMtZG9jcy9ibG9iL21hc3Rlci9zb2xpZGl0eS5odG1sLm1hcmtkb3du">https://github.com/adambard/learnxinyminutes-docs/blob/master/solidity.html.markdown<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h2 id="Solidity-코딩-예제"><a href="#Solidity-코딩-예제" class="headerlink" title="Solidity 코딩 예제"></a>Solidity 코딩 예제</h2><p>은행 업무를 미믹킹 하는 코드</p>
<p>그 이후에는 Solidity 기본기를 설명함</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// First, a simple Bank contract</span></span><br><span class="line"><span class="comment">// Allows deposits, withdrawals, and balance checks</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// simple_bank.sol (note .sol extension)</span></span><br><span class="line"><span class="comment">/* **** START EXAMPLE **** */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declare the source file compiler version</span></span><br><span class="line">pragma solidity ^<span class="number">0.6</span><span class="number">.6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Start with Natspec comment (the three slashes)</span></span><br><span class="line"><span class="comment">// used for documentation - and as descriptive data for UI elements/actions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/// @title SimpleBank</span></span><br><span class="line"><span class="comment">/// @author nemild</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 'contract' has similarities to 'class' in other languages (class variables,</span></span><br><span class="line"><span class="comment">inheritance, etc.) */</span></span><br><span class="line">contract SimpleBank { <span class="comment">// CapWords</span></span><br><span class="line">    <span class="comment">// Declare state variables outside function, persist through life of contract</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// dictionary that maps addresses to balances</span></span><br><span class="line">    <span class="comment">// always be careful about overflow attacks with numbers</span></span><br><span class="line">    mapping (<span class="function"><span class="params">address</span> =&gt;</span> uint) private balances;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// "private" means that other contracts can't directly query balances</span></span><br><span class="line">    <span class="comment">// but data is still viewable to other parties on blockchain</span></span><br><span class="line"></span><br><span class="line">    address public owner;</span><br><span class="line">    <span class="comment">// 'public' makes externally readable (not writeable) by users or contracts</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Events - publicize actions to external listeners</span></span><br><span class="line">    event LogDepositMade(address accountAddress, uint amount);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Constructor, can receive one or many variables here; only one allowed</span></span><br><span class="line">    <span class="title">constructor</span>(<span class="params"></span>) <span class="title">public</span> {</span><br><span class="line">        <span class="comment">// msg provides details about the message that's sent to the contract</span></span><br><span class="line">        <span class="comment">// msg.sender is contract caller (address of contract creator)</span></span><br><span class="line">        owner = msg.sender;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// @notice Deposit ether into bank</span></span><br><span class="line">    <span class="comment">/// @return The balance of the user after the deposit is made</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">deposit</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">payable</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>{</span><br><span class="line">        <span class="comment">// Use 'require' to test user inputs, 'assert' for internal invariants</span></span><br><span class="line">        <span class="comment">// Here we are making sure that there isn't an overflow issue</span></span><br><span class="line">        <span class="built_in">require</span>((balances[msg.sender] + msg.value) &gt;= balances[msg.sender]);</span><br><span class="line"></span><br><span class="line">        balances[msg.sender] += msg.value;</span><br><span class="line">        <span class="comment">// no "this." or "self." required with state variable</span></span><br><span class="line">        <span class="comment">// all values set to data type's initial value by default</span></span><br><span class="line"></span><br><span class="line">        emit LogDepositMade(msg.sender, msg.value); <span class="comment">// fire event</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> balances[msg.sender];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// @notice Withdraw ether from bank</span></span><br><span class="line">    <span class="comment">/// @dev This does not return any excess ether sent to it</span></span><br><span class="line">    <span class="comment">/// @param withdrawAmount amount you want to withdraw</span></span><br><span class="line">    <span class="comment">/// @return remainingBal</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">withdraw</span>(<span class="params">uint withdrawAmount</span>) <span class="title">public</span> <span class="title">returns</span> (<span class="params">uint remainingBal</span>) </span>{</span><br><span class="line">        <span class="built_in">require</span>(withdrawAmount &lt;= balances[msg.sender]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Note the way we deduct the balance right away, before sending</span></span><br><span class="line">        <span class="comment">// Every .transfer/.send from this contract can call an external function</span></span><br><span class="line">        <span class="comment">// This may allow the caller to request an amount greater</span></span><br><span class="line">        <span class="comment">// than their balance using a recursive call</span></span><br><span class="line">        <span class="comment">// Aim to commit state before calling external functions, including .transfer/.send</span></span><br><span class="line">        balances[msg.sender] -= withdrawAmount;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// this automatically throws on a failure, which means the updated balance is reverted</span></span><br><span class="line">        msg.sender.transfer(withdrawAmount);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> balances[msg.sender];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// @notice Get balance</span></span><br><span class="line">    <span class="comment">/// @return The balance of the user</span></span><br><span class="line">    <span class="comment">// 'view' (ex: constant) prevents function from editing state variables;</span></span><br><span class="line">    <span class="comment">// allows function to run locally/off blockchain</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">balance</span>(<span class="params"></span>) <span class="title">view</span> <span class="title">public</span> <span class="title">returns</span> (<span class="params">uint</span>) </span>{</span><br><span class="line">        <span class="keyword">return</span> balances[msg.sender];</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"><span class="comment">// ** END EXAMPLE **</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Now, the basics of Solidity</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. DATA TYPES AND ASSOCIATED METHODS</span></span><br><span class="line"><span class="comment">// uint used for currency amount (there are no doubles</span></span><br><span class="line"><span class="comment">//  or floats) and for dates (in unix time)</span></span><br><span class="line">uint x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// int of 256 bits, cannot be changed after instantiation</span></span><br><span class="line">int constant a = <span class="number">8</span>;</span><br><span class="line">int256 constant a = <span class="number">8</span>; <span class="comment">// same effect as line above, here the 256 is explicit</span></span><br><span class="line">uint constant VERSION_ID = <span class="number">0x123A1</span>; <span class="comment">// A hex constant</span></span><br><span class="line"><span class="comment">// with 'constant', compiler replaces each occurrence with actual value</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// All state variables (those outside a function)</span></span><br><span class="line"><span class="comment">// are by default 'internal' and accessible inside contract</span></span><br><span class="line"><span class="comment">// and in all contracts that inherit ONLY</span></span><br><span class="line"><span class="comment">// Need to explicitly set to 'public' to allow external contracts to access</span></span><br><span class="line">int256 public a = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// For int and uint, can explicitly set space in steps of 8 up to 256</span></span><br><span class="line"><span class="comment">// e.g., int8, int16, int24</span></span><br><span class="line">uint8 b;</span><br><span class="line">int64 c;</span><br><span class="line">uint248 e;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Be careful that you don't overflow, and protect against attacks that do</span></span><br><span class="line"><span class="comment">// For example, for an addition, you'd do:</span></span><br><span class="line">uint256 c = a + b;</span><br><span class="line">assert(c &gt;= a); <span class="comment">// assert tests for internal invariants; require is used for user inputs</span></span><br><span class="line"><span class="comment">// For more examples of common arithmetic issues, see Zeppelin's SafeMath library</span></span><br><span class="line"><span class="comment">// https://github.com/OpenZeppelin/zeppelin-solidity/blob/master/contracts/math/SafeMath.sol</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// No random functions built in, you can get a pseduo-random number by hashing the current blockhash, or get a truely random number using something like Chainlink VRF. </span></span><br><span class="line"><span class="comment">// https://docs.chain.link/docs/get-a-random-number</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Type casting</span></span><br><span class="line">int x = int(b);</span><br><span class="line"></span><br><span class="line">bool b = <span class="literal">true</span>; <span class="comment">// or do 'var b = true;' for inferred typing</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Addresses - holds 20 byte/160 bit Ethereum addresses</span></span><br><span class="line"><span class="comment">// No arithmetic allowed</span></span><br><span class="line">address public owner;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Types of accounts:</span></span><br><span class="line"><span class="comment">// Contract account: address set on create (func of creator address, num transactions sent)</span></span><br><span class="line"><span class="comment">// External Account: (person/external entity): address created from public key</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Add 'public' field to indicate publicly/externally accessible</span></span><br><span class="line"><span class="comment">// a getter is automatically created, but NOT a setter</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// All addresses can be sent ether</span></span><br><span class="line">owner.transfer(SOME_BALANCE); <span class="comment">// fails and reverts on failure</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Can also do a lower level .send call, which returns a false if it failed</span></span><br><span class="line"><span class="keyword">if</span> (owner.send) {} <span class="comment">// REMEMBER: wrap send in 'if', as contract addresses have</span></span><br><span class="line"><span class="comment">// functions executed on send and these can fail</span></span><br><span class="line"><span class="comment">// Also, make sure to deduct balances BEFORE attempting a send, as there is a risk of a recursive</span></span><br><span class="line"><span class="comment">// call that can drain the contract</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Can check balance</span></span><br><span class="line">owner.balance; <span class="comment">// the balance of the owner (user or contract)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Bytes available from 1 to 32</span></span><br><span class="line">byte a; <span class="comment">// byte is same as bytes1</span></span><br><span class="line">bytes2 b;</span><br><span class="line">bytes32 c;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Dynamically sized bytes</span></span><br><span class="line">bytes m; <span class="comment">// A special array, same as byte[] array (but packed tightly)</span></span><br><span class="line"><span class="comment">// More expensive than byte1-byte32, so use those when possible</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// same as bytes, but does not allow length or index access (for now)</span></span><br><span class="line">string n = <span class="string">"hello"</span>; <span class="comment">// stored in UTF8, note double quotes, not single</span></span><br><span class="line"><span class="comment">// string utility functions to be added in future</span></span><br><span class="line"><span class="comment">// prefer bytes32/bytes, as UTF8 uses more storage</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Type inference</span></span><br><span class="line"><span class="comment">// var does inferred typing based on first assignment,</span></span><br><span class="line"><span class="comment">// can't be used in functions parameters</span></span><br><span class="line"><span class="keyword">var</span> a = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">// use carefully, inference may provide wrong type</span></span><br><span class="line"><span class="comment">// e.g., an int8, when a counter needs to be int16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// var can be used to assign function to variable</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">a</span>(<span class="params">uint x</span>) <span class="title">returns</span> (<span class="params">uint</span>) </span>{</span><br><span class="line">    <span class="keyword">return</span> x * <span class="number">2</span>;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">var</span> f = a;</span><br><span class="line">f(<span class="number">22</span>); <span class="comment">// call</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// by default, all values are set to 0 on instantiation</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Delete can be called on most types</span></span><br><span class="line"><span class="comment">// (does NOT destroy value, but sets value to 0, the initial value)</span></span><br><span class="line">uint x = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Destructuring/Tuples</span></span><br><span class="line">(x, y) = (<span class="number">2</span>, <span class="number">7</span>); <span class="comment">// assign/swap multiple values</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. DATA STRUCTURES</span></span><br><span class="line"><span class="comment">// Arrays</span></span><br><span class="line">bytes32[<span class="number">5</span>] nicknames; <span class="comment">// static array</span></span><br><span class="line">bytes32[] names; <span class="comment">// dynamic array</span></span><br><span class="line">uint newLength = names.push(<span class="string">"John"</span>); <span class="comment">// adding returns new length of the array</span></span><br><span class="line"><span class="comment">// Length</span></span><br><span class="line">names.length; <span class="comment">// get length</span></span><br><span class="line">names.length = <span class="number">1</span>; <span class="comment">// lengths can be set (for dynamic arrays in storage only)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// multidimensional array</span></span><br><span class="line">uint[][<span class="number">5</span>] x; <span class="comment">// arr with 5 dynamic array elements (opp order of most languages)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Dictionaries (any type to any other type)</span></span><br><span class="line">mapping (<span class="function"><span class="params">string</span> =&gt;</span> uint) public balances;</span><br><span class="line">balances[<span class="string">"charles"</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">// balances["ada"] result is 0, all non-set key values return zeroes</span></span><br><span class="line"><span class="comment">// 'public' allows following from another contract</span></span><br><span class="line">contractName.balances(<span class="string">"charles"</span>); <span class="comment">// returns 1</span></span><br><span class="line"><span class="comment">// 'public' created a getter (but not setter) like the following:</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">balances</span>(<span class="params">string _account</span>) <span class="title">returns</span> (<span class="params">uint balance</span>) </span>{</span><br><span class="line">    <span class="keyword">return</span> balances[_account];</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Nested mappings</span></span><br><span class="line">mapping (<span class="function"><span class="params">address</span> =&gt;</span> mapping (<span class="function"><span class="params">address</span> =&gt;</span> uint)) public custodians;</span><br><span class="line"></span><br><span class="line"><span class="comment">// To delete</span></span><br><span class="line"><span class="keyword">delete</span> balances[<span class="string">"John"</span>];</span><br><span class="line"><span class="keyword">delete</span> balances; <span class="comment">// sets all elements to 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Unlike other languages, CANNOT iterate through all elements in</span></span><br><span class="line"><span class="comment">// mapping, without knowing source keys - can build data structure</span></span><br><span class="line"><span class="comment">// on top to do this</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Structs</span></span><br><span class="line">struct Bank {</span><br><span class="line">    address owner;</span><br><span class="line">    uint balance;</span><br><span class="line">}</span><br><span class="line">Bank b = Bank({</span><br><span class="line">    owner: msg.sender,</span><br><span class="line">    balance: <span class="number">5</span></span><br><span class="line">});</span><br><span class="line"><span class="comment">// or</span></span><br><span class="line">Bank c = Bank(msg.sender, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">c.balance = <span class="number">5</span>; <span class="comment">// set to new value</span></span><br><span class="line"><span class="keyword">delete</span> b;</span><br><span class="line"><span class="comment">// sets to initial value, set all variables in struct to 0, except mappings</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Enums</span></span><br><span class="line">enum State { Created, Locked, Inactive }; <span class="comment">// often used for state machine</span></span><br><span class="line">State public state; <span class="comment">// Declare variable from enum</span></span><br><span class="line">state = State.Created;</span><br><span class="line"><span class="comment">// enums can be explicitly converted to ints</span></span><br><span class="line">uint createdState = uint(State.Created); <span class="comment">//  0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Data locations: Memory vs. storage vs. calldata - all complex types (arrays,</span></span><br><span class="line"><span class="comment">// structs) have a data location</span></span><br><span class="line"><span class="comment">// 'memory' does not persist, 'storage' does</span></span><br><span class="line"><span class="comment">// Default is 'storage' for local and state variables; 'memory' for func params</span></span><br><span class="line"><span class="comment">// stack holds small local variables</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// for most types, can explicitly set which data location to use</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. Simple operators</span></span><br><span class="line"><span class="comment">// Comparisons, bit operators and arithmetic operators are provided</span></span><br><span class="line"><span class="comment">// exponentiation: **</span></span><br><span class="line"><span class="comment">// exclusive or: ^</span></span><br><span class="line"><span class="comment">// bitwise negation: ~</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. Global Variables of note</span></span><br><span class="line"><span class="comment">// ** this **</span></span><br><span class="line"><span class="built_in">this</span>; <span class="comment">// address of contract</span></span><br><span class="line"><span class="comment">// often used at end of contract life to transfer remaining balance to party</span></span><br><span class="line"><span class="built_in">this</span>.balance;</span><br><span class="line"><span class="built_in">this</span>.someFunction(); <span class="comment">// calls func externally via call, not via internal jump</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ** msg - Current message received by the contract ** **</span></span><br><span class="line">msg.sender; <span class="comment">// address of sender</span></span><br><span class="line">msg.value; <span class="comment">// amount of ether provided to this contract in wei, the function should be marked "payable"</span></span><br><span class="line">msg.data; <span class="comment">// bytes, complete call data</span></span><br><span class="line">msg.gas; <span class="comment">// remaining gas</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ** tx - This transaction **</span></span><br><span class="line">tx.origin; <span class="comment">// address of sender of the transaction</span></span><br><span class="line">tx.gasprice; <span class="comment">// gas price of the transaction</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ** block - Information about current block **</span></span><br><span class="line">now; <span class="comment">// current time (approximately), alias for block.timestamp (uses Unix time)</span></span><br><span class="line"><span class="comment">// Note that this can be manipulated by miners, so use carefully</span></span><br><span class="line"></span><br><span class="line">block.number; <span class="comment">// current block number</span></span><br><span class="line">block.difficulty; <span class="comment">// current block difficulty</span></span><br><span class="line">block.blockhash(<span class="number">1</span>); <span class="comment">// returns bytes32, only works for most recent 256 blocks</span></span><br><span class="line">block.gasLimit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// ** storage - Persistent storage hash **</span></span><br><span class="line">storage[<span class="string">'abc'</span>] = <span class="string">'def'</span>; <span class="comment">// maps 256 bit words to 256 bit words</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. FUNCTIONS AND MORE</span></span><br><span class="line"><span class="comment">// A. Functions</span></span><br><span class="line"><span class="comment">// Simple function</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">increment</span>(<span class="params">uint x</span>) <span class="title">returns</span> (<span class="params">uint</span>) </span>{</span><br><span class="line">    x += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Functions can return many arguments, and by specifying returned arguments</span></span><br><span class="line"><span class="comment">// name don't need to explicitly return</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">increment</span>(<span class="params">uint x, uint y</span>) <span class="title">returns</span> (<span class="params">uint x, uint y</span>) </span>{</span><br><span class="line">    x += <span class="number">1</span>;</span><br><span class="line">    y += <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"><span class="comment">// Call previous functon</span></span><br><span class="line">uint (a,b) = increment(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 'view' (alias for 'constant')</span></span><br><span class="line"><span class="comment">// indicates that function does not/cannot change persistent vars</span></span><br><span class="line"><span class="comment">// View function execute locally, not on blockchain</span></span><br><span class="line"><span class="comment">// Noted: constant keyword will soon be deprecated.</span></span><br><span class="line">uint y = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">increment</span>(<span class="params">uint x</span>) <span class="title">view</span> <span class="title">returns</span> (<span class="params">uint x</span>) </span>{</span><br><span class="line">    x += <span class="number">1</span>;</span><br><span class="line">    y += <span class="number">1</span>; <span class="comment">// this line would fail</span></span><br><span class="line">    <span class="comment">// y is a state variable, and can't be changed in a view function</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 'pure' is more strict than 'view' or 'constant', and does not</span></span><br><span class="line"><span class="comment">// even allow reading of state vars</span></span><br><span class="line"><span class="comment">// The exact rules are more complicated, so see more about</span></span><br><span class="line"><span class="comment">// view/pure:</span></span><br><span class="line"><span class="comment">// http://solidity.readthedocs.io/en/develop/contracts.html#view-functions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 'Function Visibility specifiers'</span></span><br><span class="line"><span class="comment">// These can be placed where 'view' is, including:</span></span><br><span class="line"><span class="comment">// public - visible externally and internally (default for function)</span></span><br><span class="line"><span class="comment">// external - only visible externally (including a call made with this.)</span></span><br><span class="line"><span class="comment">// private - only visible in the current contract</span></span><br><span class="line"><span class="comment">// internal - only visible in current contract, and those deriving from it</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generally, a good idea to mark each function explicitly</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Functions hoisted - and can assign a function to a variable</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">a</span>(<span class="params"></span>) </span>{</span><br><span class="line">    <span class="keyword">var</span> z = b;</span><br><span class="line">    b();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">b</span>(<span class="params"></span>) </span>{</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// All functions that receive ether must be marked 'payable'</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">depositEther</span>(<span class="params"></span>) <span class="title">public</span> <span class="title">payable</span> </span>{</span><br><span class="line">    balances[msg.sender] += msg.value;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Prefer loops to recursion (max call stack depth is 1024)</span></span><br><span class="line"><span class="comment">// Also, don't setup loops that you haven't bounded,</span></span><br><span class="line"><span class="comment">// as this can hit the gas limit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// B. Events</span></span><br><span class="line"><span class="comment">// Events are notify external parties; easy to search and</span></span><br><span class="line"><span class="comment">// access events from outside blockchain (with lightweight clients)</span></span><br><span class="line"><span class="comment">// typically declare after contract parameters</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Typically, capitalized - and add Log in front to be explicit and prevent confusion</span></span><br><span class="line"><span class="comment">// with a function call</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declare</span></span><br><span class="line">event LogSent(address indexed <span class="keyword">from</span>, address indexed to, uint amount); <span class="comment">// note capital first letter</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Call</span></span><br><span class="line">LogSent(<span class="keyword">from</span>, to, amount);</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">For an external party (a contract or external entity), to watch using</span></span><br><span class="line"><span class="comment">the Web3 Javascript library:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">// The following is Javascript code, not Solidity code</span></span><br><span class="line"><span class="comment">Coin.LogSent().watch({}, '', function(error, result) {</span></span><br><span class="line"><span class="comment">    if (!error) {</span></span><br><span class="line"><span class="comment">        console.log("Coin transfer: " + result.args.amount +</span></span><br><span class="line"><span class="comment">            " coins were sent from " + result.args.from +</span></span><br><span class="line"><span class="comment">            " to " + result.args.to + ".");</span></span><br><span class="line"><span class="comment">        console.log("Balances now:\n" +</span></span><br><span class="line"><span class="comment">            "Sender: " + Coin.balances.call(result.args.from) +</span></span><br><span class="line"><span class="comment">            "Receiver: " + Coin.balances.call(result.args.to));</span></span><br><span class="line"><span class="comment">    }</span></span><br><span class="line"><span class="comment">}</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Common paradigm for one contract to depend on another (e.g., a</span></span><br><span class="line"><span class="comment">// contract that depends on current exchange rate provided by another)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C. Modifiers</span></span><br><span class="line"><span class="comment">// Modifiers validate inputs to functions such as minimal balance or user auth;</span></span><br><span class="line"><span class="comment">// similar to guard clause in other languages</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// '_' (underscore) often included as last line in body, and indicates</span></span><br><span class="line"><span class="comment">// function being called should be placed there</span></span><br><span class="line">modifier <span class="function"><span class="title">onlyAfter</span>(<span class="params">uint _time</span>)</span> { <span class="built_in">require</span> (now &gt;= _time); _; }</span><br><span class="line">modifier onlyOwner { <span class="built_in">require</span>(msg.sender == owner) _; }</span><br><span class="line"><span class="comment">// commonly used with state machines</span></span><br><span class="line">modifier onlyIfStateA (State currState) { <span class="built_in">require</span>(currState == State.A) _; }</span><br><span class="line"></span><br><span class="line"><span class="comment">// Append right after function declaration</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">changeOwner</span>(<span class="params">newOwner</span>)</span></span><br><span class="line"><span class="function"><span class="title">onlyAfter</span>(<span class="params">someTime</span>)</span></span><br><span class="line"><span class="function"><span class="title">onlyOwner</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"><span class="title">onlyIfState</span>(<span class="params">State.A</span>)</span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    owner = newOwner;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// underscore can be included before end of body,</span></span><br><span class="line"><span class="comment">// but explicitly returning will skip, so use carefully</span></span><br><span class="line">modifier <span class="function"><span class="title">checkValue</span>(<span class="params">uint amount</span>)</span> {</span><br><span class="line">    _;</span><br><span class="line">    <span class="keyword">if</span> (msg.value &gt; amount) {</span><br><span class="line">        uint amountToRefund = amount - msg.value;</span><br><span class="line">        msg.sender.transfer(amountToRefund);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. BRANCHING AND LOOPS</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// All basic logic blocks work - including if/else, for, while, break, continue</span></span><br><span class="line"><span class="comment">// return - but no switch</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Syntax same as javascript, but no type conversion from non-boolean</span></span><br><span class="line"><span class="comment">// to boolean (comparison operators must be used to get the boolean val)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// For loops that are determined by user behavior, be careful - as contracts have a maximal</span></span><br><span class="line"><span class="comment">// amount of gas for a block of code - and will fail if that is exceeded</span></span><br><span class="line"><span class="comment">// For example:</span></span><br><span class="line"><span class="keyword">for</span>(uint x = <span class="number">0</span>; x &lt; refundAddressList.length; x++) {</span><br><span class="line">    refundAddressList[x].transfer(SOME_AMOUNT);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Two errors above:</span></span><br><span class="line"><span class="comment">// 1. A failure on transfer stops the loop from completing, tying up money</span></span><br><span class="line"><span class="comment">// 2. This loop could be arbitrarily long (based on the amount of users who need refunds), and</span></span><br><span class="line"><span class="comment">// therefore may always fail as it exceeds the max gas for a block</span></span><br><span class="line"><span class="comment">// Instead, you should let people withdraw individually from their subaccount, and mark withdrawn</span></span><br><span class="line"><span class="comment">// e.g., favor pull payments over push payments</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 7. OBJECTS/CONTRACTS</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// A. Calling external contract</span></span><br><span class="line">contract InfoFeed {</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">info</span>(<span class="params"></span>) <span class="title">payable</span> <span class="title">returns</span> (<span class="params">uint ret</span>)  </span>{ <span class="keyword">return</span> <span class="number">42</span>; }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">contract Consumer {</span><br><span class="line">    InfoFeed feed; <span class="comment">// points to contract on blockchain</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set feed to existing contract instance</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">setFeed</span>(<span class="params">address addr</span>) </span>{</span><br><span class="line">        <span class="comment">// automatically cast, be careful; constructor is not called</span></span><br><span class="line">        feed = InfoFeed(addr);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set feed to new instance of contract</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">createNewFeed</span>(<span class="params"></span>) </span>{</span><br><span class="line">        feed = <span class="keyword">new</span> InfoFeed(); <span class="comment">// new instance created; constructor called</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">callFeed</span>(<span class="params"></span>) </span>{</span><br><span class="line">        <span class="comment">// final parentheses call contract, can optionally add</span></span><br><span class="line">        <span class="comment">// custom ether value or gas</span></span><br><span class="line">        feed.info.value(<span class="number">10</span>).gas(<span class="number">800</span>)();</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// B. Inheritance</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Order matters, last inherited contract (i.e., 'def') can override parts of</span></span><br><span class="line"><span class="comment">// previously inherited contracts</span></span><br><span class="line">contract MyContract is abc, <span class="function"><span class="title">def</span>(<span class="params"><span class="string">"a custom argument to def"</span></span>)</span> {</span><br><span class="line"></span><br><span class="line"><span class="comment">// Override function</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">z</span>(<span class="params"></span>) </span>{</span><br><span class="line">        <span class="keyword">if</span> (msg.sender == owner) {</span><br><span class="line">            def.z(); <span class="comment">// call overridden function from def</span></span><br><span class="line">            <span class="built_in">super</span>.z(); <span class="comment">// call immediate parent overridden function</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// abstract function</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">someAbstractFunction</span>(<span class="params">uint x</span>)</span>;</span><br><span class="line"><span class="comment">// cannot be compiled, so used in base/abstract contracts</span></span><br><span class="line"><span class="comment">// that are then implemented</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C. Import</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"filename"</span>;</span><br><span class="line"><span class="keyword">import</span> <span class="string">"github.com/ethereum/dapp-bin/library/iterable_mapping.sol"</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 8. OTHER KEYWORDS</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// A. Selfdestruct</span></span><br><span class="line"><span class="comment">// selfdestruct current contract, sending funds to address (often creator)</span></span><br><span class="line">selfdestruct(SOME_ADDRESS);</span><br><span class="line"></span><br><span class="line"><span class="comment">// removes storage/code from current/future blocks</span></span><br><span class="line"><span class="comment">// helps thin clients, but previous data persists in blockchain</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Common pattern, lets owner end the contract and receive remaining funds</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">remove</span>(<span class="params"></span>) </span>{</span><br><span class="line">    <span class="keyword">if</span>(msg.sender == creator) { <span class="comment">// Only let the contract creator do this</span></span><br><span class="line">        selfdestruct(creator); <span class="comment">// Makes contract inactive, returns funds</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// May want to deactivate contract manually, rather than selfdestruct</span></span><br><span class="line"><span class="comment">// (ether sent to selfdestructed contract is lost)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 9. CONTRACT DESIGN NOTES</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// A. Obfuscation</span></span><br><span class="line"><span class="comment">// All variables are publicly viewable on blockchain, so anything</span></span><br><span class="line"><span class="comment">// that is private needs to be obfuscated (e.g., hashed w/secret)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Steps: 1. Commit to something, 2. Reveal commitment</span></span><br><span class="line">keccak256(<span class="string">"some_bid_amount"</span>, <span class="string">"some secret"</span>); <span class="comment">// commit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// call contract's reveal function in the future</span></span><br><span class="line"><span class="comment">// showing bid plus secret that hashes to SHA3</span></span><br><span class="line">reveal(<span class="number">100</span>, <span class="string">"mySecret"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// B. Storage optimization</span></span><br><span class="line"><span class="comment">// Writing to blockchain can be expensive, as data stored forever; encourages</span></span><br><span class="line"><span class="comment">// smart ways to use memory (eventually, compilation will be better, but for now</span></span><br><span class="line"><span class="comment">// benefits to planning data structures - and storing min amount in blockchain)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Cost can often be high for items like multidimensional arrays</span></span><br><span class="line"><span class="comment">// (cost is for storing data - not declaring unfilled variables)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// C. Data access in blockchain</span></span><br><span class="line"><span class="comment">// Cannot restrict human or computer from reading contents of</span></span><br><span class="line"><span class="comment">// transaction or transaction's state</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// While 'private' prevents other *contracts* from reading data</span></span><br><span class="line"><span class="comment">// directly - any other party can still read data in blockchain</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// All data to start of time is stored in blockchain, so</span></span><br><span class="line"><span class="comment">// anyone can observe all previous data and changes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// E. Oracles and External Data</span></span><br><span class="line"><span class="comment">// Oracles are ways to interact with your smart contracts outside the blockchain. </span></span><br><span class="line"><span class="comment">// They are used to get data from the real world, send post requests, to the real world</span></span><br><span class="line"><span class="comment">// or vise versa.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Time-based implementations of contracts are also done through oracles, as </span></span><br><span class="line"><span class="comment">// contracts need to be directly called and can not "subscribe" to a time. </span></span><br><span class="line"><span class="comment">// Due to smart contracts being decentralized, you also want to get your data</span></span><br><span class="line"><span class="comment">// in a decentralized manner, other your run into the centralized risk that </span></span><br><span class="line"><span class="comment">// smart contract design matter prevents. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// To easiest way get and use pre-boxed decentralized data is with Chainlink Data Feeds</span></span><br><span class="line"><span class="comment">// https://docs.chain.link/docs/get-the-latest-price</span></span><br><span class="line"><span class="comment">// We can reference on-chain reference points that have already been aggregated by </span></span><br><span class="line"><span class="comment">// multiple sources and delivered on-chain, and we can use it as a "data bank" </span></span><br><span class="line"><span class="comment">// of sources. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can see other examples making API calls here:</span></span><br><span class="line"><span class="comment">// https://docs.chain.link/docs/make-a-http-get-request</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// And you can of course build your own oracle network, just be sure to know </span></span><br><span class="line"><span class="comment">// how centralized vs decentralized your application is. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Setting up oracle networks yourself</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// D. Cron Job</span></span><br><span class="line"><span class="comment">// Contracts must be manually called to handle time-based scheduling; can create external</span></span><br><span class="line"><span class="comment">// code to regularly ping, or provide incentives (ether) for others to</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// E. Observer Pattern</span></span><br><span class="line"><span class="comment">// An Observer Pattern lets you register as a subscriber and</span></span><br><span class="line"><span class="comment">// register a function which is called by the oracle (note, the oracle pays</span></span><br><span class="line"><span class="comment">// for this action to be run)</span></span><br><span class="line"><span class="comment">// Some similarities to subscription in Pub/sub</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This is an abstract contract, both client and server classes import</span></span><br><span class="line"><span class="comment">// the client should implement</span></span><br><span class="line">contract SomeOracleCallback {</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">oracleCallback</span>(<span class="params">int _value, uint _time, bytes32 info</span>) <span class="title">external</span></span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">contract SomeOracle {</span><br><span class="line">    SomeOracleCallback[] callbacks; <span class="comment">// array of all subscribers</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Register subscriber</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">addSubscriber</span>(<span class="params">SomeOracleCallback a</span>) </span>{</span><br><span class="line">        callbacks.push(a);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">notify</span>(<span class="params">value, time, info</span>) <span class="title">private</span> </span>{</span><br><span class="line">        <span class="keyword">for</span>(uint i = <span class="number">0</span>;i &lt; callbacks.length; i++) {</span><br><span class="line">            <span class="comment">// all called subscribers must implement the oracleCallback</span></span><br><span class="line">            callbacks[i].oracleCallback(value, time, info);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">doSomething</span>(<span class="params"></span>) <span class="title">public</span> </span>{</span><br><span class="line">        <span class="comment">// Code to do something</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Notify all subscribers</span></span><br><span class="line">        notify(_value, _time, _info);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Now, your client contract can addSubscriber by importing SomeOracleCallback</span></span><br><span class="line"><span class="comment">// and registering with Some Oracle</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// F. State machines</span></span><br><span class="line"><span class="comment">// see example below for State enum and inState modifier</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Solidity, Blockchain, and Smart Contract 강의 정리</category>
      </categories>
      <tags>
        <tag>Blockchain</tag>
        <tag>Smart contract</tag>
        <tag>Bitcoin</tag>
        <tag>Ethereum</tag>
        <tag>DAO</tag>
        <tag>DeFi</tag>
        <tag>Solidity</tag>
      </tags>
  </entry>
  <entry>
    <title>아니 왜 C++ 언어가 GPL 라이센스지?? (libc++ / libstdc++)</title>
    <url>/20220115-libstdc-gpl/</url>
    <content><![CDATA[<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><p>C++17에서 <code>std::filesystem</code>을 자주 쓰는 편인데, C++11에서 작업을 해야할 일이 생겼다. 그래서 <code>std::experimental::filesystem</code>을 사용해보고 있었다.</p>
<p>어쩌다가 <code>&lt;experimental/filesystem&gt;</code> 헤더에 들어가게 되었다.</p>
<p>근데 이 헤더가 GPLv3 라이센스라고 한다.</p>
<p>??? GPL 라이센스는 상업적으로 사용하려면 무조건 소스공개를 해야하는 라이센스가 아닌가??</p>
<p>그렇다면 C++ 프로그램들은 모두 GPL 라이센스를 따라서 소스공개를 해야하는게 아닌가…? 근데 그럴리가 없는데…? 내가 GPL 라이센스를 잘못 알고 있는건가…?</p>
<img src="/20220115-libstdc-gpl/gpl.png" class="" title="gpl">

<p> </p>
<h2 id="알고보니…"><a href="#알고보니…" class="headerlink" title="알고보니…"></a>알고보니…</h2><p>GCC Runtime Library Exception에 다음과 같은 내용이 들어있다.</p>
<blockquote>
<p>“실행파일을 생성하기 위해 해당 파일을 컴파일하고 링크하는 것은 GPL 라이선스의 적용을 받지 않는다”</p>
</blockquote>
<p><strong>결국 사용해도 된다는 것 같다 ㅎㅎ</strong></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>libc++</tag>
        <tag>libstdc++</tag>
        <tag>GPL</tag>
        <tag>License</tag>
      </tags>
  </entry>
  <entry>
    <title>더 안전하게 C++ 코드를 작성하는 법</title>
    <url>/20220115-safer-c/</url>
    <content><![CDATA[<h2 id="이-글에서-이야기하는-‘안전한-코드’의-의미"><a href="#이-글에서-이야기하는-‘안전한-코드’의-의미" class="headerlink" title="이 글에서 이야기하는 ‘안전한 코드’의 의미"></a>이 글에서 이야기하는 ‘안전한 코드’의 의미</h2><blockquote>
<p>적어도 빌드는 돼야지…<br>적어도 왕초보 실수는 없어야지…<br>적어도 돌리다가 터지진 않아야지…</p>
</blockquote>
<p>적어도 이게 ‘작동하는 코드’인가 에 대한 내용이 주가 됩니다.</p>
<p>해킹 시도에 안전한지, 무적의 자율주행 알고리즘을 만든다던지와는 거리가 멉니다 ㅎㅎ</p>
<p> </p>
<h2 id="간단요약-TLDR"><a href="#간단요약-TLDR" class="headerlink" title="간단요약 / TLDR;"></a>간단요약 / TLDR;</h2><ul>
<li>gcc/clang/MSVC 컴파일러 다 써서 빌드하세요</li>
<li><code>-Wall -Werror -Wpedantic -Werror</code> 빌드 플래그 쓰세요</li>
<li>유닛테스트 꼭 하세요</li>
<li>clang-tidy나 cppcheck 꼭 쓰세요</li>
<li>sanitizer 꼭 쓰세요</li>
<li>MISRA 적용도 고려해보세요</li>
</ul>
<p> </p>
<hr>
<h2 id="컴파일러-종류는-최소-2개를-사용하세요"><a href="#컴파일러-종류는-최소-2개를-사용하세요" class="headerlink" title="컴파일러 종류는 최소 2개를 사용하세요"></a>컴파일러 종류는 최소 2개를 사용하세요</h2><p>PC에서 사용하는 C++ 컴파일러는 주로 3가지가 있습니다.</p>
<ul>
<li>Visual Studio에서 지원하는 MSVC</li>
<li>Apple 진영이 많이 사용하는 Clang</li>
<li>GNU/Linux 진영이 많이 사용하는 gcc</li>
</ul>
<p>저는 <strong>코드를 작성할 때 최소 2개의 컴파일러를 기준으로 빌드</strong>해보는 것을 추천합니다.</p>
<p>각각의 컴파일러들마다 경고/에러를 걸러주는 기준도 조금씩 다르고, 성능도 꽤 다르기 때문입니다.</p>
<p>우선 <strong>경고/에러</strong>에 대해서 이야기 해봅시다. gcc에서는 빌드가 잘 되는데, clang에서는 빌드가 안될 수도 있습니다. 이 경우에는 clang에서 경고/에러를 수정하면 보통 <strong>gcc/clang를 모두 통과하는 깔끔한 코드</strong>를 만들 수 있습니다. 왠만한 경우에서 컴파일러 두개 중 하나만 선택해야하는 경우는 없습니다.</p>
<p>다음은 <strong>성능</strong>에 대한 부분입니다. gcc로 빌드한 결과가 MSVC보다 10% 더 빠르다고 해봅시다. <strong>동일한 작업을 수행하는데 더 빠른 결과를 내는 컴파일러를 쓰는게 더 좋은 선택</strong>일 겁니다. MSVC로만 빌드하고 있었다면 평생 얻지 못할 10% 성능개선입니다. 물론 이 부분은 코드의 안정성과는 거리가 먼 부분이긴 합니다.</p>
<p>물론 멀티 컴파일러 빌드에도 단점은 있습니다. 컴파일러의 개수마다 빌드를 돌려야하기 때문에 오래걸린다는 점인데요, 이 부분은 CI 서버에 다중 agent를 둬서 MSVC, clang, gcc를 모두 한번에 빌드하고 성능을 측정하게 자동화해주면 되는 부분입니다. 나는 중저가용 노트북 하나로 모든걸 다 해야하고 내일 모레가 데드라인이다, 하시는 분들에게만 gcc 원툴 테크트리를 추천드립니다.</p>
<p> </p>
<hr>
<h2 id="모호한-코딩을-검수하는-컴파일러-플래그를-활성화하세요"><a href="#모호한-코딩을-검수하는-컴파일러-플래그를-활성화하세요" class="headerlink" title="모호한 코딩을 검수하는 컴파일러 플래그를 활성화하세요"></a>모호한 코딩을 검수하는 컴파일러 플래그를 활성화하세요</h2><p>gcc/clang 컴파일러에는 다음과 같은 컴파일러 경고 플래그를 설정할 수 있습니다. 경고 플래그는 모두 <code>-W</code>로 시작합니다.</p>
<ul>
<li><code>-Wall</code></li>
<li><code>-Wextra</code></li>
<li><code>-Wpedantic</code></li>
<li><code>-Werror</code></li>
</ul>
<p><code>-Wall</code>은 왠만한 모호한 코딩들을 잡아내는 경고 플래그들을 활성화합니다. 이 기능을 키는 것만 해도 꽤나 많은 이슈를 잡을 수 있습니다. ‘all’이라는 이름만 보면 모든 경고 플래그를 활성화하는 것 같지만, 실제로는 얼마 몇개 없습니다 (<span class="exturl" data-url="aHR0cHM6Ly9nY2MuZ251Lm9yZy9vbmxpbmVkb2NzL2djYy9XYXJuaW5nLU9wdGlvbnMuaHRtbA==">링크<i class="fa fa-external-link-alt"></i></span>).</p>
<p>여기에 <code>-Wextra</code>를 써서 모호한 코딩들을 더 잡아주는 extra 플래그도 활성화합니다.</p>
<p><code>-Wpedantic</code> 플래그도 활성화해줍니다. ISO C++에서 요구하는 사항들을 빡빡하게 걸러내주는 플래그입니다. 대충 어디어디 사전에 따르면 pedantic의 의미는 아래와 같습니다. 좋은 코드만 짤 수 있다면 좀 pedantic 하면 어떤가요 ㅎㅎ</p>
<blockquote>
<p>pedant는 형식주의와 정확성에 지나치게 관심이 있거나 과시하고 오만한 학습 표시를하는 사람입니다.</p>
</blockquote>
<p>마지막으로 <code>-Werror</code> 플래그를 활성화 해줍니다. 이러면 모든 ‘경고’는 ‘에러’가 됩니다. 즉, 이제 당신은 이 경고를 해결하기 전 까지 프로그램을 쓸 수 없습니다. 빨리 가서 경고를 다 고치십시오 후후</p>
<blockquote>
<p>MSVC에서도 비슷한 플래그들이 있습니다. 다만 저는 SLAM쟁이라 MSVC에는 큰 관심이 없으니 직접 찾아보시는걸 추천드립니다.</p>
</blockquote>
<p> </p>
<hr>
<h2 id="Unit-test를-쓰세요"><a href="#Unit-test를-쓰세요" class="headerlink" title="Unit test를 쓰세요"></a>Unit test를 쓰세요</h2><blockquote>
<p>Unit test에 대해 알아보고, 직접 데모를 돌려보고 싶으시다면 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjEwOTI1LWNwcC1iZXN0LXByYWN0aWNlcy1hdXRvbWF0ZWQtdGVzdHMv">링크<i class="fa fa-external-link-alt"></i></span>를 따라가세요</p>
</blockquote>
<p>TDD 찬양론이 아니라는 점 우선 먼저 밝힙니다. 하지만 ‘그 코드 잘 작동해?’ 라고 물어봤을 때 ‘음 잘 몰라, 아마 그럴걸?’라는 답변을 듣고 반기는 프로그래머는 없을 겁니다.</p>
<p>정말 정말 백보 물러나서 얘기하는건데, 적어도 하나의 테스트는 꼭 돌려주시길 바랍니다. 테스트도 안되고 도큐먼팅도 안된 코드를 사용해야하는 상황이 오면, 당신은 그날 저녁부터 이 상황으로부터 도망가기 위해 이직 준비를 시작하게 될겁니다. 그러다가 극심한 스트레스로 두통과 몸살이 동시에 와 침대에서 끙끙거리다가 ‘내 인생 왜 이 모양 이 꼴이냐’ 하며 눈물 한방울을 흘리게 될겁니다.</p>
<p>정말 코드에 자신이 있으려면, 작성하신 모든 함수 하나하나마다 유닛테스트를 만들어서 ‘내 코드는 주어진 시나리오에서 잘 작동합니다’ 라고 얘기할 수 있도록 합시다.</p>
<img src="/20220115-safer-c/result.png" class="" title="result">

<p> </p>
<hr>
<h2 id="정적분석-Static-analysis"><a href="#정적분석-Static-analysis" class="headerlink" title="정적분석 (Static analysis)"></a>정적분석 (Static analysis)</h2><p>정적분석은 소스코드를 분석해서 좋지 않은 코드가 있으면 알려주는 툴입니다.</p>
<p>Visual Studio에는 기본적으로 적용되어있는 기능이며, 위험한 코드를 짜게 되면 ‘이렇게 짜지 않을래?’하고 제안을 하는 친구입니다. VSCode에서는 이 기능이 자동으로 지원되지 않는데, 그러다보니 VSCode 유저들 중에 코드 퀄리티가 엉망이신 분들도 꽤 많습니다. 하지만 다 핑계입니다. <strong><code>clang-tidy</code> 익스텐션</strong> 받으면 곧바로 정적 분석이 돌아가기 때문입니다. CLion IDE를 사용하면 기본 적용되어있습니다.</p>
<p>clang-tidy도 플래그를 추가로 적용해서 더 좋은 코드를 만들 수 있습니다. 아래는 제가 좋아하는 플래그들입니다.</p>
<ul>
<li>-*<ul>
<li>기본 체크</li>
</ul>
</li>
<li>bugprone-*<ul>
<li>버그 잡이</li>
</ul>
</li>
<li>clang-analyzer-*<ul>
<li>버그 잡이 2</li>
</ul>
</li>
<li>concurrency-*<ul>
<li>동시성 프로그래밍 버그 잡이</li>
</ul>
</li>
<li>cppcoreguidelines-*<ul>
<li>C++ Core guideline 준수</li>
</ul>
</li>
<li>hicpp-*<ul>
<li>High integrity C++ 가이드라인 준수</li>
</ul>
</li>
<li>linuxkernel-*<ul>
<li>리눅스 커널에 맞는 코드 준수</li>
</ul>
</li>
<li>modernize-*<ul>
<li>모던 C++ 사용</li>
</ul>
</li>
<li>openmp-*<ul>
<li>OpenMP 사용 법 준수</li>
</ul>
</li>
<li>performance-*<ul>
<li>성능 향상 제안</li>
</ul>
</li>
<li>portability-*<ul>
<li>이식성 제안</li>
</ul>
</li>
</ul>
<p>이러한 체커도 하나만 쓰면 좀 아쉽습니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Rhbm1hci9jcHBjaGVjay8=">CppCheck<i class="fa fa-external-link-alt"></i></span>라는 정적분석 툴도 좋으니, clang-tidy와 함께 돌립시다.</p>
<p>아래는 clang-tidy를 돌렸을 때 나오는 코드 제안들의 예시입니다 (출처: <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLndob2xldG9tYXRvLmNvbS8yMDIxLzAxLzA4L2EtYnJpZWYtaW50cm9kdWN0aW9uLXRvLWNsYW5nLXRpZHktYW5kLWl0cy1yb2xlLWluLXZpc3VhbC1hc3Npc3Qv">https://blog.wholetomato.com/2021/01/08/a-brief-introduction-to-clang-tidy-and-its-role-in-visual-assist/<i class="fa fa-external-link-alt"></i></span>)</p>
<img src="/20220115-safer-c/clang.png" class="" title="clang">

<p> </p>
<hr>
<h2 id="동적분석-메모리-주소-메모리-leak-쓰레드-관리"><a href="#동적분석-메모리-주소-메모리-leak-쓰레드-관리" class="headerlink" title="동적분석 (메모리 주소, 메모리 leak, 쓰레드 관리)"></a>동적분석 (메모리 주소, 메모리 leak, 쓰레드 관리)</h2><p>동적 분석은 실제로 코드가 돌아가면서 생기는 허점들을 잡아줄 수 있습니다.</p>
<p>가장 대표적인건 1. <strong>메모리 주소</strong>, 2. <strong>메모리 leak</strong>, 3. <strong>쓰레드 관리</strong> 쪽 허점들입니다.</p>
<p>Google에서 만든 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS9zYW5pdGl6ZXJz">Sanitizer<i class="fa fa-external-link-alt"></i></span>가 있습니다. 뭘 써야할지 모르겠을 때는 그냥 다 적용합시다.</p>
<p> </p>
<hr>
<h2 id="MISRA"><a href="#MISRA" class="headerlink" title="MISRA"></a>MISRA</h2><p>MISRA C++은 실제로 안전이 중요한 시스템에서 지켜져야하는 C++ 프로그래밍 가이드입니다.</p>
<p>프로토타입 단계에서 필요한건 아니지만, 제품으로 만들려고 하면 꼭 거쳐야합니다. 어차피 거칠거면 처음부터 오토체커를 사용해서 잘 짜면 되지 않을까요?</p>
<p>오토체커 방법은 두가지가 있습니다.</p>
<p>첫번째는 <strong>CLion IDE에서 지원하는 오토체커</strong>를 사용하는겁니다. CLion에서는 정적분석 툴로 MISRA 오토체커가 들어가있습니다. 하지만 그렇게 막 좋은 편은 아닙니다. 2022년 1월 16일 기준으로, 211개 체크 중 65개만 구현이 되어있습니다. 구현 상황은 <span class="exturl" data-url="aHR0cHM6Ly95b3V0cmFjay5qZXRicmFpbnMuY29tL2FydGljbGVzL0NQUC1BLTE5MTQzMDY4Mj9fZ2E9Mi4xNTE2Mzg3NzguMjExMzM0NTA5NS4xNjQyMjUwNjAxLTIzMTMwODMwMi4xNjI1NjE2NTU0Jl9nbD0xKmJzYWtidypfZ2EqTWpNeE16QTRNekF5TGpFMk1qVTJNVFkxTlRRLipfZ2FfMFdRMlpGNVZHVCpNVFkwTWpJMk1UTTFNUzR4TGpFdU1UWTBNakkyTVRVeU1DNHc=">링크<i class="fa fa-external-link-alt"></i></span>에 잘 나와있습니다. 하지만 그래도 65개라도 돌아가는거 어디입니까 헛헛</p>
<blockquote>
<p>물론 이 방법을 사용하려면 CLion IDE를 써야합니다. 유료이긴 하지만 성능이 빵빵하니 개인적으로 추천하는 IDE입니다. 학생이시라면 교육용 라이센스로, 직장인이라면 회사에 사달라고 하세요. 개인 사용은 선택에 맡기겠습니다.</p>
</blockquote>
<p>두번째는 <strong>CppCheck 기반 오토체커</strong>를 쓰는겁니다. CppCheck는 clang-tidy와 같은 정적분석 툴인데, 이 안에도 MISRA 체커 기능이 있습니다. 대신 CLion에서 지원하는 것과는 다르게, 직접 MISRA 도큐먼트를 15 파운드 (약 3만원)으로 구매한 후 직접 설정해줘야하는게 매우 귀찮습니다.</p>
<p>제 코드는 원래 경고/에러 없이 깔끔했습니다만, CLion의 MISRA 체커를 키니까 안전하지 않은 코드라고 호되게 혼나는 모습을 아래 사진에 담았습니다. 노란줄은 경고, 빨간줄은 에러인데, 211개의 체크를 다 거치면 얼마나 고쳐야할지 모르겠네요 허허</p>
<img src="/20220115-safer-c/misra.png" class="" title="misra">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>STL</tag>
        <tag>gcc</tag>
        <tag>MISRA</tag>
        <tag>clang</tag>
      </tags>
  </entry>
  <entry>
    <title>C++11에서 C++17의 std::filesystem 사용하기</title>
    <url>/20220115-std-experimental-filesystem/</url>
    <content><![CDATA[<h2 id="Filesystem"><a href="#Filesystem" class="headerlink" title="Filesystem"></a>Filesystem</h2><p><code>std::filesystem</code>은 진짜 좋습니다…</p>
<p>아쉬운 점은 <code>std::filesystem</code>은 <strong>C++17에서 채택</strong>되었다는 점입니다.</p>
<p>그러면 C++11/14쟁이들은 영원히 path에서 고통받아야하는건가요?</p>
<p>회사 업무로 인해 C++17에서 작업을 하다가 C++11로 내려와야했던 저는, 이 발암을 없애기 위해 가장 먼저 filesystem의 대체제를 찾았습니다.</p>
<h3 id="Filesystem이-왜-좋은가"><a href="#Filesystem이-왜-좋은가" class="headerlink" title="Filesystem이 왜 좋은가?"></a>Filesystem이 왜 좋은가?</h3><p>C++ 코딩을 하다보면 다들 다르게 경로를 적는 방법이 다릅니다.</p>
<p>Windows에서는 경로를 <code>\\</code>로 가르고, UNIX 기반의 Linux와 MacOS는 경로를 <code>/</code>로 가릅니다.</p>
<p>사람마다도 경로를 다르게 적습니다. 누구는 절대경로를 좋아하고, 누구는 상대경로를 좋아합니다.</p>
<p>그래서인지 다른 컴퓨터에서 코드를 끌고와서 빌드하면 파일을 못찾고 다 터집니다 허허</p>
<p>이런 열받는 상황을 타파하는 솔루션은 <code>std::filesystem</code>입니다.</p>
<p>경로 관련 작업을 1. 안전하게, 2. 크로스플랫폼 지원하며, 3. 가독성 좋게 만들어줄 수 있습니다.</p>
<p> </p>
<hr>
<h2 id="대안-1-Boost-fileystem"><a href="#대안-1-Boost-fileystem" class="headerlink" title="대안 1: Boost/fileystem"></a>대안 1: Boost/fileystem</h2><blockquote>
<p>장점: 쓰기 쉽다. Boost의 다른 기능들도 함께 쓸 수 있다.<br>단점: 겁.나. 무겁다</p>
</blockquote>
<p>가장 뻔한 대안으로는 <span class="exturl" data-url="aHR0cHM6Ly90aGVib29zdGNwcGxpYnJhcmllcy5jb20v">Boost<i class="fa fa-external-link-alt"></i></span> 프레임워크를 사용하는 것입니다. Boost에는 C++에 적용될 수 있는 많은 모던한 기능들이 다 들어있습니다.</p>
<p>Boost 빌드를 하면서 filesystem을 타겟으로 빌드를 하면 Boost의 코어와 몇가지 라이브러리들을 기반으로 filesystem 구현체가 설치됩니다.</p>
<p>장점이라면, <code>boost::filesystem</code>은 <code>std::filesystem</code>과 <strong>거의 API가 동일</strong>하며, 왠만한 경우 C++17로 작성된 코드에서 std-&gt;boost로 바꾸면 잘 작동합니다.</p>
<p>단점이라면, Boost filesystem은 설치를 하면 Debug/Release가 각각 약 150MB 정도로 굉장히 무겁습니다. Path 기능을 아무리 잘 짜봐야 전체 알고리즘에 비해 얼마나 차지하겠습니까? 1%도 안되는 부분인데, OpenCV의 3배가 되는 심볼이 올라가게 되는거라면 용량이 정말 걱정되는 수준입니다. <strong>Boost가 무겁다</strong>는건 듣긴 했지만 이정도일줄은… ㅋㅋ</p>
<p> </p>
<hr>
<h2 id="대안-2-std-experimental-filesystem"><a href="#대안-2-std-experimental-filesystem" class="headerlink" title="대안 2: std::experimental::filesystem"></a>대안 2: std::experimental::filesystem</h2><blockquote>
<p>장점: 외부 라이브러리가 필요없다. API가 동일하다.<br>단점: 컴파일러 버전 바뀌면 언제 없어질 지 모른다. 임베디드 컴파일러에서 지원 안할수도 있다.</p>
</blockquote>
<p>그 다음 대안으로는 <code>std::experimental</code>을 사용하는 것입이다.</p>
<p><code>std::experimental</code>은 현재 STL에서 추후 C++ standard에 포함시키려고 하는 기능들을 테스트하기 위해 만든 기능들입니다. 대부분 std::experimental에 올라간 기능들은 거의 확정으로 추후 C++ standard에 올라가게 됩니다.</p>
<p>특이한 점은, C++17에 이미 filesystem이 올라가있는데도 최신 GCC 컴파일러에 <code>std::experimental::filesystem</code>이 올라가있습니다. 기존의 C++17 코드에서 <code>std::filesystem</code> -&gt; <code>std::experiemental::filesystem</code>으로 변경하고 C++11로 스탠다드를 내렸는데, 잘 작동했습니다.</p>
<p>이 방식은 Boost를 사용하는 방식에 비해, <strong>전혀 외부 라이브러리를 이용하지 않아도 된다는 큰 강점</strong>을 가집니다.</p>
<p>단점이라면, <strong>컴파일러 버전이 높아지면서 언제 사라질지 모릅니다</strong>. 또, 임베디드 타겟의 컴파일러라면 로딩되는 심볼을 줄이기 위해 experimental과 같은 기능들은 로딩하지 않았을 수도 있습니다. 로딩되지 않았다면 이 기능은 사용하지 못할 겁니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>STL</tag>
        <tag>Boost</tag>
        <tag>std::experimental</tag>
      </tags>
  </entry>
  <entry>
    <title>Static lib vs Shared lib (.lib vs .dll)</title>
    <url>/20220120-static-vs-shared-libs/</url>
    <content><![CDATA[<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><p>C++ 프로그램을 만들 때 외부 라이브러리를 사용해서 재사용 가능한 함수/루틴/클래스 등을 사용할 수 있습니다.</p>
<p>이러한 라이브러리들은 2가지 형태로 사용할 수 있습니다.</p>
<p> </p>
<hr>
<h2 id="Static-library-lib-a"><a href="#Static-library-lib-a" class="headerlink" title="Static library (.lib / .a)"></a>Static library (.lib / .a)</h2><p>컴파일 단계에서 외부 함수나 루틴을 링크하는 방법입니다.</p>
<p>코드 자체가 프로그램 내부에 들어가게 되는데, 이 때문에 프로그램의 용량이 커지게 됩니다.</p>
<p>하지만 컴파일 단계에서 코드가 링크되기 때문에, 런타임에서 굉장히 빠르게 작동합니다.</p>
<h2 id="Shared-library-dll-so-dylib"><a href="#Shared-library-dll-so-dylib" class="headerlink" title="Shared library (.dll / .so / .dylib)"></a>Shared library (.dll / .so / .dylib)</h2><p>런타임에서 다이나믹 링크하는 방법입니다.</p>
<p>함수나 루틴이 사용될 때마다 해당 기능을 외부 라이브러리에 참조하여 사용하는 방식입니다.</p>
<p>코드 자체가 프로그램 내부에 들어가지 않기 때문에 프로그램의 용량은 작게 유지할 수 있습니다.</p>
<p>하지만 외부 라이브러리를 사용하기 위해서 라이브러리 파일을 항상 가지고 다녀야합니다.</p>
<p>또, 함수나 루틴을 사용할 때 마다 외부 라이브러리를 참조해야하기 때문에 런타임에 cost가 생겨납니다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Unused parameter 경고 잠재우기</title>
    <url>/20220120-unused-parameter/</url>
    <content><![CDATA[<h2 id="Unused-parameter-warning"><a href="#Unused-parameter-warning" class="headerlink" title="Unused parameter warning"></a>Unused parameter warning</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTE1LXNhZmVyLWMv">더 안전하게 C++ 코드를 작성하는법<i class="fa fa-external-link-alt"></i></span> 글에 나왔던 것 처럼 <code>-Wall</code> 플래그를 사용해서 빌드를 할 때, <strong>함수 인자로 받은 변수를 함수에서 사용하지 않을 경우 경고</strong>가 나타나게 됩니다.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">In file included from /home/hyunggi/c++/directory/sample_code.cpp:10:</span><br><span class="line">/hyunggi/c++/directory/sample_code.hpp: In member function ‘virtual bool someClass::someNamespace::someFunction(const std::string&amp;)’:</span><br><span class="line">/hyunggi/c++/directory/sample_code.hpp:30:53: error: unused parameter ‘slam_is_awesome’ [-Werror=unused-parameter]</span><br><span class="line">   30 |   virtual bool someFunction(const std::string&amp; slam_is_awesome)</span><br><span class="line">      |                                  ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~</span><br><span class="line">cc1plus: all warnings being treated as errors</span><br></pre></td></tr></table></figure>

<p> </p>
<p>이 경고의 의미는 아래와 같습니다.</p>
<blockquote>
<p>함수에서 쓰겠다고 인자를 받았는데, 왜 안써? 너 이거 쓰는거 까먹은거 아냐?? 변수명 잘못쓴거 아냐??? 아니 <strong>그럴꺼면 인자로 왜 받은거야????</strong></p>
</blockquote>
<p> </p>
<p>인자로 받은건 꼭 써줘야합니다. 그게 안전한 프로그램입니다.</p>
<p>하지만 레거시 코드를 리팩토링 할 때는 막 함수 몇백개에서 이러한 경고가 나타나는 경우가 있습니다.</p>
<p>함수 하나씩 리팩토링을 하며 해결해나가고 싶은데, <code>-Werror</code> 플래그 때문에 빌드 자체가 안되기도 합니다.</p>
<p><code>-Werror</code>를 끄고 싶은 마음도 들지만, 그러면 수많은 warning이 있을 때 진짜 크리티컬한 warning을 못보게 되지 않을까 걱정이 됩니다.</p>
<p>이번 글에서는 이런 <strong>unused parameter warning을 잠시동안 해제하는 방법</strong>에 대해 알아봅니다.</p>
<p> </p>
<h2 id="TLDR"><a href="#TLDR" class="headerlink" title="TLDR;"></a>TLDR;</h2><ul>
<li>C++17 이상이라면 <strong><code>[[maybe_unused]]</code></strong> 씁시다</li>
<li>C++17보다 낮은 스탠다드를 쓰지만 Boost 프레임워크를 쓴다면 **<code>boost::ignore_unused()</code>**를 씁시다</li>
<li><strong>왠만하면 처음부터 잘 짭시다</strong></li>
<li>여의치 않는다면 <code>std::ignore</code>를 써서 표시해두고, 빠르게 리팩토링해서 경고가 안나게 합시다</li>
<li><strong>경고는 절대 끄지 맙시다</strong> =.=</li>
</ul>
<p> </p>
<hr>
<h2 id="쓰면-안되는-방법들"><a href="#쓰면-안되는-방법들" class="headerlink" title="쓰면 안되는 방법들"></a>쓰면 안되는 방법들</h2><p>우선 <strong>쓰지 말아야 할 방법</strong>들에 대해 먼저 알아봅시다.</p>
<p> </p>
<h3 id="Wno-unused-parameter-플래그-쓰지마세요"><a href="#Wno-unused-parameter-플래그-쓰지마세요" class="headerlink" title="-Wno-unused-parameter 플래그 쓰지마세요"></a><code>-Wno-unused-parameter</code> 플래그 쓰지마세요</h3><p>컴파일러 경고 자체를 꺼버리는 방법입니다.</p>
<p>감히 제가 뭔데 컴파일러를 무시할까요… 컴파일러 경고는 끄는게 아닙니다.</p>
<p>절대 쓰지 맙시다.</p>
<p> </p>
<h3 id="void-cast-하지마세요"><a href="#void-cast-하지마세요" class="headerlink" title="void cast 하지마세요"></a>void cast 하지마세요</h3><p>아래 방법과 같은 방식으로, void로 사용하지 않은 변수를 캐스팅하는 겁니다.</p>
<p>void casting은 전혀 아무런 의미도 가지지 않지만, 적어도 변수에 어떠한 액션을 취했기 때문에 컴파일러 경고가 사라지게 됩니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">T <span class="title">foo</span><span class="params">(T var1, T var2, T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    (<span class="keyword">void</span>)(unused_parameter); <span class="comment">// 또는 static_cast&lt;void&gt;(unused_parameter);</span></span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>근데 아니 아무 죄도 없는 변수는 왜 공허로 보냅니까?</p>
<p><strong>void casting의 목적은 컴파일러 경고를 끄기 위함이 아닙니다.</strong></p>
<p>그렇다고 주석으로 ‘컴파일러 경고를 끄기 위해 사용함’ 이라고 적는건 더 이상합니다.</p>
<p>이럴거면 차라리 함수를 제대로 리팩토링 해줍시다.</p>
<p> </p>
<h3 id="ignore-함수나-매크로-지정-하지마세요"><a href="#ignore-함수나-매크로-지정-하지마세요" class="headerlink" title="ignore 함수나 매크로 지정 하지마세요"></a>ignore 함수나 매크로 지정 하지마세요</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> UNUSED(x) (void)(x)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ignore_unused</span><span class="params">(T &amp;&amp;)</span></span></span><br><span class="line"><span class="function"></span>{ }</span><br><span class="line"></span><br><span class="line"><span class="function">T <span class="title">foo</span><span class="params">(T var1, T var2, T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    UNUSED(unused_parameter); <span class="comment">// 매크로 방법</span></span><br><span class="line">    ignore_unused(unused_parameter); <span class="comment">// 함수 방법</span></span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>void cast 방식보다는 낫지만, C++ 에서 이런 매크로 사용하지 맙시다.</p>
<p>매크로는 C++ 코드가 C 코드 처럼 보이게 만듭니다.</p>
<p>함수로 만들면 그래도 가독성은 좀더 좋아지지만… 똑같습니다.</p>
<p> </p>
<hr>
<h2 id="적당히-써줄만한-방법들"><a href="#적당히-써줄만한-방법들" class="headerlink" title="적당히 써줄만한 방법들"></a>적당히 써줄만한 방법들</h2><p>그래도 봐줄만한 방법들을 소개합니다.</p>
<p> </p>
<h3 id="attribute-unused"><a href="#attribute-unused" class="headerlink" title="attribute((unused))"></a><strong>attribute</strong>((unused))</h3><p><strong>gcc/clang에서 정의한 ‘사용하지 않는 인자’ attribute를 사용</strong>하는 방법입니다.</p>
<p>gcc와 clang에서 사용하는건 저희도 써도 괜찮지 않을까 생각합니다.</p>
<p>단점이라면 함수 인자 리스트가 좀 길어집니다.</p>
<p>하지만 적당히 정상적으로 코드를 짰다면 (i.e. unused parameter가 2개 이상이 아니라면), 함수 인자 리스트가 그렇게 길어질리가 없습니다.</p>
<p>Unused parameter가 2개 이상이라면, 그냥 함수 자체를 다시 짜시죠…</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">T <span class="title">foo</span><span class="params">(T var1, T var2, __attribute__((unused)) T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h3 id="변수-주석처리"><a href="#변수-주석처리" class="headerlink" title="변수 주석처리"></a>변수 주석처리</h3><p>좀 애매한 방법이지만 작동하는 방법입니다.</p>
<p>하지만 IDE에서 검색해서 리팩토링하기 어렵기 때문에 추천하지 않습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">T <span class="title">foo</span><span class="params">(T var1, T var2, T <span class="comment">/*unused_parameter*/</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h3 id="std-ignore"><a href="#std-ignore" class="headerlink" title="std::ignore"></a>std::ignore</h3><p><code>std::ignore</code>는 원래 <span class="exturl" data-url="aHR0cHM6Ly9lbi5jcHByZWZlcmVuY2UuY29tL3cvY3BwL3V0aWxpdHkvdHVwbGU=">std::tuple<i class="fa fa-external-link-alt"></i></span>을 쓸 때 특정 변수를 무시하도록 만든 헬퍼 기능입니다.</p>
<p>근데 그냥 변수에도 쓸 수 있습니다.</p>
<p>그냥 변수에서 쓰라고 만든 기능은 아니지만, <code>std::ignore</code>라는 이름답게 <strong>개발자들이 읽을 때 ‘아 이 변수는 무시하라고 만든거구나’ 라고 이해할 수 있습니다</strong>. </p>
<p>이런 면에서는 void cast보다 훨씬 좋다고 생각합니다.</p>
<p>나중에 리팩토링할 때 <code>std::ignore</code>를 찾아서 고칠수도 있습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">T <span class="title">foo</span> <span class="params">(T var1, T var2, T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::ignore = unused_parameter;</span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="추천하는-방법들"><a href="#추천하는-방법들" class="headerlink" title="추천하는 방법들"></a>추천하는 방법들</h2><p>제일 좋은 방법들이라고 생각합니다.</p>
<p> </p>
<h3 id="C-17의-maybe-unused-attribute"><a href="#C-17의-maybe-unused-attribute" class="headerlink" title="C++17의 [[maybe_unused]] attribute"></a>C++17의 [[maybe_unused]] attribute</h3><p><strong>‘이 변수는 아마 안쓸수도?’</strong></p>
<p><strong>제일 좋은 방법</strong>입니다.</p>
<p>컴파일러도, 나도, 내 동료들도, 모두가 바로 이해하는 방법입니다.</p>
<p>단점이라면 C++17을 요구합니다. 현재 자율주행 쪽 C++ 프로그래밍 규제로 C++11에서 작업하는데, 이거 못써서 미칠 노릇입니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">T <span class="title">foo</span> <span class="params">(T var1, T var2, [[maybe_unused]] T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h3 id="boost-ignore-unused"><a href="#boost-ignore-unused" class="headerlink" title="boost::ignore_unused()"></a>boost::ignore_unused()</h3><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTA2LWJvb3N0LXZzLWFic2VpbC8/aGlnaGxpZ2h0PWJvb3N0">Boost 프레임워크<i class="fa fa-external-link-alt"></i></span>를 쓴다면 고려해볼만한 방법입니다.</p>
<p>C++11등에서 작업하는데 C++17/20의 기능을 불러오기 위해 Boost 프레임워크를 쓰는 경우가 있습니다.</p>
<p><code>boost::ignore_unused()</code>는 boost의 내장함수이며, <strong>좋은 함수 이름으로 unused parameter를 무시</strong>할 수 있게 해줍니다.</p>
<p>이거도 나중에 <code>boost::ignore_unused</code>로 찾아서 리팩토링 해주기 좋습니다.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"boost/core/ignore_unused.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">T <span class="title">foo</span> <span class="params">(T var1, T var2, T unused_parameter)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    boost::ignore_unused(unused_paramter);</span><br><span class="line">    <span class="keyword">return</span> var1 + var2;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Open3D 빌드했는데 라이브러리가 너무 크네 어쩌지</title>
    <url>/20220121-open3d-build-story/</url>
    <content><![CDATA[<img src="/20220121-open3d-build-story/intro.png" class="" title="intro">

<p> </p>
<h2 id="TLDR"><a href="#TLDR" class="headerlink" title="TLDR;"></a>TLDR;</h2><ul>
<li>Open3D를 그냥 빌드했더니 1.2GB로 엄청나게 컸다</li>
<li>시스템에 BLAS를 설치하고, shared library로 설치해서 106MB로 다이어트 성공했다.</li>
</ul>
<p> </p>
<hr>
<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lzbC1vcmcvT3BlbjNE">Open3D<i class="fa fa-external-link-alt"></i></span>는 Intel에서 만든 3D 데이터 프로세싱 라이브러리이다.</p>
<p>개인적으로 느끼기에는 PCL + Pangolin의 상위 호환 정도로 생각하고 있었다. 듣기로는 포인트 클라우드 프로세싱은 좋은 알고리즘 + SIMD 적용이 되어있어서 빠르다고 했고, Viewing 쪽은 Filament 백엔드를 통해 이쁘게 렌더링 할 수 있을 뿐만이 아니라 WebRTC를 적용해서 웹 브라우저에서도 원격으로 볼 수 있다는 점이 끌렸다. 특히, 임베디드 보드에서 아까운 디스플레이 렌더링 자원을 쓰지 않고, 원격통신을 통해 렌더링을 하면 훨씬 편하게 구현을 할 수있지 않을까란 기대가 컸다.</p>
<p>그래서 일단 받아서 빌드를 해봤다.</p>
<p> </p>
<hr>
<h2 id="첫번째-시도-Naive-빌드"><a href="#첫번째-시도-Naive-빌드" class="headerlink" title="첫번째 시도: Naive 빌드"></a>첫번째 시도: Naive 빌드</h2><p>GitHub에서 소스코드를 땡겨왔다.</p>
<p>기본적인 CMake 빌드를 돌리면 되겠지~ 하고 아래 커맨드를 입력했다. 필자는 시스템 설치보다는 로컬 설치를 좋아해서 추가 옵션을 사용했다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir open3d &amp;&amp; <span class="built_in">cd</span> open3d</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/isl-org/Open3D.git .</span><br><span class="line">mkdir build</span><br><span class="line">mkdir install</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line"></span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../install ..</span><br><span class="line">make -j</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>일단 cmake configure할 때 <code>-GNinja</code> 옵션을 통해  Ninja 빌드도 돌려보려고 했는데, ninja는 의외로 작동하지 않았다. 그래도 make로는 잘 돌아가니 얼마나 다행인가.</p>
<p>빌드를 하며 깜짝 놀랐는데, 필요한 3rdParty 라이브러리들을 다 땡겨와서 소스빌드를 하고있다는 점이였다. CMake 스크립트를 어떻게 짰는지는 모르겠는데 굉장히 엘레강스 해보였다. 대신, 빌드하는데에 걸리는 전체 시간을 예상할 수 없는 점은 조금 아쉬웠다.</p>
<p>약 10분 정도 시간이 지난 후 빌드가 완료되었다.</p>
<p>그러고 내용을 봤는데 대충격!</p>
<p><strong>빌드 된 라이브러리들의 총 용량이 1.2GB</strong> 였다.</p>
<p>아니 세상에 좀 이쁜 뷰어 쓰겠다고 1.2GB 라이브러리를 쓰는건 말이 안되잖아요</p>
<p>대체 어떤 라이브러리가 용량을 많이 차지하는지 분석을 해봤더니, <strong>BLAS와 MKL을 통합시킨 라이브러리가 약 700MB</strong> 정도 차지했다.</p>
<p>내가 아는 BLAS는 이정도까지 크지 않은데… MKL이 좀 많이 무겁긴 하지.</p>
<p>근데 내가 왜 굳이 MKL을 빌드해야하지? MKL은 Intel 쟁이들의 물건이 아닌가?? 하씨 어쩐지 Intel이 만든 라이브러리더라,, 근데 난 ARM CPU 쓸건데??</p>
<p>그래서 다음 단계로 나는 이 라이브러리를 빌드하지 않을 방법을 찾기 시작했다.</p>
<p> </p>
<hr>
<h2 id="두번째-시도-BLAS-라이브러리를-교체해서-빌드"><a href="#두번째-시도-BLAS-라이브러리를-교체해서-빌드" class="headerlink" title="두번째 시도: BLAS 라이브러리를 교체해서 빌드"></a>두번째 시도: BLAS 라이브러리를 교체해서 빌드</h2><p>Open3D CMakeLists.txt를 보니 <code>USE_BLAS</code>라는 옵션이 보였다.</p>
<p><strong>MKL 대신 BLAS를 사용</strong>한다? 그러면 MKL을 다운받을 필요가 없을테니, 700MB나 되는 라이브러리가 더 작아질 것이다.</p>
<img src="/20220121-open3d-build-story/two.png" class="" title="two">

<p>그래서 다음과 같은 커맨드로 빌드를 해봤다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir open3d &amp;&amp; <span class="built_in">cd</span> open3d</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/isl-org/Open3D.git .</span><br><span class="line">mkdir build</span><br><span class="line">mkdir install</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line"></span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../install -DUSE_BLAS=ON ..</span><br><span class="line">make -j</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>또 빌드가 10분정도 돌았는데, 의외로 빌드가 중간에 터졌다.</p>
<p>나타는 에러는 다음과 같았다 - <code>libopenblas.a: undefined reference to '_gfortran_concat_string'</code></p>
<img src="/20220121-open3d-build-story/three.png" class="" title="three">

<p>gfortran???? GNU Fortran 컴파일러가 여기서 왜 나와???</p>
<p>추정하기로는, Open3D에서 <code>USE_BLAS</code> 옵션을 키면 선형대수 계산 가속을 OpenBLAS 라이브러리를 통해서 하는데, 이 라이브러리를 빌드할 때 gfortran 링크가 안된 것이다.</p>
<p>그러면 OpenBLAS를 다시 깔아야한다.</p>
<p><code>sudo apt-get install libopenblas-dev</code>로 다시 설치를 해봤다.</p>
<p>하지만 에러는 계속 나타났다. 음? 그러면 시스템에 있는 openblas가 잘못된건가? 그럴리는 없을 거 같다.</p>
<p>그렇다면 아마 openblas를 로컬에 설치하고 있지 않을까해서 build 폴더를 들어가봤는데, 역시나 OpenBLAS를 직접 설치하고 있었다.</p>
<p>OpenBLAS가 설치가 안될 이유가 뭐가 있을까… gfortran을 다시 깔아봤는데도 동일한 에러가 나타났다.</p>
<p>일단 첫날은 여기까지 하고, Open3D GitHub에 issue를 남겼다.</p>
<p>“컴파일 된 라이브러리 크기를 줄일 방법이 없나요?”</p>
<img src="/20220121-open3d-build-story/one.png" class="" title="one">

<p>답변은 생각보다 굉장히 빠르게 왔다. <code>USE_BLAS</code> 옵션을 줘서 빌드를 해보면 된다고 한다.</p>
<p>원작자는 제대로 빌드하면 100MB 정도로 나온다고 한다.</p>
<p>근데 그거 내가 하고있는거잖아!</p>
<p> </p>
<h2 id="세번째-시도-OpenBLAS-제대로-고쳐서-빌드"><a href="#세번째-시도-OpenBLAS-제대로-고쳐서-빌드" class="headerlink" title="세번째 시도: OpenBLAS 제대로 고쳐서 빌드"></a>세번째 시도: OpenBLAS 제대로 고쳐서 빌드</h2><p>일단 제대로 빌드하면 100MB 로 나온다는걸 보니, 분명 이 방법이 맞기는 한 것 같다.</p>
<p>하지만 난 빌드가 안되는걸 보니 분명 내 OpenBLAS에 문제가 있다는거겠지.</p>
<p>OpenBLAS를 삭제하고, 이걸 빌드할 때 쓰일만한 라이브러리들을 다시 깔아보았다.</p>
<p>BLAS… ATLAS… LAPACK…</p>
<p>응? LAPACK이 다시 깔렸다. 나 LAPACK 안깔아놨나?</p>
<p>Open3D를 다시 빌드해봤고, 빌드는 성공했다.</p>
<p><strong>아마 LAPACK을 통해 OpenBLAS를 설치하면 gfortran 링크 문제가 없는게 아닐까</strong> 생각이 든다.</p>
<p>빌드의 결과물은 <strong>약 400MB</strong> 였고, 정확히 1.2GB에서 700MB정도 되는 MKL 라이브러리가 빌드되지 않아 남은것만 빌드가 된거였다.</p>
<p>그래도 400MB는 꽤 큰 라이브러리였다. OpenCV가 50MB인걸 생각하면 말도 안되는 크기인거 같다.</p>
<p>혹시 OpenBLAS를 소스 빌드했나? 하고 확인해봤더니 역시 OpenBLAS를 소스코드를 땡겨와서 빌드한 것 같았다.</p>
<p>OpenBLAS는 시스템에 깔린걸 쓰면 좀 덜 무겁지 않나 생각이 들었다.</p>
<p>찾아봤더니 <code>BUILD_BLAS_FROM_SOURCE</code>라는 빌드 옵션이 있는거 같은데, 이걸 OFF하면 사이즈가 줄지 않을까 생각한다.</p>
<p>Open3D의 원작자가 코멘트를 남겨줬는데, <strong>shared library 형태로 빌드를 하면 사이즈가 많이 작아진다고 한다</strong>.</p>
<p>Shared library가 뭔지 정확하게 몰라서 조금 공부를 하고 노트를 남겼다. (<span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTIwLXN0YXRpYy12cy1zaGFyZWQtbGlicy8=">Static library vs Shared library 정리 글<i class="fa fa-external-link-alt"></i></span>).</p>
<p>그리고 shared library 형태로 빌드를 해봤다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir open3d &amp;&amp; <span class="built_in">cd</span> open3d</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/isl-org/Open3D.git .</span><br><span class="line">mkdir build</span><br><span class="line">mkdir install</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line"></span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../install -DUSE_BLAS=ON -DBUILD_SHARED_LIBS=ON -DBUILD_BLAS_FROM_SOURCE=OFF ..</span><br><span class="line">make -j</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>그래서 나온 결과물은 단 하나의 <code>libOpen3D.so</code> 파일. <strong>크기는 106 MB</strong>이다. 홀리몰리과카몰리~~~</p>
<img src="/20220121-open3d-build-story/five.png" class="" title="five">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>Visual-SLAM</tag>
        <tag>Intel</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Open3D</tag>
      </tags>
  </entry>
  <entry>
    <title>인생 첫 맥북! 맥북프로 M1</title>
    <url>/20220123-new-mac/</url>
    <content><![CDATA[<h2 id="사진"><a href="#사진" class="headerlink" title="사진"></a>사진</h2><img src="/20220123-new-mac/mac.jpg" class="" title="mac">

<p> </p>
<hr>
<h2 id="언박싱"><a href="#언박싱" class="headerlink" title="언박싱"></a>언박싱</h2><img src="/20220123-new-mac/1.gif" class="" title="gif1">

<img src="/20220123-new-mac/2.gif" class="" title="gif2">

<p> </p>
<hr>
<h2 id="M1-맥북을-산-이유"><a href="#M1-맥북을-산-이유" class="headerlink" title="M1 맥북을 산 이유"></a>M1 맥북을 산 이유</h2><blockquote>
<p>TLDR: ARM CPU, iOS 프로그래밍, 영상편집, 음악.</p>
</blockquote>
<p>여러가지 종합적인 이유로 보았을 때, 맥북은 하나의 디바이스로 수많은 커리어의 가능성을 열어줄 것 같아서 구매하게 되었다.</p>
<h3 id="ARM-CPU"><a href="#ARM-CPU" class="headerlink" title="ARM CPU"></a>ARM CPU</h3><p>SLAM은 모바일 로봇에서 나온 기술이다.</p>
<p>아카데믹한 연구는 많이들 x86 PC에서 하지만, 실제로 SLAM이 적용되는 제품들은 왠만하면 다 ARM CPU를 탑재한 보드에서 작동한다.</p>
<p>로봇에 탑재되는 보드도, 엔비디아에서 만든 edge 보드도, 스마트폰도 모두 ARM CPU를 사용한다.</p>
<p>그나마 x86이 대세로 적용되는 SLAM 분야라면 워크스테이션이나 서버에서 작동하는 3D reconstruction일텐데, 종종 나오는 얘기들을 들어보면 전력효율이 좋은 서버용 ARM CPU가 나오면 이거도 다 뒤집힐 수 있다는 이야기가 있다.</p>
<p>이렇게 봤을 때 ARM CPU에서 코딩을 하는 것을 공부하는 것은 크게 잃을 게 없는 선택이라고 생각된다.</p>
<h3 id="iOS-프로그래밍"><a href="#iOS-프로그래밍" class="headerlink" title="iOS 프로그래밍"></a>iOS 프로그래밍</h3><p>내가 지금까지 만든 영상처리 / SLAM 알고리즘들은 모두 회사가 가지고 있다.</p>
<p>물론 이렇게해서 월급도 잘 받고 좋지만, 회사에서는 내 코드를 정확하게 판단해줄 수 없다.</p>
<p>그냥 ‘우리 직원이 짠 현재 상태의 코드다’ 정도로 생각하지…</p>
<p>그리고 내 코드의 좋고 나쁨에 따라 비즈니스가 결정되는게 아니라 수많은 다른 요인들로 인해 결정되는 것이기 때문에, 정말 내 코드가 좋은건지 아닌건지 판단을 하기가 어렵다고 생각한다.</p>
<p>가장 정확한 판단은 오픈소스로 내놓았을 때 사람들이 써보고 평가를 했을 때 얻을 수 있는게 아닐까?</p>
<p>나도 뭔가 만들어서 직접 팔아보고 싶다.</p>
<p>오픈소스를 해보고싶다.</p>
<h3 id="영상-편집"><a href="#영상-편집" class="headerlink" title="영상 편집"></a>영상 편집</h3><p>유투브랑 강의를 하고싶다.</p>
<p>돈 벌고 싶다.</p>
<h3 id="음악"><a href="#음악" class="headerlink" title="음악"></a>음악</h3><p>이건 그냥 취미.</p>
<p> </p>
<hr>
<h2 id="스펙"><a href="#스펙" class="headerlink" title="스펙"></a>스펙</h2><p>2021년형 맥북프로</p>
<ul>
<li>CPU: M1 Max CPU</li>
<li>RAM: 64GB</li>
<li>GPU: 32코어 GPU</li>
<li>SSD: 4GB</li>
</ul>
<p>애플케어 포함 600정도 들었다.</p>
<p>아마 오버스펙인거 같은데, 일단 써보고 너무 오버스펙이면 다운그레이드 할 예정.</p>
<p>괜찮은 거 같으면 최소 5년은 써야지.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>3. Life</category>
        <category>2.1 Software Engineering</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>Apple</tag>
        <tag>Macbook Pro</tag>
        <tag>M1</tag>
        <tag>ARM</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 1 - Professionalism 좋은 문장들</title>
    <url>/20220124-clean-coders-professionalism/</url>
    <content><![CDATA[<h1 id="프로는-믿음직스러워야한다"><a href="#프로는-믿음직스러워야한다" class="headerlink" title="프로는 믿음직스러워야한다"></a>프로는 믿음직스러워야한다</h1><p>“이 사람은 진짜 프로야”, 라고 표현되는 사람은 어떤 특징을 가지고 있을까?</p>
<p>‘프로’라는건 아무래도 신뢰의 상징인 것 같다.</p>
<p>내가 돈을 내서라도 이 사람의 서비스를 받고싶다, 그만큼 이 사람의 서비스는 좋고 안정적이여야한다는 것이다.</p>
<p>하지만 아무래도 소프트웨어는 수정하기가 쉽다보니, 비-프로의 세계에서는 이 ‘신뢰’가 상당히 가볍게 여겨지는 것 같다.</p>
<p>책에서는 프로의 신뢰를 의사에 비교할 정도로 중요한 가치라고 이야기한다.</p>
<p>프로그래머가 <strong>‘내가 이 프로그램을 X월 XX일까지, 무조건적으로 네가 원하는대로 완벽하게 만들어줄게’</strong> 라고 말할 수 있고 또 그걸 지킬 수 있다는 것이, 바로 <strong>비-프로와 프로를 가르는 경계</strong>이지 않을까 생각이 든다.</p>
<p> </p>
<blockquote>
<p>“맞아요, 소프트웨어를 버그 없이 만드는건 불가능할정도로 복잡한 일입니다.<br>하지만 그렇다고 당신이 버그가 있는 소프트웨어를 만들어도 된다는 면죄부를 받는건 아니에요.<br>사람의 몸도 전부 이해하는건 불가능하지만, 의사들은 환자를 무조건 좋은 방향으로 치료하기 위해 선서를 하잖아요?”</p>
</blockquote>
<blockquote>
<p>“프로가 되기 위해서 첫번째로 할 일은 ‘사과하는걸 연습하는 것’이에요.<br>무언가를 잘못했을 때 우리는 사과를 하지만, 사과 하나로 모든게 다 해결되지는 않잖아요.<br>같은 실수를 계속 반복하면 안될거잖아요.<br>당신이 프로가 되면서 이러한 ‘사과하는 행동’은 0으로 수렴해야해요.”</p>
</blockquote>
<img src="/20220124-clean-coders-professionalism/1.gif" class="" title="trust_me"> 

<p> </p>
<hr>
<h1 id="내-버그는-내가-제일-잘-알아야한다"><a href="#내-버그는-내가-제일-잘-알아야한다" class="headerlink" title="내 버그는 내가 제일 잘 알아야한다"></a>내 버그는 내가 제일 잘 알아야한다</h1><p><strong>프로그래머에 대한 신뢰</strong>는 ‘다 된다고 내놨는데 버그가 나올 때’ 깨지게 된다.</p>
<p>아무 베이스도 없으면서 ‘아 내가 코드 좀 잘 짜는데요, 다 잘 될겁니다’ 라는 느낌으로 다 된다고 이야기한거면, 그건 그냥 (당연하게도) 100% 오만이지 않을까 생각한다.</p>
<p>‘내 코드는 A라는 시나리오에서 잘 작동합니다. B 라는 시나리오에서는 작동하지 못합니다’ 라고 <strong>명확하게 기능과 한계점을 찝어줄 수 있어야 한다</strong>.</p>
<p>그리고 작동/비작동의 여부를 알기 위해서는 코드를 무조건 테스트 목적을 가지고 돌려봐야한다.</p>
<p> </p>
<blockquote>
<p>“당신의 코드가 작동하는건지 어떻게 알까요? 정답은 간단합니다. 테스트를 하세요. 또 테스트 하세요. 위 아래로 테스트하고, 월화수목금토일 테스트하세요.”</p>
</blockquote>
<blockquote>
<p>“하루 종일 테스트만 하고 있다면 코드를 짤 시간이 없겠죠? 맞는 말씀이십니다. 그러니까 테스트를 자동화하세요!”</p>
</blockquote>
<blockquote>
<p>“착각하실까봐 말씀드리는건데, 코드 테스팅은 ‘추천하는 행동’이 아니에요.<br>이건 필수에요.<br>당신이 짠 코드는 한줄한줄 전부 다 책임지고 테스트 하세요.”</p>
</blockquote>
<blockquote>
<p>“테스트 코드 작성이 어렵다구요?<br>그건 테스트 하기 어렵게 당신이 짰기 때문이에요.”</p>
</blockquote>
<p> </p>
<hr>
<h1 id="유연한-코드를-가지기-위해서는"><a href="#유연한-코드를-가지기-위해서는" class="headerlink" title="유연한 코드를 가지기 위해서는"></a>유연한 코드를 가지기 위해서는</h1><p>산업공학을 전공하면서 배운 것 중 하나는, ‘시작 단계의 고객 요구사항은 항상 불완전하며, 끝 단계의 고객 요구사항은 이미 과거의 것이다’ 라는 것이다.</p>
<p>즉, 고객 요구사항은 계속 바뀌게 된다 (제조업이 그래서 어려웠다 ㅜㅜ)</p>
<p>소프트웨어는 다행히도 코드만 바꾸면 고객 요구사항에 맞춰서 금방 따라갈 수 있다.</p>
<p>하지만 유연하지 못한 코드를 짜게 된다면, 하드웨어 생산보다도 더 경직된 유연성을 가지게 된다.</p>
<p>고객 만족도를 최상을 높이는 ‘프로’가 되기 위해서는, 무조건 유연한 코드를 만들어야한다.</p>
<p>책에서는 유연한 코드륾 만들기 위해서는 1. <strong>좋은 아키텍처</strong>, 2. <strong>코드를 항상 깔끔하게 유지하려는 마음</strong>, 3. <strong>테스트</strong> 가 필요하다고 한다.</p>
<p> </p>
<blockquote>
<p>“당신의 코드가 유연하려면 우선 코드 구조가 유연해야합니다.<br>구조를 대충 짜는 행위는 당신의 미래를 희생하는 거에요.”</p>
</blockquote>
<blockquote>
<p>“무자비하게 리팩토링하세요 (merciless refactoring).<br>당신이 어떤 코드를 보기 전과 후를 비교했을 때, 그 결과물은 이전보다 더 깔끔해야합니다.”</p>
</blockquote>
<blockquote>
<p>“소프트웨어가 바뀌지 않고 있을 때가 가장 위험한겁니다”</p>
</blockquote>
<blockquote>
<p>“개발자들은 왜 소프트웨어를 바꾸는걸 무서워할까요?<br>아마 본인들이 무언가를 잘못 수정해서 코드가 작동하지 않을까봐 일겁니다.<br>왜 코드가 작동하지 않을 것을 무서워할까요? 코드를 테스트되지 않았기 때문입니다.<br>코드를 테스트하면 코드가 작동할 것을 확신할 수 있습니다.”</p>
</blockquote>
<p> </p>
<hr>
<h1 id="프로의-마음가짐"><a href="#프로의-마음가짐" class="headerlink" title="프로의 마음가짐"></a>프로의 마음가짐</h1><p>소프트웨어 업계는 빠르게 변한다.</p>
<p>새로운 프로그래밍 언어나 프레임워크는 매년 소개가 되고 있고, 새로운 논문은 매일 몇개씩 쏟아져나온다.</p>
<p>구글 검색 몇번이면 새 지식을 쉽게 접할 수 있다보니 세상도 함께 빠르게 변한다.</p>
<p>2015년만해도 AI 개발자 포지션은 찾기 어려운 수준이였지만, 2018년에는 사람이 없어서 못 뽑았다.</p>
<p>2020년에 NeRF 논문이 공개되었고, 2022년 컴퓨터 비전 컨퍼런스에서는 어딜가나 NeRF 논문이 보인다.</p>
<p>개인 사정 때문에, 아이를 돌보기 위해, 이직을 준비하느라, 여행을 하고 싶어서 - 어떠한 이유에서든 1-2년만 쉬어도 순식간에 뒤쳐질 수 있다.</p>
<p>그리고 나 자신에게 질문을 해보자 - ‘뒤쳐진 개발자에게 돈을 주고 일을 맡기고 싶을까?’</p>
<p> </p>
<blockquote>
<p>“당신의 커리어는 당신이 책임져야합니다.<br>당신의 고용주가 당신을 트레이닝 시켜주고, 테크 컨퍼런스에 보내주고, 프로그래밍 책을 사줄 이유는 단 1도 없어요.<br>당신의 고용주가 이미 이런 것들을 제공해주고 있다면, 이런 부분들을 ‘당연히 고용주가 제공해야하는 것’이라고 <strong>착각</strong>하지 마십시오.”</p>
</blockquote>
<blockquote>
<p>“고용주는 당신의 실력이 좋아지는 걸 기다려줄 책임이 없습니다.”</p>
</blockquote>
<blockquote>
<p>“개발자라면 매주 60시간은 일 해야합니다.<br>40시간은 당신의 고용주를 위해, 20시간은 당신을 위해. 20시간은 약 하루에 3시간 정도입니다.<br>점심시간에 책을 읽어도 좋고, 출퇴근 시간에 팟캐스트를 들어도 좋고, 하루 2시간 새로운 언어를 배워도 좋습니다.”</p>
</blockquote>
<blockquote>
<p>“1주일은 168 시간입니다.<br>고용주에게 40시간과 자신을 위한 20시간을 빼면 108시간이 남습니다.<br>매일 8시간 수면을 취해 56시간을 빼면, 나머지 시간은 52시간이 남습니다.<br>충분히 가족과 시간을 보낼 수 있고, 개인 시간을 가질 수 있습니다”</p>
</blockquote>
<blockquote>
<p>“프로는 본인의 커리어에 시간과 노력을 쏟을 줄 알아야합니다”</p>
</blockquote>
<blockquote>
<p>“자신을 위한 20시간 동안에는 고용주를 위해 일하면 안됩니다.<br>당신 자신의 커리어를 위해 일해야합니다.”</p>
</blockquote>
<blockquote>
<p>“코딩을 그만둔 아키텍트들은 곧 업무에서 본인이 쓸모없음을 느낄겁니다.<br>새로운 프로그래밍 언어를 습득하지 않는 프로그래머는 곧 본인이 업계에서 밀려남을 느낄겁니다.<br>새로운 소프트웨어 방법론과 기술을 익히지 않는 개발자들은 곧 본인의 동료들이 승진할 때 본인의 가치는 내려감을 느낄 겁니다”</p>
</blockquote>
<blockquote>
<p>“의학 논문을 읽지 않는 의사에게 진단을 받고 싶으신가요?<br>최신 법률을 따라가지 못하는 변호사에게 상담을 받고싶나요?”</p>
</blockquote>
<p> </p>
<hr>
<h1 id="전문지식을-익히자"><a href="#전문지식을-익히자" class="headerlink" title="전문지식을 익히자"></a>전문지식을 익히자</h1><p>프로그래머에게 비용을 주며 개발을 맡기는 이유는 돈으로 프로그래머의 개발 능력과 개발 시간을 산다고 볼 수 있다.</p>
<p>여기서 프로그래머가 가진 지식의 수준이 얕다면, 50%는 빈 쭉정이인게 아닐까?</p>
<p>진짜 ‘프로’는 빈 쭉정이 같은 소리를 듣지 않는다.</p>
<p> </p>
<blockquote>
<p>“Nassi-Schneiderman 차트가 무엇인지 아십니까? 모른다면, 왜 모르나요?<br>Mealy state machine과 Moore state machine의 차이를 아시나요? 알아야합니다.<br>Quick sort를 인터넷 안 보고 바로 짤 수 있나요?<br>Transform analysis가 무엇인가요?<br>Data flow diagram에 기능 분해를 하실 수 있나요?<br>Tramp data란 무엇일까요? Conascene이라는 단어를 들어본적이 있나요?<br>파르나스 테이블이 무엇인가요?”</p>
</blockquote>
<blockquote>
<p>“지난 50년간 소프트웨어 분야에는 다양한 지식들이 축적되어 왔습니다.<br>그에 비해 현대 소프트웨어 기술은 굉장히 빠르게 발전하고 있습니다.<br>하지만 그렇다고해서 옛 것의 지식을 익히지 않을 이유가 되지 않습니다.<br>과거에 만들어진 대부분의 개념들은 현대에서 사용하는 개념들에 그대로 계승되었기 때문입니다”</p>
</blockquote>
<blockquote>
<p>“역사를 기억하지 못하는 자는, 같은 실수를 반복하기 마련입니다”</p>
</blockquote>
<blockquote>
<p>“제가 개인적으로 생각하는 ‘프로라면 갖춰야할 지식들’ 입니다”</p>
</blockquote>
<ul>
<li>디자인 패턴<ul>
<li>GOF 책에 있는 24개의 패턴을 전부 설명할 수 있음</li>
<li>POSA 책에 나오는 많은 패턴들을 설명할 수 있음.</li>
</ul>
</li>
<li>디자인 방침<ul>
<li>SOLID principle을 이해함</li>
</ul>
</li>
<li>개발 방법론<ul>
<li>XP, Scrum, Lean, Kanban, Waterfall, Structured analysis, Structured design</li>
</ul>
</li>
<li>소프트웨어 방법론<ul>
<li>TDD, Object-Oriented design, Structured programming, Continuous integration, Pair Programming</li>
</ul>
</li>
<li>산출물<ul>
<li>UML, DFD, Structure chart, Petri Nets, State Transition Diagrams, flow chart, decision table</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h3 id="Continuous-learning"><a href="#Continuous-learning" class="headerlink" title="Continuous learning"></a>Continuous learning</h3><p> </p>
<h3 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h3><p>“피아니스트, 바이올리니스트 들이 어떻게 아름다운 선율을 만들 수 있을까요? 수많은 연습으로써 만들어진 결과물입니다. 수많은 스케일과 에튀드를 통해 손가락을 단련시킵니다. 무대에 자주 올라가서가 아닙니다.”</p>
<p>“소프트웨어 개발자들도 연습을 해야합니다. 저는 이 연습을 kata라고 불러요. 저는 하루에 kata를 하나에서 두개 정도 합니다.”</p>
<p>“아침에 10분 kata를 통해 웜업을, 저녁에 10분 kata를 통해 쿨다운을 하시죠”</p>
<p> </p>
<h3 id="Mentoring"><a href="#Mentoring" class="headerlink" title="Mentoring"></a>Mentoring</h3><p>“가장 빠르게 공부하는 방법은 가르치는 겁니다. 당신이 책임져야하는 학생들에게 정보를 정확하게 전달하기 위해서는 누구보다 빠르고 정확하게 공부해야합니다. 그러니 가르치는 행위는, 사실 선생님에게 가장 큰 이득이 돌아가는 학습 방법입니다.”</p>
<p>“프로들이 주니어들을 멘토링을 책임져야합니다”</p>
<p> </p>
<h3 id="Know-your-domain"><a href="#Know-your-domain" class="headerlink" title="Know your domain"></a>Know your domain</h3><p>“개발자는 본인이 프로그래밍하고 있는 분야에 대한 이해도가 있어야합니다.”</p>
<p>“도메인 전문가가 되라는건 아닙니다. 하지만 적어도 프로젝트를 시작할 때, 해당 분야에 대한 책 한두권 정도는 읽으세요”</p>
<p>“가장 안 좋은건, 비즈니스가 어떻게 굴러가는지 단 1도 이해하지 못한 채 코드만 치는겁니다”.</p>
<p> </p>
<h3 id="Identify-with-your-employer-customer"><a href="#Identify-with-your-employer-customer" class="headerlink" title="Identify with your employer / customer"></a>Identify with your employer / customer</h3><p>“당신의 고용주의 문제가 곧 당신의 문제입니다”</p>
<p>“절대 어떠한 상황에서도 ‘당신 vs 고용주’ 형태로 문제를 끌고가지 마세요. 프로들은 어떠한 대가를 치뤄서라도 이런 구도를 피합니다.”</p>
<p> </p>
<h3 id="Humility"><a href="#Humility" class="headerlink" title="Humility"></a>Humility</h3><p>“연차가 쌓이고 실력이 쌓이면서, 어느정도 리스크를 가지고 계산된 행동을 할 수 있습니다. 하지만 연차와 실력에 관계없이 누구나 실수는 합니다.”</p>
<p>“진짜 프로는 실수를 했을 때 가장 먼저 웃을 수 있는 사람입니다.”</p>
<p>“프로는 다른 사람이 실수 했을 때 비난하지 않습니다. 왜냐면 언제든지 자기 자신도 실수할 수 있기 때문입니다”</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>MacOS에 Rosetta &amp; Homebrew 설치</title>
    <url>/20220128-homebrew-install/</url>
    <content><![CDATA[<p>원글 링크: <span class="exturl" data-url="aHR0cHM6Ly9zdWJpY3VyYS5jb20vbWFjL2Rldi9hcHBsZS1zaWxpY29uLmh0bWwjYXBwbGUtc2lsaWNvbi1tMQ==">Subicura 블로그 - macOS 안내서<i class="fa fa-external-link-alt"></i></span></p>
<p>개발 일지 저장용 / 노트 용도로 작성했습니다.</p>
<h2 id="Rosetta-2-설치"><a href="#Rosetta-2-설치" class="headerlink" title="Rosetta 2 설치"></a>Rosetta 2 설치</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">softwareupdate --install-rosetta --agree-to-license</span><br></pre></td></tr></table></figure>

<p>Rosetta가 설치되고나면, x86용 Terminal 앱을 만들자.</p>
<p>우선 Terminal 앱을 검색해서 찾고, 클론을 만든다음 이름을 ‘Terminal (x86)’으로 바꾼다.</p>
<p>이 클론에 마우스 오른쪽 클릭 -&gt; <code>Get Info</code>를 선택하고 <code>Open using Rosetta</code>를 한다.</p>
<h2 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># install homebrew (arm64)</span></span><br><span class="line">arch -arm64 /bin/bash -c <span class="string">"<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)</span>"</span></span><br><span class="line"><span class="comment"># install homebrew (x86_64)</span></span><br><span class="line">arch -x86_64 /bin/bash -c <span class="string">"<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set homebrew env</span></span><br><span class="line">cat &lt;&lt;<span class="string">'EOF'</span> &gt;&gt; ~/.zprofile</span><br><span class="line">CPU=$(uname -m)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$CPU</span>"</span> == <span class="string">"arm64"</span> ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">eval</span> <span class="string">"<span class="subst">$(/opt/homebrew/bin/brew shellenv)</span>"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">export</span> PATH=/opt/homebrew/bin:<span class="variable">$PATH</span></span><br><span class="line">  <span class="built_in">eval</span> <span class="string">"<span class="subst">$(/usr/local/bin/brew shellenv)</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">alias</span> ibrew=<span class="string">"arch -x86_64 /usr/local/bin/brew"</span></span><br><span class="line"><span class="built_in">alias</span> abrew=<span class="string">"arch -arm64 /opt/homebrew/bin/brew"</span></span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Apple</tag>
        <tag>Macbook Pro</tag>
        <tag>M1</tag>
        <tag>ARM</tag>
        <tag>brew</tag>
      </tags>
  </entry>
  <entry>
    <title>M1 chip에서 OpenCV 빌드하기 (C++)</title>
    <url>/20220128-m1-opencv-build/</url>
    <content><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>M1 맥북을 샀으니 이제 개발이 해보고싶다.</p>
<p>OpenCV 부터 빌드를 해보자!</p>
<p> </p>
<h2 id="사전-단계"><a href="#사전-단계" class="headerlink" title="사전 단계"></a>사전 단계</h2><h3 id="Homebrew-설치"><a href="#Homebrew-설치" class="headerlink" title="Homebrew 설치"></a>Homebrew 설치</h3><p>MacOS에는 apt repository가 없으니, brew를 설치한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ /bin/bash -c <span class="string">"<span class="subst">$(curl -fsSL &lt;https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&gt;)</span>"</span></span><br></pre></td></tr></table></figure>

<h3 id="컴파일러-설치"><a href="#컴파일러-설치" class="headerlink" title="컴파일러 설치"></a>컴파일러 설치</h3><p>brew로 gcc를 깔아줘도 되곘지만, MacOS에서는 주로 이 방법을 쓴다고 해서 해봤다.</p>
<p>MacOS에서 주로 쓰는 개발툴인 XCode를 설치할 때 같이 딸려오는 컴파일러를 쓰는 것이다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xcode-select install</span><br></pre></td></tr></table></figure>

<p>이 후 <code>gcc --version</code> 을 치면 이렇게 나온다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ~/o/build ❯❯❯ gcc --version</span><br><span class="line">$ Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX12.1.sdk/usr/include/c++/4.2.1</span><br><span class="line">$ Apple clang version 13.0.0 (clang-1300.0.27.3)</span><br><span class="line">$ Target: arm64-apple-darwin21.2.0</span><br><span class="line">$ Thread model: posix</span><br><span class="line">$ InstalledDir: /Library/Developer/CommandLineTools/usr/bin</span><br></pre></td></tr></table></figure>

<p>??? 분명 gcc를 깔았는데 Apple clang이 나온다 ㅋㅋ</p>
<p>나중에 알고보니, Mac에서는 gcc 프론트엔드 + LLVM 백엔드를 써서 gcc 인터페이스를 지원한다고 한다. [링크]</p>
<p>그러면 결국 Apple clang이 맞는거겠지…</p>
<p> </p>
<hr>
<h2 id="OpenCV-설치"><a href="#OpenCV-설치" class="headerlink" title="OpenCV 설치"></a>OpenCV 설치</h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW5jdi9vcGVuY3Y=">OpenCV GitHub 링크<i class="fa fa-external-link-alt"></i></span>로 가서 최신 소스를 긁어온다. 쓱싹!</p>
<p>물론 이 방법보다는 Release 페이지로 들어가서 가장 최신 버전을 가져오는게 좋다.</p>
<p>현재 이 글을 작성하는 시기의 최선 버전은 4.5.5 이다.</p>
<p>아래 커맨드를 이용해서 코드를 땡겨온다. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir opencv</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/opencv/opencv.git .</span><br></pre></td></tr></table></figure>

<p>그리고 build/install 폴더를 설정한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">mkdir install</span><br></pre></td></tr></table></figure>

<p>그리고 빌드를 시작한다.</p>
<p>빌드 타입은 Release로, install의 위치는 시스템이 아닌 로컬폴더 (i.e. install 폴더)로, generator는 Ninja로 한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../install -GNinja ..</span><br></pre></td></tr></table></figure>

<p>위 커맨드를 실행했을 때 다음과 같은 로그가 나왔다.</p>
<p>기이이인 로그가 나오지만, 눈에 띄는 부분마다 짤라서 코멘트를 정리한다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- The CXX compiler identification is AppleClang 13.0.0.13000027</span><br><span class="line">-- The C compiler identification is AppleClang 13.0.0.13000027</span><br></pre></td></tr></table></figure>

<p>Apple Clang 컴파일러가 잡혔다.</p>
<p>M1 맥북에는 AppleClang13 - 제일 최신 버전이 자동으로 설치되나보다. 아주 굿!</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped</span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped</span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Detected processor: arm64</span><br><span class="line">-- Found PythonInterp: /usr/bin/python2.7 (found suitable version <span class="string">"2.7.18"</span>, minimum required is <span class="string">"2.7"</span>) </span><br><span class="line">-- Found PythonLibs: /usr/lib/libpython2.7.dylib (found suitable exact version <span class="string">"2.7.18"</span>) </span><br><span class="line">-- Found PythonInterp: /opt/homebrew/bin/python3 (found suitable version <span class="string">"3.9.10"</span>, minimum required is <span class="string">"3.2"</span>) </span><br><span class="line">-- Could NOT find PythonLibs (missing: PYTHON_LIBRARIES PYTHON_INCLUDE_DIRS) (Required is exact version <span class="string">"3.9.10"</span>)</span><br><span class="line">-- Looking <span class="keyword">for</span> ccache - not found</span><br><span class="line">-- Performing Test HAVE_CXX_FSIGNED_CHAR</span><br><span class="line">-- Performing Test HAVE_CXX_FSIGNED_CHAR - Success</span><br><span class="line">-- Performing Test HAVE_C_FSIGNED_CHAR</span><br><span class="line">-- Performing Test HAVE_C_FSIGNED_CHAR - Success</span><br><span class="line">-- Performing Test HAVE_CXX_W</span><br><span class="line">-- Performing Test HAVE_CXX_W - Success</span><br><span class="line">-- Performing Test HAVE_C_W</span><br><span class="line">-- Performing Test HAVE_C_W - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WALL</span><br><span class="line">-- Performing Test HAVE_CXX_WALL - Success</span><br><span class="line">-- Performing Test HAVE_C_WALL</span><br><span class="line">-- Performing Test HAVE_C_WALL - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_RETURN_TYPE</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_RETURN_TYPE - Success</span><br><span class="line">-- Performing Test HAVE_C_WERROR_RETURN_TYPE</span><br><span class="line">-- Performing Test HAVE_C_WERROR_RETURN_TYPE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_NON_VIRTUAL_DTOR</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_NON_VIRTUAL_DTOR - Success</span><br><span class="line">-- Performing Test HAVE_C_WERROR_NON_VIRTUAL_DTOR</span><br><span class="line">-- Performing Test HAVE_C_WERROR_NON_VIRTUAL_DTOR - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_ADDRESS</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_ADDRESS - Success</span><br><span class="line">-- Performing Test HAVE_C_WERROR_ADDRESS</span><br><span class="line">-- Performing Test HAVE_C_WERROR_ADDRESS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_SEQUENCE_POINT</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_SEQUENCE_POINT - Success</span><br><span class="line">-- Performing Test HAVE_C_WERROR_SEQUENCE_POINT</span><br><span class="line">-- Performing Test HAVE_C_WERROR_SEQUENCE_POINT - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WFORMAT</span><br><span class="line">-- Performing Test HAVE_CXX_WFORMAT - Success</span><br><span class="line">-- Performing Test HAVE_C_WFORMAT</span><br><span class="line">-- Performing Test HAVE_C_WFORMAT - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_FORMAT_SECURITY</span><br><span class="line">-- Performing Test HAVE_CXX_WERROR_FORMAT_SECURITY - Success</span><br><span class="line">-- Performing Test HAVE_C_WERROR_FORMAT_SECURITY</span><br><span class="line">-- Performing Test HAVE_C_WERROR_FORMAT_SECURITY - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WMISSING_DECLARATIONS</span><br><span class="line">-- Performing Test HAVE_CXX_WMISSING_DECLARATIONS - Success</span><br><span class="line">-- Performing Test HAVE_C_WMISSING_DECLARATIONS</span><br><span class="line">-- Performing Test HAVE_C_WMISSING_DECLARATIONS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WMISSING_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_CXX_WMISSING_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_C_WMISSING_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_C_WMISSING_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WSTRICT_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_CXX_WSTRICT_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_C_WSTRICT_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_C_WSTRICT_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WUNDEF</span><br><span class="line">-- Performing Test HAVE_CXX_WUNDEF - Success</span><br><span class="line">-- Performing Test HAVE_C_WUNDEF</span><br><span class="line">-- Performing Test HAVE_C_WUNDEF - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WINIT_SELF</span><br><span class="line">-- Performing Test HAVE_CXX_WINIT_SELF - Success</span><br><span class="line">-- Performing Test HAVE_C_WINIT_SELF</span><br><span class="line">-- Performing Test HAVE_C_WINIT_SELF - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WPOINTER_ARITH</span><br><span class="line">-- Performing Test HAVE_CXX_WPOINTER_ARITH - Success</span><br><span class="line">-- Performing Test HAVE_C_WPOINTER_ARITH</span><br><span class="line">-- Performing Test HAVE_C_WPOINTER_ARITH - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WSHADOW</span><br><span class="line">-- Performing Test HAVE_CXX_WSHADOW - Success</span><br><span class="line">-- Performing Test HAVE_C_WSHADOW</span><br><span class="line">-- Performing Test HAVE_C_WSHADOW - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WSIGN_PROMO</span><br><span class="line">-- Performing Test HAVE_CXX_WSIGN_PROMO - Success</span><br><span class="line">-- Performing Test HAVE_C_WSIGN_PROMO</span><br><span class="line">-- Performing Test HAVE_C_WSIGN_PROMO - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WUNINITIALIZED</span><br><span class="line">-- Performing Test HAVE_CXX_WUNINITIALIZED - Success</span><br><span class="line">-- Performing Test HAVE_C_WUNINITIALIZED</span><br><span class="line">-- Performing Test HAVE_C_WUNINITIALIZED - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DELETE_NON_VIRTUAL_DTOR</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DELETE_NON_VIRTUAL_DTOR - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_DELETE_NON_VIRTUAL_DTOR</span><br><span class="line">-- Performing Test HAVE_C_WNO_DELETE_NON_VIRTUAL_DTOR - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNNAMED_TYPE_TEMPLATE_ARGS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNNAMED_TYPE_TEMPLATE_ARGS - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNNAMED_TYPE_TEMPLATE_ARGS</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNNAMED_TYPE_TEMPLATE_ARGS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_COMMENT</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_COMMENT - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_COMMENT</span><br><span class="line">-- Performing Test HAVE_C_WNO_COMMENT - Success</span><br><span class="line">-- Performing Test HAVE_CXX_FDIAGNOSTICS_SHOW_OPTION</span><br><span class="line">-- Performing Test HAVE_CXX_FDIAGNOSTICS_SHOW_OPTION - Success</span><br><span class="line">-- Performing Test HAVE_C_FDIAGNOSTICS_SHOW_OPTION</span><br><span class="line">-- Performing Test HAVE_C_FDIAGNOSTICS_SHOW_OPTION - Success</span><br><span class="line">-- Performing Test HAVE_CXX_QUNUSED_ARGUMENTS</span><br><span class="line">-- Performing Test HAVE_CXX_QUNUSED_ARGUMENTS - Success</span><br><span class="line">-- Performing Test HAVE_C_QUNUSED_ARGUMENTS</span><br><span class="line">-- Performing Test HAVE_C_QUNUSED_ARGUMENTS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SEMICOLON_BEFORE_METHOD_BODY</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SEMICOLON_BEFORE_METHOD_BODY - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_SEMICOLON_BEFORE_METHOD_BODY</span><br><span class="line">-- Performing Test HAVE_C_WNO_SEMICOLON_BEFORE_METHOD_BODY - Success</span><br><span class="line">-- Performing Test HAVE_CXX_FFUNCTION_SECTIONS</span><br><span class="line">-- Performing Test HAVE_CXX_FFUNCTION_SECTIONS - Success</span><br><span class="line">-- Performing Test HAVE_C_FFUNCTION_SECTIONS</span><br><span class="line">-- Performing Test HAVE_C_FFUNCTION_SECTIONS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_FDATA_SECTIONS</span><br><span class="line">-- Performing Test HAVE_CXX_FDATA_SECTIONS - Success</span><br><span class="line">-- Performing Test HAVE_C_FDATA_SECTIONS</span><br><span class="line">-- Performing Test HAVE_C_FDATA_SECTIONS - Success</span><br><span class="line">-- Performing Test HAVE_CPU_NEON_SUPPORT (check file: cmake/checks/cpu_neon.cpp)</span><br><span class="line">-- Performing Test HAVE_CPU_NEON_SUPPORT - Success</span><br><span class="line">-- Performing Test HAVE_CPU_FP16_SUPPORT (check file: cmake/checks/cpu_fp16.cpp)</span><br><span class="line">-- Performing Test HAVE_CPU_FP16_SUPPORT - Success</span><br><span class="line">-- Performing Test HAVE_CPU_BASELINE_FLAGS</span><br><span class="line">-- Performing Test HAVE_CPU_BASELINE_FLAGS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_FVISIBILITY_HIDDEN</span><br><span class="line">-- Performing Test HAVE_CXX_FVISIBILITY_HIDDEN - Success</span><br><span class="line">-- Performing Test HAVE_C_FVISIBILITY_HIDDEN</span><br><span class="line">-- Performing Test HAVE_C_FVISIBILITY_HIDDEN - Success</span><br><span class="line">-- Performing Test HAVE_CXX_FVISIBILITY_INLINES_HIDDEN</span><br><span class="line">-- Performing Test HAVE_CXX_FVISIBILITY_INLINES_HIDDEN - Success</span><br><span class="line">-- Performing Test HAVE_C_FVISIBILITY_INLINES_HIDDEN</span><br><span class="line">-- Performing Test HAVE_C_FVISIBILITY_INLINES_HIDDEN - Success</span><br><span class="line">-- Performing Test HAVE_LINK_AS_NEEDED</span><br><span class="line">-- Performing Test HAVE_LINK_AS_NEEDED - Failed</span><br><span class="line">-- Looking <span class="keyword">for</span> posix_memalign</span><br><span class="line">-- Looking <span class="keyword">for</span> posix_memalign - found</span><br><span class="line">-- Looking <span class="keyword">for</span> malloc.h</span><br><span class="line">-- Looking <span class="keyword">for</span> malloc.h - not found</span><br><span class="line">-- Looking <span class="keyword">for</span> fseeko</span><br><span class="line">-- Looking <span class="keyword">for</span> fseeko - found</span><br><span class="line">-- Looking <span class="keyword">for</span> unistd.h</span><br><span class="line">-- Looking <span class="keyword">for</span> unistd.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/types.h</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/types.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> stdint.h</span><br><span class="line">-- Looking <span class="keyword">for</span> stdint.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> stddef.h</span><br><span class="line">-- Looking <span class="keyword">for</span> stddef.h - found</span><br><span class="line">-- Check size of off64_t</span><br><span class="line">-- Check size of off64_t - failed</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHORTEN_64_TO_32</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHORTEN_64_TO_32 - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_ATTRIBUTES</span><br><span class="line">-- Performing Test HAVE_C_WNO_ATTRIBUTES - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_STRICT_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_C_WNO_STRICT_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISSING_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISSING_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISSING_DECLARATIONS</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISSING_DECLARATIONS - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHIFT_NEGATIVE_VALUE</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHIFT_NEGATIVE_VALUE - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNDEF</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNDEF - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_IMPLICIT_FALLTHROUGH</span><br><span class="line">-- Performing Test HAVE_C_WNO_IMPLICIT_FALLTHROUGH - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_PARAMETER</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_PARAMETER - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_SIGN_COMPARE</span><br><span class="line">-- Performing Test HAVE_C_WNO_SIGN_COMPARE - Success</span><br><span class="line">-- libjpeg-turbo: VERSION = 2.1.2, BUILD = opencv-4.5.5-dev-libjpeg-turbo</span><br></pre></td></tr></table></figure>

<p>마지막 줄은 보아하니, turbo-jpeg로 자동으로 깔려있나보다. 야호!</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Check size of size_t</span><br><span class="line">-- Check size of size_t - <span class="keyword">done</span></span><br><span class="line">-- Check size of unsigned long</span><br><span class="line">-- Check size of unsigned long - <span class="keyword">done</span></span><br><span class="line">-- Performing Test HAVE_BUILTIN_CTZL</span><br><span class="line">-- Performing Test HAVE_BUILTIN_CTZL - Success</span><br><span class="line">-- Looking <span class="keyword">for</span> include file locale.h</span><br><span class="line">-- Looking <span class="keyword">for</span> include file locale.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> include file stdlib.h</span><br><span class="line">-- Looking <span class="keyword">for</span> include file stdlib.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> include file sys/types.h</span><br><span class="line">-- Looking <span class="keyword">for</span> include file sys/types.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> assert.h</span><br><span class="line">-- Looking <span class="keyword">for</span> assert.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> dlfcn.h</span><br><span class="line">-- Looking <span class="keyword">for</span> dlfcn.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> fcntl.h</span><br><span class="line">-- Looking <span class="keyword">for</span> fcntl.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> inttypes.h</span><br><span class="line">-- Looking <span class="keyword">for</span> inttypes.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> io.h</span><br><span class="line">-- Looking <span class="keyword">for</span> io.h - not found</span><br><span class="line">-- Looking <span class="keyword">for</span> limits.h</span><br><span class="line">-- Looking <span class="keyword">for</span> limits.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> memory.h</span><br><span class="line">-- Looking <span class="keyword">for</span> memory.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> search.h</span><br><span class="line">-- Looking <span class="keyword">for</span> search.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> string.h</span><br><span class="line">-- Looking <span class="keyword">for</span> string.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strings.h</span><br><span class="line">-- Looking <span class="keyword">for</span> strings.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/time.h</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/time.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> unistd.h</span><br><span class="line">-- Looking <span class="keyword">for</span> unistd.h - found</span><br><span class="line">-- Performing Test C_HAS_inline</span><br><span class="line">-- Performing Test C_HAS_inline - Success</span><br><span class="line">-- Check size of signed short</span><br><span class="line">-- Check size of signed short - <span class="keyword">done</span></span><br><span class="line">-- Check size of unsigned short</span><br><span class="line">-- Check size of unsigned short - <span class="keyword">done</span></span><br><span class="line">-- Check size of signed int</span><br><span class="line">-- Check size of signed int - <span class="keyword">done</span></span><br><span class="line">-- Check size of unsigned int</span><br><span class="line">-- Check size of unsigned int - <span class="keyword">done</span></span><br><span class="line">-- Check size of signed long</span><br><span class="line">-- Check size of signed long - <span class="keyword">done</span></span><br><span class="line">-- Check size of signed long long</span><br><span class="line">-- Check size of signed long long - <span class="keyword">done</span></span><br><span class="line">-- Check size of unsigned long long</span><br><span class="line">-- Check size of unsigned long long - <span class="keyword">done</span></span><br><span class="line">-- Check size of unsigned char *</span><br><span class="line">-- Check size of unsigned char * - <span class="keyword">done</span></span><br><span class="line">-- Check size of ptrdiff_t</span><br><span class="line">-- Check size of ptrdiff_t - <span class="keyword">done</span></span><br><span class="line">-- Check size of INT8</span><br><span class="line">-- Check size of INT8 - failed</span><br><span class="line">-- Check size of INT16</span><br><span class="line">-- Check size of INT16 - failed</span><br><span class="line">-- Check size of INT32</span><br><span class="line">-- Check size of INT32 - failed</span><br></pre></td></tr></table></figure>

<p>흐음… INT8, INT16, INT32의 사이즈 체크는 왜 fail인 것일까…</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Looking <span class="keyword">for</span> floor</span><br><span class="line">-- Looking <span class="keyword">for</span> floor - found</span><br><span class="line">-- Looking <span class="keyword">for</span> pow</span><br><span class="line">-- Looking <span class="keyword">for</span> pow - found</span><br><span class="line">-- Looking <span class="keyword">for</span> sqrt</span><br><span class="line">-- Looking <span class="keyword">for</span> sqrt - found</span><br><span class="line">-- Looking <span class="keyword">for</span> isascii</span><br><span class="line">-- Looking <span class="keyword">for</span> isascii - found</span><br><span class="line">-- Looking <span class="keyword">for</span> memset</span><br><span class="line">-- Looking <span class="keyword">for</span> memset - found</span><br><span class="line">-- Looking <span class="keyword">for</span> mmap</span><br><span class="line">-- Looking <span class="keyword">for</span> mmap - found</span><br><span class="line">-- Looking <span class="keyword">for</span> getopt</span><br><span class="line">-- Looking <span class="keyword">for</span> getopt - found</span><br><span class="line">-- Looking <span class="keyword">for</span> memmove</span><br><span class="line">-- Looking <span class="keyword">for</span> memmove - found</span><br><span class="line">-- Looking <span class="keyword">for</span> setmode</span><br><span class="line">-- Looking <span class="keyword">for</span> setmode - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strcasecmp</span><br><span class="line">-- Looking <span class="keyword">for</span> strcasecmp - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strchr</span><br><span class="line">-- Looking <span class="keyword">for</span> strchr - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strrchr</span><br><span class="line">-- Looking <span class="keyword">for</span> strrchr - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strstr</span><br><span class="line">-- Looking <span class="keyword">for</span> strstr - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strtol</span><br><span class="line">-- Looking <span class="keyword">for</span> strtol - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strtol</span><br><span class="line">-- Looking <span class="keyword">for</span> strtol - found</span><br><span class="line">-- Looking <span class="keyword">for</span> strtoull</span><br><span class="line">-- Looking <span class="keyword">for</span> strtoull - found</span><br><span class="line">-- Looking <span class="keyword">for</span> lfind</span><br><span class="line">-- Looking <span class="keyword">for</span> lfind - found</span><br><span class="line">-- Performing Test HAVE_SNPRINTF</span><br><span class="line">-- Performing Test HAVE_SNPRINTF - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_BUT_SET_VARIABLE</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_BUT_SET_VARIABLE - Failed</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_CAST_ALIGN</span><br><span class="line">-- Performing Test HAVE_C_WNO_CAST_ALIGN - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHADOW</span><br><span class="line">-- Performing Test HAVE_C_WNO_SHADOW - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_MAYBE_UNINITIALIZED</span><br><span class="line">-- Performing Test HAVE_C_WNO_MAYBE_UNINITIALIZED - Failed</span><br><span class="line">-- Performing Test HAVE_C_WNO_POINTER_TO_INT_CAST</span><br><span class="line">-- Performing Test HAVE_C_WNO_POINTER_TO_INT_CAST - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_INT_TO_POINTER_CAST</span><br><span class="line">-- Performing Test HAVE_C_WNO_INT_TO_POINTER_CAST - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISLEADING_INDENTATION</span><br><span class="line">-- Performing Test HAVE_C_WNO_MISLEADING_INDENTATION - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISSING_DECLARATIONS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISSING_DECLARATIONS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_PARAMETER</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_PARAMETER - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISSING_PROTOTYPES</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISSING_PROTOTYPES - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNDEF</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNDEF - Success</span><br><span class="line">-- Performing Test HAVE_C_STD_C99</span><br><span class="line">-- Performing Test HAVE_C_STD_C99 - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_VARIABLE</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_VARIABLE - Success</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_FUNCTION</span><br><span class="line">-- Performing Test HAVE_C_WNO_UNUSED_FUNCTION - Success</span><br><span class="line">-- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version &gt;= 2.3.1). OpenJPEG will be built from sources</span><br><span class="line">-- Performing Test HAVE_C_WNO_IMPLICIT_CONST_INT_FLOAT_CONVERSION</span><br><span class="line">-- Performing Test HAVE_C_WNO_IMPLICIT_CONST_INT_FLOAT_CONVERSION - Success</span><br><span class="line">-- OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.5.5-dev-openjp2-2.4.0</span><br><span class="line">-- Looking <span class="keyword">for</span> stdio.h</span><br><span class="line">-- Looking <span class="keyword">for</span> stdio.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> math.h</span><br><span class="line">-- Looking <span class="keyword">for</span> math.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> float.h</span><br><span class="line">-- Looking <span class="keyword">for</span> float.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> time.h</span><br><span class="line">-- Looking <span class="keyword">for</span> time.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> stdarg.h</span><br><span class="line">-- Looking <span class="keyword">for</span> stdarg.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> ctype.h</span><br><span class="line">-- Looking <span class="keyword">for</span> ctype.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> stdint.h</span><br><span class="line">-- Looking <span class="keyword">for</span> stdint.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> inttypes.h</span><br><span class="line">-- Looking <span class="keyword">for</span> inttypes.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/stat.h</span><br><span class="line">-- Looking <span class="keyword">for</span> sys/stat.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> include file malloc.h</span><br><span class="line">-- Looking <span class="keyword">for</span> include file malloc.h - not found</span><br><span class="line">-- Looking <span class="keyword">for</span> _aligned_malloc</span><br><span class="line">-- Looking <span class="keyword">for</span> _aligned_malloc - not found</span><br><span class="line">-- Looking <span class="keyword">for</span> posix_memalign</span><br><span class="line">-- Looking <span class="keyword">for</span> posix_memalign - found</span><br><span class="line">-- Looking <span class="keyword">for</span> memalign</span><br><span class="line">-- Looking <span class="keyword">for</span> memalign - not found</span><br><span class="line">-- Performing Test HAVE_C_WNO_CAST_FUNCTION_TYPE</span><br><span class="line">-- Performing Test HAVE_C_WNO_CAST_FUNCTION_TYPE - Failed</span><br><span class="line">-- OpenJPEG libraries will be built from sources: libopenjp2 (version <span class="string">"2.4.0"</span>)</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SHADOW</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SHADOW - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SIGN_COMPARE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SIGN_COMPARE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNINITIALIZED</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNINITIALIZED - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SWITCH</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SWITCH - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_PARENTHESES</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_PARENTHESES - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_ARRAY_BOUNDS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_ARRAY_BOUNDS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_EXTRA</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_EXTRA - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DEPRECATED_DECLARATIONS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DEPRECATED_DECLARATIONS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISLEADING_INDENTATION</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_MISLEADING_INDENTATION - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DEPRECATED</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_DEPRECATED - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SUGGEST_OVERRIDE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SUGGEST_OVERRIDE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_INCONSISTENT_MISSING_OVERRIDE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_INCONSISTENT_MISSING_OVERRIDE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IMPLICIT_FALLTHROUGH</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IMPLICIT_FALLTHROUGH - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_TAUTOLOGICAL_COMPARE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_TAUTOLOGICAL_COMPARE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_REORDER</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_REORDER - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_RESULT</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_RESULT - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IMPLICIT_CONST_INT_FLOAT_CONVERSION</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IMPLICIT_CONST_INT_FLOAT_CONVERSION - Success</span><br><span class="line">-- Could not find OpenBLAS include. Turning OpenBLAS_FOUND off</span><br><span class="line">-- Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off</span><br><span class="line">-- Could NOT find Atlas (missing: Atlas_CBLAS_INCLUDE_DIR Atlas_CLAPACK_INCLUDE_DIR Atlas_BLAS_LIBRARY) </span><br><span class="line">-- Looking <span class="keyword">for</span> sgemm_</span><br><span class="line">-- Looking <span class="keyword">for</span> sgemm_ - not found</span><br><span class="line">-- Found Threads: TRUE  </span><br><span class="line">-- Looking <span class="keyword">for</span> dgemm_</span><br><span class="line">-- Looking <span class="keyword">for</span> dgemm_ - found</span><br><span class="line">-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX12.1.sdk/System/Library/Frameworks/Accelerate.framework  </span><br><span class="line">-- Looking <span class="keyword">for</span> cheev_</span><br><span class="line">-- Looking <span class="keyword">for</span> cheev_ - found</span><br><span class="line">-- Found LAPACK: /Library/Developer/CommandLineTools/SDKs/MacOSX12.1.sdk/System/Library/Frameworks/Accelerate.framework;-lm;-ldl  </span><br><span class="line">-- LAPACK(LAPACK/Apple): LAPACK_LIBRARIES: /Library/Developer/CommandLineTools/SDKs/MacOSX12.1.sdk/System/Library/Frameworks/Accelerate.framework;-lm;-ldl</span><br><span class="line">-- Looking <span class="keyword">for</span> Accelerate/Accelerate.h</span><br><span class="line">-- Looking <span class="keyword">for</span> Accelerate/Accelerate.h - found</span><br><span class="line">-- Looking <span class="keyword">for</span> Accelerate/Accelerate.h</span><br><span class="line">-- Looking <span class="keyword">for</span> Accelerate/Accelerate.h - found</span><br><span class="line">-- LAPACK(LAPACK/Apple): Support is enabled.</span><br></pre></td></tr></table></figure>

<p>MacOS에서 지원하는 BLAS와 LAPACK이 따로 있다고 들었다.</p>
<p>그걸 마지막 줄에서 실제로 보게 되었다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_LOCAL_TYPEDEFS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_LOCAL_TYPEDEFS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SIGN_PROMO</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SIGN_PROMO - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_TAUTOLOGICAL_UNDEFINED_COMPARE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_TAUTOLOGICAL_UNDEFINED_COMPARE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IGNORED_QUALIFIERS</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_IGNORED_QUALIFIERS - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_FUNCTION</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_FUNCTION - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_CONST_VARIABLE</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_CONST_VARIABLE - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SHORTEN_64_TO_32</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_SHORTEN_64_TO_32 - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_INVALID_OFFSETOF</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_INVALID_OFFSETOF - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_ENUM_COMPARE_SWITCH</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_ENUM_COMPARE_SWITCH - Success</span><br><span class="line">-- Could NOT find JNI (missing: JAVA_INCLUDE_PATH JAVA_INCLUDE_PATH2 JAVA_AWT_INCLUDE_PATH) </span><br><span class="line">-- VTK is not found. Please <span class="built_in">set</span> -DVTK_DIR <span class="keyword">in</span> CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file</span><br><span class="line">-- Looking <span class="keyword">for</span> dlerror <span class="keyword">in</span> dl</span><br><span class="line">-- Looking <span class="keyword">for</span> dlerror <span class="keyword">in</span> dl - found</span><br><span class="line">-- ADE: Download: v0.1.1f.zip</span><br><span class="line">-- Allocator metrics storage <span class="built_in">type</span>: <span class="string">'int'</span></span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.sse2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.sse3.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.ssse3.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.sse4_1.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.sse4_2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.avx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin128.avx512_skx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin256.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin256.avx512_skx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/core/<span class="built_in">test</span>/test_intrin512.avx512_skx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/imgproc/src/corner.avx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/imgproc/src/imgwarp.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/imgproc/src/imgwarp.sse4_1.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/imgproc/src/resize.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/imgproc/src/resize.sse4_1.cpp</span><br></pre></td></tr></table></figure>

<p>오… M1 Chip이다보니 SSE와 AVX를 쓸 일이 없어서 x86 Intel CPU 가속 관련 코드를 빌드하지 않는 모습을 볼 수 있다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Registering hook <span class="string">'INIT_MODULE_SOURCES_opencv_dnn'</span>: /Users/hyunggi/opencv/modules/dnn/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake</span><br><span class="line">-- opencv_dnn: filter out ocl4dnn <span class="built_in">source</span> code</span><br><span class="line">-- opencv_dnn: filter out cuda4dnn <span class="built_in">source</span> code</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/layers/layers_common.avx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/layers/layers_common.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/layers/layers_common.avx512_skx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/layers/layers_common.rvv.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/int8layers/layers_common.avx2.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: &lt;BUILD&gt;/modules/dnn/int8layers/layers_common.avx512_skx.cpp</span><br><span class="line">-- Excluding from <span class="built_in">source</span> files list: modules/features2d/src/fast.avx2.cpp</span><br></pre></td></tr></table></figure>

<p>다시 DNN 모듈에서도 AVX 관련 소스코드를 빌드하지 않는 모습을 볼 수 있다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- imgcodecs: OpenEXR codec is disabled <span class="keyword">in</span> runtime. Details: https://github.com/opencv/opencv/issues/21326</span><br><span class="line">-- Performing Test HAVE_CXX_FOBJC_EXCEPTIONS</span><br><span class="line">-- Performing Test HAVE_CXX_FOBJC_EXCEPTIONS - Success</span><br><span class="line">-- highgui: using <span class="built_in">builtin</span> backend: COCOA</span><br><span class="line">-- Found <span class="string">'misc'</span> Python modules from /Users/hyunggi/opencv/modules/python/package/extra_modules</span><br><span class="line">-- Found <span class="string">'mat_wrapper;utils'</span> Python modules from /Users/hyunggi/opencv/modules/core/misc/python/package</span><br><span class="line">-- Found <span class="string">'gapi'</span> Python modules from /Users/hyunggi/opencv/modules/gapi/misc/python/package</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_OVERLOADED_VIRTUAL</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_OVERLOADED_VIRTUAL - Success</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_PRIVATE_FIELD</span><br><span class="line">-- Performing Test HAVE_CXX_WNO_UNUSED_PRIVATE_FIELD - Success</span><br><span class="line">-- Found <span class="string">'misc'</span> Python modules from /Users/hyunggi/opencv/modules/python/package/extra_modules</span><br><span class="line">-- Found <span class="string">'mat_wrapper;utils'</span> Python modules from /Users/hyunggi/opencv/modules/core/misc/python/package</span><br><span class="line">-- Found <span class="string">'gapi'</span> Python modules from /Users/hyunggi/opencv/modules/gapi/misc/python/package</span><br><span class="line">-- </span><br><span class="line">-- General configuration <span class="keyword">for</span> OpenCV 4.5.5-dev =====================================</span><br><span class="line">--   Version control:               4.5.5-80-g9cab808c5d</span><br><span class="line">-- </span><br><span class="line">--   Platform:</span><br><span class="line">--     Timestamp:                   2022-01-28T07:40:38Z</span><br><span class="line">--     Host:                        Darwin 21.2.0 arm64</span><br><span class="line">--     CMake:                       3.22.1</span><br><span class="line">--     CMake generator:             Ninja</span><br><span class="line">--     CMake build tool:            /opt/homebrew/bin/ninja</span><br><span class="line">--     Configuration:               Release</span><br><span class="line">-- </span><br><span class="line">--   CPU/HW features:</span><br><span class="line">--     Baseline:                    NEON FP16</span><br><span class="line">-- </span><br></pre></td></tr></table></figure>

<p>Host는 Darwin (i.e. MacOS)에 arm64 CPU 아키텍처를 찾은 것을 확인했다.</p>
<p>CPU/HW feature도 Baseline을 ARM Neon 가속을 적용한 것을 확인할 수 있는데, CPU 특징상 Floating point를 16비트로 표현한 것으로 가속을 하는 것을 확인할 수 있다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--   C/C++:</span><br><span class="line">--     Built as dynamic libs?:      YES</span><br><span class="line">--     C++ standard:                11</span><br><span class="line">--     C++ Compiler:                /Library/Developer/CommandLineTools/usr/bin/c++  (ver 13.0.0.13000027)</span><br><span class="line">--     C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wno-delete-non-virtual-dtor -Wno-unnamed-type-template-args -Wno-comment -fdiagnostics-show-option -Qunused-arguments -Wno-semicolon-before-method-body -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG</span><br><span class="line">--     C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wno-delete-non-virtual-dtor -Wno-unnamed-type-template-args -Wno-comment -fdiagnostics-show-option -Qunused-arguments -Wno-semicolon-before-method-body -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG</span><br><span class="line">--     C Compiler:                  /Library/Developer/CommandLineTools/usr/bin/cc</span><br><span class="line">--     C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wno-delete-non-virtual-dtor -Wno-unnamed-type-template-args -Wno-comment -fdiagnostics-show-option -Qunused-arguments -Wno-semicolon-before-method-body -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG</span><br><span class="line">--     C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wno-delete-non-virtual-dtor -Wno-unnamed-type-template-args -Wno-comment -fdiagnostics-show-option -Qunused-arguments -Wno-semicolon-before-method-body -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG</span><br><span class="line">--     Linker flags (Release):      -Wl,-dead_strip  </span><br><span class="line">--     Linker flags (Debug):        -Wl,-dead_strip  </span><br><span class="line">--     ccache:                      NO</span><br><span class="line">--     Precompiled headers:         NO</span><br><span class="line">--     Extra dependencies:</span><br><span class="line">--     3rdparty dependencies:</span><br><span class="line">-- </span><br><span class="line">--   OpenCV modules:</span><br><span class="line">--     To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo python2 stitching ts video videoio</span><br><span class="line">--     Disabled:                    world</span><br><span class="line">--     Disabled by dependency:      -</span><br><span class="line">--     Unavailable:                 java python3</span><br><span class="line">--     Applications:                tests perf_tests apps</span><br><span class="line">--     Documentation:               NO</span><br><span class="line">--     Non-free algorithms:         NO</span><br><span class="line">-- </span><br><span class="line">--   GUI:                           COCOA</span><br><span class="line">--     Cocoa:                       YES</span><br><span class="line">--     VTK support:                 NO</span><br><span class="line">-- </span><br><span class="line">--   Media I/O: </span><br><span class="line">--     ZLib:                        build (ver 1.2.11)</span><br><span class="line">--     JPEG:                        build-libjpeg-turbo (ver 2.1.2-62)</span><br><span class="line">--     WEBP:                        build (ver encoder: 0x020f)</span><br><span class="line">--     PNG:                         build (ver 1.6.37)</span><br><span class="line">--     TIFF:                        build (ver 42 - 4.2.0)</span><br><span class="line">--     JPEG 2000:                   build (ver 2.4.0)</span><br><span class="line">--     OpenEXR:                     build (ver 2.3.0)</span><br><span class="line">--     HDR:                         YES</span><br><span class="line">--     SUNRASTER:                   YES</span><br><span class="line">--     PXM:                         YES</span><br><span class="line">--     PFM:                         YES</span><br><span class="line">-- </span><br><span class="line">--   Video I/O:</span><br><span class="line">--     DC1394:                      NO</span><br><span class="line">--     FFMPEG:                      NO</span><br><span class="line">--       avcodec:                   NO</span><br><span class="line">--       avformat:                  NO</span><br><span class="line">--       avutil:                    NO</span><br><span class="line">--       swscale:                   NO</span><br><span class="line">--       avresample:                NO</span><br><span class="line">--     GStreamer:                   NO</span><br><span class="line">--     AVFoundation:                YES</span><br><span class="line">-- </span><br><span class="line">--   Parallel framework:            GCD</span><br><span class="line">-- </span><br><span class="line">--   Trace:                         YES (with Intel ITT)</span><br><span class="line">-- </span><br><span class="line">--   Other third-party libraries:</span><br><span class="line">--     Lapack:                      YES (/Library/Developer/CommandLineTools/SDKs/MacOSX12.1.sdk/System/Library/Frameworks/Accelerate.framework -lm -ldl)</span><br><span class="line">--     Eigen:                       NO</span><br><span class="line">--     Custom HAL:                  YES (carotene (ver 0.0.1))</span><br><span class="line">--     Protobuf:                    build (3.19.1)</span><br><span class="line">-- </span><br><span class="line">--   OpenCL:                        YES (no extra features)</span><br><span class="line">--     Include path:                NO</span><br><span class="line">--     Link libraries:              -framework OpenCL</span><br><span class="line">-- </span><br><span class="line">--   Python 2:</span><br><span class="line">--     Interpreter:                 /usr/bin/python2.7 (ver 2.7.18)</span><br><span class="line">--     Libraries:                   /usr/lib/libpython2.7.dylib (ver 2.7.18)</span><br><span class="line">--     numpy:                       /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/core/include (ver 1.8.0rc1)</span><br><span class="line">--     install path:                lib/python2.7/site-packages/cv2/python-2.7</span><br><span class="line">-- </span><br><span class="line">--   Python (<span class="keyword">for</span> build):            /usr/bin/python2.7</span><br><span class="line">-- </span><br><span class="line">--   Java:                          </span><br><span class="line">--     ant:                         NO</span><br><span class="line">--     JNI:                         NO</span><br><span class="line">--     Java wrappers:               NO</span><br><span class="line">--     Java tests:                  NO</span><br><span class="line">-- </span><br><span class="line">--   Install to:                    /Users/hyunggi/opencv/install</span><br><span class="line">-- -----------------------------------------------------------------</span><br><span class="line">-- </span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /Users/hyunggi/opencv/build</span><br></pre></td></tr></table></figure>

<h2 id="빌드"><a href="#빌드" class="headerlink" title="빌드"></a>빌드</h2><p>아래 커맨드를 이용해서 빌드했다. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">time ninja</span><br></pre></td></tr></table></figure>

<p>참고로 M1 vs x86 워크스테이션 두개 설정에서 빌드를 해봤는데, 충격적인 결과가 나왔다.</p>
<p>결과는 다음 <a href="www.cv-learn.com/20220130-m1-opencv-build-sse/">링크</a>에…</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
      </categories>
      <tags>
        <tag>Apple</tag>
        <tag>OpenCV</tag>
        <tag>Macbook Pro</tag>
        <tag>M1</tag>
        <tag>ARM</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern javascript 튜토리얼 레퍼런스</title>
    <url>/20220128-modern-javascript/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9rby5qYXZhc2NyaXB0LmluZm8v">https://ko.javascript.info/<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="Javascript-진짜"><a href="#Javascript-진짜" class="headerlink" title="Javascript? 진짜?"></a>Javascript? 진짜?</h2><p>슬램쟁이가 javascript를 공부한다니??</p>
<p>사실 필수는 아니다.</p>
<p>Pangolin에 질려서 D3.js 로 뷰어를 만들어보고 싶다는 생각도 있고, 데스크탑 앱 같은걸 만들 줄 알면 도움이 되겠다고 생각이 들었다.</p>
<p>그리고 블록체인 쪽도 요즘 공부를 하고 있는데, 이쪽은 smart contract를 작성하는 것도 중요하지만 현재 대부분은 앱개발이 꼭 붙어야하는 것 같다.</p>
<p>찍먹 해보기 딱 좋을듯!</p>
<h2 id="기대하는-점"><a href="#기대하는-점" class="headerlink" title="기대하는 점"></a>기대하는 점</h2><p>사실 Solidity를 공부하면서 javascript 생태계를 간접적으로 체험해본 적이 있다.</p>
<p>npm 서버를 통해 굉장히 좋은 API가 많이 나와있는데, 정말 진짜 코드 한두줄 추가해서 완벽하게 잘 만든 기능을 쓸 수 있다는 점에 놀랐다.</p>
<p>C++에서는 vcpkg에서 옵션도 많이 막혀있고, 소스빌드를 하려면 cmake에 make에 컴파일러에 이것저것 알아야할게 산더미같기 때문이다 으아악</p>
<p>생산성이 훨씬 뛰어난 개발자들이 작업하는 생태계는 어떤 느낌일까, 라는 질문을 가지고 javasript 공부를 하고 생태계를 직접 경험해보려고 한다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.5 Javascript / Typescript</category>
      </categories>
      <tags>
        <tag>Javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>Senior 엔지니어의 이직 방법</title>
    <url>/20220128-senior-dev-jobsearch/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuam9ic2VhcmNoLmRldi8=">https://www.jobsearch.dev/<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="100일-동안의-job-search"><a href="#100일-동안의-job-search" class="headerlink" title="100일 동안의 job search"></a>100일 동안의 job search</h2><p>미국 내 큰 회사로의 이직을 목표로 하는 엔지니어들을 위한 이직 가이드. 프론트엔드/백엔드/풀스택 개발자들을 타겟으로 작성되었지만, 타 분야도 어느정도 적용 가능할 듯.</p>
<h2 id="Study-Plan"><a href="#Study-Plan" class="headerlink" title="Study Plan"></a>Study Plan</h2><p>Week 1 ~ 14까지 플랜이 있음.</p>
<ul>
<li>W1: 이직하고 싶은 회사 탐색 &amp; 리스트업</li>
<li>W2-3: 과제 진행 + 알고리즘 인터뷰 연습</li>
<li>W4-6: 기술면접 준비 + 시스템 아키텍처 디자인 면접 준비</li>
<li>W7-8: 기술면접 진행 + 시스템 아키텍처 디자인 면접 준비</li>
<li>W9-10: 대면면접 진행</li>
<li>W11-14: 오퍼 받기</li>
</ul>
<h2 id="코스-구성"><a href="#코스-구성" class="headerlink" title="코스 구성"></a>코스 구성</h2><ul>
<li>Mental Game: 구직을 위한 Mindset 만들기</li>
<li>Communication : 모든 인터뷰에서 사용할 언어 스킬</li>
<li>Interview Phases : 인터뷰 단계들 소개 및 단계별 필요한 것들</li>
<li>Algorithms &amp; UI Interviews : 기술 인터뷰 준비</li>
<li>System Design Interviews : 기술 리더들과의 인터뷰 대비 하기</li>
<li>Experience Interviews : 경험과 컬쳐핏에 대해 이야기</li>
<li>Offer Stage : 최고의 연봉 협상 하는 법</li>
</ul>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>이직</tag>
      </tags>
  </entry>
  <entry>
    <title>CppCon 2015 - &quot;Stop Teaching C&quot; (Kate Gregory)</title>
    <url>/20220128-stop-teaching-c/</url>
    <content><![CDATA[<h2 id="시작하기에-앞서"><a href="#시작하기에-앞서" class="headerlink" title="시작하기에 앞서"></a>시작하기에 앞서</h2><p>“Stop Teaching C” 토크는 C++ 컨퍼런스인 CppCon에서 Kate Gregory가 발표한 토크의 제목입니다.</p>
<p>C 언어를 쓰지 말라는 의미가 아니라, <strong>C++을 가르치는데 C처럼 가르치지 말라</strong>는 의미입니다.</p>
<p>C 언어는 좋은 언어입니다. 오해가 없길 바랍니다 ㅎㅎ,,</p>
<p> </p>
<hr>
<h2 id="C-gt-C-공부-방법이-만드는-문제"><a href="#C-gt-C-공부-방법이-만드는-문제" class="headerlink" title="C->C++ 공부 방법이 만드는 문제"></a>C-&gt;C++ 공부 방법이 만드는 문제</h2><p>현재 많은 C++ 커리큘럼들이 C를 먼저 공부한 후 C++을 공부하는 방향을 지향합니다.</p>
<p>이는 두 언어가 상당히 비슷해 C를 아는 사람에게 C++을 가르치기 쉽기 때문입니다.</p>
<p>상당수의 현업 C++ 유저들이 C++이 생겨나기 전 C를 먼저 사용하다가 넘어온 경우가 많은데, 본인들이 겪은 공부 과정을 전수하기 위해 이 방법을 사용하기도 합니다.</p>
<p>하지만 이 방법을 채택하면 제대로 된 C++를 공부하는데에 더 오랜 시간이 걸리게 됩니다.</p>
<p> </p>
<p>C와 C++에는 극명한 차이가 있습니다.</p>
<p>C에는 RAII, type-safety, template, lambda, exceptions, iterator와 같은 기능이 없습니다.</p>
<p>C로는 Object-oriented design을 공부할 수 없습니다.</p>
<p>C를 먼저 배운 후 C++을 공부하게 되면, <strong>이미 익숙해진 C스타일을 버리고 C++을 새롭게 공부</strong>해야합니다.</p>
<p>그럴바에는 <strong>Day 1 부터 C++ 스타일을 공부하는게 더 효율적</strong>일 겁니다.</p>
<p> </p>
<hr>
<h2 id="C-코스-Day-1에-가르쳐서는-안될-것들"><a href="#C-코스-Day-1에-가르쳐서는-안될-것들" class="headerlink" title="C++ 코스 Day 1에 가르쳐서는 안될 것들"></a>C++ 코스 Day 1에 가르쳐서는 안될 것들</h2><p>아래에 있는 내용들은 ‘C++에서 쓰면 안되는 것들’이 아닙니다.</p>
<p>C++을 공부하는데에 Day 1에 가르치면, 공부하는 사람 입장에서 굉장히 헷갈리게 되는 것들입니다.</p>
<p>대학교에서 C++ 코스를 듣고 ‘와 에반데- C++은 내 길이 아닌가보다’ 하고 생각하게 만드는 것들입니다.</p>
<h3 id="char-string"><a href="#char-string" class="headerlink" title="char* string"></a>char* string</h3><p><code>char*</code>로 string을 다루기 위해서는 pointer를 이해해야합니다.</p>
<p>기본적인 Hello world까지야 <code>printf()</code>와 <code>sscanf()</code>로 어떻게든 됩니다.</p>
<p>하지만 조금만 더 깊게 들어가 string을 다루기 위해서는 1. char array에서 pointer를 이동하는 방법, 2. off-by-one error 이해, 3. null terminator의 대한 개념까지 이해해야합니다.</p>
<p>그 이후에 구현을 위해 여러가지 C에서 제공하는 string 함수들을 외워야합니다 (e.g. <code>strcmp()</code>).</p>
<p>5분만 들어도 **’와 쓰읍 C++은 어렵구나’**라고 생각하게 만드는 주범 입니다.</p>
<h4 id="해결책"><a href="#해결책" class="headerlink" title="해결책"></a>해결책</h4><p>C++ 방식으로 처음부터 가르치면 <code>char*</code> 대신에 <code>&lt;string&gt;</code>을 사용할 수 있습니다.</p>
<p><code>&lt;string&gt;</code>은 <code>char*</code>에 비해 이해하기 훨씬 쉬우며, Python이나 javascript와 같이 string이 built-in 기능으로 포함되어있는 다른 프로그래밍 언어를 겪은 사람들은 string을 곧바로 이해할 수 있습니다.</p>
<p>1980~년대에 공부하신 분들은 char*로 배우셨을 텐데, 이는 그 당시에 <code>&lt;string&gt;</code>이라는 기능 자체가 최신 기능처럼 느껴졌을 것 이기 때문입니다. (그리고 2021년 현대에 string은 그냥 어디에든 있는 기본 기능입니다)</p>
<p><code>&lt;string&gt;</code>을 사용하면서 C++의 강점인 operator overloading과 strong type을 소개할 수도 있습니다.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span> name;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">string</span> greeting = <span class="string">"Hello, "</span> + name;</span><br><span class="line"><span class="keyword">if</span> (name == <span class="string">"Kate"</span>)</span><br><span class="line">    greeting += <span class="string">", I know you!"</span>;</span><br></pre></td></tr></table></figure>

<p> </p>
<h3 id="printf"><a href="#printf" class="headerlink" title="printf"></a>printf</h3><p>변수의 값을 확인하기 위해 <code>printf()</code>를 가르칩니다.</p>
<p><code>printf()</code>를 사용하기 위해서는 format specifier를 다 외워야합니다.</p>
<p>그리고 많은 경우 초보자들은 format specifier로 인해 에러를 경험합니다. (e.g. <code>%c</code> vs <code>%s</code>)</p>
<p>오전 10시 반 Day1 C++ 클래스를 들으면서, 초보자들은 벌써 복잡한 에러메세지를 마주하며 본인이 짠 간단한 프로그램의 변수 값도 확인하지 못하게 됩니다.</p>
<h4 id="해결책-1"><a href="#해결책-1" class="headerlink" title="해결책"></a>해결책</h4><p>Debugger를 써서 코드 한줄한줄 넘어갈 때 변수 값이 어떻게 바뀌는지 보여줍시다.</p>
<p>실제로 출력도 해야하는 경우에는 <code>cout</code>, <code>cin</code>, <code>cerr</code>로 빠르게 결과를 보여줍시다.</p>
<p>stream 방식을 사용할 경우 속도가 좀 느려질 수 있다는 걸 언급만 하고 넘어가도 괜찮습니다.</p>
<p>Day1 이니까요.</p>
<p> </p>
<h3 id="arrays"><a href="#arrays" class="headerlink" title="[] arrays"></a>[] arrays</h3><p><code>char*</code>와 비슷하게, 초보자에게 array의 기본 기능을 가르치기도 전 부터 포인터 개념을 요구합니다.</p>
<p>기본적으로 address와 reference의 개념을 익혀야하고, 그 후 bounds checking, off-by-one 과 같은 에러에 부닥치며, array를 익히기도 전에 여러가지 개념을 헷갈리기 시작합니다.</p>
<p>심지어 대부분의 [] array를 사용한 예제는 더 쉽게 적을 수 있는 방식이 있음에도 raw for loop 내부에 알고리즘을 직접 구현하는 방식을 씁니다.</p>
<p>예를 들어, “이 array 안에 3이 몇개가 있을까요?”라는 문제를 푸는 코드를 작성할 때, 쉽게 하려면 range-based for loop를 써서 가독성을 높이거나, 또는 <code>std::count</code>를 쓰면 됩니다. raw for-loop에서 i 값 하나하나 세면서 하지 말구요.</p>
<h4 id="해결책-2"><a href="#해결책-2" class="headerlink" title="해결책"></a>해결책</h4><p><code>std::vector</code>를 쓰면 됩니다.</p>
<p>초심자에게는 <code>std::vector</code>로 거의 모든 케이스를 커버할 수 있습니다.</p>
<p>기본기를 익히고나서, 성능이 중요하게 될 때 STL에 성능을 개선할 수 있는 다른 형태의 자료구조들이 존재한다고 알려주기만 해도 충분합니다.</p>
<p>하지만 처음부터 iterator를 알려주는건 추천하지 않습니다.</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/YnWhqhNdYyk" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>CppCon</tag>
        <tag>Kate Gregory</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 2 - Saying No 좋은 문장들</title>
    <url>/20220130-clean-coders-saying-no/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>“프로들은 매니저들에게 ‘안됩니다’라고 말할 수 있는 용기를 가지고 있습니다”</p>
<p>“노예들은 ‘안됩니다’라고 말할 수 없습니다. 노동자는 ‘안됩니다’라고 말하기 꺼려할 겁니다. 하지만 프로(전문가)들은 ‘안됩니다’라고 말할 수 있는 사람이여야합니다”</p>
<p> </p>
<hr>
<h2 id="Adversarial-roles"><a href="#Adversarial-roles" class="headerlink" title="Adversarial roles"></a>Adversarial roles</h2><p>“매니저와 당신, 양쪽 모두가 <strong>각자의 임무</strong>를 완수하기 위해 최선을 다 해야만 ‘최선의 결과’를 얻을 수 있습니다” (매니저는 시간을 완수해야하고, 개발자는 좋은 제품을 만들어야한다)</p>
<p> </p>
<blockquote>
<p>케이스 1:<br><strong>매니저:</strong> “내일까지 A 기능을 만들어주세요”<br><strong>개발자:</strong> “내일까지요? 흠, 해볼게요”<br><strong>매니저:</strong> “좋아요, 고마워요!”</p>
</blockquote>
<p>“여기서 매니저는 ‘해볼게요’를 ‘만들겠습니다’로 인지했습니다. 멍청한 생각이에요. <strong>‘정말 내일까지 만들 수 있나요?</strong>‘라고 했었어야합니다”</p>
<p> </p>
<blockquote>
<p>케이스 2:<br><strong>매니저:</strong> “내일까지 A 기능을 만들어주세요”<br><strong>개발자:</strong> “시간이 더 필요할 것 같은데요?”<br><strong>매니저:</strong> “얼마나 더 필요한데요?”<br><strong>개발자:</strong> “2주 정도면 어떨까요?”<br><strong>매니저:</strong> “좋아요, 2주 후에 봅시다”</p>
</blockquote>
<p>“개발자는 2주면 괜찮을지 물어볼 것이 아니라, <strong>정확하게 ‘2주 걸릴 것 같다’라고 얘기</strong>했어야합니다. 매니저는 이 2주라는 시간을 <strong>질문도 없이 그대로 받아드리면 안됬습니다</strong>.”</p>
<p> </p>
<blockquote>
<p>케이스 3:<br><strong>매니저:</strong> “내일까지 A 기능을 만들어주세요”<br><strong>개발자:</strong> “안됩니다. 그건 2주짜리 개발 업무에요”<br><strong>매니저:</strong> “2주요?? 아키텍트들은 3일을 예상했는데, 벌써 5일이나 지났잖아요!”<br><strong>개발자:</strong> “아키텍트들이 잘못 예상한거에요. 3일을 예상했던 작업은 지난번 마케팅 회의 때 요구사항이 뒤엎어졌어요. 새로운 요구사항은 아직 최소 열흘은 더 걸립니다. 위키에 업데이트 해놨는데, 안보셨어요?”<br><strong>매니저:</strong> “(부들부들대며) 이건 말도 안돼요. 고객들은 당장 내일 데모를 보기를 원한다구요. 그리고 이 A 기능이 작동해야한다구요.”<br><strong>개발자:</strong> “A 기능의 어떤 부분이 필요한데요?”<br><strong>매니저:</strong> “A 라는 기능 자체가 필요하다구요!”<br><strong>개발자:</strong> “A의 기능을 보여주는 mock-up 기능을 만들 수는 있어요. 실제 서비스되는 A 기능에 필요한 서버 기능, 쿠키 데이터 저장, 보안 체크, 데이터베이스 통신과 같은 기능은 작동하지는 않겠지만, 데모에 필요한 필수 클라이언트 기능들은 작동하는 모습은 보여줄 수 있습니다. 이건 어떨까요?”<br><strong>매니저:</strong> “A가 작동하는게 보여진다구요?”<br><strong>개발자:</strong> “네, 고객들 입장에서는 작동하는 것 처럼 보여질거에요”<br><strong>매니저:</strong> “완벽합니다! 고마워요! (내적 “예쓰으으!!”를 외치며 자리를 떠남)</p>
</blockquote>
<p>“양쪽 모두가 각자의 임무를 지키려고 함으로써, 양쪽 모두가 원하는 바를 얻는 ‘최선의 결과’를 얻었습니다.”</p>
<p>“제 경험 상, fact가 why보다 더 중요합니다.”</p>
<p> </p>
<hr>
<h2 id="High-stakes"><a href="#High-stakes" class="headerlink" title="High stakes"></a>High stakes</h2><p>“‘안됩니다’라고 말할 수 있는 가장 중요한 때는 <strong>사활이 걸려있을 때</strong> 입니다”.</p>
<p> </p>
<blockquote>
<p>케이스 4:<br>(기존의 예상보다 더 개발에 딜레이가 걸린다는 사실을 알렸을 때)</p>
<p><strong>CEO:</strong> “이 건을 따내지 못하면 우린 진짜… 당신도 알고 있죠? 이렇게나 많이 늦어진다면… 하… 투자자들한테는 뭐라고 얘기를 해야하지… 당신, 뭐든 상관없으니 어떻게든 해봐요.”<br><strong>개발자:</strong> “이미 제가 할 수 있는건 다 했어요. 대표님도 아시잖아요. 3개월 전에 개발 명세 조정을 요청했던게 거절당한거. 고객사에서는 개발 요구사항을 줄여주지도 않고, 안정성 테스트도 할 여건을 만들어주지 않는걸요. 대표님도 그 떄 고객사 편을 들어줬구요. 지금 저희 팀은 할 수 있는 모든걸 쏟아붓고 있어요. 지금 하는거보다 더 빠르게 작업할 순 없어요.”<br><strong>CEO:</strong> “젠장, 당신 지금 그 말에 책임 질 수 있어요?”<br><strong>개발자:</strong> <strong>“날 짜른다고 해서 개발 스케줄이 바뀌진 않아요.”</strong><br><strong>CEO:</strong> “알았어요. 지금이라도 액션을 취해볼게요. 당신은 계속 개발을 진행해주세요.”</p>
</blockquote>
<p> </p>
<hr>
<h2 id="Being-a-‘team-player’"><a href="#Being-a-‘team-player’" class="headerlink" title="Being a ‘team player’"></a>Being a ‘team player’</h2><p>“팀플레이어가 된다는건 마치 ‘내 할일도 잘 하고, 팀원이 막힐 때 도와줄 수 있는 사람’과 같은 사람처럼 들립니다.”</p>
<p>“하지만 사실 <strong>팀플레이이어는 ‘팀원이 원하는대로 다 해주는 사람’이 아닙니다</strong>“</p>
<p> </p>
<blockquote>
<p>케이스 5:<br><strong>개발자:</strong> “팀원들과 상의해서 개발 스케줄을 새로 짜왔습니다. 8주 후면 데모를 보여줄 수 있을 것 같네요. +- 1주 정도 오차가 있을 수 있습니다.”<br><strong>매니저:</strong> “안됩니다. 6주 후에 고객 데모를 진행하기로 이미 예정되어있어요.”<br><strong>개발자:</strong> “우리 의견은 듣지도 않고요?”<br><strong>매니저:</strong> “이미 위에서 정해진 내용입니다.”<br><strong>개발자:</strong> “(한숨) 알았어요, 팀원들과 6주 안에 개발 할 수 있는 내용들로 다시 정리해올게요. 대신 기존에 상정했던 개발 요구사항에서 몇개 빠질거고, 데이터 로딩 부분은 미완성으로 나갈겁니다.”<br><strong>매니저:</strong> “안돼요. 그 기능들도 다 포함되야합니다. 고객들은 완성된 데모를 보고싶어해요.”<br><strong>개발자:</strong> “그건 불가능합니다. 이미 우리는 팀 리소스를 전부 투입하고 있어요.”<br><strong>매니저:</strong> “젠장, 알았어요. 우선 팀원들과 상의해보고 내일까지는 계획을 공유해주세요.”<br><strong>개발자:</strong> “알겠습니다.”<br><strong>매니저:</strong> “이해가 안되네요. 정말 더 해볼 방법이 없어요? 좀 더 똑똑하게 일할 방법을 찾는다던지, 뭔가 창의적인 방법이 있겠죠.”<br><strong>개발자:</strong> “우린 이미 충분히 창의적이에요, 매너저님. 업무 내용도 정확하게 다 파악하고 있어요. 8-9주의 시간이 필요해요. 6주는 절대 안된다구요.”<br><strong>매니저:</strong> “야근이랑 휴일업무를 해도 안될까요?”<br><strong>개발자:</strong> “더 느려지기만 할거에요. 지난번에 초과업무 3주 내내 하다가 팀원들 다 지쳐서 에러 투성이 코드를 릴리즈한거 기억 안나요?”<br><strong>매니저:</strong> “그게 꼭 이번 릴리즈에 반복되라는 법은 없잖아요.”<br><strong>개발자:</strong> “아뇨, 이번 릴리즈에 정확히 똑같이 반복될거에요. 이 부분은 저희 쪽 의견 믿어주셔야해요. 이거 다 하려면 8-9주 걸려요. 6주론 부족해요.”<br><strong>매니저:</strong> “알았어요, 내일까지 계획 세워서 공유해주세요. 하지만 6주 안에 가능하게 만들 수 있는 방법도 계속 고민해주세요. 똑똑한 분들이시니까 분명 방법을 찾으실 수 있을 거에요. 이거 진짜 중요한 데모거든요.”<br><strong>개발자:</strong> “아뇨, 안된다니까요. 6주짜리 계획은 세워서 줄 수 있지만, 아까 얘기했듯이 기능이 몇개 빠질거에요. 이거말고는 방법이 없어요.”<br><strong>매니저:</strong> “알았어요, 하지만 우리 회사에서 제일 똑똑한 분들이신데, 분명 방법을 찾을 수 있을거에요!”<br>…<br>(오후에 있는 PM 미팅에서…)<br><strong>CEO:</strong> “매니저님, 6주 후에 데모 있는거 아시죠? 고객들은 이번에 완벽한 데모를 기대하고 있어요”<br><strong>매니저:</strong> “데모는 계획했던 대로 진행될겁니다. 저희 개발 팀이 엄청 열심히 일하고 있어요. 일정이 타이트해서 초과근무는 하겠지만, 다들 똑똑한 사람들이라서 잘 해낼겁니다!”<br><strong>CEO:</strong> “허허, 저희 매니저님과 개발팀원분들은 팀의 목표를 이루기 위해 무엇이든 해내는 팀플레이어들이라서 너무 다행입니다”</p>
</blockquote>
<p><strong>(쓰으으으으으으으읍)</strong></p>
<p>“여기서 누가 진짜 팀플레이어일까요? 팀을 위해 <strong>1. 현실을 직시하게 해주고</strong>, <strong>2. 최선의 결과를 낼 수 있도록 각자의 임무에 충실하게 한</strong> 개발자가 팀플레이어입니다.”</p>
<p>“여기서 매니저는 자신의 목표 달성에만 충실했습니다. 개발자의 편도 아니였지만, CEO의 편도 아니였어요. 매니저는 CEO가 자신을 팀플레이어처럼 보기를 원했기 때문에, 개발자에게 온갖 사탕발린 말과 비꼬기 등으로 개발자를 휘두르려고 했습니다.”</p>
<p> </p>
<hr>
<p>“이 대화에서 개발자가 가장 잘 한 것은 ‘알았어요, 한번 해볼게요’라고 말하지 않은 것 입니다. <strong>개발의 세계에서 ‘해본다’나 ‘최선을 다해보겠습니다’라는 말은 존재하지 않아요</strong>.”</p>
<p>“<strong>‘한번 해보겠습니다’라는건, 원래의 계획과는 다르게 더 어려운 목표에 도전한다는 의미</strong>입니다.” </p>
<p>“더 어려운 목표에 도전해서 성공할 수 있는거라면, 당신이 처음에 생각했던 목표/스케줄은 부족했던게 아닐까요?” 더 어려운 목표를 해결하기 위해서, 당신이 처음에 생각했던 스케줄과는 어떤 점을 다르게 할 건데요? 문제에 대한 접근 방법이 달라지나요? 마인드셋이 달라지나요? 당신이 ‘해보겠다’라는거로 어떤 점이 달라지는데요? “</p>
<p>“‘최선을 다 해보겠습니다’라는 말은, 평소에는 대충한다는 뜻인가요? <strong>평소에 아껴둔 에너지를 이번에 쓰면 목표를 달성할 수 있는건가요?</strong> 그게 아니면 당신은 그냥 지키지 못할 약속을 하는게 아닐까요?”</p>
<p>“<strong>새로운 계획도 없고, 새롭게 문제를 해결할 접근법도 없고, 추가로 가용할만한 에너지도 없고, 처음에 세웠던 계획에도 자신감이 있었다면</strong>, ‘해보겠습니다’나 ‘최선을 다해보겠습니다’와 같이 ‘<strong>약속</strong>‘을 하는 행위는 <strong>단순히 ‘거짓말’을 하는 행위</strong>에 불과합니다. <strong>논쟁을 피하고 싶어서 하는 거짓말</strong>이죠.”</p>
<p> </p>
<hr>
<h2 id="Passive-aggression"><a href="#Passive-aggression" class="headerlink" title="Passive aggression"></a>Passive aggression</h2><blockquote>
<p>케이스 6:<br><strong>매니저:</strong> “저희 데모가 3주 앞으로 임박했습니다. 고객들은 기능 A.12를 보기 원해요”<br><strong>개발자:</strong> “매니저님, 그건 저희가 개발하기로 협의한 기능이 아니잖아요.”<br><strong>매니저:</strong> “알아요. 근데 고객들이 보고싶어합니다.”<br><strong>개발자:</strong> “(한숨) 알았어요. 그러면 기능 A.11이나 A.13은 데모에서 제외해야합니다. 시간이 부족해요”<br><strong>매니저:</strong> “절대 안됩니다. 고객들은 그 기능들도 보고싶어해요.”<br><strong>개발자:</strong> “이정도면 그냥 모든 기능을 다 보고싶다는거 아니에요? 이건 안된다고 이미 몇번이나 말씀드렸잖아요.”<br><strong>매니저:</strong> “미안합니다. 하지만 고객들이 마음을 바꾸지 않는걸요. 모든 기능들을 다 보고싶어해요.”<br><strong>개발자:</strong> “안됩니다 매니저님. 이건 진짜 안돼요.”<br><strong>매니저:</strong> “진짜 왜 그러는거에요? 고객들이 원한다구요. <em>시도</em>는 해볼 수 있잖아요?”<br><strong>개발자:</strong> “매니저님, 제가 공중부양을 하는걸 <em>시도</em>해볼순 있죠. 납을 금으로 바꾸려고 <em>시도</em>해볼 수도 있구요. 태평양을 수영해서 건너는걸 <em>시도</em>해볼 수도 있어요. 근데 제가 성공할거 같아요?”<br><strong>매니저:</strong> “이제는 말도 안되는 소리나 하고 있군요. 제가 <em>불가능</em>한걸 요구하는거도 아니잖아요.”<br><strong>개발자:</strong> “아뇨 매니저님. 지금 <em>불가능</em>한걸 요구하고 계셔요.”<br><strong>매니저:</strong> “(장난조로 웃으며, 뒤 돌아 자리를 떠나며) 개발자님 진짜 개그센스가 좋으시네요. 잘 하실 수 있는거 다 알아요. 조금만 더 고생 해주세요.”<br><strong>개발자:</strong> “(멀어지는 매니저를 향해 말하며) 진짜 안된다니까요? 이렇게 피하기만 하면 진짜 안 좋게 끝날거에요”</p>
</blockquote>
<p>“이제 개발자는 결정을 내려야합니다. 개발자는 이제 당연히 매니저가 CEO에게 새 개발 계획을 공유하지 않았을 것이라는걸 알아요. 아마 매니저는 CEO에게 개발자가 약속된 일정을 지키지 못했다고 얘기하겠죠. 여기서 개발자는 2가지 방법을 택할 수 있습니다”</p>
<p>“첫째는 매니저가 알아서 죽도록 놔두는 겁니다. 문제가 터졌을 때, 개발자는 지금까지의 위키 업데이트, 대화 메모, 주변 사람들의 증언등을 모아 ‘몇번이나 매니저에게 이야기했다고’ CEO에게 알리는 겁니다.”</p>
<p>“둘째는, 지금 바로 CEO에게 가서 이야기하는 겁니다. 물론 여기에 리스크가 있지만, 이 방법이 진정으로 팀플레이어가 되는 방법입니다. <strong>저기 멀리서 폭주기관차가 우리를 향해 오고있다는걸 당신만 인지하고 있다면, 혼자만 레일에서 내려오는것보다는, 모두가 도망칠 수 있게 알리는게 팀플레이어입니다</strong>.”</p>
<p> </p>
<blockquote>
<p>케이스 7:<br><strong>개발자:</strong> “매니저님, CEO님에게 개발 일정을 공유드렸나요? CEO님이 고객들에게 A.12 기능은 작동하지 않을 거라고 알렸나요?”<br><strong>매니저:</strong> “개발자님, 지난번에 그거 개발하기로 했잖아요.”<br><strong>개발자:</strong> “아뇨 매니저님, 안된다고 분명히 말씀드렸어요. 불가능하다고 말씀드렸고, 그 때 저희 얘기했던 회의록도 있어요.”<br><strong>매니저:</strong> “그랬죠, 근데 그래도 <em>시도</em>해보겠다고 했잖아요?”<br><strong>개발자:</strong> “그 때 대화 기억 안나요? 태평양 수영?”<br><strong>매니저:</strong> “(한숨) 개발자님. 이건 진짜 어쩔수 없이 그냥 하셔야해요. 어떤 수단과 방법을 써도 되니까, 이거만큼은 절 위해서라도 꼭 해주셔야해요.”<br><strong>개발자:</strong> “아뇨 매니저님. 제가 매니저님을 위해서 이걸 해야할 이유는 없어요. 제가 <em>해야하는건</em>, 매니저님이 직접 안하신다면, CEO님께 이 상황을 알리는거에요.”<br><strong>매니저:</strong> “진짜 말도 안되는 소리 하지 마세요. 안 그러실거잖아요.”<br><strong>개발자:</strong> “저도 그러고 싶진 않아요. 하지만 매니저님이 저를 그렇게 만들고 있잖아요.”<br><strong>매니저:</strong> “아, 제발”<br><strong>개발자:</strong> “매니저님. 그 기능들은 시간에 맞춰서 개발을 <em>못해요</em>. 이걸 제대로 알아주셔야해요. 저보고 일을 더 하라고 하지 마세요. 제가 어떻게 마법을 부려서 해낼거라고 믿는거 그만두셔야해요. 제대로 현실을 보고 CEO님에게 알리세요. <em>오늘요</em>.”<br><strong>매니저:</strong> “(크게 눈을 뜨며) 오늘?!”<br><strong>개발자:</strong> “네. 오늘요. 왜냐면 내일 CEO님과 제가 데모에 들어갈 기능을 리뷰하는 미팅을 잡아놨거든요. 내일 이 미팅 전에 말씀드릴거 아니면, 제가 직접 말씀드릴 수 밖에 없어요.”<br><strong>매니저:</strong> “당신 지금 이게 말이 된다고 생각해? 당신 밥통만 챙기면 다야? 일도 제대로 안하면서”<br><strong>개발자:</strong> “매니저님, 저 지금 <em>우리 둘 모두의 밥통</em>을 챙기는거거든요? 고객들이 다 모시고 그 앞에서 데모 터지는 꼴 보고 싶으세요?”</p>
</blockquote>
<p>“지금까지 개발자는 프로답게 행동했습니다. <strong>매니저가 개발자를 가스라이팅하고, 모욕하고, 업신여기고, 떄로는 빌면서 부탁을해도 현실만을 직시하며 ‘안됩니다’라고 얘기했습니다</strong>. 또, 개발자는 매니저가 가지고 있는 환상을 깨부셔주었습니다”.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>(충격!) OpenCV 빌드 벤치마크 - M1 (ARM) vs Workstation CPU (x86)</title>
    <url>/20220130-m1-opencv-build-sse/</url>
    <content><![CDATA[<h2 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTI4LW0xLW9wZW5jdi1idWlsZC8=">지난 글<i class="fa fa-external-link-alt"></i></span>에서 M1 칩에서 OpenCV 빌드를 해봤다.</p>
<p>생각보다 빠르게 빌드가 되길래, x86 CPU와 비교해서 벤치마크를 해보았다.</p>
<p> </p>
<h2 id="Native-CPU-가속-설정-빌드-시간"><a href="#Native-CPU-가속-설정-빌드-시간" class="headerlink" title="Native CPU 가속 설정 빌드 시간"></a>Native CPU 가속 설정 빌드 시간</h2><p>이 프로젝트에 대한 설정은 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTI4LW0xLW9wZW5jdi1idWlsZC8=">지난 글<i class="fa fa-external-link-alt"></i></span>과 동일하다.</p>
<p>아래 커맨드를 이용해서 빌드하는데 걸린 시간을 측정하였다. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">time ninja</span><br></pre></td></tr></table></figure>

<p>x86 CPU와 비교하기 위해 귀가해서 x86 워크스테이션 급 CPU로도 빌드를 해봤다.</p>
<p>아래 테이블에 사용한 장비들의 스펙을 작성해놨다.</p>
<div class="tabs" id="장비-비교"><ul class="nav-tabs"><li class="tab active"><a href="#장비-비교-1">M1 맥북</a></li><li class="tab"><a href="#장비-비교-2">AMD Threadripper 2950X 워크스테이션</a></li></ul><div class="tab-content"><div class="tab-pane active" id="장비-비교-1"><p>2021 MacBook Pro M1 Max<br>10 코어 CPU (8코어 메인, 2코어 보조)<br>64GB RAM<br>기본 스왑 메모리</p></div><div class="tab-pane" id="장비-비교-2"><p>데스크탑 워크스테이션<br>CPU - AMD Threadripper 2950X - 16코어 32쓰레드. 모든 코어 4.8GHz 오버클럭<br>RAM - 64GB, DDR4 3200 MHz (3400 MHz 오버클럭), 16-18-18-38 타이밍. 램카드 4개 모두 동일 타이밍.</p></div></div></div>

<p>동일한 셋팅으로, 뒤에 크롬 탭 몇개 켜있는거 빼고는 다른 작업은 하지 않았다.</p>
<p>결과는 다음과 같이 나왔다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># M1 Max</span></span><br><span class="line">ninja  1665.10s user 55.95s system 837% cpu 3:25.47 total</span><br><span class="line"></span><br><span class="line"><span class="comment"># x86 Workstation</span></span><br><span class="line">ninja  7395.31s user 300.40s system 2793% cpu 4:35.47 total</span><br></pre></td></tr></table></figure>

<p>M1 max는 실제 흐른 시간 <strong>56초</strong>. 전체 CPU 시간 1665초.<br>x86 워크스테이션은 실제 흐른시간 <strong>300초</strong>, 전체 CPU 시간 7395초.</p>
<p>…??????</p>
<p>노트북이 워크스테이션보다 빌드가 6배나 빠르게 나왔다.</p>
<p>물론 SSE/AVX의 종류도 많고 적용된 분야가 많아서 아무래도 빌드할 때 더 많은 링크가 들어갔겠고, ARM CPU에서는 이걸 빌드하지 않으니 절대적인 용량이 더 작을 수도 있겠다.</p>
<p>M1 Max에서 x86 크로스컴파일 빌드를 해보면 다른 결과가 나올 수도 있다는 점은 유의해둬야한다.</p>
<p>하지만 기본 빌드만 돌리는 단순 유저의 입장에서는 당연히 M1 맥북이 더 선호되겠다!!!</p>
<p> </p>
<hr>
<h2 id="x86-CPU-빌드로-크로스컴파일-벤치마크"><a href="#x86-CPU-빌드로-크로스컴파일-벤치마크" class="headerlink" title="x86 CPU 빌드로 크로스컴파일 벤치마크"></a>x86 CPU 빌드로 크로스컴파일 벤치마크</h2><p>그렇다면 둘 다 동일 코드 베이스로 빌드를 하면 얼마나 차이가 날까?</p>
<p>이 방법만이 정확하게 CPU를 비교할 수 있지 않을까 생각이 들었다.</p>
<p>x86 빌드로 기준을 맞춰서 빌드를 해보았다.</p>
<p>M1 칩에서는 CMake 설정을 건드려서 강제로 SSE/AVX를 빌드하게 해야한다. 가능한 모든 옵션을 빌드하게 했다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../install_sse -GNinja -DCPU_BASELINE=SSE3 -DCPU_DISPATCH=SSE4_1,SSE4_2,FP16,AVX,AVX2,AVX512-SKX ..</span><br></pre></td></tr></table></figure>

<p>…를 하면 될 줄 알았는데, 컴파일러가 SSE/AVX 관련을 지원하지 않는다고 다 스킵해버리네? ㅠㅠ… 아직까지 제대로 성공하지 못했다.</p>
<p>다음과 같은 로그가 나타난다. 심지어 x86 CPU를 에뮬레이션 하는 Rosetta 까지 사용해도 같은 로그가가 난다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- Optimization SSE3 is not available, skipped                                                                            </span><br><span class="line">-- Dispatch optimization SSE4_1 is not available, skipped                                                                 </span><br><span class="line">-- Dispatch optimization SSE4_2 is not available, skipped                                                                 </span><br><span class="line">-- Dispatch optimization AVX is not available, skipped                                                                    </span><br><span class="line">-- Dispatch optimization AVX2 is not available, skipped                                                                   </span><br><span class="line">-- Dispatch optimization AVX512-SKX is not available, skipped  </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
      </categories>
      <tags>
        <tag>Apple</tag>
        <tag>OpenCV</tag>
        <tag>Macbook Pro</tag>
        <tag>M1</tag>
        <tag>ARM</tag>
      </tags>
  </entry>
  <entry>
    <title>전문연구요원 논산 생활 꿀팁</title>
    <url>/20220130-nonsan/</url>
    <content><![CDATA[<h2 id="먼저-챙겨보세요"><a href="#먼저-챙겨보세요" class="headerlink" title="먼저 챙겨보세요"></a>먼저 챙겨보세요</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tydGEyL2F3ZXNvbWUtbm9uc2Fu">Awesome Nonsan 링크<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tydGEyL2F3ZXNvbWUtbm9uc2FuL2lzc3Vlcy8xOA==">코로나19 기간 논산 훈련소 생활 정보<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>어썸한 readme 레포입니다. 논산은 어썸하지 않습니다.</p>
<p>왠만한 정보는 여기서 다 얻을 수 있습니다. 이 글의 나머지 컨텐츠는 생활 꿀팁만 이야기합니다.</p>
<p> </p>
<hr>
<h2 id="꿀팁"><a href="#꿀팁" class="headerlink" title="꿀팁"></a>꿀팁</h2><ol>
<li>신분증, 입영통지서, 나라사랑카드 챙겨가세요.</li>
<li>체크/신용카드 챙겨가세요. 현금은 필요없어요.</li>
<li>휴대폰/담배는 꼭 제출하세요.</li>
<li>보조배터리 완충해서 챙겨가세요. 하지만 꼭 제출하세요.</li>
<li>평소 드시는 약 있으면 챙기세요.</li>
<li>비염 있거나 목이 예민하신 분들은 코 스프레이 / 용각산 챙겨가세요.</li>
<li>피부 쉽게 트는 분들, 겨울에 가시는 분들은 로션 챙겨가세요.</li>
<li> 겨울에는 기침약, 타이레놀 꼭 챙겨가세요. </li>
<li> 레모나, 카누스틱, 우표 최대한 많이 챙겨가세요.</li>
<li>우표는 익일특급 선납등기라벨로 챙기세요. 선납우표 안쓰면 퇴소하고 얼굴 볼 때 쯤에 편지도 같이 도착할거에요.</li>
<li>샴푸는 올인원으로 챙겨가세요. 그게 편해요.</li>
<li>밤귀 밝으신 분들은 3M 귀마개 챙겨가세요. 다들 코 엄청 골아요.</li>
<li>여자친구 있으신 분들은 여자친구 사진 챙겨가시고, 편지 보낼 주소 받아오세요. 우편번호도 꼭 같이 받아오세요.</li>
<li>심심하면 연락 받아줄 친구들한테 전화할수도 있다고 미리 얘기해두고, 전화번호 적어가세요.</li>
<li>가족의 전화번호/주소도 적어가세요 (당연히!)</li>
<li>학구파라면 논문/전공책 챙겨가세요. 쉬고싶다면 평소에 안 읽어본 분야의 책 5~7권 정도 챙겨가세요.</li>
<li>쿠션이 두툼히 들어간 팔꿈치 &amp; 무릎 보호대 챙겨가세요.</li>
<li>칫솔 치약 챙기세요. 거기서 주는거 별로에요.</li>
<li>두루마리 휴지 2개, 물티슈, 면봉 챙기세요.</li>
<li>텀블러 하나 챙겨가세요.</li>
<li>마스크 챙기세요.</li>
<li>드로즈 속옷 입는 분들은 속옷 챙겨가세요.</li>
<li>야광 들어오는 손목시계 챙겨가세요. 후레쉬도 나오면 좋아요. </li>
<li>위장크림 챙기지 마세요. 요즘 안써요.</li>
<li>훈련소 앞에서 파는 물건은 사지 마세요. 최장 수명 4시간이에요.</li>
<li>머리는 적당히 깎고 들어가세요.</li>
<li>머리가 좀 긴 것 같다 싶으면, 퇴소하고 얼마 후 상견례가 있다고 하세요. 봐주실 수도요.</li>
<li>입소 첫날 첫 자유시간에 꼭 제일 먼저 재밌는 책 찾아서 가져오세요. </li>
<li>지인들한테 편지 많이 보내달라고 얘기해두세요. 편지 보낼 때 인증 과정이 엄청 귀찮으니 잘 부탁하세요 ㅎㅎ</li>
<li>편지로 뉴스 받아보면 좋아요.</li>
<li>편지로 주식은 받아보지 마세요. 어차피 아무것도 못해요 ㅎㅎ…</li>
<li>결혼 하신 분들은 특별히 전화 찬스가 더 많을거에요.</li>
<li>전화 할 때는 꼭 돈 충전해서 전화하세요. 그냥 전화하면 10배 가격이에요.</li>
<li>설거지 힘들어요. 설거지 중 물 퍼는 사람 있는데 그건 가능하면 꼭 피하세요…</li>
<li>공기청정기 청소 개꿀이에요.</li>
<li>훈련 시 퇴소 규정 잘 확인하시고 (X 시간 이상 열외 시 퇴소), 열외 적절하게 잘 쓰세요. 몸 다치면 나만 손해에요.</li>
<li>차등제도 교육시간 이수에요. 적절하게 잘 쓰세요.</li>
<li>여름에는 훈련할 때 땀 많이 흘리고 피부가 많이 탈거에요.</li>
<li>겨울에는 무조건 감기가 돌아서 훈련기간 내내 감기에 걸려있을거에요.</li>
<li>종교 행사에서는 종교 상관없이 모두 동일한 부식줘요 (사이다 + 초코파이2개). </li>
<li>불교 행사에 요즘 로터스 안와요 ㅋㅋ 코로나 때문에…</li>
<li>평소 좋아하는 노래 있으면 가사 적어가세요. 거기 가면 가사 기억이 안나요. (어차피 군가 부르게 되겠지만요)</li>
<li>1째 주는 덤덤해요. 중간에는 몸이 피곤하기 쉬우니 다치지 않게 조심하세요. 마지막 주는 시간이 남아돌거니 책 많이 읽고 낮잠 많이 자고 푹 쉬세요.</li>
<li>무엇보다 몸 아끼시고 조심히 다녀오세요</li>
</ol>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>전문연구요원</tag>
        <tag>논산훈련소</tag>
      </tags>
  </entry>
  <entry>
    <title>Stanford CS193P - Developing Applications for iOS using SwiftUI 레퍼런스</title>
    <url>/20220205-CS193P/</url>
    <content><![CDATA[<h2 id="Why-Swift"><a href="#Why-Swift" class="headerlink" title="Why Swift?"></a>Why Swift?</h2><p>아이폰에 컴퓨터 비전 앱을 만들어서 얹어보고싶다.</p>
<h2 id="Youtube-Link"><a href="#Youtube-Link" class="headerlink" title="Youtube Link"></a>Youtube Link</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/bqu6BquVi2M" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.6 Swift</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>MacOS</tag>
        <tag>Apple</tag>
        <tag>Stanford</tag>
        <tag>CS193P</tag>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title>Go 언어 강의 레퍼런스</title>
    <url>/20220206-go-lecture/</url>
    <content><![CDATA[<h2 id="Go-를-보는-이유"><a href="#Go-를-보는-이유" class="headerlink" title="Go 를 보는 이유?"></a>Go 를 보는 이유?</h2><p>(요즘 자꾸 여러 언어를 기웃기웃하는거 같은데… ㅋㅋ)</p>
<p>나는 대부분의 chore 스크립트를 파이썬으로 작성하고 있다.</p>
<p>Sequential한 작업에는 꽤 좋은 편이고, matplotlib과 같은 시각화 라이브러리와 엮기 좋아서였다.</p>
<p>근데 스크립트가 꽤 길어지면서, 동시에 진행될 수 있는 task들이 분명히 있는데 파이썬의 특성 상 sequential하게 돌아가야한다는 점이 굉장히 아쉬웠다.</p>
<p>(물론 파이썬도 멀티쓰레딩이 가능하다고 하지만… 듣기로는 ‘진짜 멀티쓰레딩’이 아니라고 들었다.)</p>
<p>이런 부분은 Go로 짜서 빠르게 멀티 쓰레드로 끝내버리면 어떨까…! 라는 생각을 하면서 Go를 기웃거리게 된다.</p>
<h2 id="링크"><a href="#링크" class="headerlink" title="링크"></a>링크</h2><p><span class="exturl" data-url="aHR0cHM6Ly9ub21hZGNvZGVycy5jby9nby1mb3ItYmVnaW5uZXJz">https://nomadcoders.co/go-for-beginners<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>딥러닝 면접 질문 모음 (Shlomo Kashani 책 레퍼런스)</title>
    <url>/20220207-deep-learning-interview/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDEuMDA2NTAucGRm">https://arxiv.org/pdf/2201.00650.pdf<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="책의-목표"><a href="#책의-목표" class="headerlink" title="책의 목표"></a>책의 목표</h2><ul>
<li>석사/박사를 막 졸업한 학생들이 취직을 할 때 보게되는 딥러닝 관련 기술면접 질문들 모음<ul>
<li>혼자 공부할 때도 굉장히 좋을 듯</li>
</ul>
</li>
</ul>
<h2 id="목차"><a href="#목차" class="headerlink" title="목차"></a>목차</h2><ul>
<li>난이도: 어린이집<ul>
<li>Logistic regression<ul>
<li>예시 - Sigmoid, Logit function &amp; entropy</li>
</ul>
</li>
<li>Probabilistic programming &amp; Bayesian DL<ul>
<li>예시 - Expectation &amp; variance, Bayes rule, Maximum Likelihood estimation</li>
</ul>
</li>
</ul>
</li>
<li>난이도: 고등학교<ul>
<li>Information theory<ul>
<li>예시 - Shannon’s entropy, KL Divergence</li>
</ul>
</li>
<li>Deep Learning: Calculus, Algorithmic differentitation<ul>
<li>예시 - Gradient descent &amp; Backpropagation, Partial derivatives, Activation functions</li>
</ul>
</li>
</ul>
</li>
<li>난이도: 학부생<ul>
<li>Deep Learning: NN Ensembles<ul>
<li>예시 - Bagging / Boosting / Stacking, Snapshot Ensembling, Multi-model Ensembling</li>
</ul>
</li>
<li>Deep Learning: CNN feature extraction<ul>
<li>예시 - Fine-tuning CNNs, Neural Style transfer</li>
</ul>
</li>
<li>Deep Learning<ul>
<li>예시 - Convolution and correlation, Training &amp; hyperparameters, Optimization &amp; loss</li>
</ul>
</li>
</ul>
</li>
<li>Mock exam</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Software Engineering at Google 책 레퍼런스</title>
    <url>/20220207-software-engineering-at-google/</url>
    <content><![CDATA[

<h2 id="책-링크"><a href="#책-링크" class="headerlink" title="책 링크"></a>책 링크</h2><p>O’Reily 에서 출판한 책이지만, Google Abseil 프레임워크 웹사이트에서 PDF를 공개 링크해두었다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hYnNlaWwuaW8vcmVzb3VyY2VzL3N3ZV9hdF9nb29nbGUuMi5wZGY=">https://abseil.io/resources/swe_at_google.2.pdf<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="이-책을-추천하는-이유"><a href="#이-책을-추천하는-이유" class="headerlink" title="이 책을 추천하는 이유"></a>이 책을 추천하는 이유</h2><p>신입 / 주니어 엔지니어들이 읽으면 좋을 것 같다.</p>
<p>개발팀에서 사용하기 좋은 best practices가 다 들어있다. (특정 부분에 있어서 의견이 있을 수 있지만 이 부분은 선호도의 차이라고 본다)</p>
<p>본인의 회사에서 적용되지 않은 부분에 대해 고민하고, 필요 시 도입을 해보는게 좋을 것 같다.</p>
<p>목차는 다음과 같다. 신입/주니어가 꼭 읽었으면 하는 부분에는 강조를 했다.</p>
<ol>
<li>소프트웨어 엔지니어링이란?</li>
<li><strong>팀원들과 함께 일을 잘 하는 방법</strong></li>
<li>지식 공유</li>
<li>모두를 위한 엔지니어링</li>
<li>팀을 이끄는 방법</li>
<li>스케일을 키우면서 팀을 이끄는 방법</li>
<li>생산성을 측정하는 방법</li>
<li><strong>스타일 가이드 및 규칙</strong></li>
<li><strong>코드 리뷰</strong></li>
<li><strong>도큐먼트 (개발자 문서)</strong></li>
<li><strong>테스트</strong></li>
<li><strong>유닛 테스트</strong></li>
<li>테스트 더블</li>
<li>Larger test</li>
<li>사용 중단</li>
<li><strong>버전 컨트롤 (VCS) 및 브랜치 전략</strong></li>
<li>코드 탐색</li>
<li>빌드 시스템</li>
<li>구글의 코드 리뷰 방식</li>
<li><strong>정적 분석</strong></li>
<li><strong>디펜던시 핸들링</strong></li>
<li>대규모 코드 변환 작업</li>
<li><strong>CI</strong></li>
<li>CD</li>
<li>Compute As a Service</li>
</ol>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Software Engineering at Google</tag>
      </tags>
  </entry>
  <entry>
    <title>(0) Install Golang on M1 Mac</title>
    <url>/20220208-go-1/</url>
    <content><![CDATA[<h2 id="Install-Go"><a href="#Install-Go" class="headerlink" title="Install Go"></a>Install Go</h2><p>Go 웹사이트에서 가장 최신의 Golang을 다운받자.</p>
<p>M1 Mac으로 들어가도 왠진 모르겠지만 AMD64를 추천한다. 이거는 x86 아키텍처 CPU용이니 받지 말자.</p>
<p>대신, <span class="exturl" data-url="aHR0cHM6Ly9nby5kZXYvZGwv">Other downloads<i class="fa fa-external-link-alt"></i></span>로 들어가서 <code>goX.XX.X.darwin-arm64.pkg</code> 을 받자. X는 버전 넘버이다.</p>
<p>Darwin은 MacOS를 의미한다. arm64는 M1의 아키텍처를 의미한다. pkg는 인스톨러의 파일 익스텐션이다.</p>
<p> </p>
<h2 id="Go-작업공간-만들기"><a href="#Go-작업공간-만들기" class="headerlink" title="Go 작업공간 만들기"></a>Go 작업공간 만들기</h2><p><code>/Users/유저이름/</code>으로 이동한다. 이후, 아래의 커맨드를 입력해서 기본적인 Go 작업공간을 만든다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /Users/유저이름</span><br><span class="line"></span><br><span class="line">mkdir go &amp;&amp; <span class="built_in">cd</span> go</span><br><span class="line"></span><br><span class="line">mkdir bin pkg src</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> src</span><br><span class="line">mkdir github.com</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> github.com</span><br><span class="line">mkdir 깃허브아이디</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> 깃허브아이디</span><br><span class="line">mkdir go-tutorial</span><br></pre></td></tr></table></figure>

<p>위와 같은 커맨드를 통해 go-tutorial이라는 프로젝트를 생성했다.</p>
<p>Go는 파이썬이나 javascript와는 다르게, 원하는 곳에 프로젝트를 생성할 수 없다. 대신 go 폴더 밑에 모든 프로젝트를 관리한다.</p>
<p> </p>
<h2 id="Hello-go-world"><a href="#Hello-go-world" class="headerlink" title="Hello go world!"></a>Hello go world!</h2><p>아래 커맨드를 입력해서 <code>main.go</code> 파일을 만들고 VSCode를 연다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> go-tutorial</span><br><span class="line"></span><br><span class="line">touch main.go</span><br><span class="line"></span><br><span class="line">code .</span><br></pre></td></tr></table></figure>

<p>VSCode에서 go extension을 설치해준다.</p>
<p>이후, 아래의 스크립트를 <code>main.go</code> 파일에 입력한다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">fmt.Println(<span class="string">"Hello, Go World!"</span>)</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>이후, 터미널을 열어 <code>go run main.go</code>를 입력해서 실행되는 것을 본다.</p>
<p> </p>
<h2 id="Hello-go-world-분석"><a href="#Hello-go-world-분석" class="headerlink" title="Hello go world 분석"></a>Hello go world 분석</h2><p><code>package main</code>을 선언함으로써 이 파일이 main문을 가지고 있다는 것을 표기한다.</p>
<p>main문이 있으면, 이 파일은 컴파일되어 실행가능한 형태가 된다.</p>
<p>main문이 없으면, 라이브러리 형태가 된다. </p>
<p>라이브러리 형태로 만들고 싶다면 <code>package 라이브러리이름</code> 으로 만들면 된다.</p>
<p><code>import "fmt"</code>를 사용해서 fmt 라이브러리를 불러온다. fmt 라이브러리는 Go의 기본 라이브러리 중 하나이다ㅣ.</p>
<p>fmt.Println을 통해 string 출력을 한다. Println은 print line의 줄인 말이다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>(1) Go의 기본 - 패키지, 변수, 함수</title>
    <url>/20220208-go-2/</url>
    <content><![CDATA[<h2 id="Package-만들기"><a href="#Package-만들기" class="headerlink" title="Package 만들기"></a>Package 만들기</h2><p>패키지를 만들려면 폴더를 만들고, 그 안에 go 파일을 만든다.</p>
<p>패키지 이름을 선언하고, 구현체를 만든다.</p>
<p>main.go에서 해당 패키지를 부를 때는 import에 경로를 지정해준다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// samplePackage/samplePackage.go</span></span><br><span class="line"><span class="keyword">package</span> samplePackage</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sayHello</span><span class="params">()</span></span>{</span><br><span class="line">    fmt.Println(<span class="string">"Hello!"</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SayBye</span><span class="params">()</span></span>{</span><br><span class="line">    fmt.Println(<span class="string">"Bye!"</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// main.go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line">	<span class="string">"github.com/changh95/go-tutorial/samplePackage"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	fmt.Println(<span class="string">"Hello, Go World!"</span>)</span><br><span class="line">	samplePackage.SayBye()</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<h2 id="Public-Private-함수"><a href="#Public-Private-함수" class="headerlink" title="Public / Private 함수"></a>Public / Private 함수</h2><p>Public 함수는 대문자로 시작한다.</p>
<p>Private 함수는 소문자로 시작한다.</p>
<h2 id="변수"><a href="#변수" class="headerlink" title="변수"></a>변수</h2><p>Go 는 Typed language이다. </p>
<p>아래와 같은 타입을 사용할 수 있다.</p>
<ul>
<li>bool</li>
<li>int8, uint8, int16, uint16, int32, uint32, int64, uint64, int, uint, uintptr</li>
<li>float32, float64.</li>
<li>complex64, complex128.</li>
<li>string</li>
</ul>
<p>const와 non-const도 존재한다. const는 <code>const</code>로 시작하고, non-const는 <code>var</code>로 시작한다.</p>
<p><code>var</code>의 또다른 표기법으로는 <code>:=</code>가 있다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	<span class="keyword">const</span> str_const <span class="keyword">string</span> = <span class="string">"Const"</span></span><br><span class="line">	<span class="keyword">var</span> str_non_const <span class="keyword">string</span> = <span class="string">"non-const"</span></span><br><span class="line">	str_non_const2 := <span class="string">"non-const2"</span></span><br><span class="line"></span><br><span class="line">    fmt.Println(str_const)</span><br><span class="line">	fmt.Println(str_non_const)</span><br><span class="line">	fmt.Println(str_non_const2)</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="함수"><a href="#함수" class="headerlink" title="함수"></a>함수</h2><p>Go의 함수는 아래와 같이 생겼다.</p>
<p><code>func</code> 키워드로 시작하며, arguments마다 type을 작성하고 return type도 작성한다.</p>
<p>Multiple returns가 가능하다. Multiple returns에서 어떤 return 값을 무시하려면 <code>_</code>를 사용하면 된다.</p>
<p>Variadic argument를 받고 싶으면 input argument type으로 <code>...type</code>을 사용하면 된다.</p>
<p>Naked return도 가능하다.</p>
<p>defer라는 기능도 있다. 이 기능은 함수가 return하고 난 후에 작동하는 기능을 정의하는 것이다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"fmt"</span></span><br><span class="line">	<span class="string">"strings"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">multiply</span><span class="params">(a <span class="keyword">int</span>, b <span class="keyword">int</span>)</span> <span class="title">int</span></span> {</span><br><span class="line">	<span class="keyword">return</span> a * b</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">lenAndUpper</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">int</span>, <span class="keyword">string</span>)</span></span> {</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">len</span>(str), strings.ToUpper(str)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">lenAndUpperNakedReturn</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(length <span class="keyword">int</span>, upper_case <span class="keyword">string</span>)</span></span>{</span><br><span class="line">    length = <span class="built_in">len</span>(str)</span><br><span class="line">    upper_case = strings.ToUpper(str)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">lenAndUpperNakedReturnWithDefer</span><span class="params">(str <span class="keyword">string</span>)</span> <span class="params">(length <span class="keyword">int</span>, upper_case <span class="keyword">string</span>)</span></span>{</span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(<span class="string">"I'm' done"</span>)</span><br><span class="line"></span><br><span class="line">    length = <span class="built_in">len</span>(str)</span><br><span class="line">    upper_case = strings.ToUpper(str)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">repeatMe</span><span class="params">(str ...<span class="keyword">string</span>)</span></span> {</span><br><span class="line">	fmt.Println(str)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">	a := <span class="number">2</span></span><br><span class="line">	b := <span class="number">3</span></span><br><span class="line"></span><br><span class="line">	answer := multiply(a, b)</span><br><span class="line">	fmt.Println(answer)</span><br><span class="line"></span><br><span class="line">	str := <span class="string">"Hello"</span></span><br><span class="line">	<span class="built_in">len</span>, strUpper := lenAndUpper(str)</span><br><span class="line">	fmt.Println(<span class="built_in">len</span>, strUpper)</span><br><span class="line"></span><br><span class="line">	len2, _ := lenAndUpper(str)</span><br><span class="line">	fmt.Println(len2)</span><br><span class="line"></span><br><span class="line">    repeatMe(<span class="string">"Hi"</span>, <span class="string">"Hello"</span>, <span class="string">"Bye"</span>)</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>(2) Go의 기본 - For loop, range, ...args</title>
    <url>/20220210-go-3/</url>
    <content><![CDATA[<h2 id="For-loop"><a href="#For-loop" class="headerlink" title="For loop"></a>For loop</h2><p>Golang에는 <code>for</code>만 있다. for each, for in, for of 등등은 없다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">addInForLoop</span><span class="params">(numbers ...<span class="keyword">int</span>)</span> <span class="title">int</span></span> {</span><br><span class="line">	total := <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(numbers); i++ {</span><br><span class="line">		total += numbers[i]</span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">return</span> total</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="Range"><a href="#Range" class="headerlink" title="Range"></a>Range</h2><p>range를 통해 array 에 for loop를 적용하여 iteration을 할 수 있다. range를 통해 index를 받을 수 있다. 실제 값을 받고 싶다면 <code>index, value</code>를 받으면 된다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">addInForLoop</span><span class="params">(numbers ...<span class="keyword">int</span>)</span> <span class="title">int</span></span> {</span><br><span class="line">	total := <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> _, number := <span class="keyword">range</span> numbers {</span><br><span class="line"></span><br><span class="line">		total += number</span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">return</span> total</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>(3) Go의 기본 - if-else, switch</title>
    <url>/20220210-go-4/</url>
    <content><![CDATA[<h2 id="if-else"><a href="#if-else" class="headerlink" title="if-else"></a>if-else</h2><p>기본적인 if-else는 다음과 같다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">canDrinkAlcohol</span><span class="params">(age <span class="keyword">int</span>)</span> <span class="title">bool</span></span> {</span><br><span class="line">	<span class="keyword">if</span> age &lt; <span class="number">18</span> {</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="Go만의-특성-if를-할-때-variable-생성하기"><a href="#Go만의-특성-if를-할-때-variable-생성하기" class="headerlink" title="Go만의 특성 - if를 할 때 variable 생성하기"></a>Go만의 특성 - if를 할 때 variable 생성하기</h2><p>go에서는 if를 할 때 변수를 생성할 수 있다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">canDrinkAlcoholKoreanAge</span><span class="params">(western_age <span class="keyword">int</span>)</span> <span class="title">bool</span></span> {</span><br><span class="line">	<span class="keyword">if</span> korean_age := western_age + <span class="number">2</span>; korean_age &lt; <span class="number">18</span> {</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<h2 id="Switch"><a href="#Switch" class="headerlink" title="Switch"></a>Switch</h2><p>C나 java 처럼 switch문도 사용 가능하다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">canDrinkAlcoholKoreanAgeSwitch</span><span class="params">(western_age <span class="keyword">int</span>)</span> <span class="title">bool</span></span> {</span><br><span class="line">	<span class="keyword">switch</span> korean_age := western_age + <span class="number">2</span>; korean_age {</span><br><span class="line">	<span class="keyword">case</span> <span class="number">10</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">	<span class="keyword">case</span> <span class="number">18</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">	}</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>(4) Go의 기본 - Pointer</title>
    <url>/20220210-go-5/</url>
    <content><![CDATA[<h2 id="Pointer"><a href="#Pointer" class="headerlink" title="Pointer"></a>Pointer</h2><p>C의 포인터와 상당히 비슷하다.</p>
<p>a라는 변수를 만들고 <code>b := a</code>를 하면 copy가 일어난다. copy가 나지 않게 하려면 메모리 주소를 참조해야한다.</p>
<p>보통 굉장히 큰 자료구조를 넘길 때 pointer를 사용해서 넘긴다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span>{</span><br><span class="line">    a := <span class="number">2</span></span><br><span class="line">    b := &amp;a</span><br><span class="line">    fmt.Println(a, *b)</span><br><span class="line"></span><br><span class="line">    *b = <span class="number">20</span></span><br><span class="line">    fmt.Println(a, *b)</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>(5) Go의 기본 - 자료 구조 - Arrays &amp; slices, Map, Structs</title>
    <url>/20220210-go-6/</url>
    <content><![CDATA[<h2 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h2><p>크기가 정해져있는 array를 만들 때 사용한다</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">names := [<span class="number">3</span>]<span class="keyword">string</span>{<span class="string">"Hi"</span>, <span class="string">"Hello"</span>, <span class="string">"Bye"</span>}</span><br><span class="line">fmt.Println(names, names[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="Slice"><a href="#Slice" class="headerlink" title="Slice"></a>Slice</h2><p>C++의 <code>std::vector</code>와 비슷하다. 크기가 정해져있지 않은 array이며, 원하는대로 element를 계속 추가할 수 있다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">names_slice := []<span class="keyword">string</span>{<span class="string">"Hi"</span>, <span class="string">"Hello"</span>, <span class="string">"Bye"</span>}</span><br><span class="line">names_slice = <span class="built_in">append</span>(names_slice, <span class="string">"Hi again!"</span>)</span><br><span class="line">fmt.Println(names_slice)</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p>Hash table 형태로 저장할 수 있는 자료구조이다. Python에서 dictionary, 또는 C++ 에서 std::unordered_map과 비슷하다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">    name_and_age := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span>{<span class="string">"Harry"</span>:<span class="number">15</span>,<span class="string">"Ron"</span>:<span class="number">16</span>}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, value := <span class="keyword">range</span> name_and_age {</span><br><span class="line">    fmt.Println(key, value)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, value := <span class="keyword">range</span> name_and_age {</span><br><span class="line">    fmt.Println(value)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, _ := <span class="keyword">range</span> name_and_age {</span><br><span class="line">    fmt.Println(key)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="Struct"><a href="#Struct" class="headerlink" title="Struct"></a>Struct</h2><p>C++에서 사용하는 struct처럼 만들 수 있다. 하지만 default constructor가 없다.</p>
<p>method도 구현할 수 있다.</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> person <span class="keyword">struct</span> {</span><br><span class="line">name <span class="keyword">string</span></span><br><span class="line">age <span class="keyword">int</span></span><br><span class="line">favourite_food []<span class="keyword">string</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> {</span><br><span class="line">    favourite_food := []<span class="keyword">string</span>{<span class="string">"ramen"</span>, <span class="string">"kimchi"</span>}</span><br><span class="line">    </span><br><span class="line">    harry := person{<span class="string">"harry"</span>, <span class="number">15</span>, favourite_food}</span><br><span class="line">    fmt.Println(harry)</span><br><span class="line">    fmt.Println(harry.name)</span><br><span class="line"></span><br><span class="line">    ron := person{name: <span class="string">"ron"</span>, age: <span class="number">16</span>, favourite_food: favourite_food}</span><br><span class="line">    fmt.Println(ron)</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.7 Go</category>
        <category>Nomad Coder 강의</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Go</tag>
        <tag>Nomad Coder</tag>
      </tags>
  </entry>
  <entry>
    <title>내가 좋아하는 빈브라더스 커피 블로그 레퍼런스</title>
    <url>/20220214-coffee-bean-brothers/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9iZWFuYnJvdGhlcnMub29weS5pby8=">https://beanbrothers.oopy.io/<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="빈브라더스"><a href="#빈브라더스" class="headerlink" title="빈브라더스"></a>빈브라더스</h2><p>제일 좋아하는 국내 카페 브랜드 Top 3 중 하나이다 (Top 3 에서 우위를 가릴 수 없다 ㅎㅎ).</p>
<p>이 3개의 카페 브랜드는 각각 1. 에스프레소 류, 2. 블렌드/큐레이션, 3. 로스팅/카페인테리어 에 대한 특성을 가지고 있는데, 빈브라더스는 이 중 ‘블렌드 커피 / 커피 큐레이션’에 대해 가장 잘 하는 곳이라고 생각한다.</p>
<p>커피 입문도 빈브라더스의 블랙수트 드립백 커피로 시작하게 되었고, 종종 마포점에 찾아가 깔끔한 롱블랙을 즐기러 간다 ㅎㅎ. (강남점에도 있었는데, 사람이 엄청나게 많아지면서 분위기가 해이해지더니 어느날 사라지고 말았다…!)</p>
<p> </p>
<h2 id="블렌드-커피"><a href="#블렌드-커피" class="headerlink" title="블렌드 커피"></a>블렌드 커피</h2><p>예전부터 항상 보이던 라인으로는 1. 블랙수트, 2. 화이트벨벳 이 있다.</p>
<p>블랙수트는 고소하고 묵직한 편, 화이트벨벳은 산미가 적당하게 있는 편이며, 둘 다 과하지 않아 커피에 입문하는 사람들에게 이 둘의 차이를 알려주기 딱 좋다고 생각한다.</p>
<p>어떻게 매번 이렇게 맛이 똑같을까? 싶을 정도로 퀄리티 컨트롤을 잘하는거 같다 (그게 아니면 그냥 내가 둔한걸수도 ㅋㅋ)</p>
<p>드립백 정기배송으로 입문하면서 항상 아침을 열어주는(?) 느낌이 좋았다.</p>
<p> </p>
<h2 id="큐레이션"><a href="#큐레이션" class="headerlink" title="큐레이션"></a>큐레이션</h2><p>최근에는 온라인 구매가 가능한 원두의 폭이 엄청나게 늘어난 듯!</p>
<p>코로나 시대로 들어가면서 온라인 판매쪽에 다양한 시도를 하는 것 같다.</p>
<p>싱글오리진 커피의 종류가 엄청 많아졌고, 굉장히 특이한+특별한 취향의 사람들을 위한 커피도 있는 것 같다. (물론 호불호가 갈릴 수 있다)</p>
<p>뉴스레터를 통해서 로스팅, 브루잉 등등 꿀팁을 많이 알려주기도 하고, 커피 이론도 배울 수 있어서 좋다.</p>
<p>무엇보다 뉴스레터의 톤이 굉장히 나긋나긋하면서 편안하게 스토리텔링을 하는데, 이런 부분이 뭔가 느긋하게 커피 한잔 마시면서 아침을 여는(?) 느낌과 비슷해서 더 좋은 것 같다.</p>
<p>커피에 대해 열정이 넘치는 분들도 있는 것 같아 더 신뢰가 간다.</p>
<h2 id="위에-올린-링크"><a href="#위에-올린-링크" class="headerlink" title="위에 올린 링크"></a>위에 올린 링크</h2><p>빈브라더스에서 보내는 뉴스레터를 아카이빙 해놓은거라던지, 현재 판매중인 원두에 대한 사내 바리스타들의 브루잉 방법 + 후기 등을 정리해둔 내용이다.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>Coffee</tag>
        <tag>Life</tag>
        <tag>빈브라더스</tag>
      </tags>
  </entry>
  <entry>
    <title>AAAI 2021 - Graph Neural Networks - Models and Applications 레퍼런스</title>
    <url>/20220214-gnn-tutorial/</url>
    <content><![CDATA[<h2 id="Table-of-contents"><a href="#Table-of-contents" class="headerlink" title="Table of contents"></a>Table of contents</h2><ol>
<li>Introduction<ol>
<li>Graphs and Graph Structured Data</li>
<li>Tasks on Graph Structured Data</li>
<li>Graph neural networks</li>
</ol>
</li>
<li>Foundations<ol>
<li>Basic Graph Theory</li>
<li>Graph Fourier Transform</li>
</ol>
</li>
<li>Models<ol>
<li>Spectral-based GNN layers</li>
<li>Spatial-based GNN layers</li>
<li>Pooling Schemes for Graph-level Representation Learning</li>
<li>Attacks and Robustness of Graph Neural Networks</li>
<li>Deeper Graph Neural Networks</li>
<li>Scalable Learning For Graph Neural Networks</li>
<li>Self-supervised Learing for Graph Neural Networks</li>
</ol>
</li>
<li>Applications<ol>
<li>Recommendation</li>
</ol>
</li>
</ol>
<h2 id="링크"><a href="#링크" class="headerlink" title="링크"></a>링크</h2><p><span class="exturl" data-url="aHR0cHM6Ly93ZWIubmppdC5lZHUvfnltMzI5L3R1dG9yaWFscy9hYWFpMjAyMS8=">https://web.njit.edu/~ym329/tutorials/aaai2021/<i class="fa fa-external-link-alt"></i></span></p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/3vfAiEv3wZ8" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Graph Neural Network</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Szeliski - Computer Vision - Algorithms and Applications (2nd ed) 책 링크</title>
    <url>/20220214-szeliski-book/</url>
    <content><![CDATA[<h2 id="바이블"><a href="#바이블" class="headerlink" title="바이블"></a>바이블</h2><p>드디어 2nd Ed의 편집이 끝났다.</p>
<p>그러면 당연히 읽어야지!</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9zemVsaXNraS5vcmcvQm9vay8=">https://szeliski.org/Book/<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
        <tag>Szeliski</tag>
      </tags>
  </entry>
  <entry>
    <title>SOLID 디자인 법칙</title>
    <url>/20220216-solid-principles/</url>
    <content><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>프레임워크를 바닥부터 새로 짜게 되었다.</p>
<p>이번엔 제대로 짜고 싶어서 공부를 해보다보니, Software Development, Clean Coders 정리 책에서 ‘개발자라는 사람이 SOLID 원칙도 모르는거 말이 안된다’ 하는걸 보고 충격먹어서 공부하기 시작했다.</p>
<p> </p>
<hr>
<h2 id="SOLID-원칙"><a href="#SOLID-원칙" class="headerlink" title="SOLID 원칙"></a>SOLID 원칙</h2><ul>
<li>S : 단일 책임 원칙 (Single Responsibility Principle, SRP)</li>
<li>O : 열림-닫힘 원칙 (Open-Closed Principle, OCP)</li>
<li>L : 리스코프 치환 원칙 (Liskov Subsitution Principle, LSP)</li>
<li>I : 인터페이스 분리 원칙 (Interface Segregation Principle, ISP)</li>
<li>D : 의존성 역전 원칙 (Dependency Inversion Principle, DIP)</li>
</ul>
<p>이 원칙들은 2000년대 초 Clean Code의 저자인 Robert C. Martin에 의해 소개되었다.</p>
<p> </p>
<hr>
<h2 id="단일-책임-원칙-Single-Responsibility-Principle-SRP"><a href="#단일-책임-원칙-Single-Responsibility-Principle-SRP" class="headerlink" title="단일 책임 원칙 (Single Responsibility Principle, SRP)"></a>단일 책임 원칙 (Single Responsibility Principle, SRP)</h2><blockquote>
<ul>
<li>각 클래스는 단 한가지의 책임을 부여받는다.</li>
<li>클래스를 수정해야할 이유는 단 한가지여야한다 (i.e. 책임져야하는 기능에 대한 구현 수정).</li>
<li>전지전능 객체는 꼭 피하자.</li>
</ul>
</blockquote>
<p>하나의 클래스가 거의 모든 기능을 담당하는 것을 본 적이 있을 것이다. 우리는 이것을 ‘<strong>전지전능 클래스</strong>‘라고 부른다. 전지전능 클래스는 처음 짤 때는 편하고 좋으나, 나중에 기능을 세분화하려고 하거나, 기능을 따로 떼려고 하거나, 새로운 기능을 추가하려고 하거나, 기존의 기능을 수정하려고 하거나 할 때 굉장히 골치아파진다. 전지전능 클래스 내부에 작은 수정을 하기 위해서는 해당 클래스에 정보를 주거나 받는 여러 다른 클래스들의 코드도 고쳐야하는 경우가 많다. 작은 수정을 여러 클래스에 걸쳐서 해야한다면 아키텍처에 뭔가 문제가 있는 것이다. 이처럼 <strong>코드를 수정하기 어렵게 하는 주 범인 중 하나가 전지전능 클래스</strong>이다.</p>
<p>예시로, ‘공책’ 이라는 클래스를 만들었다고 해보자. 우리는 이 클래스에 멤버변수로 ‘공책 제목’을 담는 string과 ‘노트 내용’을 담는 string을 모은 dynamic array를 멤버 변수로 가지고 있다. 그리고 ‘노트 작성’이라는 public 함수를 통해 그날의 일기를 쓸 수 있다고 해보자.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Notebook</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> title;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt; note_entries;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Notebook</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; title)</span> : title</span>{title} {}</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; note_entry)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">        note_entries.push_back(<span class="built_in">std</span>::to_string(count++) + <span class="string">": "</span> + note_entry);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></table></figure>

<p>이 구조는 ‘공책’의 기능을 잘 표현하고 있다. 일기를 적는데에 필요한 정보가 클래스 멤버 변수로 들어있고, 적절한 메소드를 통해 멤버변수와 교류할 수 있다.</p>
<p>이제 이 ‘공책’에 적힌 정보를 파일로 저장하는 기능을 만든다고 해보자. 다음과 같은 코드를 짤 수 있다.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Notebook::save</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; filename)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::ofstream <span class="title">ofs</span><span class="params">(filename)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; entry : entries)</span><br><span class="line">    {</span><br><span class="line">        ofs &lt;&lt; entry &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>전지전능 클래스라면 Notebook 클래스가 1. 내용 기입, 2. 저장 기능을 모두 다 가지고 있을 것이다. 호환성 좋게 2개 기능을 쓴다고 얘기할 수도 있겠지만, 기능이 조금만 세분화되고 많아지기만 해도 감당이 되지 않을 것이다. Notebook의 내용을 수정한다면? 내용을 셔플링한다면? 파일로 저장하는거 말고 클라우드에도 저장한다면? 프린트한다면? 이런것들도 다 Notebook에 들어가야하지 않을까?</p>
<p>Notebook의 책임은 ‘노트 항목들을 기입/관리’하는 것이지, 디스크에 쓰는 것이 아니다. 그러므로 <code>save</code>라는 기능은 처음부터 Notebook 클래스에 들어가면 안된다.</p>
<p>이러한 파일 저장 기능은 아예 별도로 취급하는 것이 좋다. 예를 들어, 다음과 같이 만들 수 있다.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">DiskWriter</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">(<span class="keyword">const</span> Notebook&amp; notebook, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; filename)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h2 id="열림-닫힘-원칙-Open-Closed-Principle-OCP"><a href="#열림-닫힘-원칙-Open-Closed-Principle-OCP" class="headerlink" title="열림-닫힘 원칙 (Open-Closed Principle, OCP)"></a>열림-닫힘 원칙 (Open-Closed Principle, OCP)</h2>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>SOLID</tag>
        <tag>GoF</tag>
        <tag>디자인 패턴</tag>
      </tags>
  </entry>
  <entry>
    <title>Test-Driven C++ (CppCon 2020 - Phil Nash)</title>
    <url>/20220221-tdd/</url>
    <content><![CDATA[<h2 id="Test-Driven-Developement-Test-Driven-Design"><a href="#Test-Driven-Developement-Test-Driven-Design" class="headerlink" title="Test Driven Developement, Test Driven Design"></a>Test Driven Developement, Test Driven Design</h2><p>TDD에서 가장 중요한 점은 ‘개발을 하면서 테스트를 만드는 것’이다.</p>
<p>무엇보다, ‘<strong>개발을 하기 전에 테스트를 먼저 작성한다</strong>‘.</p>
<p>이를 통해서 내가 개발을 하는 방법, <strong>코드와 아키텍처를 디자인 하는 방식이 바뀌게 된다</strong>.</p>
<p>이는 기존의 ‘구현을 다 하고나서 제대로 만들었는지 테스트’ 하는 방법과 굉장히 다르다.</p>
<p> </p>
<h2 id="TDD-Cycle"><a href="#TDD-Cycle" class="headerlink" title="TDD Cycle"></a>TDD Cycle</h2><img src="/20220221-tdd/tdd.png" class="" title="tdd">

<ol>
<li><strong>실패하는 테스트</strong>를 작성한다.</li>
<li>그 테스트가 패스할 수 있는 ‘<strong>최소한의 코드</strong>‘를 작성한다</li>
<li><strong>리팩토링</strong>한다</li>
<li>원하는 기능을 다 만들었으면 끝낸다. 그게 아니라면 1로 돌아간다.</li>
</ol>
<h3 id="실패하는-테스트"><a href="#실패하는-테스트" class="headerlink" title="실패하는 테스트"></a>실패하는 테스트</h3><p>테스트라는 말에는 여러가지 뜻이 내포된다.</p>
<p>첫번째로는 <strong>기능상의 테스트</strong>가 될 수 있다. GTest나 Catch2와 같은 프레임워크를 통해서 기능상의 테스트를 할 수 있다.</p>
<p>두번째로는 (많은 사람들이 간과하고 놓치지만) <strong>컴파일러 빌드 테스트</strong>가 있다. 컴파일러가 코드를 빌드할 수 있어야 ‘빌드 가능한’ 코드임을 증명할 수 있다. 새로운 메소드나 클래스를 만들 때 이 테스트를 거친다면, 해당 메소드/클래스의 존재를 먼저 선언하고 개발을 시작하게 할 수 있다.</p>
<h3 id="테스트가-패스할-수-있는-‘최소한의-코드’"><a href="#테스트가-패스할-수-있는-‘최소한의-코드’" class="headerlink" title="테스트가 패스할 수 있는 ‘최소한의 코드’"></a>테스트가 패스할 수 있는 ‘최소한의 코드’</h3><p>처음부터 좋은 코드를 짜려고 하면 고려해야할 점이 너무 많다. 아키텍처? Exception? 호환성?</p>
<p>하지만 테스트를 패스할 수 있는 ‘최소한의 코드’만 작성하면 1. <strong>빠르게</strong>, 2. <strong>테스트가 요구하는 기능을 충족</strong>할 수 있다. ‘기능 개발’이라는 부분에서 가장 중요한 부분에 집중할 수 있게 문제를 단순하게 만들어주는 것이다.</p>
<p>아키텍처, Exception, 호환성 같은 것들이 중요하지 않다는게 아니다. 이러한 점들은 이 다음 스텝에서 고친다.</p>
<h3 id="리팩토링"><a href="#리팩토링" class="headerlink" title="리팩토링"></a>리팩토링</h3><p>이전 단계에서 기능 개발은 끝났으니, 이번 단계에서는 진짜 ‘<strong>클린 코드</strong>‘를 만드는 것이다.</p>
<p>가독성을 높이고, 아키텍처를 고려하면 된다.</p>
<p>테스트를 패스하는 코드는 이미 작성했으니, 우리는 <strong>이전 단계에서 작성한 테스트가 깨지지 않는 선에서 코드를 개선</strong>하면 된다.</p>
<p>TDD를 할 때 얻는 이점 중 하나는 ‘<strong>테스트하기 쉬운 코드</strong>‘를 유도하게 된다는 점인데, 이를 통해 기능의 모듈화나 인터페이스 분리와 같은 점들을 빠르게 고려해서 짜게 되는 점이 굉장히 큰 이점이다. <strong>테스트하기 어려운 코드는 아무래도 좋은 디자인을 가지지 않기 떄문에, 고치게 된다</strong>.</p>
<p>리팩토링 단계가 매 TDD 사이클만다 일어난다고는 할 수 없지만, 해야한다면 제대로 해야한다는 점이 중요하다. 그렇지 않으면 더러운 구현 코드가 남게 되고, 코드 퀄리티를 높이기 위한 TDD는 실패하게 된다. 리팩토링을 잠깐 미룰 수는 있지만, 경험상 ‘아 그냥 그때 리팩토링을 할껄’ 하는 경우가 더 많다. </p>
<h3 id="종합하면…"><a href="#종합하면…" class="headerlink" title="종합하면…"></a>종합하면…</h3><p>개발을 할 때 테스트는 implementation test로써 ‘개발의 진척도’를 나타내는 지표가 된다.</p>
<p>이후, 개발이 완료되었을 때는 regression test로써 ‘코드의 안정성’을 나타내는 지표가 된다. </p>
<p>TDD를 할 때 리팩토링 단계를 거침으로써 테스트하기 쉬운 코드를 짜야한다는 ‘<strong>디자인 압박</strong>‘을 받게 된다. 이 압박을 통해 테스트하기 좋은 코드를 작성하게 되고, 사용하기 쉬운 코드를 짜게 된다.</p>
<p>TDD를 통해서 개발하면 <strong>거의 항상 100%의 coverage를 달성</strong>할 수 있다 (i.e. 모든 코드가 테스트된다). 하지만 unit test만으로는 완벽한 안정성을 얻을 수 없기 때문에, 우리는 <strong>unit test를 넘어 그 다음 단계의 test들을 고려할 수 있게 된다</strong>. 이 점이 대단한 이유는, 기존의 개발-&gt;테스트작성 방식으로는 100%의 coverage를 얻는것도 어렵기 떄문에 그 다음 단계의 test를 고려하는 것도 거의 불가능하기 때문이다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
        <category>2.1 Software Engineering</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>TDD</tag>
        <tag>CppCon</tag>
      </tags>
  </entry>
  <entry>
    <title>Go로 OpenCV 돌리기 - GoCV</title>
    <url>/20220222-go-cv/</url>
    <content><![CDATA[<img src="/20220222-go-cv/gocv.png" class="" title="gocv">

<p><span class="exturl" data-url="aHR0cHM6Ly9nb2N2LmlvLw==">https://gocv.io/<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="GoCV-OpenCV4를-Golang으로-쓰기"><a href="#GoCV-OpenCV4를-Golang으로-쓰기" class="headerlink" title="GoCV - OpenCV4를 Golang으로 쓰기"></a>GoCV - OpenCV4를 Golang으로 쓰기</h2><p>OpenCV는 이미 C++과 Python을 잘 지원한다. C++를 통해 직접 OpenCV 네이티브 코드에 접근해 CPU와 메모리 최적화가 잘 된 성능이 좋은 영상처리 프로그램을 만들 수 있다. 아니면 Python 바인딩된 OpenCV를 사용해서 내부함수는 C++의 속도를 가져가고 외부 파이프라인 개발속도는 Python의 속도를 가져갈 수도 있다. 이렇게 이미 OpenCV를 사용해서 개발하는 방법에는 성능을 가져가는 방법과 개발속도를 가져가는 방법 둘 다 잘 되어있다.</p>
<p>그렇다면 Go로 OpenCV를 써야할 이유가 뭐가 있을까?</p>
<p>Go의 장점은 <strong>손쉬운 concurrency 프로그래밍</strong>이다. 내가 생각하기엔, Python에서 제대로 지원하지 않는 concurrency 기능을 Go에서는 손쉽게 사용할 수 있기 때문에 <strong>1. 더 좋은 성능의 (i.e. 동시성 프로그래밍) 프로그램을 2. 더 빠르게 짤 수 있다</strong>.</p>
<p>물론 파이썬이 Go보다 가지는 장점은 당연히 있다. 1. OpenCV-python의 코드가 더 많고, 2. Interpreter와 Jupyter notebook 등을 통해 손쉽게 중간결과를 분석할 수 있고, 3. 시각화를 하기 쉽다.</p>
<p>하지만 이런 분석/시각화 시나리오가 아니라 단순히 스크립트 형식으로 영상처리가 필요하다면 Go가 가질 수 있는 이점이 있을 것으로 생각한다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>2. Programming</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
        <category>2.7 Go</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 3 - Saying Yes 좋은 문장들</title>
    <url>/20220303-clean-coders-saying-yes/</url>
    <content><![CDATA[<h1 id="A-language-of-Commitment"><a href="#A-language-of-Commitment" class="headerlink" title="A language of Commitment"></a>A language of Commitment</h1><p>“<strong>말을 했으면, 해야한다</strong>.”</p>
<p>“무언가를 하겠다고 말한 사람들 중, 실제로 그것을 진짜 해내는 사람을 정말 많이 없다.”</p>
<p> </p>
<h2 id="What-does-commitment-sound-like"><a href="#What-does-commitment-sound-like" class="headerlink" title="What does commitment sound like?"></a>What does commitment sound like?</h2><p>“진짜 ‘하겠다’는 건 이렇게 표현할 수 있다. 1. 언제까지, 2. 어떻게, 3. …하겠습니다.”</p>
<p>“<strong>정확히 무엇을 하겠다는건지</strong>, <strong>정확히 언제까지 하겠다는지</strong>를, <strong>‘네가’ 하겠다는 것</strong>을 fact로써 표현을 한것이다”.</p>
<p>“여기에는 ‘아마’ 라던지, ‘이쯤에’ 라던지, ‘해보겠습니다’ 같이 모호한 표현은 끼어들 곳이 없다.”</p>
<p> </p>
<h1 id="Learning-how-to-say-“Yes”"><a href="#Learning-how-to-say-“Yes”" class="headerlink" title="Learning how to say “Yes”"></a>Learning how to say “Yes”</h1><h2 id="The-other-side-of-“Try”"><a href="#The-other-side-of-“Try”" class="headerlink" title="The other side of “Try”"></a>The other side of “Try”</h2><blockquote>
<p><strong>매니저:</strong> “금요일까지 엔진 개발 마칠 수 있어요?”<br><strong>개발자:</strong> “그때까지 <strong>해볼게요</strong>.”</p>
</blockquote>
<p>“‘~해볼께요’ 라는 말에는 무슨 뜻이 숨겨져있을까? 아마 될수도, 안될수도 있다는 뜻이다.”</p>
<p> </p>
<blockquote>
<p><strong>매니저:</strong> “금요일까지 엔진 개발 마칠 수 있어요?”<br><strong>개발자:</strong> “그 작업은 도큐먼트도 적어야해서 앞으로 8시간 정도 더 필요해요. 그러니 월요일까지는 끝낼 수 있을 것 같습니다. 늦어도 화요일에는 결과물이 나옵니다.”<br><strong>매니저:</strong> “아뇨, 금요일에 끝나는지가 중요한 건이라서요. 금요일까지 할 수 있어요?”<br><strong>개발자:</strong> “그렇게 말씀하시면 불가능하다고 말씀드릴게요. <strong>확신할 수 있는건, 화요일에는 도큐먼트까지 해서 다 끝날 예정</strong>입니다.”</p>
</blockquote>
<p>“위 방법이 그나마 좀 더 솔직하고 좋다. 매니저에게 스케줄 변동의 위험성을 알게 되고, 해당 위험성에 대해 대처를 할 수 있기 때문이다.”</p>
<p> </p>
<blockquote>
<p>…<br><strong>매니저:</strong> “아뇨, 금요일에 끝나는지가 중요한 건이라서요. 금요일까지 할 수 있어요?”<br><strong>개발자:</strong> “그렇게 말씀하시면 불가능하다고 말씀드릴게요. 확신할 수 있는건, 화요일에는 도큐먼트까지 해서 다 끝날 예정입니다.”<br><strong>매니저:</strong> “<strong>화요일은 진짜 안됩니다</strong>. 월요일부터 금요일까지 다음 파트에서 작업을 하기로 했는데, 그쪽 팀도 일정을 더 미룰 수 없을만큼 타이트해요. 고객사 데드라인에 맞추려면 꼭 금요일에 끝나야해요. 어떻게 방법이 없을까요? 도큐먼트를 먼저 끝낸다던지?”<br><strong>개발자:</strong> “그건 안돼요. 이 코드 수정 작업이 끝나야 도큐먼트를 작성할 수 있는걸요.”<br><strong>매니저:</strong> “하… 어떻게든 금요일까지 코드 수정도 끝내고 도큐먼트도 적을 방법이 없을까요? 진짜 중요한 건이란 말이에요. 월요일 아침까지 꼭 필요합니다.”</p>
</blockquote>
<p>“위와 같은 상황처럼 꼭 금요일에 끝나야하는 상황에는 어떻게 해야하는가? 이러한 상황에서 개발자는 결정을 내려야한다. 기능 개발의 일부분을 포기할 수도 있고, 야근이나 주말출근처럼 초과근무를 해서 작업량을 달성할 수도 있다.”</p>
<p>“여기서 개발자는 여러 유혹에 빠져들게 된다. 테스트를 적지 않으면 더 빨리 개발할 수 있지 않을까? 리팩토링을 하지 않으면 더 빨리 개발할 수 있지 않을까? Regression 테스트를 전부 거치지 않으면 결과를 더 빨리 볼 수 있지 않을까?”</p>
<p>“여기서 프로들은 단번에 알 수 있을 것이다. 1. <strong>테스트를 적지 않는다고 개발이 빨라지지는 않는다</strong>. 2. <strong>리팩토링을 하지 않는다고 개발이 빨리지지 않는다</strong>. 3. <strong>Regression 테스트를 하지 않는다고 개발이 빨라지지는 않는다</strong>. 프로들은 수년의 경험을 통해, <strong>이 ‘룰’들을 깨면 결국 느려질 수 밖에 없다</strong>는 걸 알고 있다.”</p>
<p>“그리고 속도만 문제인게 아니다. 프로라면, 자신의 코드를 테스트 했을 것이고, 유지보수 가능한 코드를 작성했을 것이다. 고객들도 그러한 코드를 기대하고 있을 것이다. 고객이 원하는 수준에 미치지 못하는 코드를 주는건 <strong>계약 위반</strong>이며, 프로답게 행동하지 못했다는 것이다.”</p>
<p>“초과근무에 대해서도 고려해볼 부분이 있다. 개발자는 초과근무를 하기 전, 본인의 스태미나와 여분 에너지에 대해 솔직하게 판단해야한다. ‘제가 하겠습니다’라고 말하는건 굉장히 쉽다. 진짜로 해내는건 주중에 평소 컨디션으로 해내는 것 보다 훨씬 더 많은 에너지를 소요하고 훨씬 더 어렵다. ‘내가 해내고 말거야!’ 같은 생각으로 오판을 내렸을 때, 돌아오는 대가는 크다. <strong>프로들은 자기자신의 한계를 정확하게 안다</strong>.”</p>
<p> </p>
<blockquote>
<p>…<br><strong>매니저:</strong> “하… 어떻게든 금요일까지 코드 수정도 끝내고 도큐먼트도 적을 방법이 없을까요? 진짜 중요한 건이란 말이에요. 월요일 아침까지 꼭 필요합니다.”<br><strong>개발자:</strong> “알겠습니다. 그러면 일단 저는 집에 전화를 해서 가족들과의 일정을 좀 미뤄볼게요. <strong>가족들이 괜찮다고 하면, 월요일 아침까지 초과근무를 해서 개발 완료</strong> 해놓겠습니다. 월요일 아침에도 출근해서 다음 파트의 팀원들에게 넘기는 것도 문제 없이 되도록 해드릴게요. <strong>그 대신 수요일까지 휴가를 약속</strong>해주셨으면 합니다. 주말동안 쓴 에너지를 다시 채워와야 다음 스케줄을 할 수 있어요.”</p>
</blockquote>
<p>“이러한 요구는 정당하다”</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 4 - Coding 좋은 문장들</title>
    <url>/20220304-clean-coders-coding/</url>
    <content><![CDATA[<p>“당신이 어떤 에러를 만들었는지 느끼는건 정말로 중요합니다”</p>
<h1 id="Preparedness"><a href="#Preparedness" class="headerlink" title="Preparedness"></a>Preparedness</h1><p>“코딩이 어렵고 힘든 이유는, 해결해야할 문제가 여러가지로 복잡하게 얽혀있기 때문입니다.”</p>
<p>“첫째로, <strong>당신의 코드는 무조건 작동해야합니다</strong>. 당신의 코드는 당신이 풀려고 하는 문제를 정확하게 풀어내야합니다.”</p>
<p>“둘째, 고객이 요구하는 문제를 풀어야합니다. 고객이 풀려고 하는 ‘문제’와 고객의 ‘요구사항’은 종종 다를 수 있습니다. 당신은 고객의 요구사항을 맞추는 코드를 작성하는게 아니라, <strong>궁극적으로 고객의 문제를 풀어줘야합니다</strong>.”</p>
<p>“셋째, 당신의 코드는 <strong>기존의 시스템에서도 잘 작동해야합니다</strong>. 당신의 코드가 새로 적용됨으로써 기존의 시스템에 더 복잡해지면 안됩니다. 이를 위해서 당신의 코드는 여러 엔지니어링 패턴들을 잘 따라야합니다.”</p>
<p>“넷째, <strong>다른 프로그래머들이 당신의 코드를 읽을 수 있어야합니다</strong>.”</p>
<p>“당신이 충분히 집중을 하지 못할 때, 코드에는 버그가 생기고 잘못된 구조가 나타납니다. 이렇게 되면 당신의 코드는 고객이 풀려고 하는 문제를 풀 수 없게 됩니다. 그러니 <strong>피곤하거나 집중을 하지 못할 때는 그냥 코드를 짜지 마세요</strong>.”</p>
<p> </p>
<h1 id="3AM-Code"><a href="#3AM-Code" class="headerlink" title="3AM Code"></a>3AM Code</h1><p>“그건 진짜로 잘못된 솔루션이였어요. 하지만 새벽 3시에 짠 코드는 다 겁나 멋져보입니다.”</p>
<p>“당연하죠. 18시간이나 연속으로 코딩을 했으니, 그런 퀄리티가 그 당시의 저의 최선일 수 밖에 없습니다.”</p>
<p>“<strong>몇시간동안 개발을 했는지는 중요하지 않습니다.</strong>. 당신의 코드는 진정성있게 고객의 문제를 해결해주나요? 당신의 코드는 프로페셔널 수준의 퀄리티를 가지고 있나요? 자신있게 고객에게 보여줄 수 있는 코드인가요? 좋은 코드를 짜고 싶다면, 우선 <strong>충분히 잠은 자고 있는지, 건강한 생활패턴을 가지고 있는지부터 확인하세요</strong>. 그렇지 않고서는 <strong>매일 8시간을 집중해서 좋은 결과</strong>를 낼 수 없습니다.”</p>
<p> </p>
<h1 id="Worry-code"><a href="#Worry-code" class="headerlink" title="Worry code"></a>Worry code</h1><p>“혹시 <strong>남편/아내와 격하게 싸우고나서, 또는 친구/부모님과 격한 논쟁을 벌이고나서 코드를 짜보려고 한 적이 있나요?</strong>“</p>
<p>“코드에 집중하기 위해서는 background process를 꺼야합니다. 아니면 적어도 코딩에 방해가 되지 않을 정도로 중요도를 낮출 수 있어야해요.”</p>
<p>“저는 <strong>그 일을 해결할 시간을 따로 만들어서 문제를 해결</strong>했습니다. 3시간은 코딩을 하고, 1시간은 문제 해결에 시간을 썼습니다. 아이가 아프다면 집에 전화해서 상태를 확인해본다던지, 아내와 다퉜다면 전화를 해서 푼다던지의 방법으로요. 물론 이 방법은 좋지 않습니다. 개인적인 일은 오피스에 가져오지 않고 개인적인 시간에 해결하는게 제일 좋죠.”</p>
<p> </p>
<h1 id="The-Flow-zone"><a href="#The-Flow-zone" class="headerlink" title="The Flow zone"></a>The Flow zone</h1><p>“종종 집중해서 프로그래밍을 하다보면 ‘<strong>the zone</strong>‘에 들어가게 됩니다. <strong>주변 상황이 모두 차단되고, 코드에만 집중하게 되면서 생산성이 엄청나게 오르게 되는걸 느끼죠</strong>. The zone에서 나올 때 쯤에는 엄청난걸 해낸 것 같은 느낌이 납니다.”</p>
<p>“팁을 하나 드리죠. <strong>The zone에 빠지는걸 피하세요</strong>.”</p>
<p>“The zone에는 함정이 있습니다. 한가지 분야에만 집중하게 되면 큰 그림을 완전히 놓쳐버리게 되는 아주 큰 단점이 있습니다. 완전히 터널-비전이 되어버려서, 그 문제를 풀고 있을 때는 잘 하고 있는 것 같지만 나중에 큰 그림으로 봤을 때는 잘못되어서 결국 다시 고치게 되는거죠.”</p>
<p>“저는 그래서 the zone에 들어가는 것 같으면, 잠깐 일어나서 동네 한바퀴 걷고 옵니다. 이메일을 확인하거나 트위터를 보면서 머리를 비우기도 해요.”</p>
<p>“The zone에 들어가면 좋을 때는 당신이 ‘<strong>연습하고 있을 때</strong>‘ 입니다.”</p>
<p> </p>
<h1 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h1><p>“저는 음악을 듣고 있을 때 코드를 더 못치는 것 같아요. 음악을 듣다보면 거기에 에너지를 많이 뺏겨요. 근데 혹시 몰라요, 당신은 음악을 들으면 오히려 더 집중을 잘 하는 타입일수도요.”</p>
<p>“하지만 저는 <strong>음악이 the zone에 들어가기 쉽게 만들어주는 것 같아서 음악을 듣는걸 추천하지 않습니다</strong>.”</p>
<p> </p>
<h1 id="Interruption"><a href="#Interruption" class="headerlink" title="Interruption"></a>Interruption</h1><p>“<strong>누군가 질문을 하러 왔다면 어떻게 반응해야할까요?</strong>“</p>
<p>“대부분의 ‘예의 없는 반응’은 the zone에서 옵니다. 엄청나게 집중을 하고 있었는데, 외부 질문 때문에 the zone에서 끌려나와지는 거지요.”</p>
<p>“하지만 가끔은 진짜 복잡한 문제를 풀기 위해 집중하고 있을 수도 있죠.”</p>
<p>“사실 효율적인 업무를 위해서 팀원끼리 질문을 할 수도 있는건데, 혼자서 집중하는 과정은 이러한 질문 과정을 방해하는 부분이 될 수도 있습니다.”</p>
<p>“페어 프로그래밍을 하고 있다면, 여러분이 질문에 답을 해주고 있는 동안 여러분의 파트너가 문제의 컨텍스트를 그대로 잡고 있을 수 있습니다.”</p>
<p>“또는 TDD를 하고 있다면, 질문에 답을 하고 오면 failing test 사이클로 다시 돌아올 수 있죠.”</p>
<p>“결국 협업을 하는 과정에서는 무조건 질문에 답을 해야할 때가 올겁니다. 그렇게 시간을 뻇기게 되는 것도 당연한 거구요. 하지만 그런 상황이 왔을 때, <strong>당신도 언젠가는 질문을 할 것이고 누군가의 집중을 깨야할 때가 올 수 있다는 것을 인지해야합니다</strong>. 그러므로 질문에 예의있고 도움이 될 수 있는 방향으로 힘을 써주는 것이 프로다운 자세입니다.”</p>
<p> </p>
<h1 id="Writer’s-block"><a href="#Writer’s-block" class="headerlink" title="Writer’s block"></a>Writer’s block</h1><p>“가끔은 진짜 그냥 코드가 안나오는 경우도 있습니다. 잠을 충분히 못잤거나, 무언가가 걱정이 된다거나, 우울해져있다거나의 이유가 있겠습니다.”</p>
<p>“이메일을 읽거나, 트위터를 읽거나, 책을 읽거나, 스케줄을 다시 짠다거나, 도큐먼트를 적는다거나, 미팅을 한다거나, 팀원과 이야기를 한다거나 등으로 환기를 시켜볼 수 있습니다. 하지만 종종 무슨 짓을 해도 코드가 안 짜여질 때가 있습니다.”</p>
<p>“여기에는 아주 쉬운 솔루션이 있습니다. <strong>Pair programming</strong>을 같이 할 파트너를 찾으세요. 누군가와 함께 앉아서 문제에 대해 이야기하는 순간 마법같이 코드가 다시 짜지기 시작할겁니다.”</p>
<p> </p>
<h1 id="Creative-input"><a href="#Creative-input" class="headerlink" title="Creative input"></a>Creative input</h1><p>“<strong>창의적인 아웃풋을 내기 위해서는 창의적인 인풋이 필요</strong>합니다.”</p>
<p>“주변을 창의적인 것들로 가득 채우세요. 그러면 당신도 무언가 창의적인걸 만들어내야한다는 느낌을 받게 될 것입니다.”</p>
<p> </p>
<h1 id="Debugging-time"><a href="#Debugging-time" class="headerlink" title="Debugging time"></a>Debugging time</h1><p>“코딩을 하는데에 들어가는 시간과 비용이 높듯이, 디버깅을 하는데에 들어가는 시간과 비용도 높습니다.”</p>
<p>“저는 요즘에 <strong>디버깅을 하는데에 훨씬 적은 시간을 씁니다</strong>. 대충 10 분의 1 정도 되는 것 같아요. <strong>TDD를 도입함으로써 버그가 엄청나게 줄어들었기 때문</strong>입니다.”</p>
<p>“TDD를 하거나, 아니면 다른 비슷한 효율적인 방법론을 잘 쓰면… 디버깅의 시간은 0으로 수렴하게 됩니다.”</p>
<p>“<strong>의사들도 수술을 끝낸 환자를 다시 개복하고 싶지 않아합니다</strong>. 변호사들도 이미 끝난 사건을 다시 열고싶지 않아합니다. 다시 수술을 하려는 의사나 다시 케이스를 열려고 하는 변호사는 프로답지 못한 겁니다.”</p>
<p> </p>
<h1 id="Pacing-yourself"><a href="#Pacing-yourself" class="headerlink" title="Pacing yourself"></a>Pacing yourself</h1><p>“소프트웨어 개발은 단거리 달리기가 아니라 마라톤입니다.”</p>
<p>“<strong>프로들은 에너지와 창의력을 비축해서 적시적기에 사용</strong>할 줄 압니다.”</p>
<p> </p>
<h1 id="Know-when-to-walk-away"><a href="#Know-when-to-walk-away" class="headerlink" title="Know when to walk away"></a>Know when to walk away</h1><p>“문제가 풀리지 않아서 집에 갈 수 없나요? <strong>오, 천만에요. 집에 가세요!</strong>“</p>
<p>“창의성과 똑똑함은 건강한 몸에만 존재합니다.”</p>
<p>“몸이 피곤할 때는 조금 쉬어주세요. 그러면 적은 시간과 적은 노력으로도 더 많은 일을 해낼 수 있을 겁니다.”</p>
<p> </p>
<h1 id="Being-late"><a href="#Being-late" class="headerlink" title="Being late"></a>Being late</h1><p>“<strong>당신도 언젠가 한번쯤은 일정을 맞추지 못할 겁니다.</strong>“</p>
<p>“일정이 늦어지는걸 관리하려면 1. <strong>빠르게 늦어지는걸 알아차려야하고</strong>, 2. <strong>그 사실을 투명하게 공유</strong>해야합니다.”</p>
<p>“가장 최악인건, 모두에게 마지막 순간까지 잘 만들고 있다고 하다가, 정말 마지막 순간에 늦는다고 하는겁니다. 절대 이러지 마세요. 대신, <strong>정기적으로 당신이 얼마나 작업을 진행했는지 공유</strong>하고 <strong>3가지 작업종료일의 경우의 수</strong>를 알려주세요 - 제일 좋은 케이스, 적당히 잘 됬을 때의 케이스, 최악의 케이스 입니다. 그리고 제발 <strong>솔직하게</strong> 말하세요! 당신의 희망을 estimate에 넣지 마세요.”</p>
<p> </p>
<h1 id="Hope"><a href="#Hope" class="headerlink" title="Hope"></a>Hope</h1><p>“<strong>희망이 프로젝트를 말아먹습니다</strong>.”</p>
<p>“데모가 열흘 후라고 해보죠. 그리고 당신이 담당하는 작업의 3가지 작업종료일이 8/12/20 이라고 해보죠. 이런 경우에, ‘8일 안에 작업을 끝낼 수도 있지 않을까?’ 라는 희망을 갖지 마세요!”</p>
<p>“대신, <strong>고객들과 프로젝트 관리자들이 이 상황에 대해 정확하게 인지</strong>하게 하세요.”</p>
<p> </p>
<h1 id="Rushing"><a href="#Rushing" class="headerlink" title="Rushing"></a>Rushing</h1><p>“매니저가 ‘8일 안에 끝낼 수 있지 않을까요? 당신을 잘 해낼 수 있을거에요’ 라고 해도 속아넘어가지 마세요. 당신은 당신이 직접 만든 estimate를 고수하세요. 급하게 일을 끝내서 데드라인을 맞추려고 하지 마세요. 상사에게 가서 정확하게 estimate를 공유하고, 데드라인을 맞추기 위해서는 개발품목을 줄여야한다고 솔직하게 이야기하세요.”</p>
<p>“<strong>급하게 짠다고해서 당신이 코드를 짜는 속도가 더 빨라지는게 아닙니다</strong>.”</p>
<p> </p>
<h1 id="Overtime"><a href="#Overtime" class="headerlink" title="Overtime"></a>Overtime</h1><p>“상사가 매일 2시간씩 야근을 더 한다던지, 토요일 출근을 요청해서 데드라인을 맞추려고 요구하면 어떻게 해야할까요?”</p>
<p>“초과근무는 좋은 방법이긴 합니다. 가끔은 진짜 필요하기도 하구요. 하지만 정말 리스크가 큰 방법입니다. <strong>20%의 시간을 더 투입한다고 20%의 작업이 더 되는게 아닙니다</strong>.”</p>
<p>“초과근무를 해도 되는 상황은 다음과 같습니다. 1. <strong>개인적으로 시간을 낼 수 있다</strong>. 2. <strong>초과근무를 하는 시간이 2주 또는 그 이하이다</strong>. 3. <strong>초과근무를 해도 실패했을 경우 백업 플랜이 있다</strong>.”</p>
<p>“<strong>마지막 조건이 제일 중요합니다</strong>. 마지막 조건이 갖춰지지 않았다면 절대로 초과근무를 하면 안됩니다.”</p>
<p> </p>
<h1 id="False-delivery"><a href="#False-delivery" class="headerlink" title="False delivery"></a>False delivery</h1><p>“제일 나쁜건, 작업이 다 끝나지도 않았는데 다 끝났다고 해버리는겁니다.”</p>
<p>“‘다 끝났다’의 정의를 새로 만들어버리는 경우도 있습니다. 이건 진짜 위험합니다. 이 새로운 정의가 팀원들에게 퍼지게 되면 안됩니다. 이를 위해 팀원들끼리 ‘다 끝남’의 정의를 새롭게 내리거나, 또는 자동 테스트 파이프라인을 구축함으로써 정의를 내려야합니다.”</p>
<p> </p>
<h1 id="Help"><a href="#Help" class="headerlink" title="Help"></a>Help</h1><p>“당신이 얼마나 프로그래밍을 잘 하는 사람이던, 누구나 다른 프로그래머의 시각과 아이디어에서 도움을 얻을 수 있습니다.”</p>
<p>“그렇기 때문에 동료들을 도와줘야합니다. <strong>동료가 도움을 요청하는데 그걸 무시하고 진행해야할 만큼 중요한 일은 없습니다</strong>.”</p>
<p>“물론 이게 혼자만의 개발시간이 필요하지 않다는건 아닙니다. 혼자 집중할 시간은 필요하죠. 대신, 동료들과 서로 이 시간을 예의있게 존중해야합니다.”</p>
<p>“<strong>누군가 도움이 필요해보이면, 도움을 주세요.</strong> 옆에 같이 앉아서 코드를 같이 짜세요. 1시간, 또는 그 이상의 시간이 걸릴껄 생각하고 차분하게 도와주세요.”</p>
<p> </p>
<h1 id="Being-helped"><a href="#Being-helped" class="headerlink" title="Being helped"></a>Being helped</h1><p>“<strong>누군가 나를 도와줬다면, 고마움을 표현하세요</strong>.”</p>
<p>“당신이 바쁘고 여유가 없다고해서 고마움을 표현하지 않고 넘어가려고 하지 마세요.”</p>
<p>“누군가 도와주려고 하면 30분 정도 함께 하세요. 그 시간이 지났는데도 큰 도움이 되지 않는다면, 예의바르게 이야기를 하고 고마움을 표시하며 세션을 종료하세요.”</p>
<p>“<strong>도움을 요청하는 방법을 익히세요</strong>. 남들이 쉽게 도와줄 수 있는 상황인데, 혼자서만 막혀있는 것도 프로답지 못한 행동입니다.”</p>
<p> </p>
<h1 id="Mentoring"><a href="#Mentoring" class="headerlink" title="Mentoring"></a>Mentoring</h1><p>“<strong>경험이 부족한 프로그래머들을 멘토링하는 작업은 경험이 더 많은 프로그래머의 책무입니다</strong>. 온라인 코스나 책을 읽는거랑은 효율성이 비교가 되지 않습니다.”</p>
<p>“주니어 프로그래머가 열정이 있고, 거기에 맞춰 시니어 프로그래머가 좋은 멘토링을 해준다면, 이보다 더 빠른 성장은 없을 겁니다.”</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>현대차 직원이 말하는, 테슬라를 따라잡기 어려운 이유 - 글 레퍼런스</title>
    <url>/20220309-hyundai-tesla/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9icnVuY2guY28ua3IvQDEtdGVzbGEvMjE=">원글 링크<i class="fa fa-external-link-alt"></i></span></p>
<img src="/20220309-hyundai-tesla/tesla.png" class="" title="tesla">

<h1 id="필요한-기술과-인력-구성이-다르다"><a href="#필요한-기술과-인력-구성이-다르다" class="headerlink" title="필요한 기술과 인력 구성이 다르다"></a>필요한 기술과 인력 구성이 다르다</h1><ul>
<li>기존 내연기관 자동차 회사들은 전기차 개발을 위한 인력+기술+경험이 없다.<ul>
<li>내연기관 자동차는 엔진 개발 능력이 뛰어나다.</li>
<li>전기차 개발을 위해서는 모터, 배터리 개발능력이 뛰어나야한다.</li>
<li>인력+기술+경험도 없는데, 고급인재들이 가고싶어할까?</li>
</ul>
</li>
<li>SW를 개발하는 방식은 Tier 1 및 그 하단 부에 전부 외주를 주는 형태이다.<ul>
<li>전기차는 SW가 중요하다.<ul>
<li>테슬라는 전기차를 ‘바퀴달린 컴퓨터’로 표현한다. 기능이 늘어날 때 마다 업데이트를 시켜주는 방식을 사용한다.</li>
<li>테슬라는 전부 내부에서 개발한다.</li>
</ul>
</li>
<li>전부 외주를 주는 방식으로 하면, 여러개의 컴퓨터가 달릴 수 밖에 없다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Tesla</tag>
        <tag>Hyundai</tag>
      </tags>
  </entry>
  <entry>
    <title>현재 NFT 시장에 대한 내 생각</title>
    <url>/20220309-nft-my-thoughts/</url>
    <content><![CDATA[<h1 id="NFTs"><a href="#NFTs" class="headerlink" title="NFTs"></a>NFTs</h1><p>작년 4분기 부터 NFT에 관심을 가지게 되면서 시장과 흐름을 천천히 보고 있다.</p>
<p>괜찮은 아이디어도 있어서 NFT 개발 스터디에도 참여해 활동했지만, 1. 회사 스케줄의 압박, 2. 개인 스케줄 압박, 3. NFT에 대한 불안감 때문에 모티베이션이 많이 떨어지게 되어 개발을 거의 진행하지 못했다.</p>
<p>이번 글에서는 <strong>내가 NFT 시장을 보면서 느끼게 된 점들</strong>과, <strong>왜 불안하다고 느꼈는지</strong>에 대해 이야기해본다.</p>
<p> </p>
<h1 id="가상화폐의-불안정성"><a href="#가상화폐의-불안정성" class="headerlink" title="가상화폐의 불안정성"></a>가상화폐의 불안정성</h1><blockquote>
<p>TLDR;</p>
<ul>
<li>코인은 큰 가격 변동성에는 중앙집권화된 시스템의 영향도 크다.</li>
<li>NFT를 거래하는데 코인을 많이 사용하는데, 투자자들에게 이러한 가격변동성은 리스크로 다가온다.</li>
</ul>
</blockquote>
<p>우선 가장 먼저 이야기하고 싶은 점은 <strong>가상화폐의 불안정성</strong>이다.</p>
<p>가상화폐로써 가장 유명한 2개는 <strong>비트코인</strong>과 <strong>이더리움</strong>이다. 이 둘은 가상화폐의 대표주자로써 상징성이 굉장히 크고, 수많은 트렌드, 법, 규제가 이 둘을 중점으로 만들어진다. </p>
<p>비트코인과 이더리움은 몸집이 굉장히 큰 편이기 때문에 소규모 코인들에 비해서는 가격변동성이 낮지만, 그래도 <strong>주식에 비교했을 때 변동성이 굉장히 큰 편</strong>이다. </p>
<p><strong>글로벌 스케일의 거시경제 변화, 정치, 규제 변동</strong>에 따라 코인의 가격이 오르내리는 경우도 있지만, <strong>자동 트레이딩 봇이나 세력의 움직임</strong>에 따라 가격 변동성이 생기는 경우도 있고, 종종 <strong>가짜뉴스/선동/단순hype 또는 아무도 모르는 이유</strong>로 가격이 변동되는 경우도 있다.</p>
<p>그만큼 <strong>코인의 안정성은 떨어진다</strong>고 볼 수 있다. 이 때문에 <strong>투기성이 굉장히 높은 물품임에도 미국에서 비트코인 현물 ETF를 승인하지 못하는 부분</strong>이기도 하다.</p>
<p>이 부분이 NFT와 어떤 연관이 있을까? </p>
<p>첫번째로 <strong>NFT 거래는 가상화폐</strong>로 하는 경우가 많다. 내가 어떤 NFT 아트워크를 구매했다고 해보자. 내가 이걸 단순 NFT 소유권 증명용으로 구매한 거면 상관이 없지만, 아트워크는 사실 투기성 성향이 높으며 (e.g. 아트워크 재테크) NFT 아트워크는 특히나 투기성 성향이 더더욱 높다. 내가 투기목적으로 구매했다면, 가격변동성이 높은게 좋을까 낮은게 좋을까? 그건 사실 중요하지 않고, <strong>판매자에게는 NFT가 판매가격/가치가 시간이 지날수록 꾸준히 올라준다는게 중요할 것</strong>이다. 하지만 NFT의 가치가 올라도, 거래 통화인 코인의 가격이 오르내릴 수 있다. <strong>거래통화로써 코인의 원래 목적</strong>은 decentralized finance, 즉 <strong>중앙집권화 된 권력이 통화의 가격을 결정하는 것이 아닌, 사용자들의 공급과 수요에 따라 가격이 오르내려야한다</strong>. 하지만 현재 비트코인/이더리움의 가격이 오르내리는걸 보면 <strong>1. 미국/전세계의 통화정책, 2. 가상화폐 규제, 3. 정치 및 전쟁에 큰 영향을 받는데, 이는 중앙집권화된 권력이 비트코인/이더리움의 가격에 큰 영향</strong>을 미치는 걸 볼 수 있다. 그러면 사용자의 공급/수요에 따른 변화는 어떤 것들이 있을까? 물론 실제 공급/수요에 따른 변화도 있겠지만, <strong>사용자가 비트코인 소식을 접하는 뉴스 및 미디어에서는 1. 가짜뉴스, 2. 선동, 3. 알 수 없는 이유</strong> 를 많이 접하게 된다.</p>
<p>이러한 부분 때문에, <strong>NFT를 거래하는데에 생기는 불안 요소</strong>가 하나 생기게 되며, <strong>야수의 심장을 가진 사람이 아닌 이상은 거래에 부담</strong>을 느끼게 된다.</p>
<p> </p>
<h1 id="NFT의-불안정성"><a href="#NFT의-불안정성" class="headerlink" title="NFT의 불안정성"></a>NFT의 불안정성</h1><p>두번째는 <strong>NFT의 불안정성</strong>이다.</p>
<blockquote>
<p>TLDR;</p>
<ul>
<li>NFT 판에는 안전망이 충분히 구축되어있지 않다.</li>
<li>그러다보니 사기꾼들과 일확천금을 노리는 야수의 심장이 많다. </li>
<li>느리더라도 안정적으로 가격이 오를거라는 보장이 없다. Go big or go home 밖에 없다.</li>
</ul>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMzA5LW5mdC1zY2Ftcy8=">이전 글<i class="fa fa-external-link-alt"></i></span>에서 NFT 스캠 사기의 종류에 대해 알아봤다. 이 글을 적으면서 느낀 점은, ‘<strong>실제 세상에 있는 온갖 사기 방법들도 NFT 시장에 그대로 존재한다</strong>‘ 라는 점이다. 이렇게만 보면 실제 세상이나 NFT 시장이나 별 다를 바가 없기 때문에 문제가 없어 보일 수 있다.</p>
<p>하지만 <strong>NFT 시장은 아직 초창기</strong>이다. 그러다보니 <strong>거래에 필요한 최소한의 안전망이 잘 구축되어있지 않다</strong>. 실제 세상에서 사기를 당하면, 상대방의 신원정보를 가지고 신고를 하고, 고소를 하고, 금융적으로 제재를 걸고, 경찰의 추적을 요구할 수 있다. <strong>NFT 시장에서는 사기를 당하면 현재 민사소송 외로 할 수 있는 부분이 아무것도 없다</strong>. 근데 <strong>웹상에서 가짜이름을 쓰거나 익명을 쓴다면</strong> 민사소송을 걸려고 해도 대상인을 찾을 수 없기 때문에 소송 자체도 불가능하다.</p>
<p>물론 이러한 점은 ‘초창기’라는 특징으로 전부 이해할 수 있다. 아직 <strong>안전망이 구축되지도 않은 시장에 뛰어들어 초기 투자를 성공적으로 진행시키면 큰 부를 얻을 수 있을 것</strong>이다. <strong>하이리스크 하이리턴, 야수의 심장</strong>만이 성공할 수 있을 것이다.</p>
<p>하지만 반대로, <strong>안전망이 없는 곳은 무법지대나 마찬가지이니 수많은 사기꾼과 범법자를 끌어들인다</strong>. 내 안전을 내가 직접 책임질 수 있는 사람들만 투자를 하는게 좋을 것이다. </p>
<p>하지만 <strong>진짜 높은 가치의 아트워크를 소유하고 있는 투자자들이, 진짜 많은 양의 자산을 보유한 투자자들이 이 리스크를 지려고 할까?</strong> 절대 없겠다고는 못하겠지만, 안전하지만 꾸준하게 투자를 하려는 사람들도 많을 것이다. 그렇다면, <strong>지금 시장에 있는 사람들은 일확천금을 노리는 사람들이 많을 것</strong>이다.</p>
<p> </p>
<h1 id="초기-NFT-가치가-있는가"><a href="#초기-NFT-가치가-있는가" class="headerlink" title="초기 NFT, 가치가 있는가?"></a>초기 NFT, 가치가 있는가?</h1><blockquote>
<p>TLDR;</p>
<ul>
<li>대부분의 초기 NFT들은 시간이 지나며 가치를 잃어버릴 것이다.</li>
<li>많은 NFT 프로젝트 오너들은 일확천금을 벌고 은퇴할 것이다.</li>
<li>그러다보니 NFT의 가치가 시간에 따라 올라가기 전에, 서포트가 끊길 확률이 높다.</li>
</ul>
</blockquote>
<p>NFT는 블록체인의 특징을 이용해서 디지털 에셋에 unique함을 얹어줄 수 있는 장치이다. 이를 통해 <strong>디지털 아트워크에 고유소유권을 만들어 희소성을 만들어준다</strong>. 모나리자의 사진은 몇십만장이지만, 실제 모나리자는 1개만 있듯이 말이다. 모나리자는 말이 된다. 그만큼 가치가 있으니까.</p>
<p>근데 <strong>돌멩이를 그려놓은 픽셀덩어리가 진짜 80억의 가치가 있을까?</strong> 일단 지금 상황에서는 절대 아니다. 누구나 그릴 수 있는 돌멩이 픽셀아트는 NFT가 아니였다면 몇달러의 가치도 없을 것이다. 이 NFT가 80억에 팔리는 이유는, 추후 <strong>NFT의 역사에 한 획을 그은 아트워크가 될 것</strong>이라는 기대 때문이다. 실제로 나중에 가서 이게 400억 500억이 될지는 아무도 모르는 일이다.</p>
<p>진짜 찐 개인생각이지만, <strong>역사책에서 한 획을 긋는 것</strong>이라고 하면 대표적인 몇개만 되지 않나 싶다. 그리고 그런 것들은 <strong>시간이 지나도 가치가 있어야한다</strong>. 그러한 가치가 없다면 금방 hype가 빠지고 가치는 바닥으로 추락할 것이다.</p>
<p>예를 들어, 메이플스토리 게임 초창기에는 ‘냄비 뚜껑’이라는 아이템이 있었다. 고스펙 장비가 없던 시절, 낮은 드랍율을 가진 이 아이템은 고가에 거래되곤 했다. 10년이 지난 지금 메이플스토리에서 냄비뚜껑의 가치는 바닥이다. 왜냐하면 훨씬 더 좋은 아이템들이 생겼기 때문이다. 나는 <strong>지금 NFT 시장의 초기 NFT 작품들의 99%가 이런 냄비뚜껑들이라고 생각한다</strong>. </p>
<img src="/20220309-nft-my-thoughts/top.jpeg" class="" title="top">

<p>NFT가 시간이 지나도 가치가 있으려면 2가지 방법밖에 없다. <strong>1. 시장이 지금처럼 그대로 유지된다, 2. 지금의 가치를 지속적으로 발전시켜나간다</strong>.</p>
<p><strong>1번 방법은 불가능하다. 시장이 절대 지금처럼 유지될리가 없다.</strong> 한번 돈냄새가 나는 곳에는 지속해서 새로운 사람들이 들어올거고, 새롭고 기발한 아이디어로 경쟁해서 돈을 더 벌려고 할거다. </p>
<p><strong>2번 방법은 가능하긴 하다</strong>. NFT 프로젝트 <strong>오너들이 책임감을 가지고 몇십년동안 지속해서 NFT의 가치를 높여주면 된다</strong>. 초기 NFT 구매자들에게 지속해서 인센티브를 주는 것이다. <strong>근데 지금 블록체인과 NFT 유저들을 보면 단기간 하이리스크-하이리턴의 투자방식</strong>으로 순식간에 몇십/몇백억을 벌어 <strong>20~30대 fire족 은퇴를 하고싶어하는데</strong>… <strong>성공적으로 NFT 프로젝트 초기를 이끈 오너들과 개발자들이 과연 몇십년동안 지속해서 NFT의 가치를 높여주고 싶을지가 의문</strong>이다.</p>
<p> </p>
<h1 id="그래서-결론"><a href="#그래서-결론" class="headerlink" title="그래서 결론."></a>그래서 결론.</h1><p><strong>NFT 시장은 한번 패러다임이 바뀌어야한다</strong>. 규제가 생기던, 제품에 변화가 생기던.</p>
<p>NFT 기술이 단순 투기성이 아닌 진짜 uniqueness의 증명방식으로써 높은 가치를 지니게 되었을 때, 그때가 진짜 성숙한 NFT의 사용과 거래가 이뤄질 것으로 생각이 된다.</p>
<p>그리고 지금은 hype이 굉장히 많이 들어가있다. 주식도 원래 저점에서 사서 고점에서 팔아야 수익이 생긴다. 지금 NFT 시장에 섣불리 들어갔다가는 고점에서 사서 저점에서 팔 수도 있다.</p>
<p>그래서 개인적인 생각으로는… <strong>1. 야수의 심장을 가지고 높은 변동성에서 빠르게 단타수익을 내고싶다, 하면 지금 진입 추천</strong>, <strong>2. 그게 아니라 진짜 NFT를 다루고 싶다면 조금 기다리는 것을 추천</strong> 한다. </p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>NFT</tag>
        <tag>Blockchain</tag>
        <tag>Ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title>NFT 스캠 사기의 종류</title>
    <url>/20220309-nft-scams/</url>
    <content><![CDATA[<p>2021년 4분기 부터 NFT 시장을 천천히 보고 있었다.</p>
<p>개인적으로 느끼기에는 굉장히 핫했다고 느끼는데, 그 중에 스캠 사기를 보게되며 느끼게 된 점들이 있다.</p>
<p>이번 글에서는 다양한 NFT 스캠 사기의 종류들에 대해 정리한다.</p>
<h1 id="Rug-Pull-Scams"><a href="#Rug-Pull-Scams" class="headerlink" title="Rug Pull Scams"></a>Rug Pull Scams</h1><ul>
<li>Rug pull은 NFT 프로젝트 오너들이 대금을 받은 상태에서 약속했던 제품을 만들지 않고 도망가는 스캠 사기이다.<ul>
<li>가장 보편적인 사기 방법에는 두가지가 있다.<ul>
<li><strong>민팅에 필요하다고 선금을 받은 후, 그대로 돈을 들고 튀는 경우</strong></li>
<li>롱텀으로 NFT를 소유할 경우 가치가 계속 오를 것을 약속한 후 (이 경우 초기 NFT가 비싸짐), <strong>예고 없이 프로젝트를 중단하고 잠적하는 경우</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Rug pull 스캠에 당하는 경우는 대부분 해당 프로젝트 오너들을 ‘믿고 있던 경우’가 많다. 많은 경우 프로젝트 오너의 트위터 팔로우 수와 같은 지표를 믿게 된다.</p>
<p>하지만 <strong>팔로우 수 같은건 쉽게 조작할 수 있는 점을 유의</strong>하자. 프로젝트 오너가 쉽사리 도망가기 어려운 포지션인 경우에만 믿는게 맞다.</p>
<p> </p>
<h1 id="Phishing-scam"><a href="#Phishing-scam" class="headerlink" title="Phishing scam"></a>Phishing scam</h1><ul>
<li>Phishing은 허위 광고 또는 허위 가이드를 통해 유저의 지갑 비밀번호를 받아내어, 유저의 지갑 속 가상화폐 및 NFT를 모두 들고 튀는 것을 의미한다.</li>
<li>Phishing이라는 단어는 보이스피싱의 그 ‘피싱’이 맞다.</li>
</ul>
<p>Phishing의 대처법은 보이스피싱 대처법과 똑같다. 절대로 지갑 비밀번호를 아무에게도 알려주지 말자.</p>
<p> </p>
<h1 id="Bidding-scam"><a href="#Bidding-scam" class="headerlink" title="Bidding scam"></a>Bidding scam</h1><ul>
<li>Bidding scam은 NFT 경매 마켓에서 이뤄지는 사기이다.</li>
<li>당신이 NFT를 리셀 (재판매)할 때, 구매자가 거래에 필요한 코인을 판매자 몰래 가치가 낮은 코인으로 몰래 바꿔서 거래하는 것이다. 예를 들어 비트코인 1개로 팔려고 했는데 (약 5천만원), 도지코인 1개 (약 150원)으로 거래를 해버리는 것이다.</li>
</ul>
<p>팔기 전에 꼭 거래 통화를 두번 확인하자.</p>
<p> </p>
<h1 id="Counterfeit-NFT"><a href="#Counterfeit-NFT" class="headerlink" title="Counterfeit NFT"></a>Counterfeit NFT</h1><ul>
<li>Counterfeit NFT는 NFT 프로젝트 오너가 어떠한 아티스트의 작업물을 무단도용해 NFT를 팔아버리는 경우이다.<ul>
<li>이 경우, 실제 작품 소유권은 원작자에게 있기 때문에 발행된 NFT에는 아무런 가치도 없다.</li>
</ul>
</li>
</ul>
<p>NFT 아트워크를 구매하기 전에, 실제로 아트워크 원작자가 NFT 프로젝트 오너로 들어가있는지 확인해야한다.</p>
<p> </p>
<h1 id="Pump-and-dumps"><a href="#Pump-and-dumps" class="headerlink" title="Pump and dumps"></a>Pump and dumps</h1><ul>
<li>Pump and dump는 다른 사람들을 사주해서 NFT를 구매해, NFT의 희소성을 높이고 구매가격을 높여버리는 방법이다. 가격이 올라버리면, 이 NFT를 전부 높은 가격으로 팔아버리고 잠적하는 방식이다.<ul>
<li>사기를 당한 사람은 갑작스럽게 시장에 수많은 NFT가 풀려 가격이 내려가버린다.</li>
</ul>
</li>
</ul>
<p>이거는 피하기 쉽지 않다. 주가조작과 비슷한 느낌인데, 현재로써는 프로젝트 오너가 깨끗한 사람인지 판단하는 방법밖에 없다.</p>
<p> </p>
<h1 id="Airdrop-scams"><a href="#Airdrop-scams" class="headerlink" title="Airdrop scams"></a>Airdrop scams</h1><ul>
<li>NFT 프로젝트 오너들이 ‘무료 NFT’를 준다고 하며, 유저들에게 SNS 홍보를 부탁한다. 유저가 홍보를 마치고나면 ‘무료 NFT’를 주기 위해 지갑 정보를 요구하는데, 여기서 지갑 비밀번호를 주게 되면 지갑이 털리게 된다.</li>
</ul>
<p> </p>
<h1 id="Investor-scams"><a href="#Investor-scams" class="headerlink" title="Investor scams"></a>Investor scams</h1><ul>
<li>NFT 프로젝트를 진행하기 위해 초기투자자를 모으는 과정에서, 초기 투자금을 받고나면 잠적하는 사기이다.</li>
</ul>
<p> </p>
<h1 id="Customer-support-scams"><a href="#Customer-support-scams" class="headerlink" title="Customer support scams"></a>Customer support scams</h1><ul>
<li>NFT 구매 프로세스에 문제를 만든 후, ‘고객 서비스’팀으로 위장해 유저에게 연락을 취한다. 구매를 도와준다는 핑계로 지갑의 정보 및 비밀번호를 알아내어, 지갑에 있는 가상화폐와 NFT를 본인의 계좌로 송금한 후 잠적한다.</li>
</ul>
<p> </p>
<h1 id="느낀-점…"><a href="#느낀-점…" class="headerlink" title="느낀 점…"></a>느낀 점…</h1><p>사기 케이스들을 보다보니 든 생각이 있다.</p>
<blockquote>
<p>아니,, 그냥 평소의 사기 수법이랑 완전 판박이잖아…? </p>
</blockquote>
<img src="/20220309-nft-scams/wait.png" class="" title="wait">

<p>하지만 평소의 사기수법과 다른 점이라면, 실제 케이스에서는 경찰에 신고하고 소송을 걸고 사기범을 트랙다운 할 수 있지만, 블록체인/NFT 플랫폼에서는 아직 이러한 규제와 안전망 시스템이 부족하다는 점이다.</p>
<p>투자자들도 이 부분을 충분히 인지하는 것이 중요하다. 아직은 시스템이 극초기이기 때문에 안전망이 없어 리스크가 높다. 하지만 그만큼 극초기이기 때문에 적절한 투자는 큰 리턴으로 돌아올 수 있다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>NFT</tag>
        <tag>Blockchain</tag>
        <tag>Ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title>개발자가 알아야 할 스톡옵션의 모든 것 - 글 레퍼런스 및 정리</title>
    <url>/20220309-stock-option/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9ldmFuLW1vb24uZ2l0aHViLmlvLzIwMjEvMTIvMDQvd2hhdC1pcy1zdG9jay1vcHRpb25zLw==">링크<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Stock option</tag>
      </tags>
  </entry>
  <entry>
    <title>자율주행 자동차의 눈 - 라이다 - 이용이 책임 발표 노트</title>
    <url>/20220315-autonomous-driving-lidar/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/D3yUFokf4Wk" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>자율주행 주요 기술 및 데이터셋 관련 동향 - 정구민 교수님 발표 노트</title>
    <url>/20220315-autonomous-driving-tech-and-datasets/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS83Rkt2YkpJY1VWTQ==">https://youtu.be/7FKvbJIcUVM<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Event Cameras with Davide Scaramuzza 노트 (Robohub 팟캐스트 Ep.347)</title>
    <url>/20220315-event-cameras-with-davide-scaramuzza/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/-Tthu0D4dps" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h1 id="소개"><a href="#소개" class="headerlink" title="소개"></a>소개</h1><ul>
<li>Prof. Davide Scaramuzza</li>
<li>ETH-Zurich (취리히 공대)의 Robotics and Perception 그룹의 리더<ul>
<li>약 10년정도 된 그룹</li>
<li>약 15명의 연구자들이 robotics, computer vision, learning &amp; control의 융합연구를 진행중</li>
<li>로봇이 A-&gt;B로 이동하기 위해 주변 환경을 인식하고 이동하는 방법에 대해 연구중</li>
<li>주 로봇 플랫폼은 Drone임. Ground 로봇보다 훨씬 빨라서 좋다.</li>
<li>주 센서는 카메라와 IMU 임.</li>
</ul>
</li>
</ul>
<p> </p>
<h1 id="이-연구를-하게-된-계기는"><a href="#이-연구를-하게-된-계기는" class="headerlink" title="이 연구를 하게 된 계기는?"></a>이 연구를 하게 된 계기는?</h1><ul>
<li>처음에는 나도 조교수였다. 박사 학생도 아무도 없었고 (ㅋㅋ). 많은 research proposal들을 지원하다보니 약 10명정도 까지 생겼다.</li>
<li>첫 연구 주제는 drone navigation이였는데, 몇년 후 event camera쪽 연구를 시작하게 되었다.<ul>
<li>사람보다 더 빠르게 주변 환경을 인식하는 시스템을 만들고 싶었는데, 이를 위해서는 <strong>더 빠른 센서</strong>가 필요하다고 생각했기 떄문이다.</li>
<li>로봇을 쓰는 이유를 생각해보면, 다 사람보다 훨씬 효율성있기 떄문이다 (e.g. 산업로봇들이 사람들을 대체한 케이스). 그렇기 때문에, <strong>로봇으로써 가치가 있기 위해서는 사람보다 더 빠르게 주변환경을 인식해야하고 더 빠르게 결정을 내릴 수 있어야한다</strong>.</li>
</ul>
</li>
</ul>
<p> </p>
<h1 id="Event-camera란"><a href="#Event-camera란" class="headerlink" title="Event camera란?"></a>Event camera란?</h1><ul>
<li>Event camera는 모든 pixel이 각각 독립적으로 작동하는 카메라이다.<ul>
<li>각각의 pixel에는 마이크로칩이 있어서, <strong>pixel마다 intensity의 변화가 (i.e. event) 생김을 감지</strong>할 수 있다.<ul>
<li>+ve의 intensity 변화는 +ve event, -ve의 intensity 변화는 -ve event라고 할 수 있다.</li>
<li>기존의 카메라처럼 frame을 받는게 아닌, per-pixel intensity change를 받게 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/1.gif" class="" title="dfsdf"> 

<p> </p>
<ul>
<li>Event camera는 기존의 카메라처럼 discrete-time sampling을 통해 데이터를 취득하는게 아닌, <strong>continuous-time sampling</strong>을 통해 데이터를 취득한다.<ul>
<li>Frame이라는 개념 자체가 없다. 단순히 stream of data일뿐! 이 data 에는 event가 있는 pixel location과 intensity 변화 값이 있다.</li>
<li>선풍기가 event camera 앞에서 돌고 있다면, high-frame rate로 frame을 받는게 아닌, spiral events가 생긴다.<ul>
<li>이것을 spiral events of space-time 이라고 한다.</li>
</ul>
</li>
<li>기존의 카메라는 모든 pixel이 동일한 시간에 값을 취득하지만, event camera는 그렇지 않다.</li>
</ul>
</li>
<li>(솔직히 여기를 듣고나서 충분히 설득이 되진 않았지만… 모든 센서는 다 discrete-time sampling이고, event camera는 그냥 그게 엄청 빠른거 아닌가? ‘Resolution of microseconds’를 가지고 있다고 하셨는데, 이거 자체가 사실 discrete-time sampling이라는 얘기인거 같은데… 그런데 이런 주장에는 단호하게 ‘no’라고 하신 이유가 궁금하다.)</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/2.gif" class="" title="asdfasdf"> 

<p> </p>
<h1 id="Event-camera의-장점-장점이-드러나는-연구들"><a href="#Event-camera의-장점-장점이-드러나는-연구들" class="headerlink" title="Event camera의 장점 / 장점이 드러나는 연구들"></a>Event camera의 장점 / 장점이 드러나는 연구들</h1><ul>
<li>Event camera를 사용하면 ‘<strong>어떤 motion이 들어와도 깨지지 않는 state estimation</strong>‘이 가능하다.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDkuMDYzMTA=">‘Ultimate SLAM’<i class="fa fa-external-link-alt"></i></span>이라고 공개한 논문이 있는데, 기존의 카메라들이 실패하는 케이스를 event 카메라로 매우 훌륭하게 풀어냈다.</li>
<li>여기서는 카메라를 USB 케이블에 달아서 카우보이처럼 빙빙 돌렸다 ㅋㅋ</li>
<li>기존의 카메라들은 모션블러로 인해 진작에 실패했겠지만, event camera의 <strong>높은 temporal resolution</strong>을 통해 얻은 데이터에서 corner 정보를 취득해서 (물론 데이터가 다르게 생겼기 때문에 기존의 corner detection 방식을 사용하지는 못한다), IMU 정보와 fusion을 통해 아주 정확한 trajectory를 그릴 수 있었다.</li>
<li>이 연구를 통해 <strong>event camera를 사용하면 기존의 카메라를 사용해서 실패하는 경우의 85% 이상을 전부 개선할 수 있다</strong>는 결론을 내렸다.</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/3.gif" class="" title="ultimate slam"> 

<p> </p>
<ul>
<li>기존 카메라들보다 <strong>~8배 정도의 dynamic range</strong>를 가지고 있다.<ul>
<li>어두운 곳에서도 잘 볼 수 있다. 터널에서 나오거나 하는 상황에서도 굉장히 잘 볼 수 있다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDIuMTM0MDY=">‘Rotor failure’<i class="fa fa-external-link-alt"></i></span> 연구에서는 드론이 비행 중 하나의 rotor가 고장나서 비상착륙을 해야하는 시나리오를 다룬다. 이 때 드론이 아무데나 막 날라가지 않고 제자리에서 높이만 낮춰서 착륙을 해야하는데, 로터가 하나 고장났기 때문에 제자리에서 빠른 속도로 회전을 하면서 내려오게 된다.</li>
<li>광량이 충분한 상황에서는 SVO 같이 빠른 알고리즘을 사용하면 기존의 카메라로도 충분히 자세를 보정할 수 있었지만, 광량이 적은 곳에서는 motion blur가 너무 크게 나타나 불가능하다.</li>
<li>‘Rotor failure’ 연구에서는 <strong>event camera를 사용했을 때는 높은 dynamic range를 이용해서 아주 적은양의 광량에서도 안정적으로 위치를 추정할 수 있다는 것</strong>을 보여준다. 10 lux 정도까지 낮추는데에 성공했는데, 이는 보름달이 뜨는 밤 정도라고 한다.</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/4.gif" class="" title="Dynamic range"> 

<ul>
<li>Event camera는 <strong>기본적으로 엄청 빠르다</strong>.<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ycGcuaWZpLnV6aC5jaC9kb2NzL1JBTDE5X0ZhbGFuZ2EucGRm">‘Rapid, dynamic obstacle avoidance’<i class="fa fa-external-link-alt"></i></span> 연구에서는 드론에 축구공을 던졌을 때 바로 피할 수 있을 정도의 속도를 낼 수 있었다.</li>
<li>기존의 카메라로는 ‘이미지 취득 - 이미지 취득 - 트랙킹 알고리즘 수행 - 동적 객체 인지’에 보통 20~30 ms씩 걸리는데에 비해, event camera에서는 3.5 ms 밖에 걸리지 않았다.</li>
</ul>
</li>
</ul>
<p> </p>
<h1 id="Event-camera를-사람이-이해할-수-있을까"><a href="#Event-camera를-사람이-이해할-수-있을까" class="headerlink" title="Event camera를 사람이 이해할 수 있을까?"></a>Event camera를 사람이 이해할 수 있을까?</h1><ul>
<li>인터뷰어는 ‘기존의 카메라는 그래도 사람이 봤을 때 바로 이해할 수 있는데, 이벤트 카메라는 그렇지 않을 것 같다. <strong>사람이 이벤트 카메라도 이해할 수 있을까?</strong>‘ 라는 질문을 했다.</li>
<li>답변은 ‘<strong>Raw 데이터를 직접적으로 이해하기는 어렵다</strong>‘ 였다.</li>
<li>하지만 <span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWNjZXNzLnRoZWN2Zi5jb20vY29udGVudF9DVlBSXzIwMTkvcGFwZXJzL1JlYmVjcV9FdmVudHMtVG8tVmlkZW9fQnJpbmdpbmdfTW9kZXJuX0NvbXB1dGVyX1Zpc2lvbl90b19FdmVudF9DYW1lcmFzX0NWUFJfMjAxOV9wYXBlci5wZGY=">‘Event-to-Video 변환 뉴럴넷 개발’<i class="fa fa-external-link-alt"></i></span>연구에서는 RNN을 이용하여 사람이 이해할 수 있는 이미지의 형태로 event 데이터를 변환하는 네트워크를 개발하였다.<ul>
<li>처음에는 Event camera 시뮬레이터에서 학습을 하고, 아래 이미지는 실제 event camera에서 recon을 한 모습이다. 다양한 event camera의 제조사 제품에도 다 적용된다고 한다.</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/5.gif" class="" title="Event-to-Videos"> 

<ul>
<li>Event camera는 <strong>단일 센서로써 사용되면 안되고, 항상 기존의 camera와 함께 사용</strong>되어야한다고 생각한다. <ul>
<li>Event camera 자체는 하나의 high-pass filter로 볼 수 있다.</li>
<li>움직임이 전혀 없을 때, 예를 들어 자동차가 신호를 기다리면서 멈춰있을 때, 다양한 정보를 얻을 수 있다. <strong>기존의 카메라는 이렇게 움직임이 없는 상황에서 더욱 깔끔한 이미지를 얻으며 많은 정보를 높은 품질로 수집</strong>할 수 있다. </li>
<li>Event camera는 움직이지 않으면 아무런 데이터도 얻을 수 없다.</li>
</ul>
</li>
</ul>
<p> </p>
<h1 id="Event-camera를-어떻게-쓸-수-있을까"><a href="#Event-camera를-어떻게-쓸-수-있을까" class="headerlink" title="Event camera를 어떻게 쓸 수 있을까?"></a>Event camera를 어떻게 쓸 수 있을까?</h1><ul>
<li>인터뷰어는 ‘Event camera가 실제로 어떻게 적용되고 있는지? 스타트업 같은 곳에서 event camera를 사용해서 차별점을 두고 싶다면 어떻게 해야하는지?’ 에 대해 질문을 했다.</li>
<li>답변으로는 우선 ‘<strong>Top tier 회사와 automotive 목적으로 연구를 진행하고 있고, HDR imaging 쪽도 연구</strong>를 하고 있다. <strong>Pedestrian detection 연구</strong>도 하고 있다.’<ul>
<li>목표는 이러한 task들을 하는데에 10ms 미만으로 떨어트리는 것이다.</li>
</ul>
</li>
<li>Event camera를 쓰면서 느낀 점 중 하나는, <strong>기존의 카메라보다 훨씬 적은 memory를 필요로 한다는 것</strong>이다.<ul>
<li><a href="">‘TimeLens’</a>연구에서는 기존의 FullHD 카메라로 얻은 2개의 이미지 사이에 event 카메라로 취득한 event 정보를 이용해서 frame-rate를 대폭 향상시키는 연구를 진행했다.</li>
<li>High-FPS 이미지를 얻기 위해 <strong>비싼 초고속카메라를 쓸 필요가 없을 뿐</strong>더러, <strong>약 40% 정도 더 적은 메모리를 사용</strong>한다는 점이 매력적인 연구였다.</li>
<li>예를 들어, Huawei P40로 촬영한 8000FPS 이미지는 1초당 약 16GB의 데이터 저장공간을 요구하는데, event camera를 이용한 것은 4GB만 이용한다.</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/6.gif" class="" title="TimeLens"> 
<img src="/20220315-event-cameras-with-davide-scaramuzza/7.gif" class="" title="TimeLens2"> 

<p> </p>
<h1 id="Event-camera-근데-비싸지-않나…"><a href="#Event-camera-근데-비싸지-않나…" class="headerlink" title="Event camera, 근데 비싸지 않나…?"></a>Event camera, 근데 비싸지 않나…?</h1><ul>
<li><strong>현재 가격으로는 $3000~5000</strong> 정도 한다. 하지만 이는 대량생산 이전의 가격이고, killer application만 찾게 된다면 <strong>$5 정도로 낮춰질 수 있지 않을까</strong> 예상을 하고 있다.<ul>
<li>마치 예전에 depth sensor가 $10,000로 엄청나게 비쌌지만, 20년이 지난 현재는 많이 가격이 떨어진 것과 같은 행보를 걷고 있다고 생각한다.</li>
<li>시간은 걸릴거다. 하지만 언젠간 훨씬 저렴해질 것이라는걸 확신한다.</li>
<li>중국에서도 꽤 쓰고 있다고 알고있다.</li>
<li>SynSense라는 회사는 event camera를 이용해서 face recognition을 하는데 1mV의 전력을 소비하는 초저전력 시스템을 만들었다고 한다. 이 회사는 edge device / always-on-device 를 노린다고 한다.</li>
<li>로켓이나 미사일 추격시스템 같은 시스템에서도 쓰려고 한다.</li>
<li>Un-blurring image로도 쓸 수 있다.</li>
</ul>
</li>
</ul>
<p> </p>
<h1 id="Event-camera로-거리-추정도-가능한지"><a href="#Event-camera로-거리-추정도-가능한지" class="headerlink" title="Event camera로 거리 추정도 가능한지?"></a>Event camera로 거리 추정도 가능한지?</h1><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ycGcuaWZpLnV6aC5jaC9kb2NzL1JBTDE2X0VWTy5wZGY=">‘EVO’<i class="fa fa-external-link-alt"></i></span> 연구에서 <strong>visual odometry</strong> 연구를 진행했다.<ul>
<li>1개의 event camera와 IMU를 섞어서 VIO를 할 수도 있고, 2개의 event camera를 사용해서 triangulation을 통해 depth 추정을 할 수도 있다. 이 중 하나는 rgb 카메라로도 바꿀 수 있다.</li>
<li>최근에는 Sony Zurich와 합작해서 event-depth sensor를 만들고 있다. 레이저 포인트 프로젝터를 연결해서 엄청나게 빠른 속도로 레이저가 환경을 스캔하고, event 카메라가 이 레이저를 트랙킹해서 60ms 마다 뎁스를 추정하는 센서를 만들었다. 근데 event camera는 사실 더 빠르게 추적할 수 있는데, 레이저 포인터의 최대 속도 떄문에 60ms가 걸린다고 한다. (<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMTEuMTU1MTAucGRm">‘Event-based structured light 연구<i class="fa fa-external-link-alt"></i></span>)</li>
</ul>
</li>
<li>최근에 Sony Zurich와는 Event-based LiDAR 연구도 하고 있다.<ul>
<li>생각해보면, LiDAR는 엄청나게 많은 전력을 소비한다. 심지어 주변의 환경이 크게 변화하지 않을 때도 말이다. Event-camera를 이용해서 event가 있는 부분만이 moving object가 있다고 판단할 수 있고, 해당 위치에만 LiDAR 레이저를 쏨으로써 전체 전력량을 획기적으로 낮출 수 있다는 방법을 제안하고 있다.</li>
</ul>
</li>
</ul>
<img src="/20220315-event-cameras-with-davide-scaramuzza/8.gif" class="" title="evo"> 

<p> </p>
<h1 id="Event-camera를-써보고-싶다면"><a href="#Event-camera를-써보고-싶다면" class="headerlink" title="Event camera를 써보고 싶다면?"></a>Event camera를 써보고 싶다면?</h1><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ycGcuaWZpLnV6aC5jaC9yZXNlYXJjaF9kdnMuaHRtbA==">‘Event camera resources’ 웹페이지<i class="fa fa-external-link-alt"></i></span>를 팔로우하자! 여기에 관련된 데이터셋, 지난 10년간의 연구, 등등이 다 있다!</li>
<li>Event camera는 살 필요가 없다. 비싸니까! 데이터셋을 쓰면 된다.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.3 Computer Vision &amp; Imaging</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>Visual-SLAM</tag>
        <tag>Computer Vision</tag>
        <tag>Event Camera</tag>
        <tag>ETH Zurich</tag>
        <tag>Davide Scaramuzza</tag>
        <tag>Drone</tag>
      </tags>
  </entry>
  <entry>
    <title>IMU와 LiDAR를 사용할 때 주의해야하는 점 (번역)</title>
    <url>/20220315-lidar-imu-pitfalls/</url>
    <content><![CDATA[<h1 id="원-저자-Mateusz-Sadowski-블로그"><a href="#원-저자-Mateusz-Sadowski-블로그" class="headerlink" title="원 저자 - Mateusz Sadowski 블로그"></a>원 저자 - Mateusz Sadowski 블로그</h1><p>M. Sadowski는 SLAM toolbox 소프트웨어와 Weekly robotics 뉴스레터로 유명한 로보틱스 개발자다!</p>
<p>그 분이 쓰신 글을 살포시 번역했다.</p>
<p>출처: <span class="exturl" data-url="aHR0cHM6Ly9tc2Fkb3dza2kuZ2l0aHViLmlvL2Jhc2ljLXNlbnNvcnMtZm9yLW1vYmlsZS1yb2JvdHMv">https://msadowski.github.io/basic-sensors-for-mobile-robots/<i class="fa fa-external-link-alt"></i></span></p>
<p> </p>
<hr>
<h1 id="IMUs"><a href="#IMUs" class="headerlink" title="IMUs"></a>IMUs</h1><p>Mobile Robot을 쓴다면 IMU를 쓸 확률이 굉장히 높다.</p>
<p>IMU를 고를 때 고려해야하는 점들은 다음과 같다.</p>
<ul>
<li>가격</li>
<li>센서의 속도<ul>
<li>바퀴로 움직이는 로봇은 ~50Hz면 충분하다</li>
<li>UAV는 100~500Hz 정도 되야한다</li>
</ul>
</li>
<li>Attitude fusion이 센서 단에서 일어나는지, 아니면 여러 센서를 퓨전해야하는지?</li>
<li>인터페이스<ul>
<li>ROS를 쓴다면 USB가 제일 쉬움</li>
</ul>
</li>
<li>해상도, 최대치, gyro drift</li>
<li>캘리브레이션의 필요 유무</li>
</ul>
<p>&amp;nbsp</p>
<h1 id="IMU-사용할-때-고려해야할-점"><a href="#IMU-사용할-때-고려해야할-점" class="headerlink" title="IMU 사용할 때 고려해야할 점"></a>IMU 사용할 때 고려해야할 점</h1><blockquote>
<p>내 경험상으로는, 모바일 로봇에서 사용하기 가장 까다로운 센서는 IMU이다.</p>
</blockquote>
<h2 id="자기장-간섭"><a href="#자기장-간섭" class="headerlink" title="자기장 간섭"></a>자기장 간섭</h2><p>내가 사용하던 IMU는 magnetometer를 사용해서 heading (i.e. 북쪽방향)을 감지하고, 이를 통해 IMU 값도 보정하는 시스템이였다. 이 IMU는 드론에 탑재되어 heading 값으로 방향도 추정하고 IMU 값으로 움직임 값도 추정했는데, 며칠을 튜닝해도 heading 값이 안정화가 되지 않았었다. 그러다가 ‘우연히’ heading 값에서 보이는 노이즈의 원인을 알게 되었는데, 드론의 메인 로터의 속도가 올라가면서 노이즈도 같이 올라가는 것을 알 수 있었다. 나중에 알고보니, 우리 <strong>드론의 날개가 철을 포함하고 있어서 자기장 간섭을 일으켰던 것</strong>이다.</p>
<blockquote>
<p>로봇에 IMU를 쓴다면, 무조건 <strong>자기장이 발생할 수 있는 위치에서 최대한 멀리 떼놔야한다</strong>. 자기장이 발생하기 쉬운 곳은 1. 철로 된 파트, 2. 액추에이터, 3. 고전류 케이블이다.</p>
</blockquote>
<h2 id="진동"><a href="#진동" class="headerlink" title="진동"></a>진동</h2><p>모든 로봇은 진동한다. 다만 그 진동의 형태는 로봇이 어떤 재료로 만들어져있고 어떤 땅을 밟고 있냐에 따라 달라진다. 우리 팀이 만든 규칙으로는, 드론을 날려서 x,y,z축의 움직임을 그렸을 때, x축이나 y축의 움직임이 z축을 교차한다면 진동이 너무 강하다고 판단하기로 했다.</p>
<blockquote>
<p><strong>로봇을 평지에 가만히 세워놨는데, x/y축 움직임이 z축을 교차한다면 아마 그 진동을 댐프닝 해야할 것</strong>이다. </p>
</blockquote>
<h2 id="축-방향-정의"><a href="#축-방향-정의" class="headerlink" title="축 방향 정의"></a>축 방향 정의</h2><p>회사마다 IMU 축을 다르게 정의한다. IMU를 쓰고 있다면, 지금 당장 당신이 생각했던 축이랑 실제 축이랑 같은지 확인하길 추천한다. 심지어 종종 IMU 회사에서 제공하는 소프트웨어에서 정의한 축과 실제 축이 다를 때도 있다. 얼른 가서 확인해라. 왜냐면 나도 엄청 많이 틀렸거든.</p>
<blockquote>
<p><strong>실제 IMU 센서의 축과, 소프트웨어의 축이 맞는지 다시 한번 확인하자</strong>.</p>
</blockquote>
<h2 id="캘리브레이션"><a href="#캘리브레이션" class="headerlink" title="캘리브레이션"></a>캘리브레이션</h2><p>종종 직접 캘리브레이션 해야하는 IMU 들이 있다. 그리고 진짜 가끔, 캘리를 해줬는데 껐다가 키면 캘리 정보가 날아가는 IMU도 있다 (너 말하는거다, BNO055…). IMU를 타겟 디바이스에 탑재하고나서 모든 축으로 회전할 수 있는지, IMU를 사기 전에 꼭 확인하자. 탑재까지 다 했는데 캘리 다시해야하면 진짜 골치아프다.</p>
<img src="/20220315-lidar-imu-pitfalls/1.gif" class="" title="rofl"> 

<p> </p>
<hr>
<h1 id="LiDARs"><a href="#LiDARs" class="headerlink" title="LiDARs"></a>LiDARs</h1><p>라이다를 쓰면 ‘스마트’해진다. 내 로봇이 드디어 주변 환경을 ‘인식’할 수 있게 되고, 또 라이다라는거 그 자체가 그냥 엄청 미래적이잖아! </p>
<p>라이다를 쓴다는건 레이저 펄스를 쏴서 돌아오는 시간이 얼마나 걸리는지 측정한다는걸 의미한다. (종종 phase shift 같은거도 고려할거다.)</p>
<p>라이다는 다음과 같은 종류가 있다.</p>
<ol>
<li>단일 점 거리 센서  (e.g. Terabee TeraRanger Evo, Lightware SF11/C)</li>
<li>2D 라이다 (e.g. RPLidar A1, Hokuyo UTM-30LX) </li>
<li>3D 라이다  (e.g. Ouster, Velodyne Puck)</li>
<li>비-반복 패턴의 라이다 (e.g. Livox Mid-40)</li>
</ol>
<p>원하는 목적에 맞춰서, 원하는 인터페이스에 맞춰서, 필요한 파워에 맞춰서 제품을 고르면 된다.</p>
<p> </p>
<h1 id="LiDAR를-사용할-때-고려해야하는-점"><a href="#LiDAR를-사용할-때-고려해야하는-점" class="headerlink" title="LiDAR를 사용할 때 고려해야하는 점"></a>LiDAR를 사용할 때 고려해야하는 점</h1><p>라이다를 써보기 전 까지는 빛이 이렇게 짜증난다는걸 몰랐다.</p>
<h2 id="눈-피로"><a href="#눈-피로" class="headerlink" title="눈 피로"></a>눈 피로</h2><p>자, 여기서 얘기할 내용에 과학적인 증거는 없다는 점을 미리 밝힌다.</p>
<p>근데 라이다 쓰다보면 <strong>눈이 겁나 빨리 피곤해진다</strong>. 뭔가 엄청 얼얼한데, 내 동료 말로는 ‘아프진 않은데 아픈 느낌’이라고 한다. 대부분의 라이다들은 ‘Eyes-safe’하다고 하지만, 그래도 라이다를 직접 쳐다보지 않는걸 추천한다.</p>
<p>라이다를 오래 쓰면서 생기는 시력문제들에 대해 아는 사람들이 있으면 댓글로 좀 알려주길.</p>
<h2 id="Flip-effect"><a href="#Flip-effect" class="headerlink" title="Flip effect"></a>Flip effect</h2><p>아주 가끔 나타나는 현상이지만, 한번 나타나면 사람 돌아버리게 하는 현상이다.</p>
<p>이건 뭐 어떻게 고쳐야할지도 모르겠다. 그냥 센서 펌웨어를 업데이트하고 기도할 수밖에.</p>
<p>정확히 어떤 현상이나면, <strong>실제 거리보다 훨씬 적은 거리로 센서 값이 나오는 것</strong>이다.</p>
<p>실제 거리는 초록색+빨간색이다. 근데 센서는 초록색이라고 말하는거고.</p>
<img src="/20220315-lidar-imu-pitfalls/2.png" class="" title="flip"> 

<h2 id="시야각"><a href="#시야각" class="headerlink" title="시야각"></a>시야각</h2><p>LiDAR에 탑재된 레이저 에미터도 시야각이 있다. 약 1~3도 정도 된다.</p>
<p>아래 그림처럼 생긴 상황에서 레이저를 쏘면, 센서는 어떤 값을 내놓을거라고 생각하는가?</p>
<p>정답: 센서마다 다르다. <strong>어떤 센서는 Y를 주고, 어떤 센서는 X,Y 다 주고, 어떤 센서는 X랑 Y의 평균값을 준다</strong>. 이런-</p>
<img src="/20220315-lidar-imu-pitfalls/3.png" class="" title="fov"> 

<h2 id="최대거리-vs-외부환경"><a href="#최대거리-vs-외부환경" class="headerlink" title="최대거리 vs 외부환경"></a>최대거리 vs 외부환경</h2><p>LiDAR의 데이터시트를 보면 최대거리를 보통 알려준다. 하지만 실제로 쓸 떄는 이 <strong>최대거리에서 센서 값 절대로 안나온다</strong>.</p>
<p>이런 라이다 스펙은 보통 어느정도 반사가 가능한 물체들에 대해서 테스트 되기 때문이다. 완전 시꺼먼 물체에서는 절대로 최대거리로 나오지 않는다.</p>
<p>아래 그림은 태양빛에 있는 여러 빛의 스펙트럼이다. 보통 라이다 센서들은 여기 빨간색으로 표시된 부분에서 푹 떨어지는 곳들의 파장으로 레이저를 쏜다. 그래야 태양빛에 대한 간섭을 덜 받게 된다.</p>
<p><strong>라이다를 사려고 하는데 몇 nm의 파장을 쓰는지 안알려준다? 그 라이다 회사는 걸러야한다</strong>.</p>
<img src="/20220315-lidar-imu-pitfalls/4.png" class="" title="fov"> ]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>IMU</tag>
        <tag>LiDAR SLAM</tag>
        <tag>LIO</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformers - 고려대학교 강필성 교수님 강의</title>
    <url>/20220315-transformers/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/Yk1tV_cXMMU" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Transformers</tag>
      </tags>
  </entry>
  <entry>
    <title>Tuple의 pair programming 가이드</title>
    <url>/20220315-tuples-pair-programming-guide/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly90dXBsZS5hcHAvcGFpci1wcm9ncmFtbWluZy1ndWlkZS8=">https://tuple.app/pair-programming-guide/<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
      </categories>
      <tags>
        <tag>Pair programming</tag>
      </tags>
  </entry>
  <entry>
    <title>EE3564A/B - Convex Optimization - Stephen Boyd 강의 레퍼런스 (Stanford)</title>
    <url>/20220319-stanford-convex-optimization/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMMzk0MEREOTU2Q0RGMDYyMg==">강의 링크<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zZWUuc3RhbmZvcmQuZWR1L0NvdXJzZS9FRTM2NEE=">렉처 노트 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9zZWUuc3RhbmZvcmQuZWR1L0NvdXJzZS9FRTM2NEI=">렉처 노트 링크2<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93ZWIuc3RhbmZvcmQuZWR1L35ib3lkL2N2eGJvb2sv">책 링크<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="강의-소개"><a href="#강의-소개" class="headerlink" title="강의 소개"></a>강의 소개</h1><p>2008년 가을 학기에 진행된 ES3564A/B Convex Optimization 렉처이다.</p>
<p>Convex Optimization 분야의 바이블인 Stephen Boyd의 수업이다.</p>
<h2 id="커리큘럼"><a href="#커리큘럼" class="headerlink" title="커리큘럼"></a>커리큘럼</h2><ul>
<li>Convex sets</li>
<li>Convex functions</li>
<li>Convex optimization problems</li>
<li>Duality</li>
<li>Approximation and fitting</li>
<li>Statistical estimation</li>
<li>Geometric problems</li>
<li>Numerical linear algebra background</li>
<li>Unconstrained minimization</li>
<li>Equality constrained minimization</li>
<li>Interior-point methods</li>
<li>Subgradient methods</li>
<li>Localization methods</li>
<li>Decomposition and distributed optimization</li>
<li>Proximal and operator splitting methods</li>
<li>Self-concordance and Interior Point Method</li>
<li>Conjugate gradients</li>
<li>Non-convex problems</li>
<li>Neural Networks</li>
</ul>
]]></content>
      <categories>
        <category>4. Maths</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Convex Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>CS223a - Introduction to Robotics 코스 레퍼런스 (Stanford)</title>
    <url>/20220319-stanford-robotics-cs223a/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9zZWUuc3RhbmZvcmQuZWR1L0NvdXJzZS9DUzIyM0E=">강의 링크<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="강의-소개"><a href="#강의-소개" class="headerlink" title="강의 소개"></a>강의 소개</h1><p>2008년 겨울~봄 학기에 진행된 전설의 CS223a 로보틱스 렉처이다.<br>수많은 로봇공학 코스들의 레퍼런스가 된 렉처이며, 주로 집중하는 분야는 로봇 모션이다.</p>
<p>이 렉처를 통해 1. 로봇이 움직일 수 있는 공간을 이해 (i.e. Forward kinematics), 2. 로봇의 움직임을 수학적으로 표현 (ie. Jacobian, Dynamics), 3. 로봇의 움직임을 플래닝 (i.e. Inverse kinematics), 4. 로봇제어 (i.e. Control)을 익힐 수 있다.</p>
<h1 id="꿀팁"><a href="#꿀팁" class="headerlink" title="꿀팁"></a>꿀팁</h1><ul>
<li>영상은 사실 유투브에 CS223a 를 찾아서 보는게 더 좋다. 그래야 자막이 같이 나오기 때문이다.</li>
<li>하지만 위 링크에는 Lecture note들도 같이 있다. 렉처 노트를 받고나서 유투브로 보면 좋다.</li>
</ul>
<h2 id="커리큘럼"><a href="#커리큘럼" class="headerlink" title="커리큘럼"></a>커리큘럼</h2><ul>
<li>Introduction</li>
<li>Spatial descriptions 1</li>
<li>Spatial descriptions 2</li>
<li>Forward Kinematics 1</li>
<li>Forward Kinematics 2</li>
<li>Jacobians: Velocities</li>
<li>Jacobians: Explicit Form</li>
<li>Jacobians: Static Forces</li>
<li>Vision in Robotics</li>
<li>Inverse Kinematics/Trajectory generation</li>
<li>Dynamics: Acceleration and Inertia</li>
<li>Dynamics: Explicit Form</li>
<li>Control: PID control</li>
<li>Control: Joint space control</li>
<li>Control: Operational space control and Force control</li>
<li>Advanced Topics</li>
</ul>
<img src="/20220319-stanford-robotics-cs223a/cs223a.png" class="" title="cs223a">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>Stanford</tag>
        <tag>CS223a</tag>
      </tags>
  </entry>
  <entry>
    <title>Is SLAM solved? 고수들에게 물어본다</title>
    <url>/20220322-is-slam-solved/</url>
    <content><![CDATA[<h1 id="인터뷰-진행-방식"><a href="#인터뷰-진행-방식" class="headerlink" title="인터뷰 진행 방식"></a>인터뷰 진행 방식</h1><p>진행일: <strong>2010년</strong> (꽤 오래된 자료이긴 하다)</p>
<p>인터뷰 참여자:</p>
<ul>
<li>Udo Frese (인터뷰어)</li>
<li>Sebastian Thrun</li>
<li>Jose Neira</li>
</ul>
<h1 id="SLAM-이-문제는-다-풀린건가"><a href="#SLAM-이-문제는-다-풀린건가" class="headerlink" title="SLAM. 이 문제는 다 풀린건가?"></a>SLAM. 이 문제는 다 풀린건가?</h1><p>Thrun:</p>
<ul>
<li><strong>정적인 환경에서는 풀렸다</strong>.<ul>
<li>이론적으로도 깊게 이해하고 있고, scale-up 하는 방법도 있다.</li>
</ul>
</li>
<li><strong>동적인 환경에서는 풀리지 않았다</strong>.<ul>
<li>정적인 물체, 동적인 물체, 동적 움직임이 가능하지만 정지해있는 물체를 구분할 수 없다.</li>
<li>Semantic understanding을 할 수 있어야한다.</li>
</ul>
</li>
</ul>
<p>Neira:</p>
<ul>
<li>“처음보는 환경에서 센서만 가지고 주변 환경에 대한 모델을 추론하고 현재 위치를 추론할 수 있는가?” -&gt; 할 수 있다.</li>
<li>하지만 그 환경에…<ul>
<li>동적인 물체가 있는가?</li>
<li>동적인 물체를 무시하고 주변 환경을 모델링 할 수 있는가?</li>
<li>엄청나게 큰 환경에서 실시간으로 작동할 수 있는가?</li>
<li>환경의 변화를 인지하고 업데이트 할 수 있는가?</li>
<li>Geometric한 정보를 뛰어넘어 Semantic한 정보도 함께 모델링 할 수 있는가?</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="지난-20년간의-연구를-봤을-때-i-e-1990-2010-우리는-좋은-연구를-하고-있는가"><a href="#지난-20년간의-연구를-봤을-때-i-e-1990-2010-우리는-좋은-연구를-하고-있는가" class="headerlink" title="지난 20년간의 연구를 봤을 때 (i.e. 1990~2010), 우리는 좋은 연구를 하고 있는가?"></a>지난 20년간의 연구를 봤을 때 (i.e. 1990~2010), 우리는 좋은 연구를 하고 있는가?</h1><p>Thrun:</p>
<ul>
<li>잘 하고 있는 것 같다. 하지만 너무 특정 문제에 집중하는 것 같다.<ul>
<li><strong>아무도 GPS를 쓰려고 하지 않는다</strong>.</li>
<li><strong>Real-world에서는 GPS가 꼭 필요할텐데</strong> 말이다.</li>
</ul>
</li>
</ul>
<p>Neira:</p>
<ul>
<li>GPS를 쓰는게 이론적으로 매력적이지 않아서 연구가 많이 되지 않는 것 같다.</li>
<li>논문을 낼 때, 새로운 이론이 아무래도 단순 구현 방식보다 더 매력적이기 마련이다.</li>
<li>그러다보니 Real-world 문제를 해결하는 것에서 점점 멀어지는 것 같다.</li>
</ul>
<p> </p>
<hr>
<h1 id="모션-플래닝-분야에서도-비슷한-트렌드가-있다-i-e-특정-연구분야에만-집중한다-이런게-AI-science-분야에서-자주-나타나는-것인가"><a href="#모션-플래닝-분야에서도-비슷한-트렌드가-있다-i-e-특정-연구분야에만-집중한다-이런게-AI-science-분야에서-자주-나타나는-것인가" class="headerlink" title="모션 플래닝 분야에서도 비슷한 트렌드가 있다 (i.e. 특정 연구분야에만 집중한다). 이런게 AI/science 분야에서 자주 나타나는 것인가?"></a>모션 플래닝 분야에서도 비슷한 트렌드가 있다 (i.e. 특정 연구분야에만 집중한다). 이런게 AI/science 분야에서 자주 나타나는 것인가?</h1><p>Thrun:</p>
<ul>
<li>종종 특정 분야에 집중하다보면 ‘큰 그림’을 보지 못하면서 쓸려가는 경우가 있다. SLAM도 마찬가지다.</li>
<li><strong>우리의 목표는 로봇을 통해 세상에 이익을 가져와야하는 것</strong>이며, SLAM은 그 컴포넌트 중 하나라는걸 잊지 말아야한다.</li>
</ul>
<p>Neira:</p>
<ul>
<li>Academia의 한계인 것 같다.<ul>
<li>연구자들의 성과는 논문을 낸 수와, 그 논문들의 피인용수가 중요하다.</li>
<li>2자리 수의 피인용수가 나지 않는 논문은 의미가 없다고 생각하는 문화가 이런 한계를 만들어내는 것 같다.</li>
<li>이 때문에 <strong>연구자들이 빠르고 간단하게 피인용수를 얻어낼 수 있는 논문에 집중하게 되고, 사회에 임팩트를 줄 수 있는 논문과는 멀어지는 것</strong> 같다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="Big-O-notation이-SLAM-연구에-영향을-줬을-것이라고-생각하는가"><a href="#Big-O-notation이-SLAM-연구에-영향을-줬을-것이라고-생각하는가" class="headerlink" title="Big-O-notation이 SLAM 연구에 영향을 줬을 것이라고 생각하는가?"></a>Big-O-notation이 SLAM 연구에 영향을 줬을 것이라고 생각하는가?</h1><p>Thrun:</p>
<ul>
<li>그렇게 생각하지 않는다.</li>
<li>이전 연구를 개선하는 방향 vs 큰 그림을 그리는 방향. 이 둘 사이에서 항상 딜레마가 있는 것 같다.</li>
<li>Computational complexity는 SLAM에서 그렇게 크게 중요하게 보는 것 같지 않다. FastSLAM이나 GraphSLAM이 훨씬 더 효율적임에도 말이다.</li>
</ul>
<p>Neira:</p>
<ul>
<li>지난 연구들을 보면 SLAM 알고리즘들의 효율성에 대해 집중하는 경우가 많은 것 같다.</li>
<li>하지만 그에 비해 robustness에는 충분히 집중하지 않은 것 같다.</li>
</ul>
<p> </p>
<hr>
<h1 id="수식을-만들어서-문제를-풀려고-하는-것과-문제를-풀면서-수식을-발견하게-되는-것-어떤-것이-더-중요할까"><a href="#수식을-만들어서-문제를-풀려고-하는-것과-문제를-풀면서-수식을-발견하게-되는-것-어떤-것이-더-중요할까" class="headerlink" title="수식을 만들어서 문제를 풀려고 하는 것과, 문제를 풀면서 수식을 발견하게 되는 것. 어떤 것이 더 중요할까?"></a>수식을 만들어서 문제를 풀려고 하는 것과, 문제를 풀면서 수식을 발견하게 되는 것. 어떤 것이 더 중요할까?</h1><p>Thrun:</p>
<ul>
<li>무엇이 되었던지 사회에 줄 수 있는 임팩트를 생각해야한다.</li>
<li>이분법으로 생각하는 것 보다 이게 더 중요하다.</li>
<li>이론이 더 중요할 때도 있고, 구현이 더 중요할 때도 있다.</li>
</ul>
<p>Neira:</p>
<ul>
<li>‘좋은 이론보다 더 practical한 것은 없다’ 라고 한다.</li>
<li>이론이던, 구현이던, 둘 다 중요하니 언제든지 어떤 방법으로도 도전해보는게 중요하다.</li>
</ul>
<p> </p>
<hr>
<h1 id="Google-Street-View에는-SLAM이-들어가있는지-Thrun-타겟-질문"><a href="#Google-Street-View에는-SLAM이-들어가있는지-Thrun-타겟-질문" class="headerlink" title="Google Street View에는 SLAM이 들어가있는지? (Thrun 타겟 질문)"></a>Google Street View에는 SLAM이 들어가있는지? (Thrun 타겟 질문)</h1><p>Thrun:</p>
<ul>
<li>조금 들어있다.</li>
<li>카메라와 라이다를 써서 포즈를 추정한다.</li>
<li>그 외로 GPS와 IMU 내비게이션도 사용한다.</li>
</ul>
<p> </p>
<hr>
<h1 id="Real-world에서-쓸-수-있는-SLAM의-다른-예제가-있나"><a href="#Real-world에서-쓸-수-있는-SLAM의-다른-예제가-있나" class="headerlink" title="Real-world에서 쓸 수 있는 SLAM의 다른 예제가 있나?"></a>Real-world에서 쓸 수 있는 SLAM의 다른 예제가 있나?</h1><p>Thrun:</p>
<ul>
<li><strong>Transportation</strong>은 엄청 큰 시장이다.</li>
<li><strong>해저 구조를 탐색</strong>하거나, <strong>산업/공장들을 맵핑</strong>해서 온실가스 맵핑을 하는 것도 좋다.</li>
</ul>
<p> </p>
<hr>
<h1 id="SLAM의-트렌드는-점점-Visual-SLAM이-되고-있다-그-중-bundle-adjustment나-structure-from-motion이-주목받고-있다-SLAM이-컴퓨터-비젼-분야에-어떤-기여를-할-수-있을까-Neira-타깃-질문"><a href="#SLAM의-트렌드는-점점-Visual-SLAM이-되고-있다-그-중-bundle-adjustment나-structure-from-motion이-주목받고-있다-SLAM이-컴퓨터-비젼-분야에-어떤-기여를-할-수-있을까-Neira-타깃-질문" class="headerlink" title="SLAM의 트렌드는 점점 Visual-SLAM이 되고 있다. 그 중 bundle-adjustment나 structure-from-motion이 주목받고 있다. SLAM이 컴퓨터 비젼 분야에 어떤 기여를 할 수 있을까? (Neira 타깃 질문)"></a>SLAM의 트렌드는 점점 Visual-SLAM이 되고 있다. 그 중 bundle-adjustment나 structure-from-motion이 주목받고 있다. SLAM이 컴퓨터 비젼 분야에 어떤 기여를 할 수 있을까? (Neira 타깃 질문)</h1><p>Neira:</p>
<ul>
<li>디지털 카메라가 빠르고, 작고, 저렴해지고 있다.</li>
<li>Scale 정보가 없다는 점이 연구적인 목적에서 문제를 흥미롭게 만든다.</li>
<li>컴퓨터 비젼에서 풀려고 하는 문제들은 efficiency에 대해 집중하는 경우가 많이 없다. 그에 비해 visual-SLAM 연구자들은 efficiency에 많이 집중하기 때문에, 이 부분에서 기여를 할 수 있을 것이다.</li>
</ul>
<p> </p>
<hr>
<h1 id="SLAM-분야가-AI-커뮤니티에-어떤-메세지를-줄-수-있을까"><a href="#SLAM-분야가-AI-커뮤니티에-어떤-메세지를-줄-수-있을까" class="headerlink" title="SLAM 분야가 AI 커뮤니티에 어떤 메세지를 줄 수 있을까?"></a>SLAM 분야가 AI 커뮤니티에 어떤 메세지를 줄 수 있을까?</h1><p>Thrun:</p>
<ul>
<li>우리 겁나 어려운 문제 풀었다! 멋지지??!!</li>
</ul>
<p>Neira:</p>
<ul>
<li>로보틱스는 AI를 실제 세상으로 끌어오기 아주 좋은 필드다.</li>
</ul>
<p> </p>
<hr>
<h1 id="SLAM의-미래는-어떤-모습일까"><a href="#SLAM의-미래는-어떤-모습일까" class="headerlink" title="SLAM의 미래는 어떤 모습일까?"></a>SLAM의 미래는 어떤 모습일까?</h1><p>Thrun:</p>
<ul>
<li>미래에는 SLAM 연구들이 실제 세상에 많은 임팩트를 줄 수 있으면 좋겠다.<ul>
<li>나도 얼마전부터 논문을 적기보다는 수백/수천만명의 삶을 개선시킬 수 있는 실제 시스템을 만드는데에 집중하고 있다.</li>
</ul>
</li>
<li>SLAM 연구 분야의 최전선을 달리는 몇몇 리더들이 너무 특정 문제에만 집중하는 것 같아서 아쉽다.</li>
</ul>
<p>Neira:</p>
<ul>
<li>SLAM 기술에 대한 기대치와 꿈을 더 높게 잡으면, 더이상 특정 분야에만 집중하지 않게 되지 않을까?</li>
<li>좀 더 general하고 complex한 문제에 집중해야한다.</li>
</ul>
<h1 id="출처"><a href="#출처" class="headerlink" title="출처"></a>출처</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc2VtYW50aWNzY2hvbGFyLm9yZy9wYXBlci9JbnRlcnZpZXclM0EtSXMtU0xBTS1Tb2x2ZWQtRnJlc2UvMzhlMDdmOTU1NTExZTVlZWExYzNiZGUzMzZjN2E2MTRmMGFhN2VlMQ==">링크<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>Visual SLAM</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Sebastian Thrun</tag>
        <tag>Jose Neira</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 5 - Test Driven Development</title>
    <url>/20220324-clean-coders-tdd/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>“처음에 그는 간단한 유닛 테스트를 먼저 적었어요. 그건 코드라고 하기에 부끄러울 정도였어요. 그 후 그는 그 테스트를 통과할 수 있는 코드를 적었어요. 그 뒤에는 아주 조금 더 테스트 코드를 적었구요, 그 다음에 다시 또 코드를 적었어요.”</p>
<p>“이 순환과정은 정말로 빨랐어요. 제가 익숙했던 방법으로는 첫번째 컴파일을 하기 위해서는 1~2시간은 족히 걸렸는데 말이죠. 근데 그는 매 30초마다 코드를 실행시키고 있었어요.”</p>
<p>“물론 뭐 Java 같은 언어로는 바로바로 실행해서 30초마다 코드를 돌릴 수 있겠죠. 근데 전 C++ 프로그래머란 말이에요? 이런 젠장-, 내 빌드는 몇분/몇시간씩 걸린단 말이에요!”</p>
<p> </p>
<h1 id="The-Jury-is-in"><a href="#The-Jury-is-in" class="headerlink" title="The Jury is in"></a>The Jury is in</h1><p>“수술을 하는 의사는 ‘손씻기’의 중요성을 굳이 말할 필요가 없습니다. 프로그래머도 똑같아요. ‘TDD’의 중요성을 굳이 말할 필요가 없습니다.”</p>
<p>“당신의 코드가 어떻게 작동하는지도 모르면 어떻게 자기자신을 프로라고 칭할 수 있을까요? 테스트를 하지 않고서는 당신이 코드를 수정할 때마다 잘 작동할거라고 확신할 수 있을까요? 제대로 자동화된 테스팅 프레임워크 없이는 어떻게 코드를 수정할 때마다 바로바로 테스트 할 수 있을까요?”</p>
<p> </p>
<h1 id="The-three-laws-of-TDD"><a href="#The-three-laws-of-TDD" class="headerlink" title="The three laws of TDD"></a>The three laws of TDD</h1><ol>
<li>유닛테스트를 작성하기 전에는 어떠한 구현 코드도 작성하면 안됩니다.</li>
<li>이 유닛테스트를 적을 때는 딱 최소한의 ‘통과 불가능’ 상태를 만드는 테스트 코드를 적습니다. 예를 들어, ‘컴파일이 되지 않는다’도 하나의 ‘통과 불가능’ 상태입니다. 한번에 여러개의 ‘통과 불가능’ 조건을 만들면 안됩니다.</li>
<li>이 ‘통과 불가능’ 상태를 해결할 수 있는 최소한의 구현코드를 작성합니다. 이것보다 더 많은 구현 코드를 작성하면 안됩니다.</li>
</ol>
<p>“이 3가지 조건이 만드는 <strong>1개의 TDD 사이클은 약 30초 정도 걸릴 것</strong>입니다.”</p>
<p>“<strong>TDD 사이클을 계속 돌리며 개발을 진행</strong>합니다. 테스트 코드를 조금 더 추가하고, 이에 맞춰 조금 더 구현 코드를 추가합니다. 2개의 코드 흐름 - 테스트와 구현 - 이 동시에 진화하며 서로를 완성해갑니다.”</p>
<p> </p>
<h1 id="The-Litany-of-Benefits"><a href="#The-Litany-of-Benefits" class="headerlink" title="The Litany of Benefits"></a>The Litany of Benefits</h1><h2 id="Certainty"><a href="#Certainty" class="headerlink" title="Certainty"></a>Certainty</h2><p>“저 (i.e. 저자)는 Java 기반의 테스팅 도구인 FitNesse 라이브러리의 저자이자 메인테이너입니다. FitNesse는 현재 64,000줄의 코드로 작성되어있고, 그 중 28,000 줄은 약 2,200의 독립적인 유닛테스트를 위한 코드입니다. 이 테스트 코드들은 약 구현코드의 90%를 커버하고, 실행할 때는 약 90초 정도 걸립니다.”</p>
<p>“FitNesse 라이브러리 코드를 수정하고나면, 저는 그냥 유닛테스트를 실행해서 제 코드가 잘 작동하는지 검증합니다.”</p>
<h2 id="Defect-Injection-Rate"><a href="#Defect-Injection-Rate" class="headerlink" title="Defect Injection Rate"></a>Defect Injection Rate</h2><p>“작년에 저는 20,000줄 정도의 코드를 새로 작성했습니다. 그 중 약 17개 정도의 버그가 있었어요. Safe-critical 한 용도가 아닌 프로그램으로써는 굉장히 적은 수의 버그가 검출되었습니다.”</p>
<h2 id="Courage"><a href="#Courage" class="headerlink" title="Courage"></a>Courage</h2><p>“사람들은 왜 안좋은 코드를 보고도 고치지 않을까요? 왜냐하면 본인이 그 코드를 건들면서 프로그램이 터지는걸 걱정하기 때문입니다.”</p>
<p>“하지만 코드를 수정해도 프로그램이 터지지 않을 거라는 확신을 줄 수 있는 방식이 있다면, 그 사람은 자신있게 코드를 수정하지 않을까요?”</p>
<p>“이것이 <strong>TDD의 가장 큰 장점</strong>입니다. <strong>‘코드를 수정하는데에 들어가는 모든 두려움’을 없앨 수 있어요</strong>.”</p>
<h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>“유닛테스트 내부에서는 우리가 만든 기능을 실제로 사용하면서 그 기능이 잘 작동하는지를 검증합니다. 그렇기 때문에 모든 object들은 생성되고, 그리고 해당 object에서 쓸 수 있는 기능들은 전부 테스트 됩니다.”</p>
<p>“그렇기 때문에 우리는 유닛테스트를 봄으로써 <strong>우리가 만든 기능들이 어떻게 사용되는지 알 수 있습니다</strong>. 마치 하나의 <strong>documentation (사용설명서)</strong> 처럼요.”</p>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>“테스트를 해야한다는 조건 자체가, 개발자에게는 좋은 디자인을 고려하게 만듭니다.”</p>
<p>“테스트를 적고, 구현을 하고, 테스트를 적고, 구현을 하고…  테스트를 적을 때 마다, 어떻게 원하는 결과를 바로바로 만들 수 있는지 고민하면서, 자연스럽게 좋은 디자인에 대해 고려하게 됩니다.”</p>
<p>“‘아, 그냥 나중에 테스트 코드를 적으면 안될까?’. 안됩니다. 절대 안됩니다. 나중에 적는 테스트는 전부 ‘방어’ 목적인거에요. 구현 전에 적는 테스트는 ‘공격’입니다. 당신의 목표는 모든 공격에 대응할 수 있는 구현 코드를 작성하는 것이구요.”</p>
<p> </p>
<h2 id="What-TDD-is-NOT"><a href="#What-TDD-is-NOT" class="headerlink" title="What TDD is NOT"></a>What TDD is NOT</h2><p>“TDD는 어떤 종교나 마법공식이 아닙니다.”</p>
<p>“TDD를 하더라도 좋지 않은 코드를 적을 수 있어요. 좋지 않은 테스트도 적을 수 있구요.”</p>
<p>“아주 가끔 TDD를 수행하기 어려운 상황들도 있을 겁니다. 당신이 프로라면 TDD를 사용할 때와 사용하면 안될 때를 구분할 수 있어야해요.”</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>투자단계별 스타트업에 취직/이직할 때 물어봐야할 질문들</title>
    <url>/20220328-startup-questions/</url>
    <content><![CDATA[<h1 id="질문을-왜-해야해요"><a href="#질문을-왜-해야해요" class="headerlink" title="질문을 왜 해야해요?"></a>질문을 왜 해야해요?</h1><blockquote>
<ol>
<li>회사의 목표와 나의 목표가 잘 맞는지 알기 위해</li>
<li>회사가 가진 리스크를 평가하기 위해</li>
<li>경영진/실무진의 마인드와 업무방식을 확인하기 위해</li>
<li>회사에 대한 나의 관심을 어필하기위해</li>
</ol>
</blockquote>
<p>회사에 지원하면 면접을 보게 됩니다.</p>
<p>많은 지원자들은 면접관들에게 잘보이고 싶은 마음에 회사의 문화나 마인드셋에 질문을 가지지 않습니다.</p>
<p>회사에 전적으로 나를 맞추겠다고 하며, 자연스럽게 ‘<strong>을</strong>‘의 포지션이 됩니다.</p>
<p> </p>
<p>저는 이 면접 과정이 사실 마치 ‘<strong>소개팅</strong>‘ 과 같다고 생각합니다.</p>
<p>단순히 회사가 우리의 능력을 평가해서 합/불을 가르는게 아니라, ‘<strong>우리도 회사를 평가하고 갈지말지 고르는</strong>‘ 과정을 거치게 됩니다.</p>
<p>성공적인 연애가 되기 위해서는 상대와 내가 둘다 깐깐하게 따지고 정말 <strong>궁합</strong>이 맞는지를 봐야하듯이, 회사와 나도 속깊은 상의를 통해 서로의 궁합을 봐야합니다.</p>
<p>그래야 <strong>회사와 지원자 모두에게 윈-윈인 계약</strong>을 체결할 수 있습니다.</p>
<p> </p>
<p>이번 글에서는 스타트업의 투자 단계별로 어떤 질문을 하면 좋을지에 대해 제가 생각하는 것들을 공유합니다.</p>
<blockquote>
<p>노트: 다 물어보기에는 아마 시간이 부족할거에요! 본인의 상황에 맞춰 적절하게 질문하시길 ㅎㅎ</p>
</blockquote>
<p> </p>
<hr>
<h1 id="공통-질문"><a href="#공통-질문" class="headerlink" title="공통 질문"></a>공통 질문</h1><ol>
<li>제가 속하게 될 팀의 분위기 / 팀 컬쳐는 어떤가요?</li>
<li>저의 팀장/매니저가 되실 분은 어떤 업무 스타일을 선호하시나요?</li>
<li>어떤 커리어적 성장을 기대할 수 있을까요?</li>
<li>팀장/PM/매니저 님과는 얼마나 투명한 소통을 기대할 수 있을까요?</li>
<li>평소 업무 페이스에서는 어느정도 워라밸이 있을까요?</li>
</ol>
<p>어떤 환경에서든 ‘<strong>내가 업무를 하게 되는 팀</strong>‘, 그리고 ‘<strong>나</strong>‘에 대한 질문은 항상 하게 됩니다 ㅎㅎ</p>
<p><strong>2번 질문</strong>은 꼭 물어보시길 바랍니다!</p>
<p> </p>
<hr>
<h1 id="Pre-A-단계"><a href="#Pre-A-단계" class="headerlink" title="Pre-A 단계"></a>Pre-A 단계</h1><ol>
<li>시리즈 A를 받기 위해 (또는 현재 상태에서 생존을 위해) 제가 가장 먼저 해결해야할 task는 무엇인가요?</li>
<li>회사가 과거에 피봇팅을 한적이 있나요? 했다면 몇번, 어떤 제품으로 피봇팅하셨나요?</li>
<li>이 제품으로 창업을 하신 이유가 뭔가요? 우리가 이 시장을 공략하는 이유가 뭔가요? 우리가 성공한다면 왜 성공하게 될까요?</li>
<li>현재 Burn rate가 어느정도인가요? (i.e. 회사가 가진 돈이 바닥나려면 얼마나 남았나요?)</li>
<li>대표님이 생각하시는 마스터 플랜 / exit 전략은 무엇인가요?</li>
<li>(처우 논의 시) 스톡은 얼마나, 베스팅 기간 얼마에, 기대가치 얼마정도로 주실 수 있나요?</li>
<li>제가 엔지니어로써 얼마나 권한을 가지게 되나요?</li>
</ol>
<p>성공할 스타트업에서는 성공에 대한 ‘갈망’이 보입니다. 아직 세상은 좋은 제품을 보지 못했으나, 적어도 이 회사 대표님 만큼은 세상을 놀라게 할 제품을 만들어낼 것이라고 확신을 가져야합니다. 이러한 <strong>확신을 가진 대표님들이 답을 할 수 있는 질문</strong>들을 늘어놨습니다 - 첫 투자를 위한 계획, 전략 변화, 제품에 대한 확신, 현재 재무 상태 등등입니다.</p>
<p><strong>신입분들께는 첫 회사로써 Pre-A 스타트업은 추천하지 않습니다</strong>. 왜냐하면 Pre-A 스타트업은 엄청나게 빠른 의사결정과 전략변화를 통해 제품을 만들어야하기 때문입니다. 회사의 입장에서는 신입분들이 빠르게 정확하게 의사결정을 내리는걸 기대하기에 무리가 있고, 신입분들께서는 잡다한 일을 정말 짧은 사이클을 가지고 업무를 하기 때문에 탄탄한 업무 프로세스를 익힐 수 없습니다. 보통 Pre-A 스타트업에서 좋지 못한 커리어의 시작을 끊으신 분들께 ‘2년간 정말 열심히 했는데, 이력서에 적을만한건 없고 잡부처럼 일했다’ 라고 하시는 안타까운 경우가 보입니다.</p>
<p>대신, <strong>어느정도 업무 경력이 있으시고 하이리스크-하이리턴을 원하시는 분들께는 Pre-A 스타트업을 추천</strong>드립니다. 본인이 가지고 있는 노하우를 쏟아부어 <strong>초창기 스타트업의 스톡</strong>을 챙기시고, 추후 그 스톡의 가치가 몇천배 몇만배가 되었을 때 회사와 같이 exit (i.e. 은퇴)를 할 수 있습니다.</p>
<p> </p>
<hr>
<h1 id="시리즈-A-B-단계"><a href="#시리즈-A-B-단계" class="headerlink" title="시리즈 A~B 단계"></a>시리즈 A~B 단계</h1><ol>
<li>후속 투자를 받기 위한 조건이 무엇인가요?</li>
<li>투자자 중 가장 많이 투자를 한 곳이 어디인가요? 경영 간섭이 혹시 있나요?</li>
<li>현재 저희가 집중하는게 어떤 분야인가요? (i.e. 제품/시장 검증, 제품 안정화, scale-up)</li>
<li>경쟁사에 대비해 우리가 갖춰야할 강점/차별점은 무엇일까요?</li>
<li>현재 단계에서 업무 페이스/분위기는 어떤가요? 리스크를 지고 빠르게 치는 스타일인가요? 아니면 안정적으로 성장을 하는 편인가요?</li>
<li>현재 진행중인 프로젝트가 몇개정도일까요? PM 분들 중 기술적인 배경이 있으신 분이 있으실까요?</li>
<li>(처우 논의 시) 스톡은 얼마나, 베스팅 기간 얼마에, 기대가치 얼마정도로 주실 수 있나요?</li>
<li>현재 Burn rate가 어느정도인가요? (i.e. 회사가 가진 돈이 바닥나려면 얼마나 남았나요?)</li>
<li>대표님이 생각하시는 마스터 플랜 / exit 전략은 무엇인가요?</li>
</ol>
<p>시리즈 A~B 단계에서 물어볼 질문들은 ‘<strong>시장을 잘 이해하는지</strong>‘와 ‘<strong>더 많은 고객을 수용할 수 있는지</strong>‘에 대해 집중합니다.</p>
<p><strong>시리즈 A~B 단계는 회사가 제품의 초기 형태로 시장/투자자에게 인정을 받은 상태</strong>입니다. 한번 검증을 거쳐 그만큼 안정성은 생겼지만, 회사의 가치는 이미 꽤 올라간 상태이고 지분도 투자자들에게 꽤 넘어간 상태입니다 (그만큼 <strong>우리한테 돌아오는 스톡이 작아졌다는 뜻</strong>입니다). 하지만 후속투자를 받기 전에 돈이 동나버리면 역시 그대로 역사속으로 사라지는 단계입니다. </p>
<p>이 때부터 회사는 살아남기 위해 ‘<strong>시장을 잘 이해</strong>‘해야합니다. <strong>제품의 차별점을 확고하게 잡아야하고</strong>, 더 많은 고객을 유치하기 위해 <strong>마케팅</strong>도 잘 해야하고 <strong>개발역량</strong>도 키워야합니다. Pre-A 단계에서는 소규모 인원으로 프로토타입 제품만 잘 만들면 됬지만, 시리즈 A~B부터는 이전보다 많은 고객에게 더 높은 품질의 제품을 제공해야하기 때문에 회사의 인원이 적으면 20, 많으면 100명을 넘어가면서 <strong>임원진에게는 다수의 인원을 조화롭게 운영할 수 있는 능력</strong>이 필요합니다. </p>
<p> </p>
<hr>
<h1 id="시리즈-C-D-단계"><a href="#시리즈-C-D-단계" class="headerlink" title="시리즈 C~D 단계"></a>시리즈 C~D 단계</h1><ol>
<li>업무에 사용되는 개발 인프라는 어떻게 구성이 되어있나요?</li>
<li>현재 시장을 어느정도 점유하고 있나요? 성장 속도가 어떻게 되나요?</li>
<li>IPO까지 구체적인 계획이 궁금합니다.</li>
<li>복지는 어떤 것들이 있나요?</li>
<li>Exit 전략이 어떻게 되시나요?</li>
<li>이 포지션을 채용하시는 이유가 어떻게 되시나요?</li>
</ol>
<p>시리즈 C~D 단계에서 물어볼 질문들은 ‘<strong>Exit 전략 / IPO 가능성</strong>‘과 ‘<strong>편한 생활</strong>‘(ㅎㅎ) 입니다.</p>
<p><strong>시리즈 C~D 단계는 이미 주력 제품의 로드맵이 확고하게 잡힌 상황이며 상장을 위해 달려가는 단계</strong>입니다. 우리에게 돌아올 <strong>스톡은 굉장히 작아졌거나 없을 확률이 높으며</strong>, 대신 회사가 그동안 많은 노하우로 구축한 <strong>탄탄한 개발 인프라가 있을 확률이 높고</strong>, 필수 시니어 인력들의 유출을 막기 위해 <strong>좋은 복지를 가지고 있을 확률</strong>이 높습니다. 이렇다보니 <strong>신입 엔지니어들이 입사 했을 때 얻는 이점이 굉장히 많습니다</strong> (i.e. 실력 좋은 시니어, 좋은 복지, 좋은 개발 인프라 및 프로세스)</p>
<p>이 떄는 exit 전략이 중요합니다. 회사는 1. 다른 큰 회사에 피인수되려고 하거나, 2. IPO (상장)을 할 수 있습니다.</p>
<p>피인수됨으로써 exit를 하는 경우에는 ‘<strong>어디에 피인수되려고 하는지</strong>‘가 중요합니다. 예를 들어 Apple에 피인수되는 상황이라면, 내 소속은 곧 Apple 엔지니어가 될겁니다 (와!). 반대로 피인수에 대해 꿈만 꾸고있고 후보군이 없다면 위험도가 많이 높을겁니다.</p>
<p>IPO를 통해 exit를 하는 경우에는 ‘<strong>IPO를 위한 조건</strong>‘을 봐야합니다. 좋은 기술을 통해 기술상장을 할 수도 있는데, 이런 경우는 지원자가 박사급인 경우 큰 인센티브를 받고 입사할 수 있습니다. Scale-up을 통해 상장을 하는 경우, 지원자 개인에게 크게 돌아오는 보상은 없겠지만 복지가 좋을 확률이 높으며 회사가 좋은 개발 인프라와 시니어 엔지니어를 보유하고 있을 확률이 높습니다. 종종 ‘<strong>신사업</strong>‘을 통해 제품의 다양성을 넓히는 경우가 있는데, 이 때 종종 시리즈 C~D 단계에서 보기 힘든 스톡이 지급되기도 합니다.</p>
<p> </p>
<hr>
<h1 id="Post-IPO"><a href="#Post-IPO" class="headerlink" title="Post-IPO"></a>Post-IPO</h1><ol>
<li>업무에 사용되는 개발 인프라는 어떻게 구성이 되어있나요?</li>
<li>회사 개발조직 / 기획조직은 어떻게 구성되어있나요? 각 조직에 어느 수준의 전문가 분들이 계신지 궁금합니다.</li>
<li>현재 어떤 challenge가 있을까요?</li>
</ol>
<p><strong>이미 상장을 한 기업</strong>입니다. 더이상 스타트업이 아니게되며 (ㅎㅎ), 회사는 굉장히 안정적인 상태가 됩니다.</p>
<p>제가 아직 Post-IPO 기업에 대해서는 경험이 부족해서 의견을 드리기 조심스럽지만, 그런 상황이 온다면 아마 ‘<strong>어떤 이유로 채용을 하는건지</strong>‘가 궁금할 것 같습니다. 초창기 스타트업에 비해 훨씬 확립된 개발 프로세스, 역으로 이야기하면 경직된 프로세스에서 작업을 할텐데, 그런 점을 뛰어넘을 수 있는 이점들이 있는지 질문을 할 것 같습니다.</p>
<p> </p>
<hr>
<h1 id="대기업"><a href="#대기업" class="headerlink" title="대기업"></a>대기업</h1><ol>
<li>신규입사자 교육기간이 어느정도 되나요?</li>
<li>(입사 후 프로젝트가 배정되는 전형이라면) 프로젝트를 배정할 때는 어떤 기준으로 배정되나요?</li>
<li>(입사 때 부터 어떤 프로젝트에 배정되는지 아는 전형이라면) 현재 프로젝트는 어떤 단계에 있나요?</li>
<li>오피스 위치가 재배치 될 가능성이 있을까요?</li>
</ol>
<p>대기업은 가본적이 없어서 -.-… </p>
<p>대기업은 교육 프로세스, 개발 프로세스, 복지에 대해서는 보장된 곳이 아닐까 생각합니다. 대신 그만큼 지원자 개인으로써는 ‘톱니바퀴’ 취급을 당하게 되는 거구요. 그러면 <strong>직원/개인으로써 보장되야하는 이점과 안정성이 최대화</strong>되야하지 않을까 생각합니다.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Start-ups</tag>
        <tag>커리어</tag>
        <tag>스타트업</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 - Machine Learning 코스 레퍼런스 (Stanford)</title>
    <url>/20220329-standford-machine-learning-cs229/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vcGxheWxpc3Q/bGlzdD1QTG9ST012b2R2NHJNaUdRcDNXWFNodE1HZ3pxcGZWZmJV">강의 링크<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2NzMjI5LnN0YW5mb3JkLmVkdS9zeWxsYWJ1cy1hdXR1bW4yMDE4Lmh0bWw=">렉처 노트 링크<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="강의-소개"><a href="#강의-소개" class="headerlink" title="강의 소개"></a>강의 소개</h1><p>2018년 가을 학기에 진행된 CS229 머신러닝 렉처이다.</p>
<h2 id="커리큘럼"><a href="#커리큘럼" class="headerlink" title="커리큘럼"></a>커리큘럼</h2><ul>
<li>Supervised Learning setup, linear regression</li>
<li>Linear algebra</li>
<li>Weighted least squares, Logistic regression, Newton’s method, Perceptron, Exponential Family, Generalized Linear Models</li>
<li>Probability</li>
<li>Gaussian Discriminant Analysis, Naive Bayes</li>
<li>Laplace Smoothing. Support Vector Machines</li>
<li>Python</li>
<li>Support Vector Machines. Kernels.</li>
<li>Bias-Variance tradeoff. Regularization and model/feature selection.</li>
<li>Learning theory</li>
<li>Tree ensembles.</li>
<li>Neural Networks:basics</li>
<li>Neutral Networks:Training</li>
<li>Evaluation metrics</li>
<li>Practical advice for ML projects</li>
<li>K-means. Mixture of Gaussians. Expectation Maximization.</li>
<li>Factor analysis.</li>
<li>Principal Component Analysis. Independent Component Analysis.</li>
<li>MDPs. Bellman Equations</li>
<li>Value iteration and Policy iteration. LQR. LQG.</li>
<li>Q-learning. Value function approximation.</li>
<li>Policy Serach. REINFORCE. POMDPs.</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
      </categories>
      <tags>
        <tag>Andrew Ng</tag>
        <tag>Machine Learning</tag>
        <tag>Stanford</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title>Clean Coders 책 챕터 6 - Practicing</title>
    <url>/20220402-clean-coders-practicing/</url>
    <content><![CDATA[<blockquote>
<p>모든 프로들은 연습을 통해 최고 상태의 실력을 유지합니다.</p>
</blockquote>
<h1 id="Turnaround-Time"><a href="#Turnaround-Time" class="headerlink" title="Turnaround Time"></a>Turnaround Time</h1><p>“1960년대에 제가 코딩을 할 때에는, 컴파일 결과를 보기 위해 하루나 이틀정도를 기다려야했습니다.”</p>
<p>“현대의 프로그래머들은 컴파일을 위해 기다릴 필요가 없습니다. 제가 작성한 64,000 줄의 Java 프로젝트인 FitNesse를 전부 새롭게 빌드하고, 모든 유닛테스트와 통합테스트를 실행하는데에 4분도 채 걸리지 않습니다.”</p>
<p>“즉, 모든 QA 과정을 실행하는데에 소스코드에서부터 배포까지 4분도 걸리지 않는다는 뜻입니다.”</p>
<p> </p>
<h1 id="Kata"><a href="#Kata" class="headerlink" title="Kata"></a>Kata</h1><p>“Kata (품새)는 격투무술에서 특정 상황을 시뮬레이션하기 위해 사용되는 움직임의 모음입니다.”</p>
<p>“프로그래밍을 위한 <strong>kata는 특정 프로그래밍 문제를 풀기 위해 필요한 움직임 - 키보드 입력과 마우스 입력 -의 모음</strong>입니다.”r</p>
<p>“Kata를 통해 다양한 단축키도 익힐 수 있고, TDD와 CI와 같은 개념도 익힐 수 있습니다.”</p>
<p>“격투무술가가 여러 품새를 알듯이, 프로그래머도 다양한 kata를 익혀야합니다. 또, 기억에서 사라지지 않도록 자주 연습해줘야합니다.” </p>
<ul>
<li><span class="exturl" data-url="aHR0cDovL2thdGFzLnNvZnR3YXJlY3JhZnRzbWFuc2hpcC5vcmcv">http://katas.softwarecraftsmanship.org<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL2NvZGVrYXRhLnByYWdwcm9nLmNvbS8=">http://codekata.pragprog.com<i class="fa fa-external-link-alt"></i></span>.</li>
<li>The Bowling Game: <span class="exturl" data-url="aHR0cDovL2J1dHVuY2xlYm9iLmNvbS9BcnRpY2xlUy5VbmNsZUJvYi5UaGVCb3dsaW5nLUdhbWVLYXRh">http://butunclebob.com/ArticleS.UncleBob.TheBowling-GameKata<i class="fa fa-external-link-alt"></i></span></li>
<li>Prime Factors: <span class="exturl" data-url="aHR0cDovL2J1dHVuY2xlYm9iLmNvbS9BcnRpY2xlUy5VbmNsZUJvYi5UaGVQcmltZUZhY3RvcnMtS2F0YQ==">http://butunclebob.com/ArticleS.UncleBob.ThePrimeFactors-Kata<i class="fa fa-external-link-alt"></i></span></li>
<li>Word Wrap: <span class="exturl" data-url="aHR0cDovL3RoZWNsZWFuY29kZXIuYmxvZ3Nwb3QuY29tLzIwMTAvMTAvY3JhZnRzbWFuLTYyLWRhcmstcGF0aC5odG1s">http://thecleancoder.blogspot.com/2010/10/craftsman-62-dark-path.html<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>“진짜 잘 하는 수준까지 되려면, 음악을 틀고 리듬에 맞춰서 코드를 칠 수 있도록 되어보세요.” (ㄷㄷ)</p>
<p> </p>
<h1 id="Wasa"><a href="#Wasa" class="headerlink" title="Wasa"></a>Wasa</h1><p>“주짓수를 배울 때, wasa를 많이 했습니다. Wasa는 마치 두명이 하는 kata와 같은겁니다. 한쪽이 공격, 한쪽이 방어를 하며 동일한 움직임을 여러번 반복하며 서로의 롤을 바꾸게 되죠.”</p>
<p>“프로그래머도 마찬가지입니다. <strong>한쪽이 유닛테스트를 작성하고, 다른 쪽이 그 테스트를 통과하는 코드를 짜는거</strong>죠.”</p>
<p>“기존의 kata를 할 수도 있습니다. 이미 결과가 어떻게 나올지 알기 떄문에, 서로의 키보드/마우스 입력을 지적해주며 서로 얼마나 kata를 잘 기억하는지 평가해줄 수 있죠.”</p>
<p> </p>
<h1 id="Randori"><a href="#Randori" class="headerlink" title="Randori"></a>Randori</h1><p>“Randori는 자율적인 스파링입니다.”</p>
<p>“프로그래머의 randori는 ㄷ이렇게도 할 수 있습니다. 프로젝터를 켜서 벽에 화면을 보이게 하고, 한명이 테스트를 적고 앉습니다. 다음 사람은 테스트를 통과하는 코드를 작성하고, 또 테스트를 하나 새로 적습니다. 이렇게 테이블에 앉은 모든 프로그래머들이 돌아가면서 테스트-코드-테스트-코드를 작성하며 마칩니다.”</p>
<p> </p>
<h1 id="Broadening-your-experience"><a href="#Broadening-your-experience" class="headerlink" title="Broadening your experience"></a>Broadening your experience</h1><p>“고용주들은 보통 개발자들에게 단 하나의 언어, 플랫폼, 도메인에 집중하기를 원합니다. 이 구조에 익숙해진다면 우리의 이력서와 마인드셋인 시간이 지날수록 점점 더 좁아지게 될 겁니다.”</p>
<h2 id="Open-source"><a href="#Open-source" class="headerlink" title="Open source"></a>Open source</h2><p>“의사나 변호사들이 하는걸 따라해보죠. <strong>본인 분야가 아닌 다른 분야의 작업에 기여를 하면서 분야를 넓혀가는겁니다</strong>.”</p>
<p>“당신이 Java 프로그래머라면 Ruby on Rails 프로젝트에 도전해보세요. 당신이 C++ 프로그래머라면 Python 프로젝트에 도전해보세요.”</p>
<h2 id="Practice-Ethics"><a href="#Practice-Ethics" class="headerlink" title="Practice Ethics"></a>Practice Ethics</h2><p>“프로들은 개인 시간에 연습합니다. 고용주가 우리의 능력을 최상위로 유지해줄 의무는 없어요. 환자들도 의사들에게 수술을 연습하라고 돈을 주는게 아닙니다. 관객들도 뮤지션들에게 스케일 연습하라고 돈주는게 아니죠.”</p>
<p>“<strong>연습을 하는 시간은 개발자 본인만의 고유의 시간이니, 어떠한 언어를 사용하던, 어떠한 플랫폼을 사용하던 상관없습니다</strong>.”</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.1 Software Engineering</category>
        <category>Clean Coders 책 정리</category>
      </categories>
      <tags>
        <tag>Clean Coders</tag>
      </tags>
  </entry>
  <entry>
    <title>int, float, double 등은 pass-by-value로 넘겨야하는 이유</title>
    <url>/20220403-pass-by-value-primitives/</url>
    <content><![CDATA[<h1 id="Pass-by-value-vs-Pass-by-reference"><a href="#Pass-by-value-vs-Pass-by-reference" class="headerlink" title="Pass-by-value vs Pass-by-reference"></a>Pass-by-value vs Pass-by-reference</h1><p>C++에서 함수에 인자를 넘기는 방법은 두가지가 있다 - <code>pass-by-value</code>와 <code>pass-by-reference</code>이다.</p>
<p>Pass-by-value는 함수 외부의 값을 <strong>copy</strong>하며 가져온다. <code>foo(int val1)</code>과 같은 방식이다.</p>
<p>Pass-by-reference는 함수 외부의 값을 <strong>참조</strong>해서 가져온다. 기존의 C에서 pointer로 가져오는 방식을 좀 더 사용하기 쉽게 바꾼 방식이. 실제 컴파일러 내부에서는 pointer로 처리한다. <code>foo(int&amp; val1)</code>과 같은 방식이다.</p>
<p>보통 큰 object를 함수 인자로 넘기는 경우, copy 연산을 하면 너무 큰 비용이 들기 때문에 pass-by-reference를 사용하는 것이 일반적이다. 이 부분에 대해서는 이전의 <span class="exturl" data-url="aHR0cDovL3d3dy5jdi1sZWFybi5jb20vMjAyMDEyMDMtZmFjb250aS1WYWx1ZS1zZW1hbnRpY3MtdnMtcmVmZXJlbmNlcy8=">‘Const reference를 쓰세요!’ 글<i class="fa fa-external-link-alt"></i></span>에서 다룬 적이 있다.</p>
<p> </p>
<h1 id="Primitive-type에는-pass-by-value-pass-by-reference"><a href="#Primitive-type에는-pass-by-value-pass-by-reference" class="headerlink" title="Primitive type에는 pass-by-value? pass-by-reference?"></a>Primitive type에는 pass-by-value? pass-by-reference?</h1><p>최근에 팀에서 이에 대해 2가지로 의견이 갈렸습니다.</p>
<ol>
<li>int, float, double 같이 작은건 copy하는게 더 빠르다. 그러니 pass-by-value를 쓰자.</li>
<li>기존의 <code>const &amp;</code> (i.e. pass-by-reference)를 쓰자. 이래야 const-ness도 명확하게 표현할 수 있다.</li>
</ol>
<p>여러 의견이 오갔지만, <code>pass-by-value</code>를 사용하는 쪽으로 의견이 수렴되었습니다. 이유는 다음과 같습니다.</p>
<p><code>int</code>, <code>float</code>과 같은 Primitive 타입은 보통 굉장히 작습니다. <code>double</code>, <code>long long</code>을 제외하면 모든 경우가 reference보다 작습니다. 그렇기 때문에 <strong>primitive type의 경우 reference를 하는 게 더 큰 비용</strong>이 들어가므로, <strong>pass-by-value</strong>를 해주는게 좋습니다.</p>
<p>또, <strong>pass-by-value를 하는 경우 alias를 피할 수 있기 때문</strong>에, 컴파일러의 optimizer가 더 효율적으로 작동하게 됩니다.</p>
<p>컴파일러에서 <code>-O3</code> 옵션을 걸어줄 경우, pass-by-value나 pass-by-reference 모두 같은 값으로 수렴하게 된다는 것을 확인했습니다. 대신 이 경우 각각 구현의 모습은 <code>foo (int val1, int val2, int val3)</code>와 <code>foo (cosnt int&amp; val1, const int&amp; val2, const int&amp; val3)</code>처럼 생겼는데, 후자의 경우 예상 외로 가독성이 오히려 떨어지는 것을 볼 수 있었습니다.</p>
<h1 id="출처"><a href="#출처" class="headerlink" title="출처"></a>출처</h1><p><span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvMTQwMTMxMzkvaXMtaXQtY291bnRlci1wcm9kdWN0aXZlLXRvLXBhc3MtcHJpbWl0aXZlLXR5cGVzLWJ5LXJlZmVyZW5jZQ==">https://stackoverflow.com/questions/14013139/is-it-counter-productive-to-pass-primitive-types-by-reference<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>2022년 4월 SLAM 뉴스</title>
    <url>/20220408-2022-april-slam-news/</url>
    <content><![CDATA[<blockquote>
<p>오랜만이네요 진짜,, 그동안은 엔지니어링 쪽에 집중하다보니 논문 팔로우를 잘 못했네요</p>
</blockquote>
<p>논문 이름 누르면 자세한 정보가 열립니다!</p>
<p>&nbsp;</p>
<h2 id="이번-달-내가-관심가지는-논문들-키노트-랜드마크-급"><a href="#이번-달-내가-관심가지는-논문들-키노트-랜드마크-급" class="headerlink" title="이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)"></a>이번 달 내가 관심가지는 논문들 (키노트/랜드마크 급)</h2><details>
  <summary> VPR-Bench : An Open-Source Visual Place Recognition Evaluation Framework with Quantifiable Viewpoint and Appearance Change. </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vc2Nob2xhcl91cmw/dXJsPWh0dHBzOi8vaW50ZWxsaWdlbnQtdmVoaWNsZXMub3JnL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIyLzAzL1phZmZhcklKQ1YyMDIxX1ZQUkJlbmNoX2FyeGl2X3ByZXByaW50MjAwNS4wODEzNV9yZWR1Y2VkRmlsZVNpemUucGRmJmhsPWVuJnNhPVgmZD0xMDU5NzA5NTA3MDM4Mjg4OTk1MyZlaT13dWxOWXJPV09wYU02clFQN3FHazZBTSZzY2lzaWc9QUFHQmZtMzJ4TXpxYjhhVGRMYzJWdGcwcXJtM1VEd3lSQSZvaT1zY2hvbGFyYWxydCZoaXN0PWc0c1l5R0lBQUFBSjo1NzM4MzMwMTc3NTA4NTgwOTYzOkFBR0JmbTJJNjQxclFPYUpaaUdfV0xMakFEOU5uX0hPVHcmaHRtbD0mcG9zPTAmZm9sdD1jaXQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>다양한 Visual Place recognition 기술들을 벤치마킹하는 소프트웨어 공개</li>
<li>벤치마크 결과:<ul>
<li>은탄환은 없다.</li>
<li>각각의 데이터셋에서 SOTA를 찍은 알고리즘들이 여러개가 있다.</li>
<li>모든 VPR 방식들이 perceptually-aliased 환경과 less-structured한 환경에서 잘 작동하지 못한다.</li>
<li>모든 VPR 방식들이 viewpoint change에 약하다. 그래도 좌/우로 움직인거는 잘 받아칠 수 있는데, 3D change는 심각하다.</li>
<li>특정 방향에서 조명이 오는 경우 난이도가 급격하게 상승한다.</details>

</li>
</ul>
</li>
</ul>
<details>
  <summary> GTP-SLAM: Game-Theoretic Priors for Simultaneous Localization and Mapping in Multi-Agent Scenarios </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMTY2OTAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>기존의 SLAM을 multi-player interaction을 고려하지 않는다.</li>
<li>새로 제안하는 방법은 multi-robot 시나리오에서 motion planning을 dynamic game theory로 푸는 방법이 있는데 (i.e. IBR - iterative best responsnse), 이 방식을 SLAM에 넣어서 하나의 player의 시점에서 모든 player들의 dynamic state, control inputs, 그리고 landmark position 까지 joint estimation을 하는 방법을 제안한다.</li>
</ul>
</details>

<details>
  <summary> 실내자율주행로봇을위한3차원다층정밀지도구축및 위치 추정 알고리즘 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cua29yZWFzY2llbmNlLm9yLmtyL2FydGljbGUvSkFLTzIwMjIwOTA2NTcwOTQ0OC5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>LiDAR + IMU 센서와 LOAM 기반의 알고리즘을 이용하여 1개 층의 맵을 획득하고, 이것을 반복하여 여러개의 층의 맵을 모은 다음, 맵에서 겹치는 부분들을 global registration해서 다층 맵을 만드는 방법</li>
</ul>
</details>

<details>
  <summary> Supervised semantic segmentation based on deep learning: a survey </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczExMDQyLTAyMi0xMjg0Mi15">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Semantic segmentation 기법들에 대한 survey. 특정 시각으로 바라보며 평가를 수행한다.<ul>
<li><ol>
<li>Reduced feature map으로 계산을 해놓고, high-resolution map으로 다시 폈을 때 정확하게 계산해내기를 바랄 수는 없다. 근데 많은 알고리즘들이 이런 방식을 계속 사용한다.</li>
</ol>
</li>
<li><ol start="2">
<li>Target이 multi-scale로 나타날 수 있을 때, 또 background에 무엇이 있냐에 따라서 문제가 어려워질 수 있다.</li>
</ol>
</li>
<li><ol start="3">
<li>Intra-class difference, inter-class similarities 때문에 잘못된 classification이 나타날 수 있다.</li>
</ol>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> SLAM-Supported Self-Training for 6D Object Pose Estimation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMDQ0MjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>John Leonard 교수님 랩실 연구</li>
<li>6DOF 딥러닝 pose estimator를 개발할 때, 직접 라벨링한 부정확한 데이터로 하지 말고, SLAM을 돌려 얻은 pose 정보를 사용함으로써 pose estimation network를 fine-tuning할 수 있는 방법을 제안.</li>
</ul>
</details>

<details>
  <summary> MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMTMzMTAucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>DETR 기반의 monocular 3D detetor이다. Depth supervision도 필요 없고, anchor나 NMS도 사요하지 않는다고 한다.</li>
<li>Transformer 쪽을 공부하고 한번 봐야겠다.</li>
</ul>
</details>

<details>
  <summary> Tune your Place Recognition: Self-Supervised
Domain Calibration via Robust SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMDQ0NDYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Visual place recognition 모듈들은 보통 학습된 모델에 오버핏하기 마련이라 새로운 환경에서는 (i.e. 새로운 domain) 잘 작동하지 않는다. 그래서 새로운 환경에서는 보통 fine-tuning을 통해 성능을 높혀줘야한다. 이 연구에서는 새로운 환경의 데이터로 fine-tuning을 하는데, 그 때 필요한 pose 데이터를 SLAM으로 뽑아줘서 self-supervised learning을 할 수 있다.</li>
</ul>
</details>

<details>
  <summary> IMOT: General-Purpose, Fast and Robust Estimation for Spatial Perception Problems with Outliers </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDQuMDEzMjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>범용적인 outlier-rejection 프레임워크이다. 마치 RANSAC 같은 용도로 사용된다고 보면 된다. 저자는 rotation averaging, rotation search, point cloud registration, SLAM 등에 쓸 수 있다고 한다. GNC나 ADAPT와 같은 robust estimator 보다 3-125배 빠르다고 한다 (근데 난 이거 처음 들어보는데…)</li>
<li>작동 방식은 간단하다. 모든 데이터에 non-minial estimation을 적용해서 inlier들이 나오면, 그 데이터에 otsu’s threshold를 적용한다. 거기서 살아남은 데이터에 한번 더 otsu’s threshold를 적용한다. 그제서야 살아남은 데이터들이 ‘진짜배기 inlier’가 되어, 다음 iteration으로 들어간다. Converge 할 때 까지 iteration을 돌리면 된다.</li>
<li>감상: 신박하다 ㅋㅋ</li>
</ul>
</details>

<details>
  <summary> Online panoptic 3D reconstruction as a Linear Assignment Problem </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDQuMDAyMzEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Panoptic image segmentation 결과를 가지고 3D reconstruction을 빠르게 하는 방법을 소개한다. 실시간으로 돌면서, 꽤 큰 환경까지 커버하기 위해 개발되었다.</li>
<li>기존의 data association 알고리즘을 개선하는것 만으로도 이게 가능해졌다고 한다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly90dXR2aXNpb24uZ2l0aHViLmlvL09ubGluZS1QYW5vcHRpYy0zRC8=">GitHub Page<i class="fa fa-external-link-alt"></i></span> 쩐다</li>
</ul>
</details>

<details>
  <summary> Leveraging Equivariant Features for Absolute Pose Regression </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDQuMDIxNjMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>저자들은 기존의 CNN은 absolute pose regression과 같이 geomtric 문제를 풀수 있을 정도의 geometric information을 충분히 담지 못한다고 판단하였다. </li>
<li>그래서 저자들은 Rotation과 translation 정보를 담는 equivariant CNN을 사용하여 camera motion 정보를 feature space에 직접 담는 방식을 사용한다.</li>
</ul>
</details>

<details>
  <summary> GPS-Denied Global Visual-Inertial Ground Vehicle State Estimation via Image Registration </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MuY211LmVkdS9+a2Flc3MvcHViL0xpdG1hbjIyaWNyYS5wZGY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Michael Kaess 교수님 랩실 연구</li>
<li>GPS가 없는 곳에서 위성 이미지를 이용해서 로봇의 위치를 찾는 방법<ul>
<li>Multi-stereo visual inertial odometry (MSVIO)로 local tracking 수행.</li>
<li>로봇이 움직이면서 probabilistic occupancy model을 이용하여 synthetic orthographic 이미지 생성. 이후, scan match를 통해서 위성이미지와 비교함으로써 위치 추정. 초기 위치는 GPS로 찾지만, GPS 신호가 끊기는 순간 위 방법을 사용함.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Semantic scan context: a novel semantic-based loop-closure method for LiDAR SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvczEwNTE0LTAyMi0xMDAzNy13">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>아직 scihub 안열림…</li>
<li>2-step global ICP와 semantic-based descriptor를 소개한다!</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xpbGluLWhpdGNydC9TU0M=">https://github.com/lilin-hitcrt/SSC<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</details>

</details>

<details>
  <summary> Multi-Robot Active Mapping via Neural Bipartite Graph Matching </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMTYzMTkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Multi-robot active mapping 문제: 최소한의 시간 안에 multi-robot을 사용하여 맵 스캐닝을 끝내는 최적화 문제<ul>
<li>효율적이게 문제를 해결하기 위해서는 정확하게 position estimation을 하는 것이 중요하다.</li>
<li>기존의 방법은 굉장히 근시안적이라서 time efficiency가 별로거나, 또는 최종 목적지를 direct regression하지만 맵의 구석구석 모두를 스캐닝을 한다는 확신이 없었다.</li>
</ul>
</li>
<li>이 논문에서는 NeuralCoMapping이라는 방식을 제안한다. 위 두가지 방식의 장점만을 따왔다.<ul>
<li>문제를 bipartite graph matching으로 해결한다.</li>
<li>Multiplex graph neural network (mGNN)으로 만들어 효과적인 graph matching을 한다. </li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Neural RF SLAM for unsupervised positioning and mapping with channel state information </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMDgyNjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>퀄컴 형님들의 RF (radio frequency)를 이용한 슬램</li>
<li>RF 송신기와 수신기들의 정보를 이용해서 실내 위치 추정 및 송수신기 맵핑 가능</li>
</ul>
</details>

<details>
  <summary> Point and Line Feature-based VIO for Mobile Devices </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ib29rcy5nb29nbGUuY28ua3IvYm9va3M/aGw9ZW4mbHI9bGFuZ19lbiZpZD1Pa2RuRUFBQVFCQUomb2k9Zm5kJnBnPVBBMjg0Jm90cz1GVnM5V1pPTDhnJnNpZz1aN3FnTF9rT2lydmVsZVhySmFXVlpoVnQzR2cmcmVkaXJfZXNjPXkjdj1vbmVwYWdlJnEmZj1mYWxzZQ==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Sliding window 기반의 point &amp; line VIO 기법. 모바일에서 돌아감! VINS-Mobile을 참고해서 만듬.</li>
</ul>
</details>

<details>
  <summary> Event-driven Feature Detection and Tracking for Visual SLAM (PhD Thesis) </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5jb20vc2Nob2xhcl91cmw/dXJsPWh0dHBzOi8vd3d3LnJlc2VhcmNoLWNvbGxlY3Rpb24uZXRoei5jaC9iaXRzdHJlYW0vaGFuZGxlLzIwLjUwMC4xMTg1MC81NDE3MDAvaWduYWNpb19hbHp1Z2FyYXlfZG9jdG9yYWxfdGhlc2lzLnBkZj9zZXF1ZW5jZT01JmhsPWVuJnNhPVgmZD05MzY2NzIyMTkwNDI4NDU0MTQwJmVpPUliTlhZcjY4SnNTNHl3VHR6Yl9RREEmc2Npc2lnPUFBR0JmbTJkYTVJdEFTSDBnWGxZTUtXUnFwZW0wTGpPTkEmb2k9c2Nob2xhcmFscnQmaGlzdD1nNHNZeUdJQUFBQUo6NTczODMzMDE3NzUwODU4MDk2MzpBQUdCZm0ySTY0MXJRT2FKWmlHX1dMTGpBRDlObl9IT1R3Jmh0bWw9JnBvcz0xJmZvbHQ9Y2l0">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Event camera의 고수인 Ignacio Alzugaray의 박사 졸업 논문</li>
<li>4 개의 논문 발표<ul>
<li>Asynchronous Corner Detection and Tracking for Event Cameras<br>in Real-Time</li>
<li>ACE: An Efficient Asynchronous Corner Tracker for Event Cameras</li>
<li>Asynchronous Multi-Hypothesis Tracking of Features with Event Cameras</li>
<li>HASTE: multi-Hypothesis Asynchronous Speeded-up Tracking of Events</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Cascaded Keypoint Detection and Description for Object Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wcm9maWxlL0FiZHVsbWFsaWstTW9oYW1tZWQtMi9wdWJsaWNhdGlvbi8zNTk3MzMzMzJfQ2FzY2FkZWRfS2V5cG9pbnRfRGV0ZWN0aW9uX2FuZF9EZXNjcmlwdGlvbl9mb3JfT2JqZWN0X1JlY29nbml0aW9uL2xpbmtzLzYyNGMyOGI1ODVkMTQxNjc3ZjQxYWRkMi9DYXNjYWRlZC1LZXlwb2ludC1EZXRlY3Rpb24tYW5kLURlc2NyaXB0aW9uLWZvci1PYmplY3QtUmVjb2duaXRpb24ucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Upright FAST-Harris Filter and Binary Robust Independent Elementary Feature descriptor를 사용한다 (UFAHB)</li>
</ul>
</details>

<details>
  <summary> NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMTEyODMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>기존의 방법들<ul>
<li>NeRF는 reconstruction/rendering 쪽에 엄청 잘되지만, 한 scene을 렌더링하는데에 엄청 오래 걸리고 MLP의 크기에 제한이 걸려서 큰 scene을 렌더링하기 어렵다.</li>
<li>그에 비해 3D recon은 이쁘게 안나온다.</li>
</ul>
</li>
<li>NeRFusion은 NeRF와 TSDF 기반 fusion 방식을 섞어서 효율적인 large-scale reconstruction + photorealistic rendering을 만든다.<ul>
<li>Direct network inference를 통해 per-frame local radiance field를 예측한다.</li>
<li>이 정보를 기반으로 새롭게 제안되는 GRU를 (RNN계열) 사용해 global + sparse한 recon을 22FPS로 수행한다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> A Real World Dataset for Multi-view 3D Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMTEzOTcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>RGB + Depth 이미지 페어로 371개의 3D 오브젝트 모델 데이터셋을 공개한다.</li>
</ul>
</details>

<details>
  <summary> AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMDk1MTYucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Multimodal 3D task (e.g. shape completion, reconstruction, generation)에 쓸 수 있는 autoregressive prior for 3D shape을 제안한다.</li>
</ul>
</details>

<details>
  <summary> Improving Monocular Visual Odometry Using Learned Depthx </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDQuMDEyNjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Sparse depth map과 RGB input을 사용해서 Dense mapping을 위한 scale-consistent depth을 만들 수 있다.</li>
<li>기존의 Learning-based VO 보다 더 다양한 환경에서 generalization이 잘 되는 것을 볼 수 있다.</li>
<li>그리고 기존의 Geometry-based VO의 성능을 더 좋게 만들 수 있다.</li>
</ul>
</details>

<details>
  <summary>  A Self-Supervised, Differentiable Kalman Filter for Uncertainty-Aware Visual-Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDMuMDcyMDcucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>VO/VIO의 방식들.<ul>
<li>기존의 Filter/Optimizaztion 기반의 VIO는 보통 잘 되는 편이지만 조명변화, 급격한 카메라 움직임, 텍스처가 없는 환경에서 잘 작동하지 않는다.</li>
<li>Learning-based VO/VIO는 이러한 환경에서 기존의 방법보다는 더 잘 작동할 수 있지만, 보통 평균 성능이 더 떨어지는 편이다.</li>
<li>Hybrid VO/VIO는 위 두가지 방식을 합치는 방식이다. </li>
</ul>
</li>
<li>우리가 만든 hybrid VIO를 소개한다.<ul>
<li>Differentiable Kalman filter를 사용한다. IMU 모델과 뉴럴넷을 사용한 relative pose measurement 모델을 사용한다.</li>
<li>Self-supervised learning을 사용하여 비슷한 방식의 supervised learning 방식보다 더 효율적이다.</li>
<li>기존의 filter/optimization 기반의 VIO가 실패하는 부분에서 우리의 hybrid VIO는 잘 된다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Efficient Large-scale Localization by Global Instance Recognition </summary>

<ul>
<li><span class="exturl" data-url="aHR0cDovL21pLmVuZy5jYW0uYWMudWsvfmNpcG9sbGEvcHVibGljYXRpb25zL2lucHJvY2VlZGluZ3MvMjAyMi1DVlBSLWxhcmdlLXNjYWxlLWxvY2FsaXNhdGlvbi1nbG9iYWwtaW5zdGFuY2VzLnBkZg==">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Large-scale visual localization에서는 hierarchical localization 기법을 많이 사용한다.<ul>
<li>Hierarchical localiazation = Coarse하게 한번, fine하게 한번</li>
<li>간단한 환경에서는 잘 되지만, 복잡하고 큰 환경에서는 잘 안되는 편이다.</li>
</ul>
</li>
<li>이번 연구에서는 빌딩을 인식하여 coarse localization을 강화하고, fine localization의 성능도 높이는 ㅂ아법을 소개한다.<ul>
<li>우선, 각가의 빌딩들마다 global ID를 부여하고, pixel-wise recognition이 가능하게 만든다.</li>
<li>Coarse localization 단계에서 효율적인 reference search 전략을 사용한다. 데이터베이스 전체를 탐색하는 것이 아닌, local map에서 탐색하는 방법이다.</li>
<li>Fine localization 단계에서는 ID를 이용하여 instance-wise feature detection과 matching을 수행한다. 이를 통해 더욱 robust한 feature correspondence를 구할 수 있다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Bi-directional Loop Closure for Visual SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDQuMDE1MjQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>자율주행 상황에서 많은 visual navigation 시스템들이 ‘아직도’ 단방향 loop closure를 사용한다.<ul>
<li>그러다보니, 데이터베이스에서 모든 방향에 대한 데이터가 쌓이지 않는 이상 + 충분히 오버랩이 있지 않는 이상 잘 작동하지 못한다.</li>
</ul>
</li>
<li>이번 연구에서는 쌍방향 loop closure (bi-directional loop closure) 방식을 소개한다.<ul>
<li>반대방향에서 옴에도 불구하고 loop closure를 성공시켜 효과적으로 drift를 줄이는 방식을 소개한다.</li>
<li>또, 큰 데이터셋에서 bi-directional 한 케이스를 골라내게 해주는 데이터 선정 방법도 제안한다.<ul>
<li>이 데이터를 이용하여 2개의 다른 CNN 네트워크를 학습하여 loop closure를 수행하고 6-DOF camera pose regression도 할 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> EDPLVO: Efficient Direct Point-Line Visual Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MuY211LmVkdS9+a2Flc3MvcHViL1pob3UyMmljcmEucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Guoquan Huang 교수님, Michael Kaess 교수님 연구.</li>
<li>기존의 Line에 direct방식을 적용하는 방식들은 대부분 line 위에 올라가있는 픽셀 값들에 대해 photometric error를 구했다.<ul>
<li>하지만 이 방식은 사실 point에 대해 적용하는 방식이지, line에 적용하려고 만든 방식이 아니였다.</li>
<li>예를 들어, DSO도 이 방식을 사용하는데, 이는 line 위에 올라가있는 픽셀들은 전부 collinear constraint가 적용된다는 점을 완전히 무시하는 것이였다.</li>
</ul>
</li>
<li>이전에는 이 문제를 풀려고 <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzk0ODQ3OTI=">DPLVO<i class="fa fa-external-link-alt"></i></span>라는 연구를 내놨다. 하지만 제대로 된 optimization이 안되어서 프로그램이 너무 무거웠다.</li>
<li>이번 연구에서는 line에 대한 photometric error를 구하는 방법을 추가하여 point+line VO를 제안한다.<ul>
<li>이미지 위에 line을 그리는 3D points (i.e. 선분의 끝점들)을 inverse depth로 표현하였을 때 closed-form solution을 구할 수 있다는 점을 이용한다.</li>
<li>이를 통해 optimization에 필요한 변수를 획기적으로 줄여 속도를 가속하고, 또 collinear constraint도 동시에 적용한다.</li>
<li>이에 더불어 optimization을 가속하는 2-step 알고리즘도 제안한다.</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> IMU Preintegrated Features for Efficient Deep Inertial Odometry </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9jbXMudGlueW1sLm9yZy93cC1jb250ZW50L3VwbG9hZHMvdGFsa3MyMDIyLzIwMDcuMDI5MjkucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>IMU는 센서의 특징을 모델링하기 어렵고, 노이즈가 심한편인 센서이기 때문에 사용하기가 쉽지 않다.<ul>
<li>이를 딥러닝으로 풀어내는 deep inertial odometry (end-to-end learning) 기법에 대해 많은 연구가 진행된 적이 있었다.</li>
<li>보통 agent의 움직임 패턴을 학습하여 더욱 정확한 odometry를 추정하려고 했는데, 이 방식은 계산량과 메모리 점유율이 높아 low-power / edge application에 쓰기 어렵다는 단점이 있었다.</li>
</ul>
</li>
<li>이번 연구는 raw IMU 데이터가 아닌 Preintegrated IMU feature를 사용함으로써 계산량과 메모리 점유율을 획기적으로 줄였다.<ul>
<li>IMU motion model의 매니폴드 구조를 이용하여, 기존의 preintegrated IMU와 같이 수많은 시간의 정보를 단 하나의 값으로 압축하는 결과를 낼 수 있다.</li>
</ul>
</li>
<li>이 기법은 리소스가 부족한 microcontroller에서도 사용할 수 있다.</li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>

<details>
  <summary>  </summary>

<ul>
<li><a href="">논문 링크</a></li>
<li></li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>천재 엔지니어에게 배운 10가지 레슨</title>
    <url>/20220418-10-lessons-from-smartest-engineer/</url>
    <content><![CDATA[<h1 id="오리지널-링크"><a href="#오리지널-링크" class="headerlink" title="오리지널 링크"></a>오리지널 링크</h1><p><span class="exturl" data-url="aHR0cHM6Ly9zdnBpbm8uY29tL2xlc3NvbnMtbGVhcm5lZC1mcm9tLXRoZS1zbWFydGVzdC1zb2Z0d2FyZS1lbmdpbmVlci1pdmUtbWV0LTM1ODk1YWM5ZmUzYQ==">https://svpino.com/lessons-learned-from-the-smartest-software-engineer-ive-met-35895ac9fe3a<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h1><ol>
<li>1등이 되는데에 집중해라.</li>
<li>기술부채는 잘 쓰면 도움이 된다.</li>
<li>너에게 도움을 줄 수 있는 사람들로 네 주변을 채워라. 질문도 많이 하고.</li>
<li>기술적으로 특출난것도 좋지만, 좋은 소통 능력이 너를 더 높은 곳으로 이끌 것이다.</li>
<li>임팩트가 큰 업무를 해라. 나머진 다 옆으로 치워버려라.</li>
<li>네가 가진 지식들은 다 공유해라.</li>
<li>문제를 발견하면 해결해라. 문제 해결에 대한 책임을 전적으로 져라.</li>
<li>코드는 많을 수록 좋지 않다. 최대한 적게 적어라.</li>
<li>자동 테스트는 기본이다. 꼭 해라.</li>
<li>목표를 더 높게 잡아라. 만족하지 말고, 항상 더 높은 곳을 노려라. 실패하고, 배우고, 성장하고, 다시 도전해라!</li>
</ol>
<p>&nbsp;</p>
<hr>
<h2 id="1등이-되는데에-집중해라"><a href="#1등이-되는데에-집중해라" class="headerlink" title="1등이 되는데에 집중해라"></a>1등이 되는데에 집중해라</h2><p>‘적당히 잘’하는 것만으로도 돈과 시간을 벌 수 있고 이목을 끌 수 있다.</p>
<p>완벽한걸 만들려고 시간을 너무 많이 쏟지 마라. 너무 많은 사람들이 너무 많은 시간을 쏟는다.</p>
<p>적당히 잘 만들고 바로 끝내라.</p>
<p>1등이 되어야한다. 무조건 1등이 되어야한다. 항상 1등이 되어야한다.</p>
<p>그래야 네가 ‘올바른 방향’이 될 수 있다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="기술-부채는-잘-쓰면-도움이-된다"><a href="#기술-부채는-잘-쓰면-도움이-된다" class="headerlink" title="기술 부채는 잘 쓰면 도움이 된다"></a>기술 부채는 잘 쓰면 도움이 된다</h2><p>기술 부채를 제대로 이해하면, ‘다른 업무는 기다려도 되는 업무다’라고 이해할 수 있다.</p>
<p>진짜 해결해야하는 문제와, 기다려도 되는 업무를 잘 분리해라. </p>
<p>쓸데없는 업무에는 시간을 쓰지 마라.</p>
<p>&nbsp;</p>
<hr>
<h2 id="질문을-해라"><a href="#질문을-해라" class="headerlink" title="질문을 해라"></a>질문을 해라</h2><p>질문 1번만 하면 풀리는 문제인데, 머리 끙끙 싸매면서 시간을 낭비하는것처럼 멍청한게 없다.</p>
<p>질문을 좀 해라.</p>
<p>머리를 싸맨다고 돈 더 받는거 아니다. 똑똑하게, 빠르게 일하는게 더 좋다.</p>
<p>이걸 꼭 기억해라 - 세상에 멍청한 질문은 없다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="소통이-기술보다-중요하다"><a href="#소통이-기술보다-중요하다" class="headerlink" title="소통이 기술보다 중요하다."></a>소통이 기술보다 중요하다.</h2><p>네가 가진 생각을 정확하게 표현한다는건, 초능력에 버금가는 능력이다.</p>
<p>코드만 잘 짠다고 세계 최고가 될 수는 없다.</p>
<p>다른 사람들과 소통하는 방법에 대해 공부를 많이 해라. 그리고 그 시간의 2배를 더 공부해라.</p>
<p>&nbsp;</p>
<hr>
<h2 id="네가-할-수-있는-일이라고-해서-네가-해야한다는-뜻은-아니다"><a href="#네가-할-수-있는-일이라고-해서-네가-해야한다는-뜻은-아니다" class="headerlink" title="네가 할 수 있는 일이라고 해서, 네가 해야한다는 뜻은 아니다."></a>네가 할 수 있는 일이라고 해서, 네가 해야한다는 뜻은 아니다.</h2><ol>
<li>중요한 업무를 한눈에 알아챌 수 있는 능력, 2. 네 시간을 낭비하는 업무를 한눈에 알아챌 수 있는 능력, 3. 다른 사람에게 업무를 분배를 할 줄 아는 능력</li>
</ol>
<p>위 3개는 정말 중요하다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="네가-가진-지식을-모두-공유해라"><a href="#네가-가진-지식을-모두-공유해라" class="headerlink" title="네가 가진 지식을 모두 공유해라."></a>네가 가진 지식을 모두 공유해라.</h2><p>사람들은 자신을 성장시키고 띄워주는 사람 옆에 있기를 원한다.</p>
<p>네가 가진 지식을 모두 공유해라. 네 주변 인물들이 잘 될수록, 네가 그 팀의 코어 인물이 된다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="끝까지-책임을-져라"><a href="#끝까지-책임을-져라" class="headerlink" title="끝까지 책임을 져라."></a>끝까지 책임을 져라.</h2><p>항상 자기 자신에게 이런 질문을 해라 - “다음에는 무엇을 다르게 해야 더 좋은 결과를 낼 수 있을까?”</p>
<p>네가 한 작업들을 꼼꼼히 분석하고 평가해라.</p>
<p>핑계를 대는건 쉽다. 무엇이 왜 잘 안풀렸는지 이유를 대는건 99.9999% 경우 다 할 수 있다. 물론 이렇게 핑계만 대면 항상 넌 아마추어에 머물러있을 것이다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="가장-좋은-코드는-아무도-코드를-적지-않는-것이다"><a href="#가장-좋은-코드는-아무도-코드를-적지-않는-것이다" class="headerlink" title="가장 좋은 코드는 아무도 코드를 적지 않는 것이다."></a>가장 좋은 코드는 아무도 코드를 적지 않는 것이다.</h2><p>코드를 적는다는건 시간을 투입하고, 네가 미래에도 그 코드에 대해 책임을 져야한다는거다.</p>
<p>그러니 문제를 해결할 때 최소한의 코드로 해결해ㅐ보자.</p>
<p>코드를 적지 않고도 문제를 해결하는건 ‘진짜 초능력’이다.</p>
<p>&nbsp;</p>
<hr>
<h2 id="테스트-안하면-그건-백퍼-터진다"><a href="#테스트-안하면-그건-백퍼-터진다" class="headerlink" title="테스트 안하면 그건 백퍼 터진다."></a>테스트 안하면 그건 백퍼 터진다.</h2><p>터질 코드는 언젠간 터지게 되어있다.</p>
<p>너는 지금 ‘테스트를 짜고 있는게’ 아니다. 너는 ‘내일 터질 코드를 수정하는 시간을 줄이고 있는것’이다.</p>
<p>테스트가 자동화되지 않았다고? 너 그거 잘못하고 있는거다. 빨리 자동화해라.</p>
<p>&nbsp;</p>
<hr>
<h2 id="실패를-받아드려라"><a href="#실패를-받아드려라" class="headerlink" title="실패를 받아드려라."></a>실패를 받아드려라.</h2><p>실패를 안하면 배울것도 없다.</p>
<p>실패해본적이 없다면 충분히 어려운 문제에 도전한 게 아니다.</p>
<p>실패해도 된다. 거기서 배워서 다시 도전해라. 더 높은 목표를 이뤄라. 무서워하지 말고.</p>
<p>쉬운거만 하는 사람은 본인의 실력에 대해 생각해볼 기회조차 없다.</p>
<p>물론 그렇다고 맨날 실패만 하는건 바보같은 짓이다. ‘이번에도 실패했어! 난 또 배울거야 ㅎㅎ’ 같은 마인드셋은 빠지지 마라. 결국 다 성공하려고 실패를 겪는거다.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Dev</tag>
      </tags>
  </entry>
  <entry>
    <title>Default constructor는 꼭 만들어야할까?</title>
    <url>/20220418-is-a-default-constructor-a-must/</url>
    <content><![CDATA[<h1 id="문제"><a href="#문제" class="headerlink" title="문제"></a>문제</h1><p>팀원과 코드리뷰를 하다가 이런 이야기가 나왔다.</p>
<blockquote>
<p>팀원: “Default constructor가 아무 행동도 하지 않는거라면 (i.e. 초기화만 하는거라면), 굳이 <code>Class() = default</code>라고 적어야할까요? 이거 굳이 코드 줄만 잡아먹는거 같은데요”</p>
</blockquote>
<p>적자니 코드 줄을 잡아먹고, 안적자니 뭔가 이상하다.</p>
<p>그래서 ‘적는다’ vs ‘안 적는다’에 대한 의견을 찾아봤다.</p>
<p> </p>
<hr>
<h1 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h1><blockquote>
<p>‘<strong>취향에 따라서… ㅋㅋ</strong>‘<br>나는 안 적는 쪽으로 골랐다.</p>
</blockquote>
<p>적었을 때의 이점도 있고, 안 적었을 때의 이점도 있다.</p>
<p> </p>
<h2 id="적었을-때의-이점-단점"><a href="#적었을-때의-이점-단점" class="headerlink" title="적었을 때의 이점 + 단점"></a>적었을 때의 이점 + 단점</h2><p>Default constructor를 명시적으로 적지 않으면, 추후에 <strong>누군가 다른 constructor를 만들었을 때 default constructor가 사라지면서 코드가 터질 수 있다</strong>. 물론 제대로 test code를 작성했다면 이 버그는 production으로 가지 않을 것이고, test code가 터지는 걸 보고 그때 가서 default constructor를 추가해주면 된다.</p>
<p><code>Class() {}</code>를 적어주면 (컴파일러에 따라 다르지만) 컴파일러가 최적화 된 방식을 사용하지 않는다고 한다. 대신 이는 <code>Class() = default</code>를 적어줌으로써 최적화 된 방식을 사용할 수 있다.</p>
<p>어떠한 형태로던지 default constructor를 명시적으로 적으면, 아주 짧지만 적는시간 + 읽는시간이 소요된다. 하지만 반대로, default constructor가 있는 방식을 선호하는 프로그래머들은 이걸 찾느라 시간을 더 쓰게 된다.</p>
<p> </p>
<h2 id="그래서-안적는-방향으로-고른-이유"><a href="#그래서-안적는-방향으로-고른-이유" class="headerlink" title="그래서 안적는 방향으로 고른 이유"></a>그래서 안적는 방향으로 고른 이유</h2><p>우리 팀은 TDD 개발 방식을 선호한다.</p>
<p>그러다보니, default constructor를 명시적으로 적지 않은 상태에서 누군가 다른 constructor를 만들어도 code review 전에 test가 터지는 코드를 다 잡아낼 수 있을 거란 자신이 있다.</p>
<p>또, 어차피 코드 리뷰를 할 때 테스트 코드부터 읽으면서 시작한다. 테스트 코드에서 parameterless constructor가 필요하다고 생각된다면, 그렇게 제안을 할 것이다. 그 때 가서 default constructor가 필요한지, 아니면 paramterless constructor가 필요한지 결정할 수 있다. 그 후, 안쓰는 constructor는 CI에서 도는 static-analysis가 제거하라고 알려줄 것이다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>10x 개발자 되는 방법</title>
    <url>/20220421-10x-developer/</url>
    <content><![CDATA[<h1 id="10x-개발자"><a href="#10x-개발자" class="headerlink" title="10x 개발자"></a>10x 개발자</h1><p>보통의 엔지니어보다 10배는 더 잘 하는 사람 - 1맨 스쿼드처럼 다른 사람보다 더 많이 성과를 낼 수 있음.</p>
<p>어떻게 10x 개발자가 될 수 있을까?</p>
<p>(개그) <del>10배 더 빠르게 타이핑, 10배 더 일하기, 10배 더 코드를 치기.</del></p>
<p>실제로 10x 엔지니어는 타이핑 속도도 느리고, 일도 남들의 절반만하고, 코드를 작성하기보다는 지우고 있을 확률이 높다.</p>
<p>&nbsp;</p>
<hr>
<p>이제 10x 개발자가 되는 법에 대해 알아본다.</p>
<h1 id="올바른-툴을-쓰기"><a href="#올바른-툴을-쓰기" class="headerlink" title="올바른 툴을 쓰기"></a>올바른 툴을 쓰기</h1><blockquote>
<p>링컨: “내게 나무를 베는데에 8시간의 주어졌다면, 7시간은 도끼를 갈고 1시간은 나무를 베겠다”</p>
</blockquote>
<p>주니어 엔지니어는 8시간동안 덜 갈린 도끼로 나무를 치고 있을 것이다.</p>
<p>시니어 엔지니어는 1시간동안 적당한 전기톱을 고를 것이다. 그리고 5분만에 나무를 잘라버리겠지.</p>
<p>…</p>
<p>주니어 엔지니어들은 보통 곧바로 코딩을 시작하곤 한다. 본인이 알고 있는 방법으로 모든 문제를 해결하려고 한다. 다른 방법은 찾아보려고 하지도 않는다. 알고보면 실제로 코드를 1도 적지 않아도 되는 경우도 많다.</p>
<p>&nbsp;</p>
<h1 id="도움을-구하기"><a href="#도움을-구하기" class="headerlink" title="도움을 구하기"></a>도움을 구하기</h1><p>주니어 엔지니어들이 흔히 하는 착각 중 하나는, ‘시니어 엔지니어들은 고독한 천재들’이라고 생각하는 것이다. 그래서 본인들도 그렇게 되려고 노력한다.</p>
<p>주니어 엔지니어들이 여기서 간과하는 점은, ‘시니어 엔지니어들은 혼자서 문제를 풀 줄 알고, 주니어들은 못한다’는 것이다.</p>
<p>그래서 주니어 엔지니어들을 문제를 풀기 위해 코드를 이해하려고 하루종일 쳐다본다. 하지만 이건 옆자리 시니어 동료에게 5분만 물어보면 바로 풀릴 문제이다.</p>
<blockquote>
<p>덜 숙련되었지만 질문을 할 줄 아는 엔지니어는, 똑똑하지만 질문을 할 줄 모르는 엔지니어에게 항상 이길 것이다.</p>
</blockquote>
<p>&nbsp;</p>
<h1 id="비즈니스적-가치를-전달하기"><a href="#비즈니스적-가치를-전달하기" class="headerlink" title="비즈니스적 가치를 전달하기"></a>비즈니스적 가치를 전달하기</h1><p>엔지니어들은 항상 기회비용에 대해 생각해야한다. “내가 할 수 있는 것들 중, 어떤게 가장 큰 가치를 전달할 수 있을까?”.</p>
<p>그 중에서도 코드를 하나도 적지 않고도 할 수 있는 것이라면 더욱 좋다.</p>
<p>종종 비즈니스 목표를 잊어버리는 엔지니어들도 있다. 예를 들어…</p>
<ul>
<li>‘새로운 기술이 나왔다. 다음주는 이걸 써서 우리 웹사이트에 올려봐야지! (제품과 연관이 없다)</li>
<li>‘지금 코드 구조 진짜 별로인거 같아. 다음 스프린트는 코드를 전부 리팩토링 해놔야겠어’ (그 시간동안 돈을 벌 수 있는 기능을 만들 수 있지 않을까?)</li>
<li>‘지금 플랫폼은 너무 옛날꺼야. 새 플랫폼으로 옮기자!’ (새 플랫폼으로 옮기면 효율성을 더 얻을 수 있는건가? 아니면 그냥 점진적으로 개선시키는 용도인가?)</li>
</ul>
<p>주니어 엔지니어는 2시간동안 돈을 벌지도 못하는 복잡한 기능에 시간을 쏟는다. 시니어는 1시간동안 간단한 코드를 바꿔서 5배의 매출을 얻어내고 10배의 업무효율성 증진을 얻는다.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>Dev</tag>
      </tags>
  </entry>
  <entry>
    <title>GTC 2022 - NVIDIA&#39;s self-driving cars</title>
    <url>/20220421-nvidia-self-driving-cars-gtc2022/</url>
    <content><![CDATA[<h1 id="원본-영상"><a href="#원본-영상" class="headerlink" title="원본 영상"></a>원본 영상</h1><p><span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9Mam94SGFEQ3VzWQ==">https://youtu.be/LjoxHaDCusY<i class="fa fa-external-link-alt"></i></span></p>
<p>&nbsp;</p>
<hr>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><ul>
<li>Ground truth 데이터를 얻는 방법<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9ibG9ncy5udmlkaWEuY29tL2Jsb2cvMjAyMS8wNi8xMC9udmlkaWEtZGVlcG1hcC1tYXBwaW5nLWRyaXZlLXBsYXRmb3JtLw==">DeepMap 회사 인수<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9zZWxmLWRyaXZpbmctY2Fycy9oZC1tYXBwaW5nLw==">HD Mapping (DRIVE MAP) 알고리즘<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9udmlkaWFuZXdzLm52aWRpYS5jb20vbmV3cy9udmlkaWEtYW5ub3VuY2VzLW9tbml2ZXJzZS1yZXBsaWNhdG9yLXN5bnRoZXRpYy1kYXRhLWdlbmVyYXRpb24tZW5naW5lLWZvci10cmFpbmluZy1haXM=">Omniverse 시뮬레이터<i class="fa fa-external-link-alt"></i></span></li>
<li>Human-labeled 데이터</li>
</ul>
</li>
<li>딥러닝 네트워크 트레이닝<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9kYXRhLWNlbnRlci9kZ3gtc3lzdGVtcy8=">DGX 학습 워크스테이션<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kcml2ZQ==">DRIVE AV 알고리즘<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>디지털 트윈 워크스테이션<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9vbW5pdmVyc2UvcGxhdGZvcm0vb3Z4Lw==">OVX 학습 워크스테이션<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
<li>자동차에 올라가는 보드<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9hdXRvbm9tb3VzLW1hY2hpbmVzL2VtYmVkZGVkLXN5c3RlbXMvamV0c29uLW9yaW4v">NVIDIA Jetson AGX Orin<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</li>
</ul>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/1.png" class="" title="overview"> 

<p>&nbsp;</p>
<hr>
<h1 id="NVIDIA-Drive-데모"><a href="#NVIDIA-Drive-데모" class="headerlink" title="NVIDIA Drive 데모"></a>NVIDIA Drive 데모</h1><h2 id="지도-생성-NVIDIA-DRIVE-Map"><a href="#지도-생성-NVIDIA-DRIVE-Map" class="headerlink" title="지도 생성 (NVIDIA DRIVE Map)"></a>지도 생성 (NVIDIA DRIVE Map)</h2><blockquote>
<p>TLDR; 자동 맵핑</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9zZWxmLWRyaXZpbmctY2Fycy9oZC1tYXBwaW5nLw==">HD Mapping (DRIVE MAP) 알고리즘<i class="fa fa-external-link-alt"></i></span>을 기반으로 사전에 이미 따놓은 지도를 사용함. 이 지도는 NVIDIA가 인수한 DeepMap이라는 회사의 알고리즘과 기존의 NVIDIA가 만든 DRIVE Map 기술의 클라우드 인프라를 합친 것임.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/2.gif" class="" title="deepmap"> 

<p>&nbsp;</p>
<h2 id="DRIVE-Concierge"><a href="#DRIVE-Concierge" class="headerlink" title="DRIVE Concierge"></a>DRIVE Concierge</h2><blockquote>
<p>TLDR; 잘되는 대화형 AI. 시선인식. 멀티모달 디텍션. 자동주차. 서라운드뷰 모니터링.</p>
</blockquote>
<p>데모에서 주인공은 차를 탄다. 차를 타면 대화형 AI인 DRIVE 아바타가 반겨준다.</p>
<p>주인공의 친구가 ‘San Jose civic에서 나 좀 태워줘’라는 문자를 받은걸 아바타가 주인공에게 알려주며, ‘거기로 갈까요?’라고 물어본다. 주인공은 ‘응 거기로 가자’라고 얘기하고, 아바타는 목적지를 그곳으로 설정한다.</p>
<p>운행을 시작하고 얼마 안되서 ‘DRIVE Pilot 실행해’라고 말함으로써 자율주행이 시작된다.</p>
<p>주인공이 ‘친구에게 가고있다고 알려줄래?’ 라고 얘기하니, 아바타는 주인공의 친구에게 ‘12분 후 도착 예정’이라는 문자를 보낸다. </p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/3.png" class="" title="avartar"> 

<p>보행자, 공사 사인 등을 잘 피해간다. 움직이는 객체들에 대해서는 confidence map을 통해 검출 정확도도 표시한다. 자전거 도로도 잘 피해낸다.</p>
<p>차는 특정 인물을 검출할 수 있는 능력이 있어, 주인공의 친구를 바로 알아챈다.</p>
<p>주인공이 바깥을 바라보며 ‘저 빌딩이 뭐야?’라고 물어본다. 이 때, 자동차는 시선트랙킹을 통해 어떤 빌딩을 이야기하는지 추론한다. 주인공이 물어본 빌딩은 쇼를 하는 곳인데, 주인공은 ‘무슨 쇼를 하는지’ 물어보고 아바타가 답을해주자 ‘토요일 저녁에 티켓 2개 예약해줘’라고 한다.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/4.png" class="" title="multimodal"> 

<p>데모에서는 실제 세상과 Omniverse 기반의 DRIVE SIM의 세상이 거의 비슷하다고 이야기한다. DRIVE SIM에서도 NVIDIA 자동차 플랫폼인 Hyperion 8과 동일한 센서를 가지고 있다.</p>
<p>좌측 이미지가 omniverse 시뮬레이션, 오른쪽이 실제이다.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/5.png" class="" title="hyperion and omniverse"> 
<img src="/20220421-nvidia-self-driving-cars-gtc2022/6.png" class="" title="omniverse vs real"> 

<p>도착하고나서, 주인공은 ‘차를 주차해줘’라고 얘기한다. 자동차는 Surround view를 이용해서 주변에 주차할 수 있는 곳을 자동으로 탐색해 주차에 성공한다.</p>
<p>&nbsp;</p>
<hr>
<h1 id="Hyperion-8-플랫폼"><a href="#Hyperion-8-플랫폼" class="headerlink" title="Hyperion 8 플랫폼"></a>Hyperion 8 플랫폼</h1><blockquote>
<p>TLDR; 잘나가는 히페리온 8</p>
</blockquote>
<p>Hyperion 8 플랫폼은 NVIDIA DRIVE의 가장 중요한 부분이다. 센서, 네트워크, 2개의 드라이빙을 위한 컴퓨터, 1개의 아바타 컴퓨터, 1개의 mission recorder, safety + cybersecurity 시스템이 있다고 한다.</p>
<p>8개의 카메라를 사용해서 360 각도를 커버한다. 레이더, 라이다, 초음파 센서를 사용하고 있다.</p>
<p>2024년부터 벤츠 차에 올라간다. 2025년에는 재규어 랜드로버에 올라간다.</p>
<p>&nbsp;</p>
<hr>
<h1 id="Hyperion-9-플랫폼"><a href="#Hyperion-9-플랫폼" class="headerlink" title="Hyperion 9 플랫폼"></a>Hyperion 9 플랫폼</h1><blockquote>
<p>TLDR; 더 잘나갈 히페리온 9</p>
</blockquote>
<p>2026년부터 차에 탑재될 새로운 <strong>Hyperion 9 플랫폼</strong>도 공개했다.</p>
<p>14 카메라, 9개 레이더, 3개 라이다, 20 초음파 센서를 사용한다. Hyperion 8보다 2배나 되는 센서의 양을 처리할 수 있다.</p>
<p>&nbsp;</p>
<hr>
<h1 id="DRIVE-map"><a href="#DRIVE-map" class="headerlink" title="DRIVE map"></a>DRIVE map</h1><blockquote>
<p>TLDR; 지구 스케일 지도 딸 예정. DRIVE map으로 딴 다음에 omniverse에 올려서 다양한 상황 시뮬레이션 가능. 또, 최근 연구하는 기술로는 NeRF를 써서 새로운 시점에서의 데이터 생성 및 물리적인 렌더링 가능.</p>
</blockquote>
<p>카메라, 레이더, 라이다로 지도를 만든다. 각각의 센서가 모든 지도 레이어에 접근해서 독립적으로 위치 추정을 수행할 수 있다.</p>
<p>DRIVE map은 2가지 모드가 있다. 1. Ground truth survey mapping, 2. Crowd-sourced fleet mapping 이다. 아마 전자는 엄청나게 센서가 많은 차량이 돌아다니면서 정확하게 맵을 잡아내는걸꺼고, 후자는 양산된 차들이 돌아다니면서 맵을 업데이트 시켜주는 걸 것이다.</p>
<p>2024년까지 북미, 유럽, 아시아의 대부분의 고속도로 지도를 따서 디지털 트윈을 만들 계획을 추진하고 있다고 한다. 목표는 지구의 스케일의 디지털 트윈을 만들어서, 자율주행 차를 출시하기 전에 엄청난 양의 데이터로 안전을 검증하는 시스템을 만드는 것이라고 한다.</p>
<p>&nbsp;</p>
<p>현재 이를 위해 2가지 방법을 쓰고 있다고 한다.</p>
<p>첫번째 방법은 NVIDIA DRIVE Map을 NVIDIA Omniverse로 로딩한 후, 실제 객체들의 위치에 가상 렌더링 객체를 올려 정확한 그래픽 요소를 채운다.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/7.gif" class="" title="drive map to omniverse"> 

<p>그 후, DRIVE Map에 있는 동적 객체들을 딥러닝으로 검출해서, 그 위치를 디지털 트윈으로 대체한다. 해당 객체들은 이제 직접 컨트롤 할 수 있게 되며, 이를 통해 다양한 상황들을 시뮬레이션으로 만들어낼 수 있다.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/8.gif" class="" title="adversarial scenarios">

<p>&nbsp; </p>
<p>두번째 방법은, 운전 중에 녹화된 영상을 기반으로 3D reconstruction을 사용해서 지도를 만든다. 동적 객체들을 검출해서 정적 물체만 가득한 지도를 만들고, 빈 곳들은 neural reconstruction (i.e. NeRF)를 이용해서 채워넣는다. 또, NeRF를 사용해서 자동차의 시점을 원하는대로 돌려 새로운 데이터를 만들어낼 수 있다.</p>
<img src="/20220421-nvidia-self-driving-cars-gtc2022/9.gif" class="" title="nerf"> ]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title>은성의 Thread 수업</title>
    <url>/20220428-thread-esyang/</url>
    <content><![CDATA[<h1 id="Thread-관련-함수들"><a href="#Thread-관련-함수들" class="headerlink" title="Thread 관련 함수들"></a>Thread 관련 함수들</h1><ul>
<li><code>std::thread</code></li>
<li><code>std::mutex</code></li>
<li><code>std::condition_variable</code></li>
<li><code>std::lock</code></li>
</ul>
<p>Thread를 사용하는 목적: 백그라운드에서 함수가 돌아가게 하는 것.</p>
<p>물리적인 thread의 수는 제한되어있지만, OS가 적절하게 스케줄링을 해서 물리적인 thread에 프로세스를 알맞게 붙혀주는 것.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo1</span><span class="params">()</span> </span>{};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo2</span><span class="params">()</span> </span>{};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::thread(foo1);</span><br><span class="line">    <span class="built_in">std</span>::thread(foo2);</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p> </p>
<hr>
<h1 id="Read-Write"><a href="#Read-Write" class="headerlink" title="Read / Write"></a>Read / Write</h1><p>아래 코드를 실행하면 어떤 결과가 나올까?</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo1</span><span class="params">(<span class="keyword">int</span> a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    a = <span class="number">1</span>;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo2</span><span class="params">(<span class="keyword">int</span> a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    a = <span class="number">2</span>;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">std</span>::thread(foo1, a);</span><br><span class="line">    <span class="built_in">std</span>::thread(foo2, a);</span><br><span class="line"></span><br><span class="line">    a = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>정답: a가 뭐가 나올지 알 수 없다 + 터질 수도 있다.</p>
<p>a의 값을 read만 하는거는 터지지는 않는다. 물론 이상한 값이 나올 수 있다.<br>위 코드는 a를 write하고 있다. Write 중에 접근하면 터질 수 있다.</p>
<p>‘터질 수도 있다’라는 이유는, Write하는 시점을 정하는거는 순전히 OS 종속적이기 때문이다.</p>
<p> </p>
<hr>
<h1 id="Unique-lock-Mutex"><a href="#Unique-lock-Mutex" class="headerlink" title="Unique lock / Mutex"></a>Unique lock / Mutex</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;condition_variable&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo1</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    a = <span class="number">1</span>;</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo2</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    a = <span class="number">2</span>;</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::mutex mutex_for_a;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread1</span><span class="params">(foo1, a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread2</span><span class="params">(foo2, a)</span></span>;</span><br><span class="line">    lock_for_a.lock();</span><br><span class="line"></span><br><span class="line">    a = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>unique_lock을 걸어서 누가 먼저 실행할지 결정할 수 있다.<br>위의 코드는 a는 무조건 3이 나오는 코드이다. lock을 걸고나서 수행하는 최종 연산이 a=3 대입연산이기 때문이다.<br>하지만 중간에 foo1이 먼저 실행될지, foo2가 먼저 실행될지는 모른다.<br>Thread가 생성되는 시간도 고려해야한다.</p>
<p> </p>
<h1 id="Dual-lock"><a href="#Dual-lock" class="headerlink" title="Dual lock"></a>Dual lock</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;condition_variable&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo1</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_b)</span></span>;</span><br><span class="line">    </span><br><span class="line">    a = <span class="number">1</span>;</span><br><span class="line">    b = <span class="number">1</span>;</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">    lock_for_b.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo2</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_b)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line"></span><br><span class="line">    a = <span class="number">2</span>;</span><br><span class="line">    b = <span class="number">2</span>;</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">    lock_for_b.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::mutex mutex_for_a;</span><br><span class="line">    <span class="built_in">std</span>::mutex mutex_for_b;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_b)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread1</span><span class="params">(foo1, a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread2</span><span class="params">(foo2, a)</span></span>;</span><br><span class="line">    lock_for_a.lock();</span><br><span class="line"></span><br><span class="line">    a = <span class="number">3</span>;</span><br><span class="line">    b = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>해결 방법:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;thread&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;condition_variable&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo1</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_b)</span></span>;</span><br><span class="line">    </span><br><span class="line">    a = <span class="number">1</span>;</span><br><span class="line">    b = <span class="number">1</span>;</span><br><span class="line">    lock_for_b.unlock();</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo2</span><span class="params">(<span class="keyword">int</span> a, <span class="built_in">std</span>::mutex&amp; mutex_for_a)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_b)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_a)</span></span>;</span><br><span class="line"></span><br><span class="line">    a = <span class="number">2</span>;</span><br><span class="line">    b = <span class="number">2</span>;</span><br><span class="line">    lock_for_b.unlock();</span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::mutex mutex_for_a;</span><br><span class="line">    <span class="built_in">std</span>::mutex mutex_for_b;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_a</span><span class="params">(mutex_for_a, <span class="built_in">std</span>::defer_lock)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lock_for_b</span><span class="params">(mutex_for_b, <span class="built_in">std</span>::defer_lock)</span></span>;</span><br><span class="line">    <span class="built_in">std</span>::lock_guard&lt;&gt;(lock_for_a, lock_for_b);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    lock_for_a.unlock();</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread1</span><span class="params">(foo1, a)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::thread <span class="title">thread2</span><span class="params">(foo2, a)</span></span>;</span><br><span class="line">    lock_for_a.lock();</span><br><span class="line"></span><br><span class="line">    a = <span class="number">3</span>;</span><br><span class="line">    b = <span class="number">3</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; a &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>전문연구요원 끝!!!!!</title>
    <url>/20220707-army-over/</url>
    <content><![CDATA[<img src="/20220707-army-over/sv.png" class="" title="sv"> 

<hr>
<p>2019년 7월 1일 등록되어서 2022년 7월 1일 소집해제 상태가 되었습니다!</p>
<p>아 신나라!!!!!!!!!!!!! 생각보다 3년 시간 되게 안 가더라구요!</p>
<p>이번 글에서는 간단하게 지난 3년 동안 좋았던 점과 느낀 점을 정리하려고 합니다.</p>
<hr>
<h1 id="시작"><a href="#시작" class="headerlink" title="시작"></a>시작</h1><blockquote>
<p>운이 좋게 첫 회사가 좋은 곳이였다</p>
</blockquote>
<p>2019년 4월 말 증강현실 분야에 컴퓨터 비전 엔지니어로 V 사에 입사했습니다. </p>
<p>V사는 제가 사회인으로써 첫 걸음을 내딛을 수 있게 해주고, 또 탄탄한 개발의 기본기를 탑재해준 정말 고마운 곳 입니다. Geometry 기반의 컴퓨터 비전 분야에서 탄탄한 개발 문화를 익히고 싶은 신입 및 주니어 엔지니어 분들께는 제가 종종 언급하며 추천하는 곳 이기도 합니다. V 사에서 어깨 넘어로 익힌 <strong>프레임워크 설계, CI 운용방법, 최적화 테크닉, 실험 설계</strong>와 같은 부분은 현재까지도 곱씹으면서 발전시키고 있는 방향이고 또 제가 팀에 기여하고 있는 부분이기도 합니다. 이러한 경험은 절대로 아무데서나 익힐 수 있는게 아니고, 훌륭하신 팀장님과 CTO님을 봴 수 있는 운이 따라야 한다고 생각합니다. 그리고 그 운이 따라줬다는 점에 있어 너무나도 감사합니다.</p>
<p>물론 좋은 부분도 있었지만 여러 업다운도 있었습니다. 전문연구요원이 조금 늦게 등록되어도 입사일부터 카운트가 된다고 했는데 그렇지 않았다던가, 조금 이상한 과제를 수주했다가 과제 주관사에게 엄청나게 갑질을 당한다던지, HR에서 엔지니어인 제게 독일어 법률 문서를 읽게 하거나 중국 회사와 영어로 전화를 부탁한다던지, 초기 스타트업에서 나타나는 문제들을 직접 겪기도 했습니다.</p>
<p>물론 제가 떠난 후 회사도 스케일 업에 성공했고 현재는 이러한 문제들이 많이 사라졌을 것이라고 생각이 됩니다. 실제로 추가 투자 라운드도 성공적으로 클로징 했고, 링크드인으로도 훨씬 긍정적이고 밝은 소식이 자주 보입니다.</p>
<p>연구소에서 일하고 있던 도중에 <strong>엄청 신나신 표정으로 한순간에 제 자리로 달려오신 전 팀장님의 모습</strong>이 아직도 잊혀지지 않습니다. 무슨 좋으신 일이 있으신가 했는데 알고보니 <strong>전 팀장님 손에 훈련소 소집 통지서</strong>가 있었던 거였을 때 (…) 그리고 심지어 그 표정을 찐으로 순간캡쳐 하신 그 때가 아직도 기억이 납니다 ㅋㅋㅋ</p>
<img src="/20220707-army-over/NOnsan.png" class="" title="sv"> 

<hr>
<h1 id="훈련소"><a href="#훈련소" class="headerlink" title="훈련소"></a>훈련소</h1><blockquote>
<p>훈련소에서는 몸이 굴려지고 머리가 쉰다</p>
</blockquote>
<p>훈련소는 은근히 괜찮았습니다. 카더라로 듣기로는 제가 전문연구요원만 모여서 훈련을 받은 마지막 기수라고 하더군요. 역시 카더라로 듣기로는 공익 친구들과 함께 훈련소를 가게 될 경우에는 상대적으로 나이가 많은 전문연구요원 분들은 힘들 수 있다고 하더라구요. 저는 운이 좋았던 편인 것 같습니다.</p>
<p>알고리즘 엔지니어로써 항상 직접 생각하고 직접 결정을 내리고 거기에 책임을 져야했지만, 훈련소에서는 아무 생각도 할 필요도 없이 시키는 것만 잘 따르면 아무 문제 없이 굴러갔었습니다. 평생 해먹을 짓은 아니지만, 마침 회사 업무에서 멘탈이 지쳐갈 때 쯤 한달정도 생각을 그만두니까 <strong>정신적으로 휴가를 다녀온 기분</strong>이였습니다. 물론 몸은 6월 말 - 7월 말 겁나 더울 때 행군하고 그래서 엄청 힘들었지만요.</p>
<p>책을 많이 읽을 수 있었던게 좋았습니다. <strong>평소에 읽지도 않았던 역사책이나 철학책을 읽을 수 있어서 좋았습니다</strong>. C++ 책도 읽으려고 가져갔지만 번역이 너무 별로여서 금방 내려놨습니다. 새벽 시간 동안 불침번을 서며 철학 입문서, 나치 독일 관련 책, 헤로도토스 전기를 읽었던게 기억이 납니다.</p>
<p>훈련소에 가기 전에 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIwMTMwLW5vbnNhbi8=">전문연구요원 선배님들로부터 대대로 내려오는 꿀팁<i class="fa fa-external-link-alt"></i></span> 덕분에 좀 더 편하게 다녀온 것 같습니다.</p>
<hr>
<h1 id="개인-활동"><a href="#개인-활동" class="headerlink" title="개인 활동"></a>개인 활동</h1><blockquote>
<p>기술 커뮤니티를 통해 배운게 엄청나게 많다.</p>
</blockquote>
<p>회사 밖에서 엄청나게 뭔가 많이 했던 것 같습니다. 전문연구요원으로써 퇴근 후와 주말의 시간은 온전히 제 것이였기 때문에 가능했던 것이라고 생각합니다. 현역보다 무려 2배의 시간을 소모하지만, 이것 덕분에 지금의 제가 있다고 생각하기에 전문연구요원을 선택한 것을 후회하지 않습니다.</p>
<p><strong>기술 커뮤니티</strong>를 통해 딥러닝 트렌드를 따라갈 수 있었고, SLAM 온라인 스터디를 할 수 있었습니다. 모두의 연구소 과정에 참석해서 의료영상 강의도 이끌고, 오프라인 SLAM 스터디도 할 수 있었구요. 그러던 와중에 <strong>SLAM 오픈카톡방</strong>도 만들게 되어서 지금은 800명이 넘는 인원들이 생겼습니다. 이 덕분에 많은 업계 관련자 분들과 네트워킹을 할 수 있었습니다. 업무 관련으로 좋은 이야기를 하는 분들도 있고, 진짜 친구가 되신 분들도 있구요. <strong>전액 기부 세미나</strong>도 할 수 있었는데, 이 세미나 덕분에 현재 저희 CEO님과 CTO님께 좋은 인상을 남길 수 있었습니다. 아마 이직에 성공한 요인에는 이 부분도 무시할 수 없겠죠. 기술 블로그도 운영하면서 노션을 거쳐 지금의 <strong>블로그</strong>도 있게 되었습니다.</p>
<p>현역으로 갔었으면 1.5년은 먼저 끝나서 더 일찍 더 편한 자유의 몸이 되었을 겁니다. 물론 그 때도 어떻게든 어디서 기회를 잡아서 뭔가를 했었을 것 같지만, 저는 지금 제가 선택한 길과 그 덕분에 좋은 팀에서 멋진 SLAM을 하고 있다는 결과에 대해 상당히 만족하고 있기 때문에 후회하지 않습니다.</p>
<hr>
<h1 id="전문연구요원의-한계"><a href="#전문연구요원의-한계" class="headerlink" title="전문연구요원의 한계"></a>전문연구요원의 한계</h1><blockquote>
<p>전문연구요원들은 불합리하다고 느낄 수 있는 점들이 꽤 있다</p>
</blockquote>
<p>그래도 불편한 점이 없진 않았습니다. 사실 엄청 많았죠. 맨날 불평하면서 병무청 욕하고 다녔습니다.</p>
<p>우선 병역업무 관련으로 병무청에 연락하면 기본 30분은 전화기를 붙잡고 기다려야합니다. 그래서 연락이 되어도 담당자는 맨날 연차라고 하구요. 업무 대리를 맡겨둔 사람도 없습니다. 무슨 똥배짱으로 일하는건지 이해가 되지 않습니다.</p>
<p>전문연구요원은 병무청이 정하는대로 민간인이 되고 또 군인이 됩니다. 위험도가 높은 코로나 델타가 떠돌면서 몇천명씩 걸릴 때도, 전문연구요원은 출근 지옥철을 뚫고 유동인구 밀도가 높은 강남으로의 출퇴근 해야합니다. 회사에서 전사 재택을 하겠다고 해도 오피스 셧다운이 아닌 이상 전문연구요원은 무조건 출퇴근해야합니다. 왜냐하면 전문연구요원이 재택근무를 한다는 것은 현역 군인 분들과 형평성이 맞지 않기 때문입니다. 전문연구요원도 군인이니 근무지에서 근무를 해야한다는게 병무청의 입장입니다. 하지만 군인들에게 제공되는 백신은 ‘민간인’ 전문연구요원들에게 제공되지 않습니다. 화이자/모더나 대란이 있었을 때 IT 쪽에 밝은 전문연구요원들은 파이썬 스크립트를 사용해서 백신을 먼저 맞기도 했습니다. 왜냐하면 백신을 맞지 않고 코로나에 걸려버리면 병무청이 카카오톡 메세지로 경고한 것 처럼 책임을 물을 수 있다고 했거든요. 이 때문에 딥러닝 커뮤니티에서 친해지게 된 분의 결혼식을 참석하고 싶었지만 혹시나 ‘책임’을 뭍게 될까봐 축의금만 전달드리게 된 적도 있습니다. 오미크론이 유행하면서 회사에서 제 앞자리, 왼쪽자리, 오른쪽 자리, 뒷자리 분 모두 돌아가면서 한번씩 걸리셨는데, 끝까지 재택을 허용하지 않더라구요. 일주일에 3번을 출근하자마자 보건소로 코 찔리러 가는데도 재택은 안되구요.</p>
<hr>
<h1 id="그래서-다음-행선지는"><a href="#그래서-다음-행선지는" class="headerlink" title="그래서 다음 행선지는?"></a>그래서 다음 행선지는?</h1><blockquote>
<p>하던거 계속 잘 하자</p>
</blockquote>
<p>보통 전문연구요원이 끝나고 나면 다른 곳으로 이직을 하는 경우가 많습니다. 연봉 재협상에 불발되는 경우도 있을 수 있고, 스타트업이 아닌 대기업에 가겠다는 목적을 가진다던지, 종종 박사를 하러 가겠다는 분들도 계십니다. 아니면 그냥 큰 의미 없이 이 참에 다른 필드를 경험해보러 가는 분들도 계시구요.</p>
<p>저는 다른 플랜 없이 같은 곳에 같은 포지션으로, 그대로 하던 일 이어서 하려고 합니다. 지금 포지션에서 <strong>Semantic SLAM 엔지니어로써의 기반</strong>을 다지고 또 20대 후반으로써 금전적인 기반을 쌓으려고 합니다. 전문연구요원 때랑 다른 점이라고 하면 조금 더 업무 외 적으로 부업을 할 수 있다는 점이 되겠습니다. 물론 전문연구요원 때에도 업무시간 외 부업을 할 수 있었지만, 조건이 엄청 깐깐했고 매번 수입 인증을 해야하는게 여간 까다로운게 아니였거든요.</p>
<p><strong>민간인으로써의 복귀를 축하해주신 저희 회사와 구성원 분들 감사합니다</strong> ㅎㅎ 선물로 주신 향수는 제가 제일 좋아하는 브랜드이고 향이에요. 열심히 하겠습니닷 (충성)</p>
<img src="/20220707-army-over/perfume.jpg" class="" title="perfume"> ]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>전문연구요원</tag>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 프로그램 성능을 높이는 방법</title>
    <url>/20221013-cpp-essential-tools/</url>
    <content><![CDATA[<blockquote>
<p><strong>Compiler Explorer, Disassembly, QuickBench, Easy-profiler, Valgrind, Massif</strong>를 소개합니다.</p>
</blockquote>
<h1 id="C-언어를-쓰는-이유"><a href="#C-언어를-쓰는-이유" class="headerlink" title="C++ 언어를 쓰는 이유"></a>C++ 언어를 쓰는 이유</h1><p>‘<span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjEwMzMwLXdoeS1jcHAv">C++을 배워야하는 이유<i class="fa fa-external-link-alt"></i></span>‘ 글에서도 소개한 바와 같이, C++ 언어는 비효율적인 개발 시간을 감수하고서라도 ‘<strong>최적화 된 성능</strong>‘을 얻어내기 위해 사용하는 언어이다.</p>
<p>빠르게 개발하고 싶다? 그러면 파이썬이나 자바스크립트를 사용하면 된다. 메모리에 대해 덜 걱정하고싶다? 그러면 garbage collection이 돌아가는 자바나 고랭을 사용하면 된다. C++을 사용한다는 것은 <strong>로우레벨 커맨드까지 내가 직접 건드려서 최대한 효율적으로 연산을 하겠다</strong>는 목표, 이것 단 하나만을 보고 가는 것이다. </p>
<p>그렇기 때문에 C++ 프로그래밍을 할 때는 내 코드가 정말로 효율적인 연산을 하고 있는지 확인을 할 필요가 있다. 왜냐하면, 내 코드가 효율적인 연산을 하고 있지 않는 경우에는 차라리 파이썬/자바스크립트로 더 짧은 시간 안에 코드를 짜내는게 더 효율적일 것이기 때문이다.</p>
<p>이번 글에서는 ‘<strong>내 코드가 정말로 효율적인 연산을 하고 있는지 알아내는 방법</strong>‘에 대해 소개한다.</p>
<p> </p>
<hr>
<h1 id="Compiler-Explorer-amp-Disassembly"><a href="#Compiler-Explorer-amp-Disassembly" class="headerlink" title="Compiler Explorer & Disassembly"></a>Compiler Explorer &amp; Disassembly</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9nb2Rib2x0Lm9yZy8=">https://godbolt.org<i class="fa fa-external-link-alt"></i></span> - 온라인 Disassembly 체커</p>
</blockquote>
<p>내 C++ 코드가 효율적으로 작성되었는지 확인하는 가장 쉬운 방법은 컴파일 후 생성된 ‘<strong>어셈블리 코드</strong>‘를 확인하는 것이다. 그리고 <strong>Godbolt</strong> 또는 <strong>Compiler Explorer</strong>라고 불리는 이 웹사이트는 <strong>온라인에서 어셈블리 코드를 확인</strong>할 수 있게 해준다. C++의 대가인 Jason Turner가 유투브 영상에서 자주 사용하는 웹사이트이기도 하다. 컴파일러의 버전도 고를 수 있고, 플랫폼도 고를 수 있다.</p>
<img src="/20221013-cpp-essential-tools/godbolt.png" class="" title="godbolt"> 

<p> </p>
<p>웹사이트로는 사실 간단한 STL 함수의 성능 정도만 체크할 수 있고, 내가 심혈을 기울여 만들고 있는 프로그램에서 성능을 체크하기는 쉽지 않다. 내가 직접 만들고 있는 프로그램에 대해서는, <strong>IDE에서 제공하는 Disassembly 기능</strong>을 사용해서 어셈블리 코드를 볼 수 있다. MS Visual Studio도 지원하고, JetBrains CLion도 지원한다. VSCode는 아마 안되는 것으로 알고 있다.</p>
<img src="/20221013-cpp-essential-tools/disassembly.png" class="" title="disassembly"> 

<p> </p>
<p>여기서 많이들 이렇게 생각하실 것 같다 - ‘C++ 프로그래밍도 어려운데, 어셈블리까지 알아야하나??’. 물론 어셈블리 까지 제대로 알고 있다면 정확히 코드가 얼마나 효율적으로 돌아가는지 알 수 있겠지만, <strong>어셈블리 특성 상 C++ 프로그래밍보다 몇배나 어렵게 느껴질 수 있다</strong>.</p>
<p>그래서 간단한 꿀팁을 소개하려고 한다.</p>
<p>첫번째 꿀팁 - ‘<strong>어셈블리 코드의 라인 수가 적으면, 아무래도 효율적인 코드일 확률이 높다</strong>‘. 어셈블리 코드는 아무래도 굉장히 작은 operation들이 많아 abstraction이 적을 것이라고 생각하기 때문에, 이런 추측을 할 수 있다.</p>
<p>두번째 꿀팁 - ‘<strong>대략적인 operation들의 CPU 사이클 수를 외워두자</strong>‘. 어셈블리 프로그래밍을 10년간 해온 장인이 Stack Overflow에서 어디선가 언급한 것을 스크린샷으로 찍어둔 것이 있다. (출처가 기억이 나지 않아서… 언젠간 추가할 예정). 이 정보를 기반으로 효율적인 프로그램을 작성하기 위한 그라운드 룰을 만들 수 있다.</p>
<img src="/20221013-cpp-essential-tools/image.png" class="" title="tips"> 

<p> </p>
<hr>
<h1 id="Quickbench-amp-Profiler"><a href="#Quickbench-amp-Profiler" class="headerlink" title="Quickbench & Profiler"></a>Quickbench &amp; Profiler</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9xdWljay1iZW5jaC5jb20v">https://quick-bench.com/<i class="fa fa-external-link-alt"></i></span> - 온라인 CPU 사이클 / 런타임 체커<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3lzZS9lYXN5X3Byb2ZpbGVy">Easy-profiler<i class="fa fa-external-link-alt"></i></span> - 로컬 런타임 체커 (멀티쓰레드 지원))</p>
</blockquote>
<p>어셈블리 라인을 보는 것도 좋지만, <strong>실제로 얼마나 빠르게 동작했는지 보는 것도 중요</strong>하다.</p>
<p>우선 툴을 소개하기 전에 ‘프로그램의 속도를 측정하면 사실 효율성은 다 본게 아닌가? 굳이 어셈블리 코드도 봐야할 필요가 있는가?’ 라는 질문이 들 수 있다. 정답부터 말하자면, <strong>어셈블리 코드 + 런타임 벤치마크 둘 다 봐야한다</strong>. 어셈블리 코드는 CPU에 내려지는 하이레벨 instruction을 의미하는데, CPU 내부에서 동작하는 로우레벨 instruction은 우리가 뜯어볼 수도 없고 상당히 복잡하게 되어있다. 현재 OS가 어떤 작업을 수행중인지, 어떤 하드웨어 포트가 열려있고 닫혀있는지, 어떤 백그라운드 프로세스가 있는지, 메모리 여유분이 어떻게 되어있는지에 따라 CPU 내부에서 동작하는 로우레벨 instruction의 동작 방법이 달라진다. 극단적인 예시를 들면, CPU 온도가 낮을 때에는 CPU가 자율적으로 단기 오버클럭을 수행해서 더 빠르게 작업을 수행할 수도 있는데, 이러한 작업은 이미 온도가 높을 때에는 동작하지 않기 때문에 runtime이 달라질 수도 있다.</p>
<p><strong>QuickBench</strong>는 온라인 런타임 체커 웹사이트다. Godbolt와 비슷하게 컴파일러를 선택한 후, Google Benchmark 코드와 비슷한 방식으로 코드를 작성해서 <strong>내 코드의 효율성을 체크</strong>할 수 있다. 하지만 이 웹사이트는 기부받은 CPU들에 작동하기 때문에, 종종 운이 나쁘면 돌릴 때 마다 다른 결과가 나타나기도 한다.</p>
<img src="/20221013-cpp-essential-tools/quickbench.png" class="" title="quickbench"> 

<p> </p>
<p>내 컴퓨터 / 내 타겟 디바이스에서 정확히 런타임이 어떻게 되는지 확인하기 위해서는 CPU profiler를 돌려보는 것이 좋다. 필자가 추천하는 방식은 <strong>Easy-profiler</strong>이다. 이름처럼 사용하기 쉬울 뿐 더러, 로깅 오버헤드가 작고, 멀티 쓰레드 성능이 시각화가 잘 되어있어서 사용하기 아주 좋다.</p>
<p>단점이 있다면, 로깅 오버헤드가 아예 없는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0tEQUIvaG90c3BvdA==">hotspot<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9wZXJmLndpa2kua2VybmVsLm9yZy9pbmRleC5waHAvTWFpbl9QYWdl">perf<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9rby53aWtpcGVkaWEub3JnL3dpa2kvRFRyYWNl">dTrace<i class="fa fa-external-link-alt"></i></span> 같은 방법들도 있지만, 개인적으로 이 방법들로는 SLAM의 멀티쓰레드 특성을 한눈에 분석하기가 어렵다고 생각이 든다. </p>
<p>개인적으로 Easy-profiler를 사용하면서 런타임을 로깅하고 체크하다가, 마지막 실제 릴리즈 직전에 easy-profiler를 빌드에서 뺌으로써 로깅을 하지 않고, 이로써 조금이라도 더 성능을 얻어내는 방법을 사용하고 있다.</p>
<img src="/20221013-cpp-essential-tools/easy_profiler.png" class="" title="easy_profiler"> 

<p> </p>
<hr>
<h1 id="Valgrind-amp-Massif"><a href="#Valgrind-amp-Massif" class="headerlink" title="Valgrind & Massif"></a>Valgrind &amp; Massif</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly92YWxncmluZC5vcmdfLw==">Valgrind<i class="fa fa-external-link-alt"></i></span> - 메모리 릭 체커<br><span class="exturl" data-url="aHR0cHM6Ly92YWxncmluZC5vcmcvZG9jcy9tYW51YWwvbXMtbWFudWFsLmh0bWw=">Massif<i class="fa fa-external-link-alt"></i></span> - 메모리 프로파일러</p>
</blockquote>
<p>Valgrind와 Massif는 메모리를 분석하는 툴이다. <strong>Valgrind</strong>는 예전부터 <strong>Memory leak을 분석하는 툴</strong>로 많이 쓰이고, <strong>Massif</strong>는 많이 알려진 툴은 아니지만 valgrind 내부에서 <strong>heap memory profiling</strong>을 하는 도구이다.</p>
<p>제발, 대체, 왜 2022년에 아직도 new/delete를 사용하는 사람이 있는지는 모르겠지만, 혹시나 new/delete를 사용하고 있는 사람이라면 valgrind를 꼭 돌려보는 것을 추천한다. Valgrind를 사용하면 1. 전체 heap allocation이 몇번 일어났는지, 2. 전체 heap deallocation이 몇번 일어났는지, 3. Dangling pointer (i.e. memory leak)이 얼마나 있는지를 알 수 있다.</p>
<p>SLAM의 경우 heap allocation을 사용하는 경우가 굉장히 많다 (물론 안 쓰고 하는 방법도 있겠지만, 대부분의 오픈소스에서 보이는 방식은 전부 힙이라고 보면 된다). 왜냐하면 맵이 점점 커지는 형태의 프로그램인데, 이 맵 포인트를 추가할 때 거의 대부분의 경우 heap으로 추가하기 때문이다. 또, 멀티쓰레딩을 위해 여러 쓰레드에서 접근할 수 있는 공유자원을 만들면서 heap에 올리는 경우도 굉장히 많다. 그렇기 때문에, SLAM을 하는 사람들에게는 필수 툴이라고 볼 수 있다.</p>
<img src="/20221013-cpp-essential-tools/valgrind.png" class="" title="valgrind"> 

<p> </p>
<p>그 다음은 massif 이다. Massif 자체만으로는 memory profiling 기능만을 지원하지만, KDE에서 만든 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0tERS9tYXNzaWYtdmlzdWFsaXplcg==">massif-visualizer<i class="fa fa-external-link-alt"></i></span>를 사용하면 프로그램이 진행되면서 얼마나 많은 stack / heap 메모리가 점유되었는지도 볼 수 있다. 어떤 프로세스 / 어떤 모듈이 얼마나 메모리를 먹는지 볼 수 있고, 이를 통해 이유없이 데이터를 저장하고 있는 부분들을 발견해서 메모리 최적화를 진행할 수도 있다. PC에서 개발할 때에는 왠만하면 CPU나 메모리가 부족할 일이 없는데 (만약 부족했다면 높은 확률로 뭔가 크게 잘못하고 있을 확률이 높다), 타겟 디바이스에서는 PC에서의 작은 실수가 아주 큰 부메랑으로 돌아오기 때문에 메모리 프로파일링이 큰 도움이 될 수 있다.</p>
<img src="/20221013-cpp-essential-tools/massif-visualizer.png" class="" title="massif-visualizer"> 

]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Pointer를 써도 될 때는 언제인가?</title>
    <url>/20221021-pointers/</url>
    <content><![CDATA[<h1 id="C-을-몇년을-했는데-아직도-이런-질문을…"><a href="#C-을-몇년을-했는데-아직도-이런-질문을…" class="headerlink" title="C++을 몇년을 했는데 아직도 이런 질문을…?"></a>C++을 몇년을 했는데 아직도 이런 질문을…?</h1><p>Object를 다룰줄 안다. <code>std::unique_ptr</code> 과 <code>std::shared_ptr</code>을 다룰 줄 안다. 기본적인 포인터 연산도 다룰 줄 안다.</p>
<p>함수에 인자로 넘길 때 nullptr로 넘겨서 optional argument로 넘길 줄도 안다.</p>
<p>근데 왜 이런 질문을 아직도 하는가?</p>
<p> </p>
<p>이전 회사에서는 C++ 프로그래밍을 할 때 포인터를 다룬 경우가 거의 없다. 대부분의 경우 object를 직접 선언해서 사용했고, object에 참조를 할 때는 <code>&amp;</code>를 사용했다.</p>
<p>이번 회사에서는 thread 들 간의 데이터를 다룰 때에는 shared_ptr로 다뤄야한다는 것을 배웠다. 아키텍트의 의견을 따라 모든 객체를 동적할당하여 사용하는 방향으로 개발을 했으며, 이 과정에서 unique_ptr과 shared_ptr을 조금 더 잘 다루게 되었다.</p>
<p>하지만 ‘모든 객체를 동적할당한다’ 라는 개념에 문제가 생겼다. 바로 속도 문제인데, stack에 올리는게 heap보다 빠르다, 라는 점 때문이였다. 모든 것을 heap에 올릴 필요가 있을까? 라는 질문을 하게 되었다. 찾아본 결과 Stack과 heap을 적절히 사용하는게 가장 좋은 성능을 낸다고 한다.</p>
<p>‘Stack과 Heap을 적절히 사용한다’라는건 어느 의미일까? 이에 대해 찾아보던 도중 Stack Overflow에서 이런 문구를 보게 되었다.</p>
<blockquote>
<p><strong>Prefer objects, before unique_ptr, before shared_ptr, before raw pointers</strong>.</p>
</blockquote>
<p><strong>위 논리에 따르면, C++ 프로그래밍을 할 때에는 거의 모든 경우 objects 위주로 프로그래밍을 하는게 맞을 것이다</strong>. Raw 포인터는 물론이고, 스마트 포인터도 볼 일이 거의 없을 것이다.</p>
<p><strong>그렇다면 나는 왜 이렇게 많은 포인터 코드를 본 것인가?</strong></p>
<p>왜 우리 아키텍트는 모든 걸 동적할당 해야한다고 한걸까? 왜 우리 아키텍트는 그게 옳은 방법이라고 배운걸까? 왜 ROS의 수많은 코드들도 다 동적할당으로 쏘아올릴까? 왜 Stack Overflow에서도 수많은 스마트 포인터 코드가 있는 것인가? 왜왜왜???</p>
<p>이 질문에 답하기 위해 여러 글을 찾았고, 적절히 조합해서 내린 결론에 대해 글을 적는다.</p>
<p> </p>
<hr>
<h1 id="동적-할당-dynamic-allocation"><a href="#동적-할당-dynamic-allocation" class="headerlink" title="동적 할당 (dynamic allocation)"></a>동적 할당 (dynamic allocation)</h1><p><code>Object myObject</code>와 같은 방식으로 객체를 <strong>정적할당</strong> 하면 <strong>객체의 수명이 자동적으로 결정</strong>된다. 객체는 scope를 벗어나는 순간 자동으로 메모리에서 삭제된다. 이 방식은 아주 유용한데, 우리가 객체 하나하나 수명을 직접 관리해줄 필요가 없기 때문이다. 즉, 잘못 관리해서 메모리를 차지하기만 하고 아무것도 안하는 메모리가 없을 것이라는 것이다.</p>
<p>정적할당의 반대는 <strong>동적할당 (dynamic allocation)**이다. 그리고 동적할당을 할 때 <code>Object* myObject = new Object;</code> 와 같은 방식으로 포인터를 사용하게 된다. 생성한 후에 객체를 다룰 때는 <code>myObject-&gt;foo()</code> 와 같은 형태로 사용한다. Raw pointer로 생성하면 추후에 <code>delete</code>를 직접 해줘야하는데, 이 경우 위 정적할당 부분에서 언급한 것 처럼 ‘해제를 잊어버리면 메모리에 죽은 메모리가 남는다’라는 문제가 생긴다 (i.e. **Memory leak</strong>). 하지만 이 문제를 파훼하기 위해 <code>std::unique_ptr</code>이나 <code>std::shared_ptr</code> 같은 스마트 포인터라는 개념이 생겼다. <code>std::unique_ptr</code>은 정적할당 object처럼 scope를 벗어나게 되면 메모리 오너십이 사라지며 자동으로 해제되고, <code>std::shared_ptr</code>은 공유 메모리 오너십이 사라지면 자동으로 해제된다. 이렇게 스마트 포인터가 raw pointer보다 안전하기 때문에, 모던 C++에서는 ‘<strong>Raw pointer 보다 스마트 포인터를 사용하라</strong>‘ 라는 룰이 생기게 되었다.</p>
<p>객체의 수명에 관련해서는 Object로 직접 다루나 스마트 포인터를 사용하나 결국 자동 수명 관리 시스템을 사용하기 때문에 차이가 없다고 볼 수 있다. <strong>그러면 언제 정적할당을 하고 언제 동적할당을 해야하는가?</strong></p>
<p>동적할당이 필요한 경우는 다음과 같다.</p>
<ol>
<li>객체가 현재 scope를 벗어나도 살아있어야한다 (복사/이동을 하지 않고 살아있어야한다).<ul>
<li>하지만 대부분의 경우 작은 객체들은 복사/이동을 해도 괜찮은 경우가 많다. </li>
</ul>
</li>
<li>대량의 메모리를 할당해야한다.<ul>
<li>예를 들어, 이미지 몇십장을 할당해야한다. 정적할당으로 생기는 메모리를 저장하는 공간인 Stack에는 충분히 공간이 나지 않을 수 있다.</li>
</ul>
</li>
</ol>
<p><strong>위 두 경우가 아닌 경우에는 무조건 object를 사용해야한다</strong>는 것이다.</p>
<p> </p>
<h1 id="‘가능’하다면-object르-사용하자-‘필요’하다면-포인터를-사용하자"><a href="#‘가능’하다면-object르-사용하자-‘필요’하다면-포인터를-사용하자" class="headerlink" title="‘가능’하다면 object르 사용하자. ‘필요’하다면 포인터를 사용하자."></a>‘가능’하다면 object르 사용하자. ‘필요’하다면 포인터를 사용하자.</h1><p>그러면 포인터는 어디에서 사용할 수 있는 것인가? </p>
<p>동적할당 외로도 쓸 수 있는 부분이 없지는 않다. 하지만 왠만해서는 좋은 practice가 아닌 경우가 많다.</p>
<p>예시를 들어보자.</p>
<p>첫째, <code>foo(Object* obj)</code>와 같이 함수의 인자로 복사 없이 큰 객체를 넘기고 싶은 경우다. 포인터를 사용하면 객체를 복사하지 않고 넘길 수 있다. 하지만 C++ 에서는 사실 <code>foo(Object&amp; obj)</code>를 더 추천한다.</p>
<p>둘째, <code>foo(Object* obj)</code>를 할 때, obj가 선택적인 인자로 사용되게 하고 싶은 경우다. obj에 <code>nullptr</code>이 들어간다면 선택적으로 인자를 넣을 수 있다. 이건 분명한 장점이지만, C++17 부터는 <code>std::optional</code>을 사용해서 이 문제를 해결할 수 있다. </p>
<p>셋째, 컴파일 시간을 개선시키기 위해 전방선언 (Forward declaration)을 사용해서 포인터만 넘김으로써 compilation unit을 분리시키는 방법이다. 엄청나게 큰 프로젝트를 할 때, 예를 들어서 chromium 브라우저를 전부 빌드한다던지, 이럴 때에는 컴파일 시간이 개선되는건 좋다. Pimpl 같은 개념이 이것에 대해 이야기한다. 하지만 내 경우에는 작고 소중한 SLAM 프로그램 하나만 빌드하는것이며, 나는 런타임 성능이 더욱 소중하다.</p>
<p>넷째, C 라이브러리와 호환을 해야할 때의 경우다. 이 경우에는 C 언어가 스마트 포인터 및 클래스를 지원하지 않으니 어쩔 수 없이 raw pointer를 사용할 수 밖에 없다. 그럼에도 불구하고, 포인터를 다룰 때는 스마트 포인터로 다루다가, <code>get()</code>을 사용해서 raw pointer로만 잠시 바꿔주면 된다.</p>
<p> </p>
<hr>
<h1 id="아니-그러면-포인터-왜-이렇게-자주-보이나"><a href="#아니-그러면-포인터-왜-이렇게-자주-보이나" class="headerlink" title="아니 그러면 포인터 왜 이렇게 자주 보이나?"></a>아니 그러면 포인터 왜 이렇게 자주 보이나?</h1><p>Stack Overflow 질문/답변과 여러 C++ 고수들의 블로그를 뒤져봐도 명확한 이유가 나오지 않았다.</p>
<p>그러다가 Stack Overflow에서 댓글 중에서 가슴에 팍 꽂히는 문장을 보게 되었다.</p>
<blockquote>
<p>‘이게 다 <strong>Java++ 유저들</strong> 때문이다. <strong>Java에서 쓰던거처럼 모든 객체를 포인터로 다루는 습관</strong>이 있는 사람들이 C++을 더럽히고 있다’</p>
</blockquote>
<p>하,,</p>
<p>Java++ 개발자 소리 듣고 싶지 않으니, 왠만하면 포인터 쓰지 말자…</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.2 C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>모두콘 2022 - Spatial AI는 미래입니다 (장형기 발표)</title>
    <url>/20221215-moducon2022/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/QGjNggk7ZYM" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<h1 id="Spatial-AI는-미래입니다"><a href="#Spatial-AI는-미래입니다" class="headerlink" title="Spatial AI는 미래입니다"></a>Spatial AI는 미래입니다</h1><blockquote>
<p>2022년 12월 15일 연세대학교 백양누리에서 열린 <span class="exturl" data-url="aHR0cHM6Ly9tb2R1Y29uLmtyLw==">모두콘 2022<i class="fa fa-external-link-alt"></i></span> 행사 AI+Research 트랙에서 첫번째 토크로써 ‘Spatial AI는 미래입니다’ 라는 주제로 발표를 하게 되었습니다.<br>구글 드라이브에 업로드한 <a href="https://docs.google.com/presentation/d/17eJu8k7VfJr167f_qPCIYOb4yGm8V7JD/edit?usp=sharing&ouid=110711989250390692879&rtpof=true&sd=true"><strong>발표 자료 링크</strong></a>도 공유합니다.</p>
</blockquote>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide12.png" class="" title="자기소개"> 

<p>발표를 시작하기 전 간단하게 제 소개를 했습니다.</p>
<p>Facebook 그룹인 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3Iv">SLAM KR<i class="fa fa-external-link-alt"></i></span> 을 소개하고, 카카오톡 오픈채팅방 그룹인 <span class="exturl" data-url="aHR0cHM6Ly9vcGVuLmtha2FvLmNvbS9vL2c4VDVreExi">저희는_SLAM_마스터가_될겁니다<i class="fa fa-external-link-alt"></i></span>를 소개했습니다. SLAM KR에는 약 3천명, SLAM 마스터 방에는 약 900명의 참여원이 있는데, 이 중에 현업자, 논문 저자, 대학원생, 기업 대표님들 및 팀장님들이 계십니다.<br>전체 청중 중 약 1/3 정도만이 SLAM 이라는 기술에 대해 알고 계신 것 같아, SLAM 기술은 로보틱스 분야에서 주변 환경에 대한 지도를 그리고 그 공간 안에서 자기 자신의 위치를 찾는 기술이라고 간단하게 소개 드렸습니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide34.png" class="" title="Spatial AI란?">

<p>Spatial AI는 공간 지능을 뜻합니다.</p>
<p>최근에 새롭게 부상하는 키워드로써, 수많은 기존의 딥러닝 테크닉들이 포함될 수 있는 포괄적인 개념이기 때문에 기존에 딥러닝을 하시는 분들께서는 spatial AI에 대해 부분적으로 익숙하실 수 있습니다 (e.g. 3D object detection, 3D reconstruction).</p>
<p>Spatial AI가 정말 유망한 기술인지 bias가 없이 확인하기 위해, <span class="exturl" data-url="aHR0cHM6Ly9vcGVuYWkuY29tL2Jsb2cvY2hhdGdwdC8=">OpenAI chatGPT<i class="fa fa-external-link-alt"></i></span> 에게 물어보았습니다. 요약하자면 아래와 같은 문장들이 나옵니다.</p>
<ul>
<li>‘현재까지도 사용되고 있는 기술이다’</li>
<li>‘시공간을 정보를 다루는 인공지능 시스템을 개발하는데에 사용됩니다.’</li>
<li>‘예를 들어, 자율주행의 경우에는…’</li>
<li>‘앞으로도 발전해 나갈 것으로 예상됩니다.’</li>
</ul>
<p>여기서 저는 ‘<strong>시공간의 정보</strong>를 다루는…’ 부분에 집중했습니다.</p>
<p>Spatial AI는 공간에 대해 이해하려고 하는 인공지능입니다. <strong>공간에 대해 이해한다</strong>는건 어떤 의미를 가질까요? 아주 쉬운 레벨에서 생각해보자면, 어떤 공간이 비어있는지/아닌지 로 간단하게 공간을 파악할 수 있습니다. 예를 들어, 길거리에 아무것도 없다면 제가 우다다 달려도 부딪칠게 없겠죠. 조금 더 높은 수준으로 공간을 이해하려면, 내 앞에 어떤 물체가 있을 때 어떤 모양을 하고 있는지도 이해할 수 있겠습니다. 길이 x 넓이 x 높이 와 같은 표현도 가능하겠고, 점/선/면/곡면 등으로 물체의 생김새를 표현할 수 있겠죠. 이러한 정보를 통해 정확히 벽이 어디에 있는지도 표현할 수 있을 겁니다. 이와 같은 이해의 수준을 ‘<strong>기하학적 이해</strong>‘라고 합니다.</p>
<p>기하학적 이해를 뛰어넘는 이해도 있습니다. ‘<strong>의미론적 이해</strong>‘나 ‘<strong>물리적 이해</strong>‘와 같은 성질이 있습니다. 내 앞에 있는 물체가 책상인지, 자동차인지, 장난감인지 구분할 수 있는게 의미론적 이해라고 볼 수 있겠습니다. 의미론적 이해를 깊은 수준까지 할 수 있다면, 내 앞에 놓인 자동차의 모양을 하고 있는 이 물체가 진짜 자동차인지, 자동차 형태를 한 장난감인지도 구분을 할 수 있을겁니다. 물리적 이해는 내 앞에 있는 물체의 물리적인 성질을 이해하는 것 입니다. 만졌을 때 단단한지 말랑말랑한지, 두드렸을 때 어떤 소리가 나타나는지, 빛을 쬐었을 때 어떻게 반사하는지 등등을 포함하겠습니다.</p>
<p>마지막으로, ‘<strong>시간의 이해</strong>‘가 있습니다. 기하학적, 의미론적, 물리적 이해는 셋 다 모두 어떤 정해진 시점에서 고정된 값이지만, 시간이 지나면서 바뀔 수 있는 것들이 있습니다. 초록색으로 무성한 산이 가을에도 겨울에도 초록색일까요? 가을에는 단풍이 들 수도, 겨울에는 눈이 덮힐 수도 있습니다. 비가 오면 어두울 수도 있겠구요. 보수를 하지 않는 건물이 100년동안 그 모습을 유지를 할 수 있을지, 얼음 조각상이 봄이 와도 형태를 유지할 수 있을지, 와 같은 상황에서는 우리가 물체를 바라보는 모습이 시간에 따라 달라진다는 것을 알 수 있습니다.</p>
<p>Spatial AI는 궁극적으로 시공간에 대한 완벽한 이해를 하려고 합니다. 그렇기 떄문에 <strong>기하학적, 의미론적, 물리적, 시간적 이해를 전부 포함</strong>합니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide56.png" class="" title="Spatial AI usages">

<p>현재까지 올라온 Spatial AI의 수준을 보겠습니다. 사실 방금 전 슬라이드에 비하면 아주 부끄러울 정도로밖에 올라오지 못했습니다 ㅎㅎ</p>
<p>Spatial AI가 현재 잘 적용되고 있는 대표적인 분야로는 <strong>자율주행</strong>과 <strong>가상/증강현실</strong>이 있습니다. 자동차 자율주행을 할 때에는 주로 기하학적 + 의미론적 이해를 달성하는데에 주로 집중합니다. 자동차 주변 환경에서 차도와 인도를 구분할 수 있어야하고, ‘인도에서는 주행하면 안돼’라는 룰을 기반으로 인도에는 절대 침범하지 않으려고 합니다. 전후좌우로 자동차가 있는지도 확인을 하고 충돌하지 않기 위해 속도를 조절하기도 하구요. ‘옆차가 나를 추월할까?’ 같은 문제를 풀 때는 아주 단기적으로 시간적인 이해를 하기도 합니다 (i.e. prediction). 드론 자율비행 같은 경우에는 자동차와는 다르게 의미론적 이해까지는 하지 않고, 충돌방지를 위한 기하학적 이해에 집중하는 경우가 많습니다. 가상/증강현실 분야에서는 자동차 자율주행 / 드론 자율비행보다는 훨씬 앞선 연구를 하지만, 현재 가장 성공적인 제품 중 하나인 <span class="exturl" data-url="aHR0cHM6Ly9wb2tlbW9uZ29saXZlLmNvbS8=">Niantic Pokemon Go<i class="fa fa-external-link-alt"></i></span>를 보았을 때는 공간적 이해만 사용하는 수준이라고 볼 수 있겠습니다. 실제 공간에 위치하는 물체와 겹쳐있지 않게 바닥과 벽을 잘 인식해서 가상의 포켓몬을 현실 세계에 렌더링하는 겁니다.</p>
<p>최신 연구개발되는 제품들을 보면 조금 더 넓은 범위의 연구를 하는 것이 보입니다. 최신 <strong>메타버스 연구</strong> 아이디어들을 보면, 컴퓨터에 직접 키보드/마우스를 사용해서 접속하는 것이 아닌 가상의 키보드/마우스로 접속하는 모습도 보입니다. 구글의 자회사인 <span class="exturl" data-url="aHR0cHM6Ly9ldmVyeWRheXJvYm90cy5jb20v">Everyday Robots<i class="fa fa-external-link-alt"></i></span>나 청소기/헤어드라이어로 유명한 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZHlzb24uY29tL2Vu">Dyson<i class="fa fa-external-link-alt"></i></span>에서도 <strong>홈 로보틱스</strong>를 연구개발하고 있는데, 복잡한 환경 속에서 물리적인 업무를 수행 가능할 정도로 상당히 높은 수준의 로봇 개발 연구를 하고 있습니다. 예를 들어서 사람이 ‘내가 커피잔을 어디에 뒀더라?’ 라고 하면, 로봇이 이 내용을 파악해서 ‘주방 창가 옆 테이블 위에 두셨어요’라고 답을 할 수 있을 수준까지 입니다. 이를 위해서는 우선 사람의 언어를 이해해 ‘커피잔’이 ‘컵’과 동일하다는 것을 이해해야하고, ‘주방’이 어딘지 이해해야하고, ‘창가 옆 테이블’이라는 정보와 ‘위’ 라는 정보까지 완벽하게 이해해야합니다. 여기서 사람이 ‘아 그래? 가져와줘’ 라고 얘기한다면 현재 위치에서부터 주방 창가 옆 테이블까지 충돌하지 않고 안전하게 이동할 수 있어야하고, 컵을 적당한 힘으로 집어서, 다시 유저에게 돌아와야합니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide78.png" class="" title="Levels of Spatial AI">

<p>Spatial AI는 <strong>4 단계로 분류</strong>할 수 있는데, 아래부터 쉬운 순서대로 쌓아올린다고 볼 수 있습니다.</p>
<p>Level 1은 Motion Tracking &amp; Sparse map 입니다. <strong>Motion tracking은 자기 자신의 움직임을 파악하는 것</strong>이고, <strong>Sparse map은 공간 속에서 어떤 물체가 존재하는지/아닌지 정도를 구분</strong>하는 간단한 문제라고 볼 수 있습니다. SLAM이나 Visual odometry로 풀어낸 문제라고 볼 수 있고, 현재 이미 기능적으로는 연구가 거의 끝난 수준이며 ‘어떻게 하면 효율적이게 할 수 있을까?’에 집중하는 추세입니다. 예시로는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9nX3dOME50MFZBVQ==">VINS-Mono<i class="fa fa-external-link-alt"></i></span>가 있겠습니다.</p>
<p>Level 2는 Dense map 입니다. <strong>Dense map은 sparse map과 다르게 더욱 뺵뺵한 지도를 만드는 것</strong>이 목표입니다. 지도가 빽뺵해지면서 선, 면, 곡면 등을 검출해낼 수 있기 때문에, <strong>로봇이 명확하게 공간 속 벽을 파악할 수 있고 이에 맞춰 액션 플래닝</strong>을 하고 <strong>장애물 회피</strong>를 할 수 있게 됩니다. 고가의 센서들로 dense mapping을 하는 연구는 끝났지만, 저렴한 센서로 dense mapping을 하는 방법, 효율적으로 dense map을 다루는 방법, 더욱 저렴한 센서를 만드는 방법과 같이 여러가지 연구가 진행되고 있습니다. 알고리즘 예시로는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9rZUlpclhyUmIxaw==">BundleFusion<i class="fa fa-external-link-alt"></i></span>이 있겠습니다.</p>
<p>Level 3는 3D Scene understanding 입니다. <strong>Scene understanding을 통해서 로봇은 이제 어떤 벽이 어떤 물체를 의미하는지</strong>를 알게 됩니다. 차도/인도의 차이를 구분해서 이동에 제한을 두는 자율주행 자동차도 간단한 수준으로 이런 기술을 사용하고 있다고 볼 수 있습니다. 홈 로봇이 ‘주방’, ‘창가 옆 테이블’, ‘커피잔’ 등을 이해하는 것이 이 단계라고 볼 수 있겠습니다. CVPR 2020 부터 지금까지 계속 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dWJlLmNvbS9wbGF5bGlzdD9saXN0PVBMNlFYcHd2S2hRSXJTa2xOc1FKcWljanR4aU9zNl9OcFk=">3D scene understanding 워크샵<i class="fa fa-external-link-alt"></i></span>이 열리고 있으니 관련 연구를 보면 좋을 것 같습니다.</p>
<p>Level 4는 Dynamic / Object-level map 입니다. <strong>물체들간의 상호관계를 이해하는 단계</strong> 라고 볼 수 있는데, 어떤 물체는 움직일 수 있고 어떤 물체는 움직일 수 없는지, 물체들간의 상호관계는 어떻게 되는지, 내가 이 물체를 만지면 형태가 바뀌는지에 대한 이해라고 볼 수 있겠습니다.</p>
<p>이렇게 결국 Spatial AI는 기하학적 + 시간적 + 의미론적인 정보를 모아, Action과 Prediction을 가능하게 만들고, 이를 통해 고급 상호작용을 가능케 한다고 생각하면 좋을 것 같습니다. (물리적…은 의미론적 내용과 함께 들어갈 수 있습니다)</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide910.png" class="" title="Object Detection, Segmentation, GAN">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p>이번 슬라이드 부터는 기존의 딥러닝을 하시던 분들이 어떻게 Spatial AI와 이어질 수 있는지 알아보겠습니다.</p>
<p>2D 컴퓨터 비젼을 하시는 분들께서는 주로 <strong>Detection / Segmentation</strong>과 같이 object semantic을 다루시는 분들께서 spatial AI 개발에 참여하시기 좋습니다. 하나 아쉬운 점이라면, 2D CV는 이미지 도메인에서만 적용되기 때문에 다시 3D 로 object semantic 정보를 쏘아올려서 3d object와 매칭을 시켜야한다는 점이 있습니다. 이에 대한 부분은 SLAM과 같이 multiple view geometry를 이용하면 좋은데, 좋은 예시로 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9RblZsZXhYaTlfYw==">cubeSLAM<i class="fa fa-external-link-alt"></i></span>이 있습니다.</p>
<p><strong>GAN</strong>을 하신 분들 중에서는 다양한 날씨나 조명에 대한 <strong>latent variable</strong>을 학습해서 실제 데이터셋에 존재하지 않는 데이터를 생성해낼 수 있습니다. 이러한 latent variable을 이용해서 spatial AI에서도 시간에 따른 3D 공간의 기하학적, 물리적, 의미론적인 변화를 표현할 수도 있겠습니다.</p>
<p>3D 컴퓨터 비젼을 하신 분들은… 사실 뭐 이미 spatial AI를 하고 있다고 봐도 괜찮지 않을까 싶습니다. <strong>3D 공간 속에서의 의미론적인 탐색</strong>을 하고 있는 거니까요. 3D Object detection / Segmentation 연구 모두 훌륭하다고 생각합니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide1112.png" class="" title="NeRF and NLP">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p>최근 그래픽스 분야에서 <strong>NeRF</strong>와 같이 렌더링 기법을 이용하면서 3D 공간에 대한 표현 방법을 찾고 있는 연구가 활발하게 진행되고 있습니다. Spatial AI에서는 <strong>3D 공간을 표현하는 방식</strong> 역시 굉장히 중요한데, 기존의 point cloud, mesh, occupancy grid map / voxel 같은 것으로 표현하는 방식 외로 새로운 neural representation이 나왔다는 건 굉장히 환영할만한 내용인 것 같아요. NeRF는 현재로써는 메모리를 잡아먹는다는 단점이 있지만, differentiable하기 때문에 다양한 뉴럴넷들과의 궁합이 잘 맞고 end-to-end 뉴럴넷을 만든다는 가능성을 열어준다는 것이 굉장히 유망합니다.</p>
<p>Spatial AI는 컴퓨터 비젼만 할 것 같지만, 의외로 <strong>자연어 처리 및 language model</strong>이 활약할 부분이 많습니다. 자연어 처리 분야는 주로 chatGPT와 같은 챗봇에서 많이 사용되며 최근 급격한 성능 개선이 이뤄졌는데, 앞으로 메타버스 및 홈 로보틱스와 같은 제품군을 생각했을 떄 ‘<strong>공간을 표현하는 언어</strong>‘ 역시 굉장히 중요해지게 됩니다. 자연어는 실제로 비전 쪽에서 semantic을 표현하고 구분하는데에 아주 좋은 정보가 될 수 있으며, 비전 데이터로 봤을 때는 비교하기 어려운 경우에도 자연어 데이터가 섞였을 때 안정적으로 비교가 가능해지기도 합니다. 좋은 예시로 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9UOVhTVTBwS1gyRQ==">CLIP<i class="fa fa-external-link-alt"></i></span>이 있습니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide1314.png" class="" title="Reinforcement learning and SLAM">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p>3D 공간 속에서 최적의 action과 planning을 하기 위해 <strong>강화학습 (reinforcement learning)</strong> 연구도 활발하게 진행되고 있습니다. 복잡한 환경 속에서 최적의 control 파라미터를 찾기 위해 보상 기반 학습을 진행하며 최적의 action을 찾아낼 수 있기 떄문에, spatial AI의 frontend로써 perception 모델이 3D 공간을 잘 이해할 수 있다면, RL이 backend에서 받아서 해당 <strong>공간 속 최적의 action / planning</strong>을 할 수 있게 됩니다.</p>
<p><strong>SLAM</strong>은 spatial AI와 아주 밀접한 관계를 가지고 있는 기술입니다. SLAM은 <strong>실시간으로 3D 공간의 형태를 파악하고 동시에 자기 자신이 해당 공간에서 어디에 위치하는지를 알아내는 기술</strong>인데, sparse map도 알아낼 수 있고 dense map도 알아낼 수 있다는 점에서 spatial AI의 level 1 및 level 2와 밀접하게 연관되어있다는 것을 알 수 있습니다. SLAM은 사실 상 motion model과 observation model을 동시 최적화하는 state estimation 기법인데, 여기에 semantic 정보나 시간에 대한 latent variable도 넣어 함께 최적화하는 방향도 최근 제안이 되고 있습니다. Spatial AI는 SLAM에서 코어엔진으로써 기하학적/의미론적/물리적/시간적 정보를 함께 최적화나는 spatial AI 기능이 될 가능성이 높습니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide15.png" class="" title="Spatial AI combination">

<p>그래서 individual contributor로써는 spatial AI 연구를 어떻게 해야할까요?</p>
<p>사실 아이디어를 낼 수 있는 경로는 무궁무진하다고 생각합니다. <strong>방금 전에 설명했던 분야들을 단순히 늘어놓고 짬뽕시키기만 시켜도 spatial AI 모듈이 나올 수 있지 않을까요?</strong> 현재까지 오버랩이 없던 분야들일 수록 더 새로운 아이디어가 나올 수 있다고 생각합니다. 다만, 어떻게 섞어야 가장 정확하고 효율적으로 spatial AI라는 문제를 풀 수 있을지에 대한 부분은 깊게 고민해볼 부분입니다.</p>
<p>다음 슬라이드부터 최신 연구에서는 이렇게 다양한 분야를 어떻게 섞었는지에 대해 소개해보겠습니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide1617.png" class="" title="3D Dynamic Scene Graph">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDIuMDYyODk=">3D Dynamic Scene Graph<i class="fa fa-external-link-alt"></i></span>는 2020년 MIT의 Luca Carlone 교수님 랩실에서 나온 연구입니다. Indoor scene을 표현할 때 단순히 포인트 클라우드나 voxel로 전부 표현하는 것이 아닌, 로봇이 제대로 위치를 인식하고 interaction을 할 수 있는 정보를 넣어주는 것을 목표로 시작된 연구입니다. 5개의 Layer이 있는데, 단계별로 abstraction이 된다는 것이 특징입니다. Layer 1에서는 Kimera VIO를 통해 위치를 추정하면서 Metric-semantic map (i.e. 실제 세상의 스케일을 가지고 있으면서 의미론적인 정보를 담고 있는 지도)를 생성할 수 있습니다. 이 지도를 기반으로 Layer 2에서는 Object 및 agent를 찾아 검출할 수 있습니다. 알고있는 형태의 오브젝트라면 CAD 모델을 fit해서 비어있는 맵을 채워주고 오브젝트를 검출할 수 있고, 알고있지 못한 오브젝트거나 fit에 실패한 오브젝트라면 clustering을 통해서 3D object bounding box를 그려줍니다. Layer 3 에서는 metric-semantic map에서 평면을 감지해서 벽을 감지하고, 이를 통해 2D 벽 지도를 만듭니다. Layer 4에서는 2D 벽 지도를 기반으로 방을 구분할 수 있고, Layer 5에서는 입/출구 기반으로 건물을 구분할 수 있게 됩니다. </p>
<p>초창기 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMDI0OTA=">Kimera<i class="fa fa-external-link-alt"></i></span> 연구부터 최근까지 계속 진행되고 있는 프로젝트인데, 오픈소스로도 잘 나와있습니다. </p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide1819.png" class="" title="LM-Nav">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3ZpZXcvbG1uYXY=">LM-Nav<i class="fa fa-external-link-alt"></i></span>는 UC Berkeley의 Sergey Levine 교수님 랩실에서 나온 연구입니다. Vision-Language model을 사용해서 Robotic navigation도 풀 수 있을지에 대한 연구인데, 유저가 ‘X를 지나 Y에서 좌회전해서 Z로 가’ 라는 명령어를 내리면 지도 없이 공간을 탐색하면서 명령을 따라 이동하는 기술을 소개합니다. Langauge parsing을 하는 LLM 모델, 로봇에서 바라보고 있는 이미지 공간을 언어 공간으로 넘기는 VLM 모델, 그리고 이미지 공간을 navigation을 위한 공간으로 넘기는 VNM 모델을 차례대로 사용합니다.</p>
<p>LM-Nav 연구는 Language도 공간을 이해하고 탐색하는데에 쓰일 수 있다는 점을 보여주며, Spatial AI 기술에 NLP 역시 큰 역할을 할 수 있음을 의미합니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide2021.png" class="" title="Do as I can, not as I say">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9zYXktY2FuLmdpdGh1Yi5pby8=">Do as I can, not as I say<i class="fa fa-external-link-alt"></i></span> 연구는 구글의 자회사인 Everyday robots에서 나온 연구입니다. 사람이 추상적인 질문 (e.g. 문제가 생겼는데 도와줘~)을 했을 때 답변을 robotic task로써 이해하고, 그것을 실제로 vision + navigation 기술로써 풀어낸다는 연구입니다. 예시로는 ‘콜라를 쏟았는데 도와줘’ 라고 했을 때, 로봇이 ‘1. 콜라 캔을 찾는다, 2. 콜라 캔을 집어든다, 3. 쓰레기통 옆으로 이동한다, 4. 콜라 캔을 내려놓는다, 5. 스펀지를 찾는다, 6. 스펀지를 집어든다, 7. 유저가 있는 테이블로 이동한다, 8. 테이블에 스펀지를 내려놓는다’ 라는 독립적인 robotic task 문제로 풀어내고, 해당 위치에 도달했을 때 각각의 문제를 해결함으로써 아주 복잡하고 추상적인 질문을 해결하는 모습을 볼 수 있습니다.</p>
<p>Say-Can 연구는 자칫 Vision + NLP + Navigation을 사용한다는 점에서 LM-Nav 연구와 비슷해보일 수 있지만, LM-Nav 연구는 명확한 task들을 주는 반면에 Say-Can 연구는 추상적인 질문 속 명확한 task들을 분류해새 수행하는 모습을 보이는 점에서 다르다고 볼 수 있습니다.</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide2223.png" class="" title="1_2_슬라이드">

<blockquote>
<p>원본 pptx 자료를 보시면 gif 자료로 보실 수 있습니다.</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIyMTAuMTczMjU=">Real-time Mapping of Physical Scene Properties with an Autonomous Robot Experimenter<i class="fa fa-external-link-alt"></i></span> 연구는 Imperial College London의 Andrew Davison 교수님 랩실에서 나온 연구입니다. 3D 공간 속에 우리가 물리적인 특성을 모르는 물체들이 있을 때, 물체들을 물리적인 특성을 기반으로 분류하고 3D 공간에 표시한다는 연구입니다. 로봇이 자율적으로 움직이면서 물체에 force/torque 센서가 부착된 팔로 만져보면서, 부드러운 물체와 딱딱한 물체를 구분할 수 있습니다. 이 때, 3D 공간 속 기하학적으로 뭉쳐있는 (i.e. 같은 물체일 확률이 높은) 공간들을 확률적으로 같은 물리적인 특성을 가지고 있다고 표현합니다. 처음에 딱딱한 물체를 만져봤을 때에는 아직 부드러운 물체를 만져본 적이 없기 때문에 세상 모든 부분이 딱딱하다고 생각하지만, 부드러운 물체와 딱딱한 물체를 번갈아가며 만져보며 부드러운 물체와 딱딱한 물체를 vision 정보와 결합하게 되며 이해하게 됩니다.</p>
<p>Spatial AI 연구 쪽에서는 아직 물리적인 특성까지 고려하는 연구가 많이 없는데, 이런 연구가 있다는 점에 감사할 따름입니다 ㅎㅎ</p>
<p> </p>
<hr>
<img src="/20221215-moducon2022/slide2425.png" class="" title="1_2_슬라이드">

<p>Spatial AI는 결국 어떤 형태가 되게 될까요? AI가 우리가 살고 있는 공간을 전부 이해한다면, 그제서야 진정으로 Digital Twin과 메타버스가 이뤄지게 되지 않을까 생각합니다. 그때 쯤 되면 로봇들이 우리 집에서도, 공장에서도, 길에서도 우리의 일을 대신 다 해주고 있겠지요. 기술이 발전하면서 자동화를 꾀하는건 지극히 자연스러운 현상이라고 생각합니다. 연구자로써, 그리고 엔지니어로써 이러한 트렌드에 올라타 기여를 할 수 있다면 그게 커리어 적 성공이 아닐까라는 생각도 해봅니다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>학회 발표 리뷰</category>
        <category>학회 발표 리뷰</category>
        <category>1.4 Robotics</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Spatial AI</tag>
        <tag>모두의연구소</tag>
        <tag>모두콘</tag>
      </tags>
  </entry>
  <entry>
    <title>빈브라더스 카페 뉴스레터 구독자 모임 후기</title>
    <url>/20221219-bean-brothers-talk/</url>
    <content><![CDATA[<blockquote>
<p>12월 29일 <span class="exturl" data-url="aHR0cDovL2JlYW5icm90aGVycy5jby5rci8=">빈브라더스<i class="fa fa-external-link-alt"></i></span> 합정점에서 진행된 ‘빈브라더스 뉴스레터 구독자 모임’ 행사에서 적은 노트입니다 :)<br>빈브라더스는 제가 좋아하는 카페인데, 이곳 소속의 커피 연구원님께서 여러 브라질 농장/공장을 투어하시면서 느낀 점들을 공유해주셨어요.<br>산업공학을 전공했던 사람의 관점에서 너무 재밌는 내용이 많아서 정리합니다.</p>
</blockquote>
<img src="/20221219-bean-brothers-talk/DSCF9643.jpg" class="" title="사진사진"> 

<p> </p>
<hr>
<h1 id="데렉의-브라질-토크"><a href="#데렉의-브라질-토크" class="headerlink" title="데렉의 브라질 토크"></a>데렉의 브라질 토크</h1><h2 id="왜-브라질인가"><a href="#왜-브라질인가" class="headerlink" title="왜 브라질인가?"></a>왜 브라질인가?</h2><ul>
<li>빈브라더스에서 가장 잘나가는 제품은 <span class="exturl" data-url="aHR0cHM6Ly93d3cuYmVhbmJyb3RoZXJzLmNvLmtyL2dvb2RzL2dvb2RzX3ZpZXcucGhwP2dvb2RzTm89MTAwMDAwMDAzMQ==">블랙수트<i class="fa fa-external-link-alt"></i></span>이다. (전체의 약 60%)<ul>
<li> 블랙수트의 60%는 브라질</li>
<li> 싱글 오리진 판매는 10%도 안됨</li>
</ul>
</li>
<li>브라질 원두의 수급이 중요하다.<ul>
<li> 퀄리티?</li>
<li> 가격?</li>
</ul>
</li>
</ul>
<h2 id="브라질-특징"><a href="#브라질-특징" class="headerlink" title="브라질 특징"></a>브라질 특징</h2><ul>
<li>브라질 비행편에는 직항이 없다 (왕복 50 시간)</li>
<li>브라질은 에티오피아/케냐에 비해 자본이 있다.<ul>
<li>그렇기 때문에 커피 생산 체계가 잘 잡혀있고 연구활동이 활발하다.<ul>
<li>예시) Passeio 농장 -&gt; 43개의 종류의 커피나무를 심어 실험을 해보는 중</li>
<li>예시2) 다른 종의 식물이 주변에서 자랄 때 더 잘 자라는 경우도 많음.<ul>
<li>주변에 바나나를 길러봤더니 더 잘 자라더라~ 원숭이가 주변에 있다고 ㅋㅋ</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>브라질 커피는 트렌디하지 않은 편. 콜롬비아 커피가 훨씬 트렌디하다.</li>
<li>전세계 커피 생산의 30-40%를 차지한다.<ul>
<li>대략 1년에 6천3백만 bag 생산 (1bag은 60kg)</li>
<li>2위 베트남 2천9백만.</li>
<li>생산 효율이 높다는 뜻. 공급이 많기 떄문에 가격이 저렴한 편.</li>
</ul>
</li>
<li>고도 1300-1400 m 정도에서 자람<ul>
<li>에티오피아는 &gt;2000m 에서 자라는데? -&gt; 적도 쪽이라서 온도가 높아 고도가 높아도 춥지 않음.</li>
</ul>
</li>
<li>브라질 사람들은 브라질 커피만 마시는 편이다. <ul>
<li>브라질 사람들은 해외 품종에 대해서는 잘 모름</li>
<li>생두 관세가 엄청나기 때문. 브라질 도메스틱 시장에서는 ‘콜롬비아/에티오피아 커피’라고 판매하지 않고 국내생산 커피의 품종이 적혀있는 편.</li>
<li>그렇다보니 브라질 농장에 에티오피아/케냐 커피를 선물로 가져가면 되게 좋아하는 편.</li>
</ul>
</li>
<li>1900년도 초 이탈리아 + 일본에서 넘어온 이민자가 많음<ul>
<li>식문화가 많이 들어온듯. 밤 10시에 에스프레소 먹을 사람 찾기도 한다고 ㅋㅋ</li>
<li>치즈볼 -&gt; 커피</li>
</ul>
</li>
</ul>
<h2 id="작년-브라질-커피-농장-한파의-여파가-있나요"><a href="#작년-브라질-커피-농장-한파의-여파가-있나요" class="headerlink" title="작년 브라질 커피 농장 한파의 여파가 있나요?"></a>작년 브라질 커피 농장 한파의 여파가 있나요?</h2><ul>
<li>작년 브라질 한파 (온도가 &lt;0도로 떨어짐) -&gt; 커피나무가 다 죽음<ul>
<li>커피 추수에는 약 2-3년 정도 걸림. (봄에 심고 가을에 추수가 아님)</li>
</ul>
</li>
<li>커피 나무들이 다들 힘이 없음 ㅠㅠ</li>
</ul>
<h2 id="Rio-Verde-공장"><a href="#Rio-Verde-공장" class="headerlink" title="Rio Verde 공장"></a>Rio Verde 공장</h2><ul>
<li>구역별로 커피나무 품종이 나눠져있고 가공 프로세스가 나눠져있음.</li>
<li>굉장히 체계적으로 나눠져있고, 잘 된 자동화 설비를 가지고 있음<ul>
<li>Wet mill -&gt; Dry mill -&gt; Export prep (이거 사진이 있었으면 좋았을텐데 ㅠㅠㅠㅠ)</li>
</ul>
</li>
<li>Rio Verde 공장의 커피는 스타벅스 리저브에 납품되고 있다.<ul>
<li>괜히 스타벅스 리저브의 선택을 받은게 아니다. 잘 잡힌 체계로 인해 품질과 납품 수량 안정성 모두 굉장히 좋다.</li>
</ul>
</li>
<li>가지고 있는 고민거리 <ul>
<li>Peaberry 문제<ul>
<li>납품하기에는 사이즈가 너무 작은 원두를 어떻게 해야할까?<ul>
<li>맛은 좋다. 하지만 사이즈가 작아서 로스터리에서 별로 안좋아한다.</li>
<li>버리기에는 아깝다.</li>
<li>방향제…? 탈취제…? 같은거로 판매하면 어떨까.</li>
<li>수확 시 사이즈를 제대로 측정할 수 있으면 어떨까. (이미 수확된건 버리거나 다른 상품으로 전환할 수 밖에 없다.)<ul>
<li>농업로봇을 만드는 사람들에게는 이게 굉장히 중요하겠다. 자동으로 사이즈를 측정해서 수확할 수 있다면 좋을텐데.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20221219-bean-brothers-talk/process.png" class="" title="커피프로세스"> 


<h2 id="Research-topic"><a href="#Research-topic" class="headerlink" title="Research topic"></a>Research topic</h2><ul>
<li>커피 묘목이 필드에 튼튼하게 뿌리내리게 하는 혁신적인 방법이 필요함<ul>
<li>Resilience? 한파 같은게 지나가도 다음 해에 정상복구가 가능해야하지 않을까.</li>
<li>커피 묘목이 뿌리내리기 위한 조건<ul>
<li>뿌리의 밀도가 높아야한다 (묘목을 기르는 토양의 질이 좋아야함)</li>
<li>뿌리가 내리는 방향이 아래로 똑바로 내려가야한다.</li>
<li>결론: 밀도 높은 뿌리가 아래로 쭉 뻗어나가게 해야한다</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="테이스팅-노트에-대한-고찰"><a href="#테이스팅-노트에-대한-고찰" class="headerlink" title="테이스팅 노트에 대한 고찰"></a>테이스팅 노트에 대한 고찰</h2><ul>
<li>그냥 마셨을때는 모르겠지만, 테이스팅 노트에 적혀있는것에 시음하는 사람이 거부감을 느끼면 싫어할 가능성이 높음<ul>
<li>발효커피가 핫했을 때 이게 좀 문제가 되었다고… (e.g. “발효커피? 예전에 먹어봤는데 청국장냄새가 나서 싫어! 절대 안마셔!!”)</li>
</ul>
</li>
<li>브라질에서도 발표커피를 만들어보려고 하는데… 트렌드에 좀 늦은 듯</li>
</ul>
<h2 id="Passeio-농장"><a href="#Passeio-농장" class="headerlink" title="Passeio 농장"></a>Passeio 농장</h2><ul>
<li>Passeio 농장 커피의 조건: 단맛, 산미, 카라멜향 약간의 과일향<ul>
<li>‘The bitterness of low quality linger long after, the sweetness of low price goes away’라는 문구가 벽에 적혀있다.<ul>
<li>비즈니스에서 단기 수익성에 눈이 멀면 종종 품질을 놓치기도 한다. 품질에 민감한 생산자일수록 더 많은 고객을 유치할 것. 이러한 점을 잘 캐치하고 있다는걸 보여준다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="이름-못들음-농장"><a href="#이름-못들음-농장" class="headerlink" title="{이름 못들음} 농장?"></a>{이름 못들음} 농장?</h2><ul>
<li>커피나무 꽃에서 꿀을 재배하고 있다고 한다.<ul>
<li>커피 꿀…? 미쳤다… 궁금하다…</li>
</ul>
</li>
<li>연구 중: 100가지 커피 나무를 재배 중. 이 중에서 제일 맛있는걸 골라내려고 한다고 함.</li>
<li>커피를 단순히 생산 제품으로 보지 않고, 무언가의 믿음이 있는 것으로 보인다.<ul>
<li>커피나무 밑에 크리스탈을 묻어놓는다던지, </li>
<li>말리고 있는 원두에 노래를 틀어준다던지(?)</li>
<li>특이한 점은, 이런 특이한 프로세스를 전혀 과시하거나 브랜딩화 하지 않는다.<ul>
<li>진정으로 믿는것 같은 모습으로 보임.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20221219-bean-brothers-talk/DSCF9664.jpg" class="" title="사진사진2"> 

<p> </p>
<hr>
<h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&A"></a>Q&amp;A</h1><ul>
<li>맛 vs 가격 -&gt; 맛 대 가격 비율이 중요함.</li>
<li>결정은 어떻게 하는지? 산지 투어를 가신 분들이 결정하시는지? -&gt; 산지 투어를 가신 분들이 커피 농장 리스트를 취합함. 귀국 후 농장에 샘플 요청해서 로스터 + 타 멤버들과 결정함. 이 때, 커피를 제일 잘 이해하고 블렌드를 제작하는 로스터의 의견이 굉장히 중요함.</li>
</ul>
<p> </p>
<hr>
<h1 id="네트워킹"><a href="#네트워킹" class="headerlink" title="네트워킹"></a>네트워킹</h1><ul>
<li>전체 인원을 3테이블로 나눈 후, 같은 테이블에 있는 인원들과 함께 질문답변을 했다.<ul>
<li>참여자들마다 유니크한 소개문구와 질문이 있었다.<ul>
<li>내 경우에는 ‘<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmVhbmJyb3RoZXJzLmNvLmtyL2dvb2RzL2dvb2RzX3ZpZXcucGhwP2dvb2RzTm89MTAwMDAwMDAyNQ==">벨벳화이트<i class="fa fa-external-link-alt"></i></span> (빈브라더스 에티오피아 블렌드)처럼 섬세한 장형기님’ 이라는 소개문구와 함께 ‘&lt;가장 사적인 커피 이야기&gt;를 쓴다면?’ 이라는 질문이 있었다.</li>
<li>당시에는 ‘병원 다닐 때 커피를 먹으면 안되서 디카페인을 먹었는데, 국내에는 내 취향이 아닌 콜롬비아 디카페인밖에 없어서 세상 너무 속상했다’ 라는 말을 했다.</li>
<li>지금 다시보니 뭔가 ‘&lt;가장 사적인 커피 이야기&gt;라는 책을 쓴다면 무슨 내용의 책을 적을거냐고 물어보는거 같은데… 독해력을 좀 더 키워야겠다 ㅠㅠ ㅋㅋ</li>
</ul>
</li>
<li>같은 테이블에 빈브라더스 바리스타님도 같이 앉아계셨다.</li>
<li>현직 바리스타이시거나 바리스타를 준비하시는 분들이 많이 계신 것 같았다. 물론 나 같은 직장인도 있었다.</li>
</ul>
</li>
</ul>
<img src="/20221219-bean-brothers-talk/DSCF9788.jpg" class="" title="사진사진3"> 



]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>빈브라더스</tag>
      </tags>
  </entry>
  <entry>
    <title>2022년 세미나 결산</title>
    <url>/20230102-2022-seminar-showdown/</url>
    <content><![CDATA[<p>2022년을 마치면 올해 진행했던 외부 세미나들을 정리하려고 합니다!</p>
<p>2022년동안 다읔과 같은 세미나를 진행했습니다.</p>
<ul>
<li>1번의 자율주행 부트캠프 (4주)<ul>
<li>“자율주행 데브코스” @프로그래머스</li>
</ul>
</li>
<li>4번의 2-day VSLAM 강의<ul>
<li>“Monocular VSLAM 실내위치탐지” @대전정보문화산업진흥원</li>
</ul>
</li>
<li>7번의 짧은 세미나 (1-3시간)<ul>
<li>“우리 모두 슬램해” @모두의연구소 [자료 공개]</li>
<li>“프로젝트 관리와 협업” @프로그래머스</li>
<li>“자율주행의 이해와 사례” @금융연수원</li>
<li>“버그없고 성능좋은 C++ 개발 가이드” @T모 회사 [자료 공개]</li>
<li>“AI+SLAM” @모두의연구소 [자료 공개]</li>
<li>“Spatial AI는 미래입니다” @모두의연구소 [자료 공개]</li>
<li>“농업분야에서 GPS를 사용할 수 없는 경우에 위치 추정 방안” @서울대학교 [자료 공개]</li>
</ul>
</li>
</ul>
<p>공개가 가능한 자료들은 공개를 하고, 후기에 대해 간단하게 작성하려고 합니다.</p>
<p> </p>
<hr>
<h1 id="“프로그래머스-자율주행-데브코스”-프로그래머스"><a href="#“프로그래머스-자율주행-데브코스”-프로그래머스" class="headerlink" title="“프로그래머스 자율주행 데브코스” @프로그래머스"></a>“프로그래머스 자율주행 데브코스” @프로그래머스</h1><p>5월 23일부터 6월 3일까지 진행한 강의입니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9zY2hvb2wucHJvZ3JhbW1lcnMuY28ua3IvbGVhcm4vY291cnNlcy8xNDk2Ni8xNDk2Ni00JUVBJUI4JUIwLWstZGlnaXRhbC10cmFpbmluZy0lRUQlOTQlODQlRUIlQTElOUMlRUElQjclQjglRUIlOUUlOTglRUIlQTglQjglRUMlOEElQTQtJUVDJTlFJTkwJUVDJTlDJUE4JUVDJUEzJUJDJUVEJTk2JTg5LSVFQiU4RCVCMCVFQiVCOCU4QyVFQyVCRCU5NCVFQyU4QSVBNA==">프로그래머스 자율주행 데브코스 3기<i class="fa fa-external-link-alt"></i></span>에서 강사로써 Visual SLAM 강의를 진행했습니다. 기본적인 파이썬/C++을 다루시는 학생분들께서 데브코스 초반과정을 거치며 Linux, ROS, OpenCV, PyTorch, 센서퓨전 등을 공부를 하신후 최종 프로젝트 직전에 VSLAM을 공부하게 되십니다. 딥러닝을 공부하시다가 geometry 기법으로 급격하게 내용이 전환되기 때문에, 충분히 VSLAM에 대해서 차근차근 설명하는 것이 필요했습니다. 또, 파이썬을 주로 사용하는 학생분들께서 C++로 언어를 바꾸시게 되다보니, 이에 맞춰서 CMake 및 빌드 시스템도 설명을 해야했습니다. 강의는 22시간 20분 분량의 사전녹화된 강의를 학생분들은 온라인으로 수강하는 시스템이였습니다. 회사의 바쁜 일정과 맞물려서 후반부 강의 퀄리티가 떨어진게 많이 아쉽습니다 ㅠㅠ 4기의 영상은 후반부 퀄리티를 개선시키려고 합니다. 영상 강의를 찍다보니 여러 잔재주가 늘게 되었습니다 ㅎㅎ 노션에 슬라이드마다 스크립트를 미리 작성하고, 녹화할 때는 그냥 읽기만 하는 식으로 해서 실수를 최소화했습니다. 추후 영상 수정이 필요할 수도 있으니, 클립은 슬라이드마다 따놓았고 파이널컷 프로젝트도 백업해두었습니다.</p>
<p>학생분들께서는 <strong>SLAM 기술을 공부하는데에 필요한 사전지식</strong>부터, <strong>기초적인 3D 기하학</strong>을 거쳐 <strong>프론트엔드 영상처리</strong>와 <strong>백엔드 비선형 최적화</strong>를 공부하시게 됩니다. 이후, <strong>PTAM, ORB-SLAM, ProSLAM 논문을 한줄한줄 같이 읽으며 논문을 완벽하게 이해</strong>를 하는 것을 목표로 합니다. 강의가 끝난 후에는 2주간 SLAM 개발 프로젝트를 진행하게 되는데, ‘기존의 오픈소스보다 더 성능이 좋은 SLAM을 만드세요’라는 목표를 가지고 1. 속도, 2. 정확도, 3. 기능확장 중 하나를 랜덤하게 배정받아 팀으로써 개발을 하게 됩니다. 데브코스에 관심이 있으신 분들께서는 제가 나온 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9FeklneVhYQ0ZiYw==">강사 소개 영상<i class="fa fa-external-link-alt"></i></span>도 봐주시면 감사하겠습니다!</p>
<p>자료는 프로그래머스와의 계약 의무로 인해 아쉽게도 공개하기는 어렵습니다. 목차만 공개하는걸로…!</p>
<img src="/20230102-2022-seminar-showdown/devcourse.png" class="" title="데브코스"> 

<p> </p>
<hr>
<h1 id="“Monocular-Visual-SLAM-실내위치탐지”-대전정보문화산업진흥원"><a href="#“Monocular-Visual-SLAM-실내위치탐지”-대전정보문화산업진흥원" class="headerlink" title="“Monocular Visual SLAM 실내위치탐지” @대전정보문화산업진흥원"></a>“Monocular Visual SLAM 실내위치탐지” @대전정보문화산업진흥원</h1><blockquote>
<p>현재 강의 자료를 공유하기는 어렵습니다만, 강의 내용을 기반으로 E-book을 작성해서 공개할 예정입니다!</p>
</blockquote>
<p>3월 17일, 8월 13일, 9월 30일, 12월 10일 진행한 강의입니다.</p>
<p>하루 8시간, 2일에 걸쳐서 진행하는 강의입니다. <strong>VSLAM을 처음 접하시는 중소기업 현업자들을 대상으로 SLAM의 이론</strong>에 대해 강의합니다. 자율주행 데브코스와 이론 부분에서 겹치는 영역이 상당히 많지만 실습/과제/코드튜토리얼이 빠져있으며, 대신 수식부분에 있어서는 조금 더 깊게 들어갑니다. 제 강의는 이론 부분에 집중을 많이 하고, 실습을 원하시는 분들은 신동원님께서 진행하시는 ‘NVIDIA Jetson을 이용한 VSLAM 실습’ 강의를 소개드렸습니다. </p>
<p>강의의 목차는 다음과 같습니다. 현재 강의 자료의 공개는 어렵습니다 ㅠㅠ</p>
<ul>
<li>1일차<ul>
<li>3D Vision의 역사</li>
<li>Localization, Mapping, SLAM</li>
<li>SLAM에서 사용하는 센서와 SLAM의 종류</li>
<li>SLAM 기술이 사용되는 분야</li>
<li>SLAM 개발 가이드</li>
<li>확률과 통계 primer</li>
<li>선형대수 primer</li>
<li>3D 회전과 이동</li>
<li>동차 좌표계</li>
<li>카메라 투영</li>
<li>특징점 검출기의 역사</li>
</ul>
</li>
<li>2일차<ul>
<li>에피폴라 기하학</li>
<li>RANSAC</li>
<li>삼각측량</li>
<li>P3P/PnP</li>
<li>최소자승법</li>
<li>번들 조정</li>
<li>Feature-based SLAM의 역사</li>
<li>Direct SLAM의 역사</li>
<li>Visual-inertial odometry (VIO)의 역사</li>
<li>최신 딥러닝 SLAM과 VSLAM의 미래 </li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="“우리-모두-슬램해”-모두의연구소"><a href="#“우리-모두-슬램해”-모두의연구소" class="headerlink" title="“우리 모두 슬램해” @모두의연구소"></a>“우리 모두 슬램해” @모두의연구소</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMUM3Y2dEWVVGTFpqTHJ2TjF6WHBnWDBhMnhJVjVjeTZPL2VkaXQ/dXNwPXNoYXJlX2xpbmsmb3VpZD0xMTA3MTE5ODkyNTAzOTA2OTI4NzkmcnRwb2Y9dHJ1ZSZzZD10cnVl">자료 링크<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>2월 26일에 진행한 세미나입니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9tb2R1bGFicy5jby5rci8=">모두의연구소<i class="fa fa-external-link-alt"></i></span> ‘Tesla 자율주행 기술 파헤치기’ 풀잎스쿨 그룹에서 SLAM에 대해 발표를 하게 되었습니다. 컴퓨터 비전에 대한 사전지식이 있는 20명의 연구원분들을 대상으로 <strong>1. SLAM이 어떤 기술이고, 2. 자율주행에서 SLAM 기술이 어떻게 사용되는지에 대해 소개</strong>했습니다.  1시간 30분 정도 세미나를 진행을 했는데, SLAM을 처음 접하시는 분들께도 SLAM 기술에 입문하기에 적당한 시간이였던 것 같습니다. 세미나가 끝나고 피드백을 받았는데, 많은 분들이 좋아해주셨습니다. 이 때부터 세미나에 중독(?)된게 아닌가 싶기도 하구요 ㅎㅎ</p>
<p>발표 자료에는 <strong>제가 SLAM 기술을 어떻게 생각하는지</strong>가 담겨있습니다. 주로 proprioceptive sensing과 exteroceptive sensing을 섞어 최적의 odometry/map을 추정하는 부분입니다. 이 때 정립된 개념 및 슬라이드들을 이후에 다른 세미나들에서도 많이 사용합니다. 비슷한 시기에 진행된 세미나 중 풀입스쿨 퍼실이시던 은수님께서 진행하신 <span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj02cUNFUXhTTk1oNA==">테슬라 자율주행 기술에 발가락 담그기<i class="fa fa-external-link-alt"></i></span> 세미나도 있습니다. 같이 보시면 좋을 것 같습니다.</p>
<p>자료는 <a href="https://docs.google.com/presentation/d/1C7cgDYUFLZjLrvN1zXpgX0a2xIV5cy6O/edit?usp=share_link&ouid=110711989250390692879&rtpof=true&sd=true"><strong>여기</strong></a>에서 확인하실 수 있고, 아래와 같은 내용을 커버합니다.</p>
<ul>
<li>컴퓨터 비전의 발전</li>
<li>자율주행 시나리오</li>
<li>SLAM의 사용처</li>
<li>SLAM이란?</li>
<li>Tesla는 SLAM을 사용할까?</li>
<li>LiDAR SLAM</li>
<li>Visual-SLAM</li>
</ul>
<img src="/20230102-2022-seminar-showdown/modu.png" class="" title="모두슬램해"> 

<p> </p>
<hr>
<h1 id="“프로젝트-관리와-협업”-프로그래머스"><a href="#“프로젝트-관리와-협업”-프로그래머스" class="headerlink" title="“프로젝트 관리와 협업” @프로그래머스"></a>“프로젝트 관리와 협업” @프로그래머스</h1><p>5월 1일 진행한 세미나입니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9zY2hvb2wucHJvZ3JhbW1lcnMuY28ua3IvbGVhcm4vY291cnNlcy8xNDk2Ni8xNDk2Ni00JUVBJUI4JUIwLWstZGlnaXRhbC10cmFpbmluZy0lRUQlOTQlODQlRUIlQTElOUMlRUElQjclQjglRUIlOUUlOTglRUIlQTglQjglRUMlOEElQTQtJUVDJTlFJTkwJUVDJTlDJUE4JUVDJUEzJUJDJUVEJTk2JTg5LSVFQiU4RCVCMCVFQiVCOCU4QyVFQyVCRCU5NCVFQyU4QSVBNA==">프로그래머스 자율주행 데브코스 2기<i class="fa fa-external-link-alt"></i></span>에서 진행한 두번째 특강입니다. SLAM이 아닌 개발 관련 발표는 이 발표가 처음이였던 것 같습니다. </p>
<p>발표 자료에는 <strong>제가 개발과정을 어떻게 대하는지에 대한 자세와 생각</strong>이 담겨있습니다. 학부생 때 공부했던 품질공학 방법론들을 개발에 적용하기 위해 한창 고민하고 실험을 하던 시기에 진행한 세미나였기 떄문에, 이상과 현실에 대한 괴리감에 대해서도 설명하고, 이 괴리감을 넘어서기 위한 노력에 대해서도 담겨있습니다.</p>
<p>아래와 같은 내용을 커버했습니다. 발표 자료는 프로그래머스와의 계약 조건으로 인해 공개가 어렵습니다.</p>
<ul>
<li>프로젝트의 조건<ul>
<li>요구사항: 비즈니스적 요구사항, 엔지니어링 요구사항, 누가 고객인가?, 고객의 소리, 겉으로 보이는 문제와 진짜 문제, 규제</li>
<li>기간: 마일스톤 설정 방법, 간트차트, 검수 일정 산출 방법, 일정 변동 가능성, 롱테일 개발의 위험, 기술 부채</li>
<li>인원: 실력과 팀워크, 채용, 인력 가성비</li>
<li>산출물: 솔루션/커스터마이즈 제품, 제품 버전관리 및 유지보수, 제품 유통망, 품질 기준</li>
</ul>
</li>
<li>협업<ul>
<li>혼자 개발할 때 vs 팀으로써 개발할 때</li>
<li>협업의 구조: 팀 리더, PM, 엔지니어, QA</li>
<li>애자일 팀 vs 기능 팀</li>
<li>효율적인 소통이란?: 코칭/세미나, 개발문서, 코드리뷰, 코드, 가이드 문서, 레퍼런스 공유</li>
<li>협업을 위한 툴</li>
</ul>
</li>
<li>효율적인 협업을 위한 업무 자동화<ul>
<li>PR 생성 자동화</li>
<li>CI/CD</li>
<li>TDD</li>
<li>코드 리뷰</li>
<li>C++/Python<ul>
<li>빌드 테스트, 코드 포매팅, 유닛/컴포넌트/인터그레이션 테스트, 성능 벤치마크</li>
<li>코딩 가이드라인</li>
<li>정적/동적 분석</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="“자율주행의-이해와-사례”-금융연수원"><a href="#“자율주행의-이해와-사례”-금융연수원" class="headerlink" title="“자율주행의 이해와 사례” @금융연수원"></a>“자율주행의 이해와 사례” @금융연수원</h1><blockquote>
<p>자료는 현재 공개가 어렵습니다</p>
</blockquote>
<p>9월 27일 진행한 세미나입니다.</p>
<p>금융연수원 세미나는 현직 은행원들을 대상으로 한 세미나입니다. 자율주행/컴퓨터비전/로보틱스에 대한 사전지식이 하나도 없는 분들을 위해 진행한 세미나는 이 때가 처음인 것 같아요. 그래서 수식도 다 빼고, SLAM 이야기도 다 뺐습니다. 세미나 소개를 할 때 ‘여러분들께서 다음에 차를 사실 때 어떤 자율주행 옵션을 사는게 이득인지 이해시켜드리는게 오늘 제 목표입니다!’ 라고 시작할 정도였습니다 ㅋㅋ </p>
<p>4시간 정도 진행한 세미나인데, 2시간정도는 기술에 대한 이야기를 했고, 나머지 2시간은 자율주행의 사회적인 효과에 대해 이야기했습니다. <strong>자율주행 필드에 대해 전체적인 이해도를 높이고 싶으신 분들께 도움</strong>이 될 것 같습니다.</p>
<p>아래와 같은 내용을 커버했습니다. 자료는 현재 공개가 어렵습니다 (추후 공개 시 업데이트 하겠습니다)</p>
<ul>
<li>자율주행 기술<ul>
<li>다음 기술은 자율주행일까요?: 스마트 크루즈 컨트롤, 로보택시, 주차보조시스템</li>
<li>운전 자동화의 단계적 구분 (Levels of autonomy)</li>
<li>모바일 로봇의 구조: 인지, 판단, 제어</li>
<li>기술과 센서</li>
</ul>
</li>
<li>자율주행의 역사<ul>
<li>2000년도 이전의 자율주행</li>
<li>DARPA 챌린지</li>
<li>AI 기술의 발전과 뉴럴 네트워크</li>
</ul>
</li>
<li>자율주행 기술 시장<ul>
<li>시장 분석<ul>
<li>기술별: 풀스택, 위치추정/맵핑, 트럭/버스 스택, 시뮬레이션 및 검증, 자율주행 개발 도구, 칩/보드, 라이다, 카메라/레이더, V2X, 온보드 통신</li>
<li>단계별: 완성차 업체, 부품 공급 업체, 차량 공유 업체, IT 기업, 반도체 제조사, 스타트업</li>
</ul>
</li>
<li>기업 역사 살펴보기: Waymo, Tesla, GM, Intel MobilEye, Ford, Amazon Zoox, NVIDIA, Aurora, Apple, Daimler group, Audi, Bosch, Toyota, Honda, Nissan, Baidu, Pony.ai, TuSimple, 현대자동차</li>
</ul>
</li>
<li>자율주행 기술과 함께 할 우리의 미래<ul>
<li>시장 분석: 자율주행 기술, 전기차, 센서, 반도체, 정밀지도, 도시 개발, 전자제어장치, 인포테인먼트, 모빌리티(MaaS), 보험, 로보택시, 트럭</li>
<li>교통사고와 안전: 교통사고, 면허</li>
<li>도시 인프라: 주차공간, 공공서비스</li>
<li>환경 변화: 에너지, CO2</li>
</ul>
</li>
</ul>
<img src="/20230102-2022-seminar-showdown/finance2.png" class="" title="금융연수원2"> 
<img src="/20230102-2022-seminar-showdown/finance.png" class="" title="금융연수원"> 

<p> </p>
<hr>
<h1 id="“버그없고-성능좋은-C-개발을-위한-가이드”-T모-회사"><a href="#“버그없고-성능좋은-C-개발을-위한-가이드”-T모-회사" class="headerlink" title="“버그없고 성능좋은 C++ 개발을 위한 가이드” @T모 회사"></a>“버그없고 성능좋은 C++ 개발을 위한 가이드” @T모 회사</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xNnBNa3NYcjkwdHVhcXY1Mm00ajZwY01SOFpISmlhT1Ivdmlldz91c3A9c2hhcmluZw==">자료 링크<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>9월 28일 진행한 세미나입니다.</p>
<p>대전의 T모 회사의 연구원님께서 VSLAM 강의 중 ‘효율적인 SLAM 개발’에 대해 좀 더 심도깊게 공부하고 싶으시다고 세미나 요청을 해주셨습니다. <strong>C++ 언어를 사용하며 로보틱스 프로그램 개발을 할 때 어떻게 하면 좀 더 효율적으로 개발을 할 수 있을지</strong>에 대해 제가 알고 있는 노하우를 담았습니다.</p>
<p>자료는 <a href="https://drive.google.com/file/d/16pMksXr90tuaqv52m4j6pcMR8ZHJiaOR/view?usp=sharing"><strong>여기</strong></a>에서 확인하실 수 있고, 아래와 같은 내용을 커버했습니다. </p>
<ul>
<li>로봇 개발팀의 숙명<ul>
<li>더 많은 고객을 유치하기 위해 제품 커스터마이제이션을 해야한다.</li>
<li>개발 역량을 높이는 방법</li>
<li>업무 자동화<ul>
<li>1달 안에 시스템 도입하기</li>
</ul>
</li>
</ul>
</li>
<li>코딩 가이드라인<ul>
<li>코딩 가이드라인</li>
<li>정적 분석</li>
</ul>
</li>
<li>테스트 주도 개발 (TDD)<ul>
<li>테스트란?</li>
<li>테스트의 구조</li>
<li>정석 TDD와 레거시 코드 TDD</li>
<li>TDD는 꼭 해야하는가?</li>
<li>TDD 데모</li>
</ul>
</li>
<li>CI/CD<ul>
<li>CI/CD란?</li>
<li>GitHub Actions 데모</li>
</ul>
</li>
</ul>
<img src="/20230102-2022-seminar-showdown/t.png" class="" title="t1"> 
<img src="/20230102-2022-seminar-showdown/t2.png" class="" title="t2"> 

<p> </p>
<hr>
<h1 id="“AI-SLAM”-모두의연구소"><a href="#“AI-SLAM”-모두의연구소" class="headerlink" title="“AI+SLAM” @모두의연구소"></a>“AI+SLAM” @모두의연구소</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMTBfLUJTWXhBRHgxeUlEcGQzNWxyZTUwZUtZbGk0UUYtL2VkaXQ/dXNwPXNoYXJpbmcmb3VpZD0xMTA3MTE5ODkyNTAzOTA2OTI4NzkmcnRwb2Y9dHJ1ZSZzZD10cnVl">자료 링크<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>10월 18일 진행한 세미나입니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9mZXN0YS5pby9ldmVudHMvMjc0OQ==">모두의연구소 MODUPOP<i class="fa fa-external-link-alt"></i></span> 행사에서 발표를 했습니다. <strong>초창기 SLAM 부터 최신 Deep SLAM까지의 기술 변화</strong>를 알아보는데에 초점을 뒀습니다. 1시간 반 동안 약 150명의 인원이 동시에 시청해주셨고, 좋은 피드백을 받아서 굉장히 뿌듯했습니다.</p>
<p>지금까지의 세미나는 대부분 정보전달의 목적이 많았는데, 이번 세미나는 특이하게 제 생각을 녹여낸 부분이 많습니다. 연구를 하고싶다는 욕망이 생기다보니 그랬던 것 같습니다 ㅎㅎ</p>
<p>자료는 <a href="https://docs.google.com/presentation/d/10_-BSYxADx1yIDpd35lre50eKYli4QF-/edit?usp=sharing&ouid=110711989250390692879&rtpof=true&sd=true"><strong>여기</strong></a>에서 확인하실 수 있고, 아래와 같은 내용을 커버했습니다.</p>
<ul>
<li>SLAM이란?<ul>
<li>Localization &amp; Mapping</li>
<li>Proprioceptive sensing, Exteroceptive sensing</li>
</ul>
</li>
<li>SLAM의 역사<ul>
<li>LiDAR SLAM<ul>
<li>EKF-SLAM, FastSLAM, GMapping, HectorSLAM, Cartographer</li>
<li>LOAM, ALOAM, HDL-SLAM, FAST-LIO   </li>
</ul>
</li>
<li>Visual SLAM<ul>
<li>Feature-based SLAM<ul>
<li>MonoSLAM, PTAM, ORB-SLAM, ORB-SLAM2, VINS-Mono, ORB-SLAM3</li>
</ul>
</li>
<li>Direct SLAM<ul>
<li>DTAM, LSD-SLAM, SVO, DSO</li>
</ul>
</li>
<li>RGB-D SLAM<ul>
<li>KinectFusion, RTAB-Map, ElasticFusion, BundleFusion</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>딥러닝 + SLAM<ul>
<li>딥러닝 기반 모듈의 등장: SuperPoint/SuperGlue, NetVLAD, Depth estimation, Depth completion, Object detection, Semantic Segmentation</li>
<li>딥러닝 기반 SLAM: UnDeepVO, TartanVO</li>
<li>딥러닝 프론트엔드: D3VO</li>
<li>Semantic Understanding: CNN-SLAM, SemanticFusion, CubeSLAM, Fusion++</li>
<li>Deep Factor optimization: code, NeRF</li>
</ul>
</li>
</ul>
<img src="/20230102-2022-seminar-showdown/ai_slam.png" class="" title="AI_SLAM"> 

<img src="/20230102-2022-seminar-showdown/ai_slam2.png" class="" title="AI_SLAM2"> 

<p> </p>
<hr>
<h1 id="“Spatial-AI는-미래입니다”-모두의연구소"><a href="#“Spatial-AI는-미래입니다”-모두의연구소" class="headerlink" title="“Spatial AI는 미래입니다” @모두의연구소"></a>“Spatial AI는 미래입니다” @모두의연구소</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xN2VKdThrN1ZmSnIxNjdmX3FQQ0lZT2I0eUdtOFY3SkQvdmlldw==">자료 링크<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>12월 15일 진행한 세미나입니다.</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9tb2R1Y29uLmtyLw==">모두콘 2022<i class="fa fa-external-link-alt"></i></span> 행사에 초청받아 AI+Research 트랙에서 첫번째 발표로 <strong>Spatial AI 기술을 소개</strong> 하게 되었습니다. SLAM 기술에 다양한 딥러닝 기술을 섞어 한단계 높은 공간지능을 만들자는 취지의 다양한 연구를 소개했습니다. 이 발표에 대해서는 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjIxMjE1LW1vZHVjb24yMDIyLw==">좀 더 자세하게 설명한 글<i class="fa fa-external-link-alt"></i></span>이 있습니다. </p>
<p>공간 컴퓨팅이 필요한 곳에서는 무조건 spatial AI를 사용하게 될 겁니다. <strong>완전한 spatial AI를 구현하기 위해서는 다양한 분야의 딥러닝과 SLAM이 필수적으로 융합</strong>되야하기 때문에, <strong>미래를 생각하며 다양한 분야를 공부하는 것이 중요하다</strong>는 메세지를 담았습니다.</p>
<p>자료는 <a href="https://drive.google.com/file/d/17eJu8k7VfJr167f_qPCIYOb4yGm8V7JD/view"><strong>여기</strong></a>에서 확인하실 수 있고, 아래와 같은 내용을 커버합니다.</p>
<ul>
<li>Spatial AI란?<ul>
<li>Spatial AI의 단계</li>
<li>사용될 수 있는 기술: 2D/3D Vision, Graphics, Language, Reinforcement learning, SLAM</li>
</ul>
</li>
<li>3D Dynamic Scene graph: 단계적 구조를 가진 3D map</li>
<li>LM-Nav: Vision-Language 모델을 이용한 항법 모델</li>
<li>Do as I can, not as I say: Vision-Language 모델을 이용한 태스크 플래닝</li>
<li>Real-time mapping of physical scene properties…: 물리적 상태 맵핑</li>
</ul>
<img src="/20230102-2022-seminar-showdown/spatial_ai.png" class="" title="spatial_ai"> 

<p> </p>
<hr>
<h1 id="“농업분야에서-GPS를-사용할-수-없는-경우에-위치-추정-방안”-서울대학교"><a href="#“농업분야에서-GPS를-사용할-수-없는-경우에-위치-추정-방안”-서울대학교" class="headerlink" title="“농업분야에서 GPS를 사용할 수 없는 경우에 위치 추정 방안” @서울대학교"></a>“농업분야에서 GPS를 사용할 수 없는 경우에 위치 추정 방안” @서울대학교</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMTdvR0lDLW10blZ3MkFQaUpDWlVzY1luQTBHNUV6LXphL2VkaXQ/dXNwPXNoYXJpbmcmb3VpZD0xMTA3MTE5ODkyNTAzOTA2OTI4NzkmcnRwb2Y9dHJ1ZSZzZD10cnVl">자료 링크<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>12월 19일 진행한 세미나입니다.</p>
<p>대전에서 진행했던 VSLAM 강의를 수강하신 학생분께서 요청하신 세미나입니다. 서울대학교 농업생명과학대학에서 세미나를 진행했는데, <strong>트랙터 자율주행을 실현시키기 위한 위치 추정 방안</strong>에 대해 고민하고 계신 것 같았다. 자료를 준비하면서 농업쪽의 특성도 알게 되었고, GPS에 대해서 좀 더 공부할 수 있게 되어서 아주 유익했습니다. 2022년에 들면서 경제가 나빠지고 많은 기업들이 인건비 삭감을 위해 자동화를 꿰하고 있는데, 자율주행 트랙터는 기술이 시장에 큰 임팩트를 줄 수 있는 곳이기 때문에 생각치도 못하게 이 분야에 조금 끌리게 된 것 같습니다.</p>
<p>자료는 <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20vcHJlc2VudGF0aW9uL2QvMTdvR0lDLW10blZ3MkFQaUpDWlVzY1luQTBHNUV6LXphL2VkaXQ/dXNwPXNoYXJpbmcmb3VpZD0xMTA3MTE5ODkyNTAzOTA2OTI4NzkmcnRwb2Y9dHJ1ZSZzZD10cnVl">여기<i class="fa fa-external-link-alt"></i></span>에서 확인하실 수 있고, 아래와 같은 내용을 커버합니다.</p>
<ul>
<li>GNSS란?<ul>
<li>SPS, DGPS, RTK-GPS의 원리</li>
</ul>
</li>
<li>Localization 방법<ul>
<li>Simple odometry: 모션센서 기반의 누적형 오도메트리</li>
<li>Localization in known environment: 마커 기반 위치 트랙킹, 자연상태 위치 트랙킹</li>
<li>Localization in unknown environment: SLAM</li>
</ul>
</li>
<li>센서 &amp; 수학적 모델링<ul>
<li>카메라, 라이다, 휠 인코더, IMU</li>
<li>Motion model, Observation model</li>
</ul>
</li>
<li>농업환경의 특징<ul>
<li>위치 추정 &amp; 지도 작성에서 생기는 어려움</li>
<li>Drift</li>
</ul>
</li>
<li>지도 작성의 방법<ul>
<li>SLAM: Visual SLAM, LiDAR SLAM<ul>
<li>각각의 예시</li>
</ul>
</li>
</ul>
</li>
<li>위치 추정의 방법<ul>
<li>Map-based localization</li>
</ul>
</li>
</ul>
<img src="/20230102-2022-seminar-showdown/agri.png" class="" title="agricultural"> 

<p> </p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>3. Life</category>
        <category>3.2 커리어</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Spatial AI</tag>
      </tags>
  </entry>
  <entry>
    <title>2022년 회고</title>
    <url>/20230115-2022-retro/</url>
    <content><![CDATA[<h1 id="회고-방식"><a href="#회고-방식" class="headerlink" title="회고 방식"></a>회고 방식</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWFyaW1iYS50ZWFtL2tyL2Jsb2cvdG9wLXJldHJvc3BlY3RpdmUtdGVtcGxhdGVzLw==">4L: Liked, Learned, Lacked, Longed for<i class="fa fa-external-link-alt"></i></span> 방법으로 간단하게 정리합니다.</p>
<p> </p>
<hr>
<h1 id="Liked"><a href="#Liked" class="headerlink" title="Liked"></a>Liked</h1><h2 id="회사"><a href="#회사" class="headerlink" title="회사"></a>회사</h2><ul>
<li>VSLAM에 진심인 사람들을 초대해 팀이 완성되었다.</li>
<li>멋진 팀원들로부터 많은 것들을 배웠다. - 기술, 지식, 마인드셋, 태도.<ul>
<li>종종 팀원과 1대1 미팅을 통해 솔직한 피드백을 주고받았는데, 내 장/단점, 팀원으로써 걱정되는 점과 기대하는 점을 파악하는데에 아주 큰 도움이 되었다.</li>
</ul>
</li>
<li>다 같이 한 마음으로 멋진걸 시도해보았다.<ul>
<li>결과를 봤을 때 그 희열!</li>
</ul>
</li>
</ul>
<h2 id="인연과-운"><a href="#인연과-운" class="headerlink" title="인연과 운"></a>인연과 운</h2><ul>
<li>새로운 분야의 사람들과 만나며 교류를 했다 - 이해의 폭이 넓어진 것 같다.<ul>
<li>Frontend 개발자, Blockchain 개발자, 교사, 바리스타, 스타트업 액셀레레이터…</li>
</ul>
</li>
<li>새로운 분야를 체험하며 어깨 넘어로 많은 것을 배웠다.<ul>
<li>Blockchain &amp; NFT 분야는 재밌었다. 루나코인이 나락가기 전 까지.</li>
</ul>
</li>
<li>SLAM이 아닌 다른 분야에 관심을 가지며 간단한 BM을 만들어보는 경험을 했다.</li>
<li>자율주행 부트캠프를 진행하며 많은 학생분들을 만났다.<ul>
<li>학생분들과 좋은 인연을 가졌고, 버넥트를 포함한 다양한 기업에 취직을 성공하셨다.</li>
</ul>
</li>
</ul>
<h2 id="노력"><a href="#노력" class="headerlink" title="노력"></a>노력</h2><ul>
<li>열심히 돈을 벌어봤다.<ul>
<li>11건의 출강 + 1건의 부트캠프 진행을 했다.</li>
<li>급하게 필요했던 돈을 채웠다 + 조금 마음의 여유가 생겼다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="Learned"><a href="#Learned" class="headerlink" title="Learned"></a>Learned</h1><h2 id="노력과-방향"><a href="#노력과-방향" class="headerlink" title="노력과 방향"></a>노력과 방향</h2><ul>
<li>열심히만 하는건 의미가 없다고 느꼈다. 중요한건 시스템.<ul>
<li>출강을 통해 돈을 벌었지만, 가성비는 오히려 전공공부를 하는 것에 비해 떨어진다는 결론을 내렸다.<ul>
<li>내년에 같은 액수를 벌고싶다면<ul>
<li>출강: 작년에 썼던 시간만큼 동일한 시간을 투자해야한다</li>
<li>전공공부: 당장 눈 앞에 떨어지는건 적을지라도, 쌓이고 쌓이면 커리어 동안 토탈 액수가 비교도 안될만큼 더 많다. 시간은 내 편이다.</li>
</ul>
</li>
</ul>
</li>
<li>가능하면 모든 수익 및 이익은 복리가 쌓이는 시스템으로 만들어야한다.</li>
</ul>
</li>
<li>공부도 똑같이 열심히만 하는건 의미가 없다.<ul>
<li>현재 가장 임팩트를 줄 수 있는 것들 위주로, 복리를 굴릴 수 있는 방향으로 공부해야한다.</li>
</ul>
</li>
</ul>
<h2 id="책을-읽어야겠다-삶을-살아가는데에는-지혜가-필요하다"><a href="#책을-읽어야겠다-삶을-살아가는데에는-지혜가-필요하다" class="headerlink" title="책을 읽어야겠다 - 삶을 살아가는데에는 지혜가 필요하다"></a>책을 읽어야겠다 - 삶을 살아가는데에는 지혜가 필요하다</h2><ul>
<li>살아가는데에 있어서 ‘아 내가 이 문제를 해결할 방법을 알고있었다면’ 이라는 생각이 자주 들었다.<ul>
<li>사람 관계라던지, 빠르게 돈을 굴리는 방법이라던지, 우리 사회를 살아가는 방법이라던지…</li>
<li>왠만한 답 - 정답은 아니더라도 내 naive 한 답 보다는 정답에 더 유사한 답 - 은 책들에 많이 나와있다.</li>
</ul>
</li>
</ul>
<h2 id="모든건-마음-먹기-나름이다"><a href="#모든건-마음-먹기-나름이다" class="headerlink" title="모든건 마음 먹기 나름이다"></a>모든건 마음 먹기 나름이다</h2><ul>
<li>지난번엔 힘들다고 느꼈던것도, 이번에는 쉽다고 생각하면 쉽게 풀린다.<ul>
<li>반대로 지난번에 쉬웠다고 느꼈던것도, 이번에는 힘들다고 생각하면 몇배로 힘들다.</li>
</ul>
</li>
<li>나는 주니어다.<ul>
<li>지금의 포지션에서 신입들을 바라보면, 내가 줄 수 있는 가치가 엄청 커 보인다.</li>
<li>지금의 포지션에서 시니어를 바라보면, 나는 별로 도움이 되지 않는 사람이다.</li>
<li>신입들에게 가치를 나눠주는건 보람찬 일이고 행복하다. 시니어가 되기 위해서 공부를 하고 어려움을 헤쳐나가는건 고통스럽다.</li>
<li>하지만 좋아보이고 쉬운 길만 택한다면 난 여기까지다.</li>
</ul>
</li>
<li>돈은 생각에 따라 쉽게 모이기도, 어렵게 모이기도 한다.</li>
<li>작은 성공이 모이면 뭐든지 쉽게 보인다. <ul>
<li>세상에서 가장 쉬운 성공은 헬스장에 가는 것이다.</li>
</ul>
</li>
<li>생각보다 나는 하나의 생각에 쉽게 휩쓸리는 사람이였다.<ul>
<li>내가 제일 조심해야하는건, 생각에 휩쓸리지 않도록 스트레스를 피해야하는 것이다.</li>
<li>스트레스는 내가 컨트롤을 할 수 없는 것 에서 온다.</li>
</ul>
</li>
</ul>
<p> </p>
<hr>
<h1 id="Lacked"><a href="#Lacked" class="headerlink" title="Lacked"></a>Lacked</h1><h2 id="지혜가-부족했다"><a href="#지혜가-부족했다" class="headerlink" title="지혜가 부족했다"></a>지혜가 부족했다</h2><ul>
<li>지혜가 있었다면, 처음부터 출강이 아니라 녹화 강의를 만들고 책을 썼을 것이다.<ul>
<li>많은 시간과 더 큰 돈을 벌 수 있는 기회를 날렸다.</li>
</ul>
</li>
<li>지혜가 있었다면, 내 주식의 시드가 더 남아있었을 것이다.<ul>
<li>더 큰 돈을 벌 수 있는 기회를 날렸다.</li>
</ul>
</li>
<li>지혜가 있었다면, 내게 해로운 사람을 더 빨리 손절했을 것이다.<ul>
<li>많은 시간, 멘탈 에너지, 돈을 날렸다.</li>
</ul>
</li>
<li>지혜가 있었다면, 더 일찍 헬스장을 끊고, 집 정리를 하고, 책을 읽었을 것이다.<ul>
<li>이걸 하지 않아 인생에서 얻을 수 있는 물리적 에너지, 멘탈 에너지, 기회의 총량을 그대로 날렸다.</li>
</ul>
</li>
</ul>
<h2 id="커리어-성장이-부족했다"><a href="#커리어-성장이-부족했다" class="headerlink" title="커리어 성장이 부족했다"></a>커리어 성장이 부족했다</h2><ul>
<li>딥러닝과 SIMD는 올해 내가 하고 싶던 것이였지만, 하나도 시작하지 못했다.<ul>
<li>상반기에는 blockchain에 빠져서, 하반기에는 집 이사 때문에, 라고 핑계를 대지만</li>
<li>시간과 기회는 많았고</li>
<li>하나의 목표에 집중하지 않고 여러가지 찍먹을 하느라 하나도 제대로 달성하지 못했다.</li>
</ul>
</li>
<li>결국 C++을 제외한 모든 기술은 작년과 별 다름이 없다.</li>
</ul>
<p> </p>
<hr>
<h1 id="Longed-for"><a href="#Longed-for" class="headerlink" title="Longed for"></a>Longed for</h1><h2 id="축적과-성장"><a href="#축적과-성장" class="headerlink" title="축적과 성장"></a>축적과 성장</h2><ul>
<li>건강, 관계, 커리어, 돈, 지혜의 축적을 시작하고, 절대 게을리하지 않는다.<ul>
<li>특히, 커리어에 대한 부분에서는 성장을 할 때가 되었다.</li>
</ul>
</li>
</ul>
<h2 id="계획"><a href="#계획" class="headerlink" title="계획"></a>계획</h2><ul>
<li>성장을 할 때는 계획적으로 해야한다.<ul>
<li>운동은 루틴에 맞춰서 한다. 제 때 운동하고, 잘 먹고 잘 자자.</li>
<li>관계에 있어서는 특정 사인이 보일 때 바로 꺼내쓸 수 있는 행동들을 정하자. 행동들은 내 가치관에 맞춘 행동이여야한다.</li>
<li>공부는 1주일 단위로 목표를 정하고 그것만 한다.</li>
<li>돈은 투자일지를 적자. Why 매수/매도. 그린라이트/레드라이트. 차트 카피 및 심리 기록.</li>
<li>책은 가리지 말고 계속 읽자. 한 책을 고르면 다 읽거나 완전히 질릴 때 까지 다 읽는다. 북다트 애용하자.</li>
</ul>
</li>
</ul>
<h2 id="거절할건-거절하자"><a href="#거절할건-거절하자" class="headerlink" title="거절할건 거절하자"></a>거절할건 거절하자</h2><ul>
<li>일을 받는건 계란으로 탑을 쌓는것과 같다.<ul>
<li>겁나 많이 쌓으면 impressive하다.</li>
<li>근데 무너지면 아무 의미없다. </li>
<li>2023년은 일을 단 하나만 하더라도 겁나 잘 끝내는게 중요하다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>회고</tag>
        <tag>Retro</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 파티션 설정하기</title>
    <url>/20230213-ubuntu-partition/</url>
    <content><![CDATA[<h1 id="설치-방법"><a href="#설치-방법" class="headerlink" title="설치 방법"></a>설치 방법</h1><p>설치를 할 때는 항상 ‘Something else’ 옵션으로 들어가서 내가 원하는 디스크 할당을 한다.</p>
<p>내가 사용하고 싶은 디스크 공간을 우선 확보한다. 약 1TB가 확보되었다고 해보자.</p>
<h2 id="swap-메모리"><a href="#swap-메모리" class="headerlink" title="swap 메모리"></a>swap 메모리</h2><p>가장 먼저 설정해야할 것은 swap 메모리이다. swap 메모리는 DRAM 크기의 2-3배를 잡는다. 내 DRAM이 32GB라면, swap 메모리는 64GB 정도를 잡으면 적당하다. swap 메모리를 생성할 때는 Primary 파티션으로 가장 앞단에 생성한다. </p>
<p>swap 메모리는 시스템이 연산중인 정보들이 DRAM 용량의 한계를 넘어설 때 SSD에 잠시 저장을 하는 기능이다. 의외로 C++ 프로그램 병렬빌드를 할 때 메모리에 올려야할 정보량이 DRAM의 크기를 넘어서는 경우가 많은데, 이 때 넘쳐나는 정보를 swap 메모리에 엄청나게 빠른 주기로 접근하며 읽고 쓰게 된다. DRAM에 읽고 쓰는 속도보다 SSD에 읽고 쓰는 속도가 훨씬 느리기 때문에, 최대한 SSD에서 정보를 빨리 읽고 쓰게 해줘야 전체적인 성능이 올라가게 된다. 이러한 이유로 swap 메모리는 모든 메모리의 최앞단에 있어야하는데, 이는 메모리가 뒷단에 위치할 수록 접근 시 속도가 느려지기 때문이다.</p>
<h2 id="EFI-파티션"><a href="#EFI-파티션" class="headerlink" title="EFI 파티션"></a>EFI 파티션</h2><p>다음은 EFI 파티션을 설정한다. 500MB 정도를 주고 설정한다. 부팅할 때 필요한 것이다.</p>
<h2 id="root-파티션"><a href="#root-파티션" class="headerlink" title="root 파티션"></a>root 파티션</h2><p>나머지 메모리는 전부 root 파티션에 할당한다. 이게 진짜 내가 쓸 수 있는 메모리이다.</p>
<p> </p>
<hr>
<h2 id="root-파티션과-home-파티션은-나누는거라고-들었는데"><a href="#root-파티션과-home-파티션은-나누는거라고-들었는데" class="headerlink" title="root 파티션과 home 파티션은 나누는거라고 들었는데?"></a>root 파티션과 home 파티션은 나누는거라고 들었는데?</h2><p>안 나눠도 된다.</p>
<p>두개를 나눠놓았을 때의 장점은 ‘또 다시 밀 때 OS/시스템만 밀 수 있다’ 이다. 예를 들어, Ubuntu 20.04를 사용하다가 Ubuntu 22.04로 업그레이드 하고 싶다면, root 파티션만 지우고 home 파티션을 그대로 놔둘 수 있다. 그러면 home 파티션에 있는 사용자의 파일은 그대로 보존되고, OS가 있는 root만 업그레이드 된다.</p>
<p>근데 우리는 C++을 다룬다. OS가 바뀌게 되면, 아랫단에 있는 라이브러리의 위치 및 버전이 전부 바뀌게 된다. 잘 도는 경우도 있고, 안도는 경우도 있다. 근데 문제는 뭐가 돌고 뭐가 안돌지 모른다. 결국 다시 다 빌드 해봐야하는거다.</p>
<p>root 파티션만 지정했을 때 장점은 다음과 같다. 1. ‘root / home 파티션에 뭐를 어디에 설치할지 고민할 필요 없이 그냥 다 편하게 깔고, 바로 작업을 시작할 수 있다’, 2. ‘한번에 다 밀고 새로 깔기 편하다’.</p>
<p>두개를 나눠놓았을 때의 단점은 다음과 같다. 1. ‘OS 업그레이드를 할 때 USB를 꽂고 파티션을 직접 지정해서 재-마운트 시킬 수 있을 정도의 리눅스 고수여야한다’, 2. ‘root는 몇GB, 홈은 몇GB를 줘야할지 적당한 기준이 없다’, 3. ‘뭐 하나 꼬이면 결국 다 밀고 다 새로 깔아야한다’.</p>
<p>어차피 OS를 민다는건 태초마을에서 새 출발 하는게 아닌가? 그냥 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NoYW5naDk1P3RhYj1yZXBvc2l0b3JpZXM=">새 환경 빌드 스크립트<i class="fa fa-external-link-alt"></i></span> 같은거 만들어놓고, 돌리는 동안 그날 일을 뭐할지 플래닝하거나 논문 읽고 있는게 편할 수도 있겠다.</p>
<p>용량을 잘 관리하고 싶다면 root/home을 나누는것도 효율적인 방법이다. 실제로 서버 관리를 하는 사람이거나 임베디드 플릿 관리를 하는 사람이라면 이렇게 해야할 것이다. 하지만 단순히 데스크탑 워크스테이션에서 실험돌리다가 우분투 뻑나면 밀고 새로 깔면 되는 사람이라면, 이런걸 고민할 필요가 없다. ‘SSD 용량이 작아서 어쩔 수 없이 나눠야한다’ 같은 상황이라면, 조금 막말처럼 보일 수 있겠지만 이런 문제를 고민하기보다는 빠르게 10만원 더 들여서 1TB 더 큰 용량의 SSD를 구입하고 더 중요한 문제를 푸는게 이득일 것 같다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.8 Ubuntu Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM 추정 효율성에 대한 고찰 - Rosen 2021 - Advances in Inference and Representation for Simultaneous Localization and Mapping</title>
    <url>/20230214-rosen-2021/</url>
    <content><![CDATA[<h1 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h1><p>1저자인 <span class="exturl" data-url="aHR0cHM6Ly9kYXZpZC1tLXJvc2VuLmdpdGh1Yi5pby8=">David Rosen<i class="fa fa-external-link-alt"></i></span>은 Northeastern 대학의 조교수이며, John Leonard 교수님, Frank Dellaert 교수님, Luca Carlone 교수님과 함께 다수의 논문을 집필하셨다. SLAM 문제에 대해 수학적으로 파고드는 연구가 많으며, 주로 Certifiable algorithm 연구를 진행하셨다. 대표적인 작업물로는 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjEwNjA3LXNvbHZlci1jb21wLz9oaWdobGlnaHQ9c2Urc3luYw==">Juric 2021 논문리뷰<i class="fa fa-external-link-alt"></i></span>에도 나왔던 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RhdmlkLW0tcm9zZW4vU0UtU3luYw==">SE-Sync<i class="fa fa-external-link-alt"></i></span> 라이브러리이다.</p>
<p>2저자 중 한명인 <span class="exturl" data-url="aHR0cHM6Ly9wZW9wbGUuY3NhaWwubWl0LmVkdS9rZG9oZXJ0eS8=">Kevin Doherty<i class="fa fa-external-link-alt"></i></span>는 Semantic SLAM 분야에서 data association 관련 연구를 많이 진행하는 연구자이다.</p>
<p>이 논문은 2가지 파트로 나눌 수 있는데, 저자들이 각각 한 파트씩 적은 것 같다.</p>
<ol>
<li>효율적인 SLAM의 representation과 inference 방법 (아마 Rosen이 적으신듯?)</li>
<li>Semantic SLAM과 learned representation (아마 Doherty가 적으신듯?)</li>
</ol>
<p>이번 글에서는 파트 1인 ‘효율적인 SLAM의 representation과 inference 방법’에 대해 적으려고 한다.</p>
<p>파트 2는 다음 글에서 적으려고 한다.</p>
<p> </p>
<hr>
<h1 id="기본적인-SLAM-문제의-구성"><a href="#기본적인-SLAM-문제의-구성" class="headerlink" title="기본적인 SLAM 문제의 구성"></a>기본적인 SLAM 문제의 구성</h1><p>Thrun의 <span class="exturl" data-url="aHR0cHM6Ly93d3cuYWxhZGluLmNvLmtyL3Nob3Avd3Byb2R1Y3QuYXNweD9JdGVtSWQ9MjQyMDg1MDY5">확률론적 로보틱스 책<i class="fa fa-external-link-alt"></i></span>, Stachniss의 <span class="exturl" data-url="aHR0cHM6Ly93d3cuc3ByaW5nZXJwcm9mZXNzaW9uYWwuZGUvZW4vc2ltdWx0YW5lb3VzLWxvY2FsaXphdGlvbi1hbmQtbWFwcGluZy8xMDUzOTE3Ng==">Handbook of robotics 책 중 SLAM 챕터<i class="fa fa-external-link-alt"></i></span>, Dellaert &amp; Kaess의 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3MuY211LmVkdS9+a2Flc3MvcHViL0RlbGxhZXJ0MTdmbnQucGRm">Factor graphs for robot perception 논문<i class="fa fa-external-link-alt"></i></span>이나 모두 입을 모아 이야기하고 있다.</p>
<blockquote>
<p>“<strong>SLAM은 local observation을 모아 global model을 만드는 것이다</strong>“.</p>
</blockquote>
<p>여기서 <strong>local observation은 특정 시점의 센서 데이터</strong> (e.g. 이미지, 라이다 스캔)을 의미하고, <strong>global model은 3D scene geometry와 trajectory 정보</strong>를 의미하는 것이다.</p>
<p>Local observation을 의미하는 Y 데이터와 global model X가 있을 때, Joint likelihood인 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.181ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3174 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(1655, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1933, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2785, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>을 작은 conditional likelihood로 표현하면 아래와 같은 식이 된다. 결국 작은 likelihood를 모아 확률론적으로 가장 정확한 global model을 찾는다는 말이 된다.</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.864ex" xmlns="http://www.w3.org/2000/svg" width="26.282ex" height="6.399ex" role="img" focusable="false" viewBox="0 -1562.5 11616.5 2828.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(892, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(268, 199)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1937.8, 0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2493.6, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(3345.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4012.3, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(5068.1, 0)"><g data-mml-node="mo"><path data-c="220F" d="M220 812Q220 813 218 819T214 829T208 840T199 853T185 866T166 878T140 887T107 893T66 896H56V950H1221V896H1211Q1080 896 1058 812V-311Q1076 -396 1211 -396H1221V-450H725V-396H735Q864 -396 888 -314Q889 -312 889 -311V896H388V292L389 -311Q405 -396 542 -396H552V-450H56V-396H66Q195 -396 219 -314Q220 -312 220 -311V812Z"></path></g><g data-mml-node="TeXAtom" transform="translate(3, -1107.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mi" transform="translate(328.6, 1150) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(6512.8, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mrow" transform="translate(7434.2, 0)"><g data-mml-node="mo"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g><g data-mml-node="msub" transform="translate(458, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(268, 199)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mi" transform="translate(768, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(1922.2, 0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2478, 0)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mi" transform="translate(828, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(3724.4, 0)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g></g></g></svg></mjx-container></p>
<p>그리고 SLAM 문제를 잘 표현하기 위해 우리는 probabilistic graphical model을 사용한다. 사실 이렇게 길게 잘 표현하지는 않고, <strong>factor graph</strong>라고 부르는 경우가 더 많다.</p>
<p>Factor graph를 통해 우리는 수많은 state 사이의 conditional dependency 및 joint distribution을 표현할 수 있다. </p>
<img src="/20230214-rosen-2021/factor_graph.png" class="" title="factor_graph">

<p>이렇게 joint distribution을 찾아낸 후, joint optimization을 통해 최적의 global model을 찾는게 SLAM이라고 볼 수 있다. Joint optimization을 위해 iSAM, GTSAM, g2o, Ceres-solver와 같은 비선형 최적화 라이브러리를 사용한다.</p>
<p>그러면 이제 질문을 해보자. </p>
<blockquote>
<p>“가장 효율적인 SLAM은 무엇일까?”</p>
</blockquote>
<p>가장 효율적인 SLAM을 하기 위해서는 2가지 고려할 점이 생긴다. </p>
<p>첫째는 <strong>Representation</strong> - ‘<strong>시스템 모델 X는 어떤 정보를 담고 있어야하는가? 이 정보는 인풋 데이터 Y와 어떤 관계를 가지고 있는가?</strong>“</p>
<p>둘째는 <strong>Inference</strong> - ‘<strong>X와 Y 사이의 Joint probability distribution을 풀기 위해 어떤 방법을 사용해야하는가?</strong>“ 이다.</p>
<p> </p>
<hr>
<h1 id="SLAM이-풀기-어려운-이유와-Nonconvexity에-대한-문제"><a href="#SLAM이-풀기-어려운-이유와-Nonconvexity에-대한-문제" class="headerlink" title="SLAM이 풀기 어려운 이유와 Nonconvexity에 대한 문제"></a>SLAM이 풀기 어려운 이유와 Nonconvexity에 대한 문제</h1><p>SLAM 문제에 대한 매니폴드를 그려보면, 굉장히 큰 고차원의 non-convex한 모양을 가질 것이다. 이 non-convex한 state space를 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container> 라고 해보자. 이런 매니폴드에서 global minimum (i.e. 에러가 가장 낮은, 가장 정답에 가까운 파라미터들의 조합)을 찾기란 굉장히 어렵다.</p>
<p>초창기 SLAM에서는 <strong>간단하지만 추적이 가능한 인퍼런스</strong> 방법을 사용해서 문제를 풀었다. 예를 들어, Extended Kalman Filter (EKF)나 Monte Carlo sampling 방법 같은 방법으로 말이다. </p>
<p>하지만 2016년 발표된 <a href="https://arxiv.org/abs/1606.05830"><strong>Past, Present and future of SLAM</strong></a>에서 새로운 SLAM의 기준을 **maximum likelihood estimation (MLE)**로 푸는 방법을 제안하면서 판도가 바뀌게 되었다. MLE 방법은 <em>이론적으로</em> 최적의 값을 찾을 수 있다. 이 때문에 SLAM 문제의 성향은 사실 Maximum-a-posteriori (MAP)문제에 가깝지만, 여러 테크닉을 사용해서 MLE 문제로 치환해 최적의 값을 찾는 문제로 방향을 바꾼다. 또, SLAM 문제의 특징 중 하나인 sparsity를 이용한다. Sparsity는 간단하게 설명하면 ‘state들끼리 연결되지 않은 경우가 많다’ 라고 할 수 있는데, 예시를 들면 SLAM을 수행하면서 이동하다보면 SLAM을 막 실행한 첫 시점에 보였던 랜드마크가 더 이상 보이지 않아 현재의 위치와 보이지 않는 랜드마크 간에 conditional probability를 연결할 수 없는 경우가 있다. Sparsity를 이용한 1차 또는 2차 미분을 이용해서 최적화를 수행하면, 모든 매니폴드를 탐색하지 않고 빠르게 critical point를 찾아 최적 값을 탐색할 수 있다는 장점이 있다. Gauss-Newton이나 Levenberg-Marquardt 최적화 기법이 여기에 해당하며, 이러한 알고리즘을 충분히 빨리 동작할 수 있게 만들어 실시간으로 동작시킨게 현재까지의 SLAM이다.</p>
<img src="/20230214-rosen-2021/l_m.png" class="" title="l_m">

<p>하지만 문제가 있다. 1차/2차 미분 값을 이용한다는건 ‘초기값’이 존재하며 최적화 수행 시 초기값 기준으로 1차/2차 미분값을 기반으로 더 좋은 값을 찾아가는 로컬 탐색을 한다는 것인데, 로컬 탐색을 ‘초기값 주변의 critical point’를 찾게 해주는 기능이지, <strong>MLE 문제의 전역 최적변수를 찾는 것이 아니다</strong> (i.e. Global minimum이 아닌 local minima를 찾는 것이다). 물론 로컬 탐색으로 찾은 변수가 global minimum일 확률이 아예 없는건 아니지만, SLAM을 해본 사람들은 대부분 최적화 실패로 인한 소위 ‘망한 값’을 본 적이 있을 것이고, 최적화가 성공했다고 해도 은근히 에러가 많이 쌓인 값을 본 적이 있을 것이다. <strong>로컬 탐색은 이처럼 전역 최적 변수를 보장할 수 없다</strong>.</p>
<p>그러면 MLE 문제의 전역 최적 변수는 어떻게 찾는 것일까? SLAM 문제처럼 고차원의 non-convex 공간에서 최적점을 찾는 것은 NP-hard 문제이기 때문에, 모든 경우의 수를 검토하는 것은 좋은 방법이 아니다. 근데 또 생각해봐야하는건, 로컬 탐색이 항상 실패하는건 아니다. 우리가 만족할 만큼 꽤나 잘 동작했으니 또 SLAM이 연구개발되온 것일텐데, 그러면 ‘잘된 SLAM’과 ‘안된 SLAM’의 경계가 무엇인지도 생각해봐야한다.</p>
<p>이렇게 SLAM inference 방법을 개발하고 평가할 때는 다음과 같은 2가지 기준으로 평가해야한다.</p>
<ol>
<li>알고리즘 - 어떤 조건에서 MLE 전역 최적점을 찾을 수 있을 것인가?</li>
<li>통계 - MLE 전역 최적점을 찾을 수 있다면, 그 확률이 어떻게 되는가?</li>
</ol>
<p> </p>
<hr>
<h1 id="Certifiable-Correct-SLAM"><a href="#Certifiable-Correct-SLAM" class="headerlink" title="Certifiable Correct SLAM"></a>Certifiable Correct SLAM</h1><p>Certifiably correct method라는 분야는 <strong>전역 최적점을 찾았다는 것을 보장할 수 있는 알고리즘</strong>을 연구하는 분야이다. 1/2차 미분으로 로컬 탐색을 하는 보통의 SLAM과는 다르게, convex relaxation 기반으로 전역 최적점을 찾는 연구이다. Convex relaxation은 간단하게 설명하자면, non-convex 문제를 convex 문제로 근사한 후, convex 문제의 최적점을 찾는 방법이다.</p>
<img src="/20230214-rosen-2021/convex_relaxation.png" class="" title="convex_relaxation_ohh">

<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDYuMDA3NDY=">Carlone et.al. 2015 - Lagrangian duality in 3D SLAM: Verification techniques and optimal solutions<i class="fa fa-external-link-alt"></i></span>에서는 Lagrangian duality를 이용해서 SLAM 문제를 2차 함수 프로그램으로 변환해 최적점을 찾았다. 이는 결국 semidefinite program의 해를 찾는 것과 같은데, 당시에는 아쉽게도 해를 찾기 위해 로컬 탐색 기법을 사용하긴 했다.</p>
<p>하지만 이런 duality 관련 연구가 발전하면서, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MTIuMDczODY=">Rosen et.al. 2016 - A certifiably correct algorithm for synchronization over the special Euclidean group<i class="fa fa-external-link-alt"></i></span> 연구를 통해 3D pose의 조합에서 해를 찾는 방법이 개발되었고, 이는 곧 3년 후 알고리즘이 몇개 더 추가된 후 SE-Sync 라이브러리가 되었다.</p>
<p>3D pose에 대해 certified algorithm이 개발되었다는건, rotation averaging, two-view registration, sensor extrinsic calibration, 3D registration, shape reconstruction, SLAM에서 사용될 수 있다는 것을 의미한다.</p>
<img src="/20230214-rosen-2021/se_sync.png" class="" title="se_sync">

<p> </p>
<hr>
<h1 id="Robust-estimation"><a href="#Robust-estimation" class="headerlink" title="Robust estimation"></a>Robust estimation</h1><p>기존 SLAM의 최적화 방식에는 또 다른 큰 문제가 있다. 바로 <strong>MLE 알고리즘의 전제가 ‘모든 데이터가 통계적으로 올바를 때 최적점을 보장한다’ 라는 점</strong>인데, 모든 센서 데이터가 SLAM에 유용한 데이터를 제공하는게 아니기 때문이다. SLAM을 해본 사람들은 알겠지만, 센서 데이터 속에서 SLAM에 유용한 데이터를 솎아주는 frontend 설계하는 부분은 굉장히 섬세하고 어려운 작업이라 많은 피,땀,눈물을 필요로 한다. 통계적으로 <strong>유용한 데이터는 inlier</strong>, <strong>유용하지 않은 데이터는 outlier</strong>라고 칭하는데, 주로 outlier는 SLAM에서 세워둔 물리적인 모델을 따르지 않는 데이터가 된다. 그리고 MLE 문제를 풀 때, outlier가 몇개 끼어있기만해도 아주 잘못된 결과가 나오는 일이 태반이다.</p>
<p>데이터 속에서 아웃라이어를 제거하기 위해 다양한 방법들이 있다. 가장 많이 사용하는 방법은 <strong>RANSAC</strong>이며, 수많은 데이터 속에서 소수의 데이터를 샘플링 한 후 나머지 데이터들과 비교해서 적절한 모델을 선택했는지 결정해 나머지 데이터를 아웃라이어 처리하는 방법이다. 또 다른 방법으로는 <strong>RRR 전략</strong> (Realizing, reversing, recovering)이 있는데, multiple hypothesis tracking이라고도 불리는 방법으로써 여러가지 가능성을 전부 계산하고, 시간이 지나면서 말이 안되는 가능성은 점점 버리는 방법이다. 최근에는 <strong>Pairwise consistency maximization</strong> (PCM) 방식이 루프 클로저를 할 때 안정적으로 성공시키는 방법으로 뜨고 있다.</p>
<p>위와 같이 특정 테크닉들을 사용하기도 하지만, 거의 모든 SLAM에 기본적으로 사용하는 기법은 robust estimator (<strong>M-estimator</strong>) 이다. 여러가지 M-estimatior 커널들이 존재하고, 각각의 커널마다 다른 방법으로 아웃라이어를 제거한다. M-estimator를 적절하게 사용하기 위해 <span class="exturl" data-url="aHR0cHM6Ly9uaWtvc3VlbmRlcmhhdWYuZ2l0aHViLmlvL2Fzc2V0cy9wYXBlcnMvSVJPUzEyLXN3aXRjaGFibGVDb25zdHJhaW50cy5wZGY=">Sunderhauf et.al. 2012 - Switchable constraints for robust pose graph SLAM<i class="fa fa-external-link-alt"></i></span> 논문에서는 여러 M-estimator 커널을 상황에 따라 바꿀 수 있는 기법을 소개했고, <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzY2MzA1NTc=">Agarwal et.al. 2014 - Dynamic covariance scaling for robust map optimization<i class="fa fa-external-link-alt"></i></span>에서는 적절히 커널의 노이즈 covariance 값을 변경하는 방법을, <span class="exturl" data-url="aHR0cHM6Ly9hcHJpbC5lZWNzLnVtaWNoLmVkdS9tZWRpYS9wZGZzL29sc29uMjAxM2lqcnIucGRm">Olson 2013 - Inference on networks of mixtures for robust robot mapping<i class="fa fa-external-link-alt"></i></span>에서는 여러개의 커널을 동시에 적용하는 max-mixture 기법을 사용했다. M-estimator는 매니폴드의 특정 부분에 non-convexity를 추가로 적용해 좋은 데이터를 섞어내는 테크닉으로써 매우 성공적이였지만, <strong>잘못 사용할 경우 오히려 더 좋지 않은 결과</strong>가 나타날 수 있기에 <strong>일종의 튜닝의 영역</strong>으로 다뤄진다. 진정한 Robust estimation을 위해 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDYuMDY3NjkucGRm">Yang et.al. 2020 - One ring to rule them all: Certifiably robust geometric perception with outliers<i class="fa fa-external-link-alt"></i></span>에서는 M-estimator와 certifiably correct optimization 기법을 혼합하는데, 조금 더 설명하자면 semidefinite relaxation을 통해 전체 매니폴드를 convex하게 풀어줌과 동시에 M-estimator를 이용해서 truncated squared-error loss를 적용해 매니폴드를 polynomial 최적화 문제로 변환한다. 이는 전체 데이터에서 아웃라이어가 50% 이상이 넘을 때도 잘 동작하며, 효율적이게 동작하는 dual problem의 해를 구하는 방식을 채택함으로써 적당한 시간 내에 연산을 끝내는 방식을 제안한다.</p>
<p> </p>
<hr>
<h1 id="그래서-앞으로-무엇이-더-연구되어야하는가"><a href="#그래서-앞으로-무엇이-더-연구되어야하는가" class="headerlink" title="그래서 앞으로 무엇이 더 연구되어야하는가?"></a>그래서 앞으로 무엇이 더 연구되어야하는가?</h1><ol>
<li>Semidefinite programming을 효율적으로 푸는 방법이 필요하다. 엄청나게 많은 데이터를 모아두고 엄청나게 큰 문제를 푸는 방식이 아닌, 실시간으로 SLAM을 돌릴면서 풀 수 있도록 최적화된 incremental semidefinite optimization 기법이 만들어져서 iSAM이나 GTSAM 같은 라이브러리에 포함되면 좋을 것 같다.</li>
<li>Certifiable correct perception 모델의 초창기를 보게 되었는데, 이걸 실시간으로 수행하고 검증할 수 있으면 좋을 것 같다. 왜냐하면, 100% correct한 정보만을 모아서 SLAM을 돌린다면 outlier rejection 과정을 전부 제거할 수 있기 때문이다. 100% 정확한 prior들만 가지고 연산을 한다? 벌써 심장이 뛴다.</li>
<li>단순히 포인트 클라우드가 아닌, 다른 형태의 맵을 사용해보면 좋을 것 같다. 포인트 클라우드에서 각각의 3D landmark들은 서로 관계가 없고, 이 때문에 서로간의 conditional probability가 전혀 없다. 언제든 랜드마크가 사라져도 할말이 없다는 것이다. 하지만 실제 환경을 보면, 동일 물체에서 나타난 포인트들은 서로 연관이 있을 확률이 굉장히 높다. 예를 들어, 하나의 line에서는 2개의 point가 나타날 것이 아닌가? line이 존재하는 한 다른 point는 절대 사라질 수 없다. 포인트를 넘어서는 representation, 예를 들어 line, plane… semantics 같은 정보를 쓸 수 있다면 더욱 안정적인 매니폴드를 구축할 수 있을 것이다.</li>
</ol>
<p>그런 의미에서 다음 글에서는 Semantic 정보에 대해 이야기해본다.</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Certified algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>자율주행 차량을 위한 센서 캘리브레이션 기법</title>
    <url>/20230214-sensor-calibration/</url>
    <content><![CDATA[<div class="video-container"><iframe src="https://www.youtube.com/embed/VPM7N6SqiEo" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>

<p> </p>
<hr>
<h1 id="캘리브레이션이-중요한-이유"><a href="#캘리브레이션이-중요한-이유" class="headerlink" title="캘리브레이션이 중요한 이유"></a>캘리브레이션이 중요한 이유</h1><p>ADAS와 같이 자동화된 퍼셉션 기능이 많아질 수록 캘리브레이션이 중요해진다. 자동차의 눈이 다른 곳을 향해있다던가 목표를 제대로 보지 못하고 있다면,차선변경이나 회피기동을 올바르게 하지 못할 수도 있다.</p>
<p>센서의 상태는 항상 바뀔 수 있다. 진동이나 온도 변화로 바뀔 수도 있고, 사고가 나서 펜더가 휘어버린다던지 차체바디가 뒤틀림으로써 생기는 문제도 있을 수 있다. 차를 한번 사면 최소 몇년씩 타기 때문에, 차의 lifecycle 동안에는 센서의 상태가 틀어질 확률은 굉장히 높다. 그에 비해 아직 제조사들은 차에 센서를 탑재해서 full lifecycle을 겪어본 경험이 적기 때문에, 센서 캘리브레이션이 틀어졌을 때의 악효과에 대해 충분히 알지 못하며 공장 캘리브레이션 + 정비 시 캘리브레이션으로 충분하지 않을까라고 생각하는 편이다.</p>
<p> </p>
<hr>
<h1 id="정비소에서-ADAS-시스템-캘리브레이션-하는-방법"><a href="#정비소에서-ADAS-시스템-캘리브레이션-하는-방법" class="headerlink" title="정비소에서 ADAS 시스템 캘리브레이션 하는 방법"></a>정비소에서 ADAS 시스템 캘리브레이션 하는 방법</h1><p>정비소에서 ADAS 시스템을 캘리브레이션 하는데에는 짧게는 15분에서 길게는 몇시간까지도 걸린다. 이 때, 정비소에는 캘리브레이션을 하기 위한 특수 장비와 차고가 있어야한다. 캘리브레이션을 하기 전 차는 완벽하게 정비가 되어있어야한다 - 수평이 잘 맞아야하고, 타이어 압력도 충분히 들어가야하며 트렁크도 비어있어야한다.</p>
<p>캘리브레이션이 잘못 될 경우 운전자도 위험하지만 주변인도 위험해진다.</p>
<p> </p>
<hr>
<h1 id="캘리브레이션의-종류"><a href="#캘리브레이션의-종류" class="headerlink" title="캘리브레이션의 종류"></a>캘리브레이션의 종류</h1><img src="/20230214-sensor-calibration/calib.png" class="" width="1"> 

<ul>
<li>Intrinsic : 센서의 내부 파라미터에 대한 캘리브레이션</li>
<li>Extrinsic : 센서들 사이를 잇는 파라미터에 대한 캘리브레이션</li>
<li>Temporal : 센서 데이터가 들어오는 타이밍/싱크를 맞추는 캘리브레이션</li>
</ul>
<p>왠만한 카메라나 라이다는 intrinsic 캘리브레이션을 쉽게 진행할 수 있다. 또, 대부분의 ADAS 시스템이 정확한 temporal 캘리브레이션이 필요하지 않은데, 왠만하면 대부분의 카메라/라이다 데이터는 가장 가까운 프레임끼리 엮어서 써도 적당히 결과가 잘 나오기 때문이다.</p>
<p>하지만 Extrinsic은 진짜 정확해야한다.</p>
<p> </p>
<hr>
<h1 id="캘리브레이션-방법의-종류"><a href="#캘리브레이션-방법의-종류" class="headerlink" title="캘리브레이션 방법의 종류"></a>캘리브레이션 방법의 종류</h1><img src="/20230214-sensor-calibration/calib2.png" class="" width="2">

<p>Target 기반 캘리브레이션 방법은 target을 바라보는 여러개의 센서 데이터를 취합해서 reprojection error를 최소화하는 최적화 기법을 사용해 센서 파라미터를 추정한다. 다양한 종류의 센서가 이 타겟을 정확하게 바라볼 수 있는 (i.e. 체커보드의 코너를 검출하게 만드는) 방법을 찾는 것이 어렵기 때문에 난이도가 높다 - 라이다/레이더/초음파로 체커보드를 검출시키는 방법이라던가, 센서들 모두 field of view가 겹쳐야한다던가… 하지만 디버깅이 제일 쉽기 때문에 하나의 데이터셋만 가지고 센서 데이터를 골라 좋은 캘리 값을 얻는 것이 가능하기 때문에 제일 인기 있는 방법으로 꼽힌다.</p>
<p>Ego-motion 기반 캘리브레이션 방법은 사전에 잘 만들어놓은 지도가 있을 때 센서들의 위치를 추정함으로써 (i.e. localization) 캘리브레이션을 진행하는 방식이다. 이 때, 위치추정 기법이 정확하게 수행될 수 있어야 좋은 캘리브레이션이 가능한데, 보통 위치추정 기법이 잘 동작하려면 6dof 방향으로 큰 모션이 있어야 가능하다 - 자동차의 경우 위/아래로 움직일 수가 없다. 그렇기 때문에 위아래로 움직일 수 있는 드론이나 소형 로봇에서만 사용하는 방법이다.</p>
<p>Appearance 기반 캘리브레이션 방법은 여러 센서들에서 나타나는 intensity 정보를 기반으로 센서들간의 자세 차이를 구하는 방법이다. (영상에서 설명해주긴 하는데 명확하게 이해가 안된다)</p>
<p> </p>
<hr>
<h1 id="대학교-연구소에서-진행중인-캘리브레이션-방법들"><a href="#대학교-연구소에서-진행중인-캘리브레이션-방법들" class="headerlink" title="대학교 연구소에서 진행중인 캘리브레이션 방법들"></a>대학교 연구소에서 진행중인 캘리브레이션 방법들</h1><h2 id="토론토-대학-STARS-Lab의-셀프-캘리브레이션"><a href="#토론토-대학-STARS-Lab의-셀프-캘리브레이션" class="headerlink" title="토론토 대학 STARS Lab의 셀프 캘리브레이션"></a>토론토 대학 STARS Lab의 셀프 캘리브레이션</h2><img src="/20230214-sensor-calibration/stars.png" class="" title="stars">

<img src="/20230214-sensor-calibration/stars2.png" class="" title="stars2">

<p>STARS Lab은 캘리브레이션 작업 자체가 사람이 하기에는 너무 정교하기 때문에, 실시간으로 센서들 사이의 셀프 캘리브레이션이 수행되어야한다고 믿고있다.</p>
<p>Ground truth로 삼을 수 있는 센서를 하나 준비한 후, Figure 8을 돈다거나 또는 주변 환경에서 물체를 검출해서 (e.g. 신호등) 검출한 물체들을 align한다. 이를 통해 각각의 센서들을 align 시킬 수 있다.</p>
<p>이 외로도 여러가지 캘리브레이션 연구를 수행중이다. 꽤나 관심가는 논문 중 하나는 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDkuMDM1NTQucGRm">Giamou 2020 - Certifiably Globally Optimal Extrinsic Calibration from Per-Sensor Egomotion<i class="fa fa-external-link-alt"></i></span> - <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3V0aWFzU1RBUlMvY2VydGlmaWFibGUtY2FsaWJyYXRpb24=">GitHub<i class="fa fa-external-link-alt"></i></span>.</p>
<p> </p>
<h2 id="스탠포드-드라이빙-팀"><a href="#스탠포드-드라이빙-팀" class="headerlink" title="스탠포드 드라이빙 팀"></a>스탠포드 드라이빙 팀</h2><p>‘캘리브레이션이 잘 된 경우’와 ‘캘리브레이션이 잘 안된 경우’에 대한 error model을 사전에 학습함. 이를 통해 센서 캘리브레이션이 잘 안되었을 때 이를 인지할 수 있으며, 시간이 지날수록 틀어지는 캘리브레이션을 검출할 수 있음.</p>
<p>Zoox의 CEO Jesse Levinson이 이 랩에서 박사 졸업을 했는데, 이 사람의 박사논문을 보면 상당 부분이 캘리브레이션 관련인 것을 알 수 있다. 이를 통해 아마도 Zoox는 캘리브레이션을 엄청 잘하지 않을까… 라는 생각도 한다.</p>
<p> </p>
<h2 id="나고야-대학-Takeda-랩"><a href="#나고야-대학-Takeda-랩" class="headerlink" title="나고야 대학 Takeda 랩"></a>나고야 대학 Takeda 랩</h2><p>‘어디서든 캘리브레이션을 할 수 있다’ (Any place, any time calibration)라는 생각은 잘못된 믿음이라고 생각하는 랩이다.</p>
<p>Target 기반 캘리브레이션이 굉장히 잘 되기 때문에, 이를 더 사용하기 쉽게 만드는 점에 초점을 둔 랩이다. </p>
<p> </p>
<hr>
<h1 id="기업에서-진행중인-캘리브레이션-방법들"><a href="#기업에서-진행중인-캘리브레이션-방법들" class="headerlink" title="기업에서 진행중인 캘리브레이션 방법들"></a>기업에서 진행중인 캘리브레이션 방법들</h1><h2 id="OEM"><a href="#OEM" class="headerlink" title="OEM"></a>OEM</h2><img src="/20230214-sensor-calibration/oem.png" class="" title="OEM">

<p>차량 제조사들은 캘리브레이션 관련 여러 특허를 걸어뒀지만, 뭔가 새로운 캘리브레이션 방법론을 찾기보다는 편하게 target 기반 캘리를 할 수 있는 악세서리 개발 등에 초점을 뒀다. 이 방법들은 사실 그렇게 scalable하다고 볼 수 없으며, OEM만 이걸 쓸게 아니라 사실 애프터서비스 쪽에서도 필요한거기 때문에 잘 된거라고 볼 수 없고, 그렇기 때문에 크게 주목할 부분은 아니다.</p>
<p> </p>
<h2 id="AV-Fleet"><a href="#AV-Fleet" class="headerlink" title="AV Fleet"></a>AV Fleet</h2><p>Waymo 같이 fleet을 운영하는 곳에서는 캘리브레이션에 대해 연구를 하고 있지만, 실제로는 3rdParty 회사에게 위탁 캘리브레이션을 맡겨서 운영하고 있다. 왜냐하면 차량 한대를 캘리하는데 약 60분 정도 시간이 들고 이걸 매일 해야하는데, 이건 결국 유지보수 비용으로 나가기 때문에 소중한 연구 시간을 뺏길 수 없기 때문이다. 역으로 생각하면 3rdParty 업자들에게는 기회가 있다고 볼 수 있다.</p>
<p> </p>
<h2 id="센서-제조업체"><a href="#센서-제조업체" class="headerlink" title="센서 제조업체"></a>센서 제조업체</h2><p>AEye 라이다의 경우 자체 SDK를 통해 셀프 캘리브레이션을 수행하고 또 테크니션들에게 diagnostic을 줄 수 있는 기능이 있다.</p>
<p>사실 차량회사들 입장에서는 캘리브레이션을 최대한 적게 수행하는게 이득이다. 정기적으로 캘리브레이션을 수행해야한다면 그만큼 고객은 불편함을 겪게 되는 것이기 때문에, 최대한 캘리브레이션이 필요하지 않은 센서들을 선호하게 된다.</p>
<p>AEye는 이와 같은 점을 고려해서 정기적인 캘리가 필요한 Rotating lidar가 아닌 Solid state lidar로 변경했다. 단단한 케이스를 만들어서 진동과 온도 변화에 강하게 만들어 캘리를 덜 자주하게 만들 수 있다.</p>
<p> </p>
<h2 id="소프트웨어-업체"><a href="#소프트웨어-업체" class="headerlink" title="소프트웨어 업체"></a>소프트웨어 업체</h2><p>많은 AI 알고리즘들이 raw 센서 데이터를 인풋으로 받아 연산을 하는 경우가 많다. 이 때문에, 캘리브레이션 값이 정확하지 않으면 동작하지 않게 되는 경우도 있고, 캘리를 다시 할 경우 AI 알고리즘도 전부 학습을 다시해야하는 경우도 있다. 이러한 부분은 연구실에서는 문제가 되지 않을 수 있으나, 양산을 하기에는 큰 문제가 될 수 있다.</p>
<p>영국 스타트업 <span class="exturl" data-url="aHR0cHM6Ly9sZ24uYWkv">LGN<i class="fa fa-external-link-alt"></i></span>은 raw 센서 값을 쓰기보다는 센서의 latent space를 인풋으로 받아서 사용하는 곳도 있다. 이 latent space는 캘리 값 변화로 인한 인풋 데이터의 변화에도 상당히 강인하며, 다양한 센서 조합을 사용하는데에도 유용하다. 또, raw 센서 데이터에 대한 redundancy로도 사용될 수 있다.</p>
<p> </p>
<h2 id="애프터마켓-솔루션"><a href="#애프터마켓-솔루션" class="headerlink" title="애프터마켓 솔루션"></a>애프터마켓 솔루션</h2><img src="/20230214-sensor-calibration/bosch.png" class="" title="bosch">

<p>Bosch와 같은 곳에서는 캘리브레이션을 쉽게 할 수 있는 솔루션을 개발해서 판매하고 있다. 하지만 엄청 비싸다. 이런 부분에 대해 <span class="exturl" data-url="aHR0cHM6Ly93d3cuYnJpZGdlc3RvbmUuY29tL3JlZ2lvbmFsL2FzaWFfcGFjaWZpYy8=">Bridgestone<i class="fa fa-external-link-alt"></i></span> 같은 3rdParty 회사들에서 기회를 보고 있다 (Bridgestone은 타이어 회사인데, 여러 정비소 지점을 가지고 있기 때문에 정비소마다 캘리툴을 배치하려는 노력을 하고 있다 - 캘리 정비소 시장을 먹겠다는 것).</p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Calibration</tag>
        <tag>Self-driving</tag>
      </tags>
  </entry>
  <entry>
    <title>Spatial AI를 만드는 방법 - Rosen 2021 - Advances in Inference and Representation for Simultaneous Localization and Mapping</title>
    <url>/20230215-rosen-2021/</url>
    <content><![CDATA[<h1 id="시작하기-전…"><a href="#시작하기-전…" class="headerlink" title="시작하기 전…"></a>시작하기 전…</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjMwMjE0LXJvc2VuLTIwMjEv">Part 1 - SLAM 추정 효율성에 대한 고찰 - Rosen 2021 - Advances in Inference and Representation for Simultaneous Localization and Mapping<i class="fa fa-external-link-alt"></i></span>을 먼저 읽으시면 좋습니다.</p>
<p> </p>
<hr>
<h1 id="Global-representation은-어떤-형태가-적합한가"><a href="#Global-representation은-어떤-형태가-적합한가" class="headerlink" title="Global representation은 어떤 형태가 적합한가?"></a>Global representation은 어떤 형태가 적합한가?</h1><p>현재까지 Point cloud, Occupancy grid map, Mesh, Voxel, surface map 등등 다양한 형태의 global representation을 사용하는 SLAM들이 있었다.</p>
<p>SLAM을 다루는 환경은 정말 다양하다. </p>
<ul>
<li>공간의 크기: 작은 공간에서 큰 공간. </li>
<li>센서의 종류: 하나의 센서에서부터 여러개의 이종센서를 사용하는 플랫폼.</li>
<li>컴퓨팅 리소스: 마이크로 로봇에서부터 클라우드 서버 스케일의 컴퓨팅 리소스.</li>
<li>공간의 종류: 정적인 공간에서부터 자세/형태가 모두 동적인 물체가 있는 공간.</li>
<li>다른 알고리즘으로 인한 제약조건: SLAM 이전에 버추얼 센서로써 인지 알고리즘이 돌고 있는지, SLAM 이후에 플래닝 및 기타 판단을 위해 SLAM 지도의 형태가 정해져야하는지. </li>
</ul>
<p>위와 같이 다양한 환경이 있고, 이러한 환경에서 정확하고, 빠르고, robust하게 돌기 위해서는 다양한 알고리즘을 조합해야하며 이로 인해 <strong>다양한 표현법이 있을 수 밖에 없다</strong> (또는, 다양한 표현법이 있을 수 밖에 없었다).</p>
<p>그리고 <strong>시장의 기대치는 빠르게 높아져가고 있다</strong>. 이제 로봇에서 정적인 공간에 대해 geomtric map을 만들고 위치추정을 하는 기능은 SLAM 알고리즘으로써 가장 기본중의 기본으로 인식된다. 컴퓨팅 보드와 센서의 성능도 날이 갈 수록 발전하고 있기 때문에, 다양한 이종 센서들을 조합해 물체를 인식한다던가, 딥러닝 정보까지 섞는다던가, 동적인 환경에서도 잘 동작한다거나, 변화하는 환경에서도 동작하고, 경로 계획까지 잘 이어지는게 최근 시장이 기대하는 수준이다.</p>
<p>최근에는 semantic perception 정보까지 섞어서 플랫폼이 공간과 상호작용을 할 수 있는 수준까지 깊게 이해하게 만드는 수준인 ‘spatial AI’라는 키워드도 떠오르고 있다.</p>
<p>그렇다면 <strong>가장 효율적인 representation은 무엇인가?</strong> 가장 안정적이고(reliable), 가장 강인하고(robust), 가장 확실한(certifiable)하며 정확하고(accurate) 가벼운(light-weight) representation은 무엇일까?</p>
<p> </p>
<hr>
<h1 id="Geometric-representation"><a href="#Geometric-representation" class="headerlink" title="Geometric representation"></a>Geometric representation</h1><p>우선 기하학적인 표현법부터 알아보자.</p>
<p>VSLAM에서는 주로 SIFT, SURF, ORB와 같은 sparse feature를 모아 <strong>sparse map</strong>을 만들었다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MTAuMDY0NzU=">ORB-SLAM<i class="fa fa-external-link-alt"></i></span> 처럼 keypoint+descriptor 조합을 사용할 수도 있고, <span class="exturl" data-url="aHR0cHM6Ly9qYWtvYmVuZ2VsLmdpdGh1Yi5pby9wZGYvRFNPLnBkZg==">DSO<i class="fa fa-external-link-alt"></i></span>처럼 굳이 그런 조합을 사용하지 않는 방법도 있다. Sparse map은 3D 공간에서 위치 추정을 하는데에는 성공적이나, 물체가 위치한 공간과 아닌 공간을(occupied space) 구분하는데에 있어서는 상당히 취약하기 떄문에 path planning이나 collision-free path 생성에는 좋은 방법이 아니였다.</p>
<p>이를 타파하기 위해 <strong>dense representation</strong>을 사용해 occupied space를 표현하기 시작했다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MTEuMDM2MzE=">Voxblox<i class="fa fa-external-link-alt"></i></span> 처럼 Occupancy grid map / volumentric map을 사용하거나, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDEuMDY4OTQ=">Kimera<i class="fa fa-external-link-alt"></i></span>처럼 mesh를, <span class="exturl" data-url="aHR0cHM6Ly93d3cuZG9jLmljLmFjLnVrL35hamQvUHVibGljYXRpb25zL25ld2NvbWJlX2V0YWxfaWNjdjIwMTEucGRm">DTAM<i class="fa fa-external-link-alt"></i></span>처럼 dense point cloud를, <span class="exturl" data-url="aHR0cDovL3d3dy5yb2JvdGljc3Byb2NlZWRpbmdzLm9yZy9yc3MxMS9wMDEucGRm">ElasticFusion<i class="fa fa-external-link-alt"></i></span>이나 <span class="exturl" data-url="aHR0cHM6Ly93d3cubWljcm9zb2Z0LmNvbS9lbi11cy9yZXNlYXJjaC93cC1jb250ZW50L3VwbG9hZHMvMjAxNi8wMi9pc21hcjIwMTEucGRm">KinectFusion<i class="fa fa-external-link-alt"></i></span>처럼 truncated signed distance function map(TSDF) 같은 것들이 이와 같은 예시이다. </p>
<p>각각의 장점과 단점들이 있지만, 모든 representation을 꿰뚫는 단점들이 있다.</p>
<ol>
<li>Geometry만으로는 공간의 특성을 이해할 수 없다. <strong>물체의 색깔</strong>, <strong>촉감</strong>, <strong>무게</strong> 와 같은 정보는 <strong>기하적인 특성만으로는 알 수 없는 것</strong>들이다.</li>
<li>공간의 특성을 넘어서는 다양한 공간과의 상호작용에서 나타는 정보도 표현할 수 없다. <strong>소리</strong>, <strong>온도</strong> 같은 것들이다.</li>
<li><strong>자연스러운 인간-로봇 상호작용(human-robot interaction)이 불가능</strong>하다. 사람은 ‘탁자 위에 컵이 있구나’ 라고 표현하는데에 비해, 로봇의 geometric representation은 pose와 coordinate로 표현할 것이다. 사람과 로봇이 자연스럽게 소통하기 위해서는 인간의 소통방식에 입건한 semantic descriptor가 필요한데, geometric representation에는 이것이 없다.</li>
<li>Localization을 넘어서는 작업에는 큰 도움이 되지 않는다. </li>
</ol>
<p> </p>
<hr>
<h1 id="Semantic-정보와-geometry-정보의-조합"><a href="#Semantic-정보와-geometry-정보의-조합" class="headerlink" title="Semantic 정보와 geometry 정보의 조합"></a>Semantic 정보와 geometry 정보의 조합</h1><img src="/20230215-rosen-2021/bowman.png" class="" title="bowman">

<p>2015년 이후 부터 SLAM 커뮤니티에서도 머신러닝 기반의 퍼셉션 모델에 관심을 가지기 시작했다. Object detection이라던지, place recognition 등을 통해서 좀 더 높은 수준의 플래닝 및 액션을 꾀하는 연구가 진행이 되었다. 좀 더 설명하자면, 거실에서 침실로 로봇을 이동해야한다는 task가 있다면 2015년 이전에는 거실의 coordinate에서 침실의 coordinate로 이동하기 위해 occupancy grid map에서 최적경로를 계산했을텐데, 2015년 이후에는 거실을 어떻게 인지하고 침실을 어떻게 인지할지에 대한 연구로 방향을 틀었다는 것이다.</p>
<p>이러한 연구는 결국 <strong>Semantic map</strong>에 대한 개념을 만들었다. Semantic map은 단순히 object와 place에 대한 정보만 가지면 되었기 떄문에, geometric map 보다 <strong>훨씬 더 적은 메모리와 계산량을 소모한다는 장점</strong>도 알게 되었다. 그리고 <strong>로봇이 만든 지도를 사람이 이전보다 훨씬 더 쉽게 이해할 수 있다는 장점</strong>도 알게 되었다.</p>
<p>하지만 <strong>semantic 정보를 어떻게 조합해야하는지에 대한 정해진 룰이 없었다</strong>. 현재까지도 몇가지 제안된 방법은 있다만, de-facto로 정해진 방법이 없다. 지금까지 정해진건 단 하나 - SLAM 이전에 <strong>머신러닝 기반 perception 모델을 돌리고, 그 결과를 마치 버추얼 센서처럼 사용</strong>하자는 것만 있다.</p>
<p>하지만 Semantic 정보를 추출하는 object detection이나 semantic segmentation 모델을 버추얼 센서처럼 사용하는건 <strong>생각보다 쉽지 않다</strong>. 두가지 이유가 있다. 첫번째로는 <strong>도메인이 살짝만 달라져도 모델 추론이 완전히 망가진다던지, 아무리봐도 터질일이 없어보이는 경우에도 터지는 경우가 많다</strong>. 이 때문에 모델 추론 결과의 uncertainty를 함께 추론하는 연구도 진행되고 있다. 두번째로는 <strong>object category는 개별적으로(discrete)하게 분류가 되는데, 이것이 SLAM 추론 내부에서 나타나는 continuous geometry와 엮을 때 굉장히 어려워진다는 것</strong>이다. Joint discrete-continuous estimation은 종종 굉장히 큰 문제 (i.e. combinatorially large state spaces)를 만든다.</p>
<p>Latent semantic class와 geomtric landmark를 동시에 추정 (i.e. joint estimation)을 하는 기본적인 수식은 아래와 같다. Y는 semantic 정보를 포함한 모든 measurement, X는 로봇 포즈, L은 landmark를 의미한다. 여기서 새롭게 중요하게 되는 정보가 D 인데, D는 Y와 L 사이의 <strong>data association</strong>을 의미한다. Semantic class는 continuous하지 않고 discrete한 성질을 띄는데 (i.e. 카테고리 형 분류), 이는 data association을 할 때의 discrete inference한 성질과 잘 맞기 때문에 궁합이 좋다고 볼 수 있다.</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.032ex" xmlns="http://www.w3.org/2000/svg" width="32.816ex" height="4.308ex" role="img" focusable="false" viewBox="0 -1006 14504.7 1904.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(277.3, 212)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(852, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1296.7, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(118.3, 212)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1977.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2422.3, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(219.6, 212)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3528.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4583.9, 0)"><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(500, 0)"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(892, 0)"></path></g><g data-mml-node="mo" transform="translate(5975.9, 0)"><path data-c="2061" d=""></path></g><g data-mml-node="munder" transform="translate(6142.6, 0)"><g data-mml-node="mo" transform="translate(100.8, 0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833, 0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(0, -661) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(852, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1130, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(1811, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2089, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g></g></g><g data-mml-node="mi" transform="translate(8371.9, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(8874.9, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9263.9, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(10115.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(10560.5, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(11241.5, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(11686.2, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(12792, 0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(13347.7, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(268, 528)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g><g data-mml-node="mo" transform="translate(14115.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p>위와 같은 문제 정의에 맞춰 여러가지 연구가 진행이 되었다. <span class="exturl" data-url="aHR0cHM6Ly93d3cuY2lzLnVwZW5uLmVkdS9+a29zdGFzL215cHViLmRpci9ib3dtYW4xN2ljcmEucGRm">Bowman 2017 - Probabilistic data association for semantic SLAM<i class="fa fa-external-link-alt"></i></span> 연구에서는 expectation maximization (EM) 방식으로 semantic/geometric 동시 최적화를 진행한다. 두 단계로 이뤄져있는데, 첫번째 단계에서는 semantic class와 geometry 사이의 data association의 확률을 전부 고정해놓고 우선적으로 pose graph optimziation만 진행을 하고, 두번째 단계에서는 로봇 포즈와 랜드마크를 고정한 상태에서 새롭게 semantic/geometric data association을 최적화하는 것이다. 이렇게 함으로써 locall optimal solution에 다다를 수 있다. <span class="exturl" data-url="aHR0cHM6Ly93d3cueWFzaXJsYXRpZi5pbmZvL21vdmluZ3NlbnNvcnMvY2FtZXJhUmVhZHkvcGFwZXIwOS5wZGY=">Sunderhauf 2015 - SLAM-quo vadis? in support of object oriented and semantic SLAM<i class="fa fa-external-link-alt"></i></span> 연구에서는 semantic label의 확률을 최적화 하기도 했고, <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg3OTQyNDQ=">Doherty 2019 - Multimodal semantic SLAM with probabilistic data association<i class="fa fa-external-link-alt"></i></span>에서는 여러 센서로부터 들어오는 센서 정보를 mixture representation으로, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDkuMTEyMTM=">Doherty 2020 - Probabilistic data association with mixture models for robust semantic SLAM<i class="fa fa-external-link-alt"></i></span>에서는 단일 센서로부터 들어오는 센서 정보도 mixture representation으로 다루며 max-mixture 방식을 통해 continuous manifold를 구축하고 최적화를 통해 가장 높은 확률의 data association을 이루는 방법을 소개한다. 위 방법과는 다른 결로 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMTIuMDQ0MjMucGRm">Bernreiter 2019 - Multiple hypothesis semantic mapping for robust data association<i class="fa fa-external-link-alt"></i></span> 연구에서는 여러 가능성을 동시에 트랙킹하며 combinatorial하게 가장 좋은 방식을 찾는 multiple hypothesis tracking 기법을 사용하기도 한다.</p>
<p> </p>
<hr>
<h1 id="Semantic-map-representation"><a href="#Semantic-map-representation" class="headerlink" title="Semantic map representation"></a>Semantic map representation</h1><p>Semantic map을 만드는 방법에는 어떤 것들이 있을까? <strong>가장 쉬운 방법은 기존에 사용하는 geometric map (e.g. point cloud, mesh)에 semantic label을 추가하는 방법</strong>이다. 실제로 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDkuMDUxMzA=">SemanticFusion<i class="fa fa-external-link-alt"></i></span>에서는 dense map에 semantic label을 넣고, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDEuMDY4OTQ=">Kimera<i class="fa fa-external-link-alt"></i></span>에서는 mesh에 semantic label을 넣어 semantic mesh를 만들어낸다.</p>
<img src="/20230215-rosen-2021/slam_pp.png" class="" title="slam_plus_plus">

<p>2010년 초반에는 object-level SLAM도 꽤 많이 연구가 되었다. Object-level SLAM이란 map을 구성하는 요소가 object밖에 없는 경우를 말한다. <span class="exturl" data-url="aHR0cHM6Ly93d3cuZG9jLmljLmFjLnVrL35hamQvUHVibGljYXRpb25zL3NhbGFzLW1vcmVub19ldGFsX2N2cHIyMDEzLnBkZg==">SLAM++<i class="fa fa-external-link-alt"></i></span>가 대표적인 예시이다. Object 단위로 map을 만든다는건, object를 인식할 수 있어야한다는 것인데, SLAM++가 나왔을 2013년에는 아직 뉴럴넷 기반의 3D object detection이 안정적이지 못했기 때문에, prior shape에 대해 fitting 작업이 주로 진행되었다. 하지만 이후 뉴럴넷 기반의 2D object detection을 거쳐 3D object detection, shape estimation, pose estimation 관련 기법이 많이 연구되면서, 차차 이 기술들을 사용하는 방향으로 트렌드가 바뀌고 있다.</p>
<img src="/20230215-rosen-2021/nodeslam.png" class="" title="nodeslam">

<p>Object 기반 map을 만들때, 가장 쉽게 만드는 방법은 object를 하나의 semantic label이 붙어있는 point landmark로써 다루는 것이다. 하지만 이러한 경우에는 object의 방향이나 경계를 알 수 없기 때문에, 이러한 정보를 담기 위한 다양한 representation이 나오고 있다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDQuMDQwMTE=">Nicholson 2018 - QuadricSLAM: Dual Quadrics from Object Detections as Landmarks in Object-oriented SLAM<i class="fa fa-external-link-alt"></i></span>이나 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDkuMDk2Mjc=">Superquadric Object Representation for Optimization-based Semantic SLAM<i class="fa fa-external-link-alt"></i></span>과 연구에서는 object를 dual-quadric 형태에 넣음으로써 방향성을 가진 공간으로 다룬다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDYuMDA1NTc=">CubeSLAM<i class="fa fa-external-link-alt"></i></span>에서는 object를 3D object detection의 결과인 3D bounding box로 다루기도 한다. 조금 더 나아가 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDQ0ODV2Mg==">NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction<i class="fa fa-external-link-alt"></i></span>에서는 뉴럴넷으로 학습한 object descriptor를 기반으로 semantic label을 표현하며, shape과 pose (i.e. geometric)에 대한 부분은 SLAM 기반 최적화를 통해 계산하는 알고리즘을 사용한다.</p>
<p> </p>
<hr>
<h1 id="세상은-절대-가만히-있지-않아-Static-vs-dynamic-vs-semistatic-vs-deformable-world"><a href="#세상은-절대-가만히-있지-않아-Static-vs-dynamic-vs-semistatic-vs-deformable-world" class="headerlink" title="세상은 절대 가만히 있지 않아 (Static vs dynamic vs semistatic vs deformable world)"></a>세상은 절대 가만히 있지 않아 (Static vs dynamic vs semistatic vs deformable world)</h1><p>SLAM의 기본 전제 중 하나는 ‘<strong>세상은 고정되어있다</strong>‘ (i.e. <strong>static world</strong>) 이다. 이러한 전제 하에 SLAM 알고리즘은 엄청난 발전을 이뤘지만, 실제로 SLAM을 돌리려고 하는 환경에서는 환경이 고정되어있지 않은 경우가 생각보다 많다. 지금까지의 SLAM은 <strong>움직이는 물체가 있는 경우 아예 그냥 없는 물체 취급을 하며 계산에서 제외해버리는 경우가 많았다</strong>. 그렇게 해야지 고정된 물체에 대한 계산을 할 때 outlier 처리를 할 수 있기 때문이다. 하지만 최근 연구에서는 움직이는 객체를 인지하고, 그들만의 <strong>dynamics를 측정해서 시간이 지남에 따라 변화하는 공간 (spatio-temporal) 정보를 측정</strong>하려는 시도가 이뤄지고 있다.</p>
<p>고정된 세상이라는 전제에서 가장 중요한 문제는 ‘<strong>움직인것은 나인가? 아니면 세상인가?</strong>‘를 알아내는 것이다. 실제로 세상은 고정되어있는데 로봇이 움직인 것이나, 로봇은 가만히 있는데 세상이 움직인것이나, 로봇의 입장에서는 둘 다 똑같이 보인다 (물론 proprioceptive sensor를 부착하면 풀리는 문제이긴 하지만 말이다). 안정적인 SLAM은 이러한 기하학적 모호함을 해결할 수 있어야할 뿐 더러, 혹시나 모호함 때문에 잘못 추정해도 문제를 발견하면 문제가 없는 상황으로 롤백을 한 후에 다시 제대로 계산을 할 수 있어야한다. 그리고 최대한 잘못 계산하는 것을 줄이기 위해 한번에 잘 계산하는 기능도 필요한데, 이 부분은 Part 1 글에서 설명했던 certifiable algorithm으로 다시 이어진다.</p>
<p>이제 세상이 고정되어있지 않다고 해보자. 세상이 부분적으로만 고정되어있다 - 예를 들어서, 바닥과 건물은 고정되어있고, 사람들은 걸어다니고, 자동차가 굴러다니며, 내 자동차도 굴러다닌다. 이 때 부터는 ‘고정된 물체’와 ‘움직이는 물체’와 ‘나’를 분리시키는 <strong>dynamic SLAM</strong>을 해야한다. <strong>동적 물체는 정적 환경과 분리되서 인식</strong>이 되야하는데, 이를 위해 semantic 정보를 사용하는 경우가 많다. 주로 ‘아마 이런 클래스는 움직일 확률이 높아’ 라는 방식으로 사람, 자전거, 자동차 등을 정적 환경과 분리해서 트랙킹 하기도 한다. 최신 연구들은 이러한 동적 물체들의 속도까지 추정하려는 연구가 진행되고 있다.</p>
<p>이제 세상이 조금씩 변화한다고 해보자. <strong>봄여름가을겨울이 지나면서 가로수의 잎이 떨어지고</strong>, 바닥에 쌓이고, 눈이 쌓이고… 뒤에는 건물이 무너지고 새로 지어지고 있다고 해보자. 실내의 환경이라면 <strong>가구의 위치를 옮기는 것</strong>도 포함될 수 있다. 이러한 <strong>시간에 따른 정적 환경의 변화는 다른 시간대에서 동일한 공간을 재관찰해야지만 검출</strong>할 수 있다. 시간에 따라 지도가 변화한다는건, SLAM으로 만드는 지도에 대한 representation도 변화할 수 있어야만 (i.e. 업데이트가 가능해야) 한다는 것이다. 그렇다면 <strong>지도를 생성할 때 어떤 방식으로 생성해야할 것인가?</strong> <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzc0ODcyMzc=">Rosen 2016 - Towards lifelong feature-based mapping in semi-static environments<i class="fa fa-external-link-alt"></i></span>에서는 map point가 지속적으로 존재하는지에 대해 질문하는 bayesian framework를 제안한다. <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzc4Nzg2ODA=">Krajnik 2017 - Fremen:Frequency map enhancement for long-term mobile robot autonomy in changing environments<i class="fa fa-external-link-alt"></i></span>에서는 지도 속에서 변화할 확률이 높은 부분을 Fourier analysis로 검출하는 방법도 제안한다. <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg1NDI3NTM=">Bore 2018 - Detection and tracking of general movable objects in large three-dimensional maps<i class="fa fa-external-link-alt"></i></span>에서는 particle filter를 이용해서 object들의 dynamics를 트랙킹하는 방법을 제안한다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDYuMTA4MDc=">Zeng 2020 - Semantic linking maps for active visual object search<i class="fa fa-external-link-alt"></i></span>에서는 landmark들 간의 semantic 정보에 대해 probabilistic spatial coupling이 가능한 semantic linking map 표현법을 제안한다. 이렇게 다양한 방법을 통해 환경 변화에 대한 연구가 진행이 되고 있는데, 이는 3D HD-Map을 이용해 주행하는 자율주행 차와 같이 안전 문제가 엮일 때 굉장히 중요한 문제가 된다.</p>
<p>이제 세상의 형태가 실시간으로 바뀐다고 해보자. <strong>Deformable environment</strong>는 환경이 말랑말랑하다는 전제를 가지고 있다. 아쉽게도 SLAM 분야에서 이 분야에 대한 연구는 그렇게 많이 진행되고 있지 않다. <span class="exturl" data-url="aHR0cHM6Ly9ncmFpbC5jcy53YXNoaW5ndG9uLmVkdS9wcm9qZWN0cy9keW5hbWljZnVzaW9uL3BhcGVycy9EeW5hbWljRnVzaW9uLnBkZg==">DynamicFusion<i class="fa fa-external-link-alt"></i></span> 같은 경우에는 RGB-D 카메라를 이용해서 deformable한 물체의 형태를 dense reconstruction하는 방법을 제안한다. 이 분야의 연구는 주로 의료 쪽 연구에서 하는 경우가 많은데, 예를 들어서 <span class="exturl" data-url="aHR0cHM6Ly9wdWJtZWQubmNiaS5ubG0ubmloLmdvdi8yMDg3OTM1Mi8=">사람의 심장이 두근두근 뛸 때 dense reconstruction을 하면서 형태를 추정하고 카메라의 위치를 추정하는 연구<i class="fa fa-external-link-alt"></i></span>가 있다. 보통 이런 연구에서는 motion 추정을 할 때 기본적인 model이 있어서 (e.g. 사람의 심장은 반복적인 모션 - cyclic motion을 하기 때문에, 이것을 잘 관찰해서 모델로 만들면 t-1, t, t+1 시점에서 어떤 움직임을 할지 추론할 수 있다), 이를 기반으로 형태 추정도 가능하다.</p>
<p> </p>
<hr>
<h1 id="Hierarchical-representation"><a href="#Hierarchical-representation" class="headerlink" title="Hierarchical representation"></a>Hierarchical representation</h1><img src="/20230215-rosen-2021/kimera.png" class="" title="kimera">

<p>기본적으로 SLAM을 돌리면 point cloud나 occupancy grid map이 나온다. Point cloud에서 모든 point는 동일한 중요성을 가지고, occupancy grid map에서 모든 grid cell은 동일한 중요성을 가진다. Point landmark가 1천만개가 쌓여도, 이 중 navigation에 중요한 point landmark가 어떤건지 알 수 없다. 이게 기본적인 SLAM이 가지는 문제 중 하나였다.</p>
<p>Navigation을 위해 중요한 정보만을 추출해야한다. 이를 위해 <strong>abstraction(추상화)**과 **hierarchy(계층구조)</strong> 개념이 생겼다. 예를 들어, 특정 부분의 point landmark들은 책상을 의미한다, 바닥을 의미한다, 벽을 의미한다, 소파를 의미한다, 이런것들로 추상화를 할 수 있다. 그리고 책상,바닥,벽,소파가 모여서 거실을 의미한다 -&gt; 거실과 화장실과 몇개의 방이 모여 하나의 층을 의미한다 -&gt; 여러 층이 모여 건물을 의미한다, 이런 것들로 계층 구조를 가질 수 있다. 이렇게 <strong>추상화와 계층구조를 통해 SLAM mapping의 확장성과 효율성이 극대화</strong> 된다. 주로 이런 구조를 spatial-semantic hierarchy라고 한다.</p>
<p>Spatial-semantic hierarchy는 사실 현대적인 SLAM 구성이 생기기 이전 - 즉 고전 로봇 맵핑에서 사용되던 <strong>topology model</strong>과 굉장히 유사하다. Topology model을 빠르게 이해하려면 자동차 내비게이션 지도를 생각하면 된다 - ‘다음 교차로에서 좌회전하세요. 그후 직진하다가 다음 교차로에서 우회전하세요’ 같이 metric한 개념이 없이 연결성에 대한 구조만 가지고 지도를 표현하는 방법이다. Topology model의 정확도는 로봇의 판단 과정에 (decision making, planning) 있어서 아주 중요한 역할을 하는데, 예를 들어 내비게이션에서 좌회전/우회전을 잘못하면 삥 돌아가는 것과 같은 효과를 낼 수 있다. Topological model은 <strong>강력하게 abstraction에만 초점을 둔 방법</strong>이라고 볼 수 있지만, <strong>엄청나게 가벼운 representation</strong>이라는 특징을 가지게 되었고 이러한 특징은 <strong>planning과 decision making에 엄청난 도움</strong>을 준다. 조금 더 개선 시킨 방법으로는 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDkuMDgxNTU=">Mu 2016 - Information-based active SLAM via topological feature graphs<i class="fa fa-external-link-alt"></i></span>에서 사용한 topological feature graph는 geometric landmark와 topology를 결합한 형태인데, graph의 vertex는 geometric feature를 의미하고 edge가 obstacle을 의미하는 방법을 사용한다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMTQzNjg=">Stein 2020 - Enabling topological planning with monocular vision<i class="fa fa-external-link-alt"></i></span>에서는 점점 topology graph를 강화시켜 planning task에 적합하게 만드는 방식을 제안하며, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDUuMTIyNTY=">Chaplot 2020 - Neural topological SLAM for visual navigation<i class="fa fa-external-link-alt"></i></span> 논문에서는 vertex는 location을 저장하고 edge에는 traversability를 저장하는 방법으로 topological graph 기법의 정점(?)을 찍었다. </p>
<p>Abstraction과 hierarchy를 동시에 취한 representation인 <strong>scene graph</strong>는 geometric한 성질과 topological한 성질을 모두 가지고 있다. 이 때문에 전체적으로 topology-only model보다는 훨씬 무겁지만, <strong>planning에 있어서는 topology를, localization에 있어서는 geometry를 이용할 수 있다는 장점</strong>이 있다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDIuMDYyODk=">Rosinol 2020 - 3D dynamic scene graph: Actionable spatial perception wiht places, objects, and humans<i class="fa fa-external-link-alt"></i></span>나 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMDI1Mjc=">Armeni 2019 - 3d scene graph: A structure for unified semantics, 3d space, and camera<i class="fa fa-external-link-alt"></i></span> 논문은 geometric-&gt;object-&gt;places 와 같이 여러개의 spatial/semantic layer를 통해 abstraction과 hierarchy를 구현하고, 적절한 object class에 맞춰서 dynamics 정보까지 갖춤으로써 scene graph를 구성한다.</p>
<p> </p>
<hr>
<h1 id="딥러닝-중심의-representation"><a href="#딥러닝-중심의-representation" class="headerlink" title="딥러닝 중심의 representation"></a>딥러닝 중심의 representation</h1><p>대부분의 semantic SLAM을 보면, 뉴럴넷의 결과를 그대로 믿고 받아쓰는 경우가 많다. 멀티뷰 데이터를 누적하다보면 조금씩 오차가 생길텐데, 그정도는 SLAM backend가 어느정도 보정을 해줄거라는 희망을 가지고 하는 것이다. 근데 사실 이건 소위 ‘블랙박스’, ‘될때까지 튜닝하기’, ‘잘 되길 기도하기’ 방법과 다를 바가 없다. 진정 하나의 perception 시스템으로써 동작하길 바란다면, 로봇 네비게이션을 하는 그 순간에도 새로운 파라미터를 학습해서 개선한다던가, 또는 뉴럴넷의 결과가 보정되야하지 않을까? SLAM이 인식하는 scene geometry와 robot motion을 기반으로 뉴럴넷을 학습할 수도 있을 것이다.</p>
<p>실제로 SLAM의 결과를 뉴럴넷 학습에 사용하기도 한다. <strong>Self-supervised learning</strong> 기법은 뉴럴넷을 학습에 사용되는 데이터에서 추가적인 정보를 구해내고 그것을 ground truth label 및 학습에 도움되는 auxiliary data로 사용함으로써 학습을 진전시키는 방법이다. SLAM으로 추정한 global representation은 충분히 다양한 뉴럴넷 네트워크를 학습하는데에 필요한 데이터로써 사용될 수 있다.</p>
<p>딥러닝 모델로써 scene에 대한 coarse한 정보를 제공하고, SLAM이 이를 받아 최적화함으로써 기존의 뉴럴넷 결과보다 더욱 좋은 결과를 이끌어낼 수도 있다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDMuMDY0ODI=">SceneCode<i class="fa fa-external-link-alt"></i></span>와 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDQ0ODU=">NodeSLAM<i class="fa fa-external-link-alt"></i></span>에서는 object들의 성질을 variational auto-encoder를 사용해서 압축한 vector의 값들에 대해 SLAM 최적화를 함으로써 reconstruction을 수행한다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDEuMDUwNDk=">DeepFactors<i class="fa fa-external-link-alt"></i></span>에서는 depth map을 압축해서 SLAM 최적화 시 depth map 최적화도 함께 수행한다.</p>
<img src="/20230215-rosen-2021/niceslam.png" class="" title="niceslam">

<p>뉴럴넷이 backpropagation을 수행하며 내부 파라미터 최적화를 하듯이, SLAM의 backend를 differentiable하게 만들려고 하는 시도도 있다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMTA2NzI=">GradSLAM<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDYuMDQ4MDc=">BANet<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDguMTA4Njk=">DROID-SLAM<i class="fa fa-external-link-alt"></i></span>과 같이 SLAM 전체를 differentiable하게 만들려고 하는 시스템부터, 최근에는 NeRF도 함께 사용하는 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIyMTAuMTM2NDE=">NeRF-SLAM<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMDMuMTIzNTI=">iMAP<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDIuMDE4Mzg=">vMAP<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIxMTIuMTIxMzA=">NICE-SLAM<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDIuMDM1OTQ=">NICER-SLAM<i class="fa fa-external-link-alt"></i></span> 등이 있다 (NeRF 관련 논문들은 이 글이 참조하고 있는 논문인 Rosen 2021에는 없다. 이 논문이 나오고 나서 공개된 논문이기 때문).</p>
<img src="/20230215-rosen-2021/lmnav.png" class="" title="lmnav">

<p>마지막으로 완전 다른 노선으로 가고 있는 연구도 있다. 바로 <strong>SLAM도, 지도도 필요 없이 navigation 자체를 학습하는 방법</strong>이다 (이 글을 적고있는 저자의 마음은 찢어집니다… 슬램 소중해…). 센서 값을 인풋으로 받아 단일 네트워크를 거쳐 아웃풋으로 navigation의 결과를 내는 <strong>End-to-end learning</strong> 기법이다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE3MDYuMDk1MjA=">Zhang 2017 - NeuralSLAM: Learning to explore with external memory<i class="fa fa-external-link-alt"></i></span>는 그래도 occupancy map을 사용해서 navigation을 하지만, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDQuMDAxNjg=">Mirowski 2018 - Learning to navigate in cities without a map<i class="fa fa-external-link-alt"></i></span> 연구에서는 아예 지도 자체를 없애버렸다. 위에서 이야기했던 Neural topological SLAM 연구도 사실 지도 없이 navigation을 하는 연구이다. <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIyMDcuMDQ0Mjk=">LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action<i class="fa fa-external-link-alt"></i></span>에서는 vision-language model을 이용해서 language prompt로 action 커맨드로 받고, visual observation으로 topological 정보만 인식해 navigation을 하는 연구도 있다.</p>
<p> </p>
<hr>
<h1 id="앞으로-연구할-거리는-어떤-것들이-있는가-정리"><a href="#앞으로-연구할-거리는-어떤-것들이-있는가-정리" class="headerlink" title="앞으로 연구할 거리는 어떤 것들이 있는가? + 정리"></a>앞으로 연구할 거리는 어떤 것들이 있는가? + 정리</h1><p>다양한 환경에서 SLAM을 운용하기 위해서는 다양한 센서들이 필요하다. 여러 센서를 조합했을 때 범용적으로 사용할 수 있는 representation이 있을텐데, 기존의 카메라, 라이다, IMU, 소나를 넘어서 이벤트 카메라, 라이트필드 카메라, 압력/토크 센서를 함께 표현할 수 있는 representation이 필요하다.</p>
<p>더욱 다양한 환경을 표현할 수 있는 hierarchical representation도 필요하다. 기존 골자를 잡아두고 환경에 따라 변화할 수 있는 flexibility가 있는 것이 좋을 것이다. 그리고 다양한 환경에 있을 수 있는 개념들에 대한 abstraction이 필요하다.</p>
<p>Data-driven 방식이 발전하면서 semantic 정보를 얻어내는 방식도 model fitting에서 뉴럴네트워크로 바뀌었듯이, 다른 정보들도 뉴럴네트워크로 충분히 얻어낼 수 있을 것이다. 그렇다면 어떤 정보들을 뉴럴넷으로 학습해야할까? 어떤 정보들을 prior로 사용해야할까? 물론 환경에 따라서 무엇을 학습할지는 task-dependent하겠다.</p>
<p>Geometric representation과 topological representation 사이를 자유롭게 오갈 수 있는 방법도 필요하다. 우리가 로봇을 서울에서 뉴욕으로 보내려면 어떤 representation을 사용해야할까? 아마 topological하게 플래닝을 하다가, 어느 순간에는 geometric하게 바뀌어야할 것이다. 이렇게 적절한 시기에 topological constraint와 geometric constraint를 섞을 수 있는 건 navigation에 있어서 큰 도움이 될 것이다. 하지만 geometric model은 메모리를 엄청나게 많이 잡아먹기 때문에, 이를 효율적으로 저장할 방법에 대해 깊게 고민해야한다.</p>
<p>Semantic 정보를 추출할 때 뉴럴네트워크로도 추출하지만, low-level feature를 합성해서 상위단의 개념으로 취합할 수 있다. 예를 들어, ‘바닥+벽+천장을’ 합성하면 ‘방’이 나올 수 있을 것이다. ‘4개의 다리 + 상판’을 취합하면 ‘책상’이 나올 수도 있다. 이렇게 다양한 shape과 appearance를 가지고 있는 물체들에 대해 semantic 정보를 추출해내는 방법에 깊게 고민해야한다.</p>
<p>결국 답은 3개의 키워드로 모인다.</p>
<ul>
<li>Long-term autonomy를 어떻게 달성할 것인가?<ul>
<li>어떻게 해야 SLAM이 실제 세상에서 안정적으로 작동할것인가?</li>
</ul>
</li>
<li>Lifelong map learning을 어떻게 달성할 것인가?<ul>
<li>시간이 지나면서 mapping 정확도가 계속 개선되는 시스템을 만들 수 있는가?</li>
<li>변화하는 환경에도 적응하는 시스템을 만들 수 있는가?</li>
</ul>
</li>
<li>SLAM과 딥러닝을 어떻게 잘 섞을 것인가?<ul>
<li>기존의 model-based state estimation의 장점을 취하면서, 새로운 semantic representation을 적절하게 섞을 수 있는가?</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Semantic SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Qin 2020 - AVP-SLAM - Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot</title>
    <url>/20230218-qin-2020-avpslam/</url>
    <content><![CDATA[<h1 id="논문-요약"><a href="#논문-요약" class="headerlink" title="논문 요약"></a>논문 요약</h1><p>이 논문에서는 지하 주차장에서 Semantic 정보로 이뤄져있는 지도를 만들고, 관리하고, visual localization 및 planning 목적으로 사용하는 프레임워크를 제안한다. </p>
<p>지도는 주차장 노면에서 검출 가능한 road marker 및 주변 환경에서 검출 가능한 물체들로 이뤄져있고, semantic 정보만을 포함하기에 굉장히 적은 용량을 요구한다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%202.34.23%20PM.png" class="" title="Semantic map of underground parking lot">

<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%202.34.40%20PM.png" class="" title="Planning and control">


<p> </p>
<hr>
<h1 id="시스템-오버뷰"><a href="#시스템-오버뷰" class="headerlink" title="시스템 오버뷰"></a>시스템 오버뷰</h1><img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%202.36.52%20PM.png" class="" title="system overview">

<p>시스템은 4가지 섹션으로 나눠져있다.</p>
<ol>
<li>4개의 카메라로 BEV 이미지를 만든 후 딥러닝 segmentation을 수행한다.</li>
<li>2개의 Wheel encoder + IMU 퓨전을 통해 odometry를 추정한다</li>
<li>Odometry + BEV seg 이미지를 통해 로컬 맵핑 및 글로벌 맵핑을 수행한다.</li>
<li>글로벌 맵 + Odometry + BEV seg 이미지를 EKF로 퓨전해서 visual localization을 수행한다.</li>
</ol>
<p> </p>
<hr>
<h1 id="IPM-이미지-생성"><a href="#IPM-이미지-생성" class="headerlink" title="IPM 이미지 생성"></a>IPM 이미지 생성</h1><img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.05.05%20PM.png" class="" title="IPM">

<p>전후좌우로 바닥을 바라보는 Fisheye 카메라가 4개가 있다.</p>
<p>Ground plane (i.e. z=0)에 이미지를 projection하는 IPM (Inverse perspective mapping)을 함으로써 Bird’s eye view (BEV) 이미지를 생성한다. IPM 수식은 상당히 스탠다드한 방식을 사용하는 것 처럼 보이는데, 사실 구현에는 굉장히 많은 트릭이 들어갈 것이다.</p>
<p>예를 들어, ground plane을 알아내는것 자체가 쉽지 않다. 자동차가 완벽하게 평평한 면에 있다고 하고 카메라 intrinsic/extrinsic을 정확하게 알고있다면 이론적으로 완벽한 BEV가 나올 것이다. 하지만 여기에는 두가지 문제가 생긴다. </p>
<p>첫번째 문제는, fisheye 카메라간의 overlap 문제이다. 인접한 카메라들끼리는 시점에 대한 오버랩이 분명히 발생하고, 바닥에 projection 했을 때 서로 다른 데이터가 동일한 공간에 projection이 될 것이다. 그러면 어떤 카메라의 데이터를 더 신뢰해야하는가? 왼쪽 카메라의 이미지를 믿어도 되고, 오른쪽 카메라의 이미지를 믿어도 되고, 왼/오 카메라의 이미지를 블렌딩 하는 것도 방법이다. 그런데, 논문에서는 어떤 방법을 사용했는지 말해주지 않는다.</p>
<p>두번째 문제는, 자동차가 운행하면서 extrinsic이 틀어진다는 점이다. 동승자가 자동차에 타면서 무게중심이 바뀌거나 하면 카메라가 고정된 위치를 벗어날 수도 있고,자동차가 가속/감속을 하거나 코너링을 돌면서 무게중심이 바뀌기도 한다. 카메라의 extrinsic이 바뀌면 카메라~바닥면에 대한 기하학적 관계도 바뀐다. 이걸 잡는 방법으로는 뭐 road depth estimation을 한다던지, IMU를 통해 gravity 방향을 받아서 바닥면의 위치를 보정할 수도 있겠다. 하지만 논문에서는 어떤 방법을  사용했는지 말해주지 않는다.</p>
<p> </p>
<hr>
<h1 id="Semantic-feature-detection"><a href="#Semantic-feature-detection" class="headerlink" title="Semantic feature detection"></a>Semantic feature detection</h1><p>Modified U-Net 기반의 CNN을 사용한다고 한다. 사실 BEV 이미지에서 segmentation만 할 수 있다면 어떤 네트워크를 사용하던 상관 없다.</p>
<p>Output class로는 lane, parking lines, guide signs, speed bumps, free space, obstacles, walls 가 있다. 여기서 parking lines, guide signs, speed bumps만 visual localization에 사용한다.</p>
<p>또, parking line을 뽑을 때 parking spot detection task도 함께 수행한다. 보통 parking spot detection 논문들을 보면 parking line을 뽑는 알고리즘도 함께 있다 (i.e. 자주 사용되는 알고리즘이다).</p>
<p> </p>
<hr>
<h1 id="Local-mapping"><a href="#Local-mapping" class="headerlink" title="Local mapping"></a>Local mapping</h1><p>BEV 이미지에서 segmentation이 끝나면, parking lines, guide signs, speed bumps에 대한 정보만 3D space로 lifting 한다. 이는 우리가 생성한 IPM 이미지에 대한 synthesized intrinsic matrix를 inverse 곱 (i.e. unprojection lifting) 하면 된다. </p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.18.49%20PM.png" class="" title="eq3">

<p>이 결과, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.804ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3007.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(850, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1294.7, 0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1784.7, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2229.3, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2729.3, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>가 나오게 될텐데 (z=0인 이유는 바닥면에 있기 때문에 높이가 0이다), 이를 IMU+Wheel 기반의 odometry로 추정한 6dof 모션에 대해 적용하면 World coordinate 중심에서 3D point를 표현한 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.5ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4641.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(278, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><g data-mml-node="mo" transform="translate(1406.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1851, 0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><g data-mml-node="mo" transform="translate(2897.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3341.9, 0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g><g data-mml-node="mo" transform="translate(4363.2, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container> 가 나오게 된다. (IMU+Wheel 기반의 odometry는 추후 섹션에서 추가 설명한다.)</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.18.53%20PM.png" class="" title="eq4">

<p>이 3D point들은 그냥 다 하나의 맵에 때려넣는다.</p>
<p>30m 단위로 local map을 관리한다.</p>
<p> </p>
<hr>
<h1 id="Loop-detection"><a href="#Loop-detection" class="headerlink" title="Loop detection"></a>Loop detection</h1><img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.46.55%20PM.png" class="" title="loop detection">

<p>아무리 IMU + Odometry를 잘 퓨전해도 drift는 누적된다. 누적되는 drift를 해소하기 위해 loop closure를 해줘야한다.</p>
<p>Loop closure를 하기 위해서는 loop detection 기법이 필요하다. 여기서는 가장 최신의 local map의 위치 주변에 존재하는 과거의 local map들에 대해 ICP (iterative closest point) 기법을 사용해서 relative pose를 구하는 방식을 제안한다.</p>
<p>ICP를 성공했을 경우 relative pose를 구할 수 있고, 이를 loop constraint 삼아서 global pose graph optimzation이 가능해진다. 최적화를 수행하고나면 drift를 해소할 수 있다.</p>
<p> </p>
<hr>
<h1 id="Global-optimization"><a href="#Global-optimization" class="headerlink" title="Global optimization"></a>Global optimization</h1><p>Loop detection이 완료되면 스탠다드한 pose graph optimization을할 수 있다. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.826ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 807 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="58" d="M324 614Q291 576 250 573Q231 573 231 584Q231 589 232 592Q235 601 244 614T271 643T324 671T400 683H403Q462 683 481 610Q485 594 490 545T498 454L501 413Q504 413 551 442T648 509T705 561Q707 565 707 578Q707 610 682 614Q667 614 667 626Q667 641 695 662T755 683Q765 683 775 680T796 662T807 623Q807 596 792 572T713 499T530 376L505 361V356Q508 346 511 278T524 148T557 75Q569 69 580 69Q585 69 593 77Q624 108 660 110Q667 110 670 110T676 106T678 94Q668 59 624 30T510 0Q487 0 471 9T445 32T430 71T422 117T417 173Q416 183 416 188Q413 214 411 244T407 286T405 299Q403 299 344 263T223 182T154 122Q152 118 152 105Q152 69 180 69Q183 69 187 66T191 60L192 58V56Q192 41 163 21T105 0Q94 0 84 3T63 21T52 60Q52 77 56 90T85 131T155 191Q197 223 259 263T362 327T402 352L391 489Q391 492 390 505T387 526T384 547T379 568T372 586T361 602T348 611Q346 612 341 613T333 614H324Z"></path></g></g></g></g></svg></mjx-container> 은 과거의 모든 pose를 의미하고, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.979ex" xmlns="http://www.w3.org/2000/svg" width="4.809ex" height="2.506ex" role="img" focusable="false" viewBox="0 -674.8 2125.8 1107.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="TeXAtom" transform="translate(465, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(465, -295.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(639, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1000, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>는 local map t와 t+1 사이의 상대적인 포즈 변화를 의미한다. 일반적인 Gauss-Newton 최적화로 풀 수 있다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.18.11%20PM.png" class="" title="eq5">

<p>Pose graph 최적화가 끝나면, 3D space로 lifting한 semantic landmark들을 최적화된 pose에 맞게 다시 그려넣으면 globally consistent한 semantic map을 만들 수 있다.</p>
<p> </p>
<hr>
<h1 id="Localization"><a href="#Localization" class="headerlink" title="Localization"></a>Localization</h1><img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%204.05.37%20PM.png" class="" title="loc">

<p>실시간 visual localization도 가능하다. 이전 과정과 동일하게 4채널 카메라 이미지를 가지고 IPM 이미지를 만든 후 BEV segmentation을 진행한다. BEV seg 이미지를 3D space로 lifting하면 3D landmark가 나올 것이다. 이를 우리가 만들었던 globally consistent한 semantic map에 대해 ICP 알고리즘 (iterative closest point 알고리즘)을 수행함으로써 6dof pose를 구할 수 있다.</p>
<p>논문에서는 ICP를 할 때 initial guess가 중요하다고 한다. 물론 least squares optimization 형태의 ICP 알고리즘은 초기값 설정이 굉장히 중요하지만, 여기서는 아마 그런 의미로 이야기하기보다는, 주차장 특성 상 landmark의 형태가 반복되는 것이 많기 때문에 초기 위치를 정확하게 잡아야 위치 탐색을 잘 할 것이라는 뜻으로 볼 수 있다.</p>
<p>즉, 주차장에서 초기 위치를 잡는 방법이 필요하다는 것이다. 논문에서는 2가지 방법을 제안하는데, 첫번째 방법이 스탠다드한 방법이고 사실 제일 말이 되는 방법이며, 두번째 방법은 feasibility가 거의 없다고 봐야한다.</p>
<p>첫번째 방법은, 주차장의 입구에서 visual localization을 시작한다는 전제를 까는 것이다. 그러면 정해진 local map과 매칭을 하는 것이기 때문에, 정확하게 global pose를 잡고 시작할 수 있을 것이다. 이 방법이 말이 되는 이유는, 주로 Auto valet parking (i.e. AVP… 논문의 이름과 같은)을 만드는 회사들에서 요구하는 시나리오가 ‘고객은 주차장 입구에서 내리고, 자동차는 알아서 빈 곳을 찾아 주차를 하는 시스템’이기 때문이다. 즉, 고객은 무조건 자동차를 주차장 입구에 가져다놔야할 의무가 있다. 이는 기술적 시나리오와 잘 맞기도 한다.</p>
<p>두번째 방법은, 실외에서 차를 끌고 오다가 주차장 입구에서 겨우 닿는 GPS 신호를 통해 초기 위치를 인식하는 방법이다. GPS랑 주차장의 semantic map을 어떻게 연결할지도 얘기가 전혀 없고, 지하주차장으로 들어오면서 날뛰는 GPS 신호를 어떻게 컨트롤 하겠다는 얘기도 없다. 그래서 이 방법은 가볍게 무시해도 될 것같다.</p>
<p>논문에서는 안정적으로 localization을 유지하는 방법도 이야기한다. Semantic feature를 사용하는 localization 방법 특성상, semantic object가 주변에 없을 때 pose tracking이 끊길 가능성이 높다. 실제로 주차장에서 코너를 돌거나 층간 이동을 할 때는 lane이 그려져있지 않거나 parking spot이 주변에 없는 경우들이 있다. 이 경우, 기본적으로 localization 프레임워크가 EKF를 통해 Map + Odometry(IMU+Wheel) 가 연결되어있기 때문에, Map 정보가 없을 경우 (i.e. semantic feature가 안 보일 경우) Odometry를 신뢰하며 이동 값을 추정할 수 있다.</p>
<p> </p>
<hr>
<h1 id="실험-결과"><a href="#실험-결과" class="headerlink" title="실험 결과"></a>실험 결과</h1><p>주로 비교 대상은 Odometry vs ORB-SLAM2 vs AVP-SLAM 이다.</p>
<p>우선 정확도를 보면 ORB-SLAM2와 AVP-SLAM이 가장 우세하다. 근데 여기서 볼 점은, Odometry도 상당히 정확하다는 점이다. AVP-SLAM은 odometry로 motion prior를 갖기 때문에, 기존의 odometry보다는 더 좋은 성능을 보이겠지만 기본적인 odometry가 좋지 않다면 어떤 결과가 나올지 보장할 수 없다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%203.55.05%20PM.png" class="" title="accuracy">

<p>그렇다면 AVP-SLAM을 왜 쓰는가? 사실 Semantic SLAM을 사용하는건 기존의 geometric SLAM 보다 훨씬 더 큰 이점이 있다.</p>
<p>첫번째로는 semantic feature를 검출하는 방법이 data-driven 방식이기 때문에 appearance 변화에 아주 강인하다는 점이다. 그렇기 때문에, ORB-SLAM 같은 경우 조명이 조금만 바뀌어도 정확도가 뚝뚝 떨어지는데에 비해, semantic SLAM의 정확도는 유지될 확률이 높다. 하지만 이건 조명이 바뀌는 경우에 대한 것이고, 지하주차장의 조명이 바뀔 일은 보통 잘 없다.</p>
<p>두번째는 우리가 semantic feature를 선택할 때 domain-specific하게 ‘무조건 static’한 것을 고를 수 있으며, 이는 SLAM기반 reconstruction에 아주 좋은 assumption을 줄 수 있다는 점이다. 주차장은 생각보다 아주 dynamic한 곳이다. 주차되어있는 차는 정적인 것 같지만, 몇시간 후면 주인이 돌아와 차를 뺐을 수도 있다. 비슷하게 생긴 차가 들어와서 repetitive local feature도 생길 수 있다. 그에 비해, parking line이나 guide sign은 절대 변하지 않는다. 이 덕분에 AVP-SLAM은 visual lcoalization을 할 때 아주 높은 Recall-rate를 가질 수 있다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%204.01.14%20PM.png" class="" title="recall rate">

<p>세번째로는 semantic feature를 어떻게 표현하냐에 따라서 최적화가 엄청 쉬워지기도 하고 용량이 줄어들 수도 있다는 점이다. 기존의 VSLAM은 포인트 클라우드 형태의 지도를 가지며, point landmark 하나마다 다수의 descriptor를 담고 있기 때문에 용량을 많이 차지한다. Semantic SLAM은 landmark geometry + class label만 가지는 경우가 많아 굉장히 가볍다. 어떻게 표현하냐에 따라 엄청 가벼워질 수 있다. AVP-SLAM은 사실 지도 용량 최적화는 전혀 하지 않았다 - 오히려 너무 대충한 느낌이다. 그럼에도 불구하고 ORB-SLAM보다 용량이 훨씬 작다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%204.03.39%20PM.png" class="" title="map size">

<p>Parking space detection까지 해서 좋은 geometric map을 가졌다면, 이를 기반으로 path planning + control까지 해서 AVP를 구현할 수 도 있을 것이다.</p>
<img src="/20230218-qin-2020-avpslam/Screenshot%202023-02-28%20at%204.04.46%20PM.png" class="" title="path planning">

<h1 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h1><p>Literature review 섹션에서 Traditional VSLAM과 Road-based localization 기법을 소개한다.</p>
<p>Traditional VSLAM은 다른 VSLAM 논문들에서도 충분히 볼 수 있는 내용이라 이번 리뷰에서는 스킵한다.</p>
<p>Road-based localization에서는 다음과 같은 논문을 추천한다.</p>
<ul>
<li><a href="">Lu 2017 - Monocular localization in urban environments using road marking</a> - Road marker 기준 non-linear optimization 기법 사용</li>
<li><a href="">Regder 2015 - Submap-based slam for road markings</a> - Odometry 기반 포즈 추정 + 이미지 기반 lane 검출 -&gt; Local grid lane map 생성</li>
<li><a href="">Jeong 2017 - Road-slam: Road marking based slam with lane-level accuracy</a> - Road marker classification</li>
</ul>
<p> </p>
<hr>
<h1 id="관련-유투브-영상"><a href="#관련-유투브-영상" class="headerlink" title="관련 유투브 영상"></a>관련 유투브 영상</h1><div class="video-container"><iframe src="https://www.youtube.com/embed/0Ow0U-G7klM" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Semantic SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Qin 2021 - RoadMap - A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving</title>
    <url>/20230218-qin-2021-roadmap/</url>
    <content><![CDATA[<h1 id="논문-요약"><a href="#논문-요약" class="headerlink" title="논문 요약"></a>논문 요약</h1><p>이 논문에서는 Semantic 정보로 이뤄져있는 지도를 만들고, 관리하고, visual localization 목적으로 사용하는 프레임워크를 제안한다. </p>
<p>지도는 노면에서 검출 가능한 road marker로 이뤄져있고, semantic 정보만을 포함하기에 굉장히 적은 용량을 요구한다.</p>
<img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%2012.57.46%20PM.png" class="" title="RTK+Wheel">

<p> </p>
<hr>
<h1 id="시스템-오버뷰"><a href="#시스템-오버뷰" class="headerlink" title="시스템 오버뷰"></a>시스템 오버뷰</h1><img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%2012.46.45%20PM.png" class="" title="system overview">

<p>시스템은 3가지 컴포넌트로 나눠진다.</p>
<ol>
<li>고성능 센서를 탑재한 지도 생성용 자동차</li>
<li>기준 맵을 만드는 중앙 클라우드 서버</li>
<li>지도를 이용해 위치를 추정하는 양산 자동차</li>
</ol>
<p> </p>
<hr>
<h1 id="로컬-지도-생성"><a href="#로컬-지도-생성" class="headerlink" title="로컬 지도 생성"></a>로컬 지도 생성</h1><p>RTK-GPS, wheel odometry, front facing camera를 가진 ‘데이터 수집용 차량’이 운행을 하면서 지도를 만든다.</p>
<p>RTK-GPS + Wheel odometry로 로봇 포즈를 구하고, Front facing camera 이미지를 이용해 bird eye view segmentation 이미지를 생성한다. 이 정보를 지도 관리 서버로 보낸다.</p>
<h2 id="RTK-GPS-Wheel-odometry를-이용한-포즈-추정"><a href="#RTK-GPS-Wheel-odometry를-이용한-포즈-추정" class="headerlink" title="RTK-GPS + Wheel odometry를 이용한 포즈 추정"></a>RTK-GPS + Wheel odometry를 이용한 포즈 추정</h2><img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%2012.55.57%20PM.png" class="" title="RTK+Wheel">

<img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%2012.53.40%20PM.png" class="" title="RTK+Wheel factor graph">

<p>RTK-GPS는 정확한 absolute pose를 구해줄 수 있지만, 종종 도심에서는 GPS multi-path 문제로 인해 RTK-GPS 조차 신호가 흔들리기도 한다. 저자는 언제나 정확한 포즈를 추정할 수 있도록 다음과 같은 알고리즘을 설계했다: RTK-GPS의 정보가 흔들리지 않을 때는 RTK-GPS + Wheel odometry의 tightly-coupled 추정 값을 사용하고, RTK-GPS의 정보가 흔들릴 때는 RTK-GPS 데이터를 제거해서 wheel odometry만 믿는다. 이를 위해 GPS와 wheel-odometry의 residual을 최소화하는 cost function을 제안한다 (굉장히 스탠다드한 방법이다.)</p>
<p>사실 이 논문은 RTK-GPS를 사용함으로써 시스템이 굉장히 간단해졌다. RTK-GPS는 absolute pose를 얻을 수 있기 때문에 다음 단계인 클라우드 서버에서 map merging을 수행할 때 multi-session map들 사이의 overlap 및 relative pose를 구하지 않고 단순 이어붙히기로 작업을 완료할 수 있다.</p>
<p>하지만 필자는 이러한 시스템이 적절치 않다고 생각하는데, RTK-GPS 센서는 굉장히 비싸고 커버리지도 작기 때문에 전혀 scalable 한 방식이 아니라고 생각되며, RTK-GPS+Wheel 방식의 odometry가 아닌 visual odometry로 추정하고 relocalization 기능을 사용해서 map merging을 하는 것이 적절하다고 생각한다. 아마 실제 저자들도 이것을 알고 있겠지만, 내부 기술을 전부 노출할 수 없으니, 대안으로 RTK+Wheel 방식으로 위장을 했다고 생각한다. 따라서 이번 리뷰에서는 RTK+Wheel 퓨전 관련 수식을 설명하지 않는다.</p>
<h2 id="BEV-segmentation"><a href="#BEV-segmentation" class="headerlink" title="BEV segmentation"></a>BEV segmentation</h2><img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%201.31.50%20PM.png" class="" title="BEV seg">

<p>Front facing camera 이미지에서 CNN 기반 semantic segmentation 모델을 돌려 segmentation image를 얻는다. 특별히 어떤 모델을 사용했는지는 나오지 않는다.</p>
<p>이후 inverse perspective mapping (IPM) 기법을 사용해서 이미지를 바닥면에 projection 한다. 이를 하기 위해서는 사전에 캘리브레이션된 camera intrinsic parameter와 extrinsic을 알고 있어야한다. IPM을 수행하면 엄청 멀리까지 맵핑되는데, 여기서 차량 전방 12 m x 8 m 만 맵핑을 한다고 한다. 이러한 IPM 기법은 저자의 이전 논문인 AVP-SLAM에서도 사용했던 방법이다.</p>
<p>사실 IPM을 제대로 하기가 굉장히 어렵다. IPM을 제대로 한다는건 제대로 바닥면을 추정했다는건데, 이를 위해서는 1. 바닥면이 완벽하게 평평하거나, 2. 바닥면이 울퉁불퉁해도 gravity 방향을 알고있어야한다. 완벽하게 평평한 바닥면은 없고, 또 차가 가속/감속을 하거나 코너를 돌 때 중심이 바뀌기 때문에 제대로 된 projection이 안되는 경우가 많다. 보통 IMU를 탑재한 자동차들만 이것이 가능한데, 저자 이름을 찾아 중국 블로그까지 스토킹해서 들어가본 결과 Huawei의 프리미엄 자동차 라인에 대해 설명하는걸 보니 그정도 되는 차에 IMU가 들어갈 것을 추정할 수 있다. IMU가 없는 차에서는 아마 IPM이 굉장히 흔들릴 가능성이 높다.</p>
<h2 id="센서-데이터-취합-gt-로컬-지도-생성"><a href="#센서-데이터-취합-gt-로컬-지도-생성" class="headerlink" title="센서 데이터 취합 -> 로컬 지도 생성"></a>센서 데이터 취합 -&gt; 로컬 지도 생성</h2><p>RTK + Wheel 포즈 그래프 최적화를 통해 정확한 포즈를 구했다. Seg + CNN을 통해 BEV seg 이미지도 있다. </p>
<p>포즈마다 BEV 이미지가 있을텐데, 이를 전부 덧대어서 그린다. 그런데, 덧대어서 그리다보면 segmentation noise로 인해 똑같은 공간에서 다른 class label이 나올 수 있다 - 예를 들어, 하나의 공간에서 road/road/road/road/lane/lane/road/road… 이렇게 값이 나올 수가 있다. 이 문제를 해결하기 위해 voting scheme을 이용한다. 0.1 x 0.1 x 0.1 m 단위로 scene을 쪼갠 후, 해당 voxel에서 가장 많이 나타난 class label을 최종 class label로 삼는 방식이다.</p>
<p>위 방법은 아마 semantic reconstruction을 하는 가장 쉬운 방법일 것이다. 보통 다른 논문들에서는 probabilistic fusion 또는 bayesian 3D update 등을 사용한다. 여기서는 왜 그런 방법을 쓰지 않을까? 이유는 두가지로 생각할 수 있을텐데, 아마 RTK+Wheel이 너무 정확해서 굳이 그런 알고리즘을 쓸 필요가 없거나, 또는 기술을 유출하기 싫어서 위장한 것이라고 볼 수 있다.</p>
<p> </p>
<hr>
<h1 id="클라우드-서버에서-지도-취합-압축"><a href="#클라우드-서버에서-지도-취합-압축" class="headerlink" title="클라우드 서버에서 지도 취합 + 압축"></a>클라우드 서버에서 지도 취합 + 압축</h1><p>지도 관리 서버에서는 여러 ‘데이터 수집용 자동차’에서 받은 BEV segmentation 정보를 수집한다. 수집한 BEV segmentation을 함께 수집한 로봇 포즈 정보를 기반으로 global map에 깐다. 여기서 contour를 계산해서 중요한 정보만 남긴다.</p>
<h2 id="지도-취합"><a href="#지도-취합" class="headerlink" title="지도 취합"></a>지도 취합</h2><p>데이터를 수집한 차량은 클라우드 서버에 semantic 지도를 보낸다. 보낼 때, 데이터 송수신량을 줄이기 위해 ‘공간을 차지하는 부분’에 대한 정보만 보낸다.</p>
<p>클라우드 서버는 다수의 차량에게서 다수의 지도를 받는다. 그리고 global map에 전부 이 지도를 덧대어서 그린다. 이전 단계와 동일하게, 0.1 x 0.1 x 0.1 m 단위로 scene을 쪼갠 후, 최종 class label을 voting scheme을 통해 정한다.</p>
<h2 id="지도-압축"><a href="#지도-압축" class="headerlink" title="지도 압축"></a>지도 압축</h2><img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%201.49.02%20PM.png" class="" title="Map compression">

<p>이렇게 만들어진 global map을 다시 운행중인 자동차로 보내기에는 너무 데이터가 클 수 있다. 지도를 압축해야하는데, 여기서 road marker에 대해 contour를 추출해서 정보를 압축할 수 있다.</p>
<p>3D 지도에서 top view만 추출한 후, contour를 추출해서 새로운 global map을 만든다. 어떻게 contour를 추출하는지, 어떤 자료구조로 저장하는지는 또 설명하지 않았다.</p>
<p> </p>
<hr>
<h1 id="서비스-차량-배포-및-지도-사용"><a href="#서비스-차량-배포-및-지도-사용" class="headerlink" title="서비스 차량 배포 및 지도 사용"></a>서비스 차량 배포 및 지도 사용</h1><p>압축이 끝나 가벼워진 지도를 서비스 중인 자율주행 차량에 보낸다. </p>
<p>서비스 중인 자율주행 차량은 저가형 카메라로 얻은 BEV segmentation 정보와 지도 정보를 매칭해서 자신의 위치를 추정한다. 추정한 위치와 wheel odometry 정보를 extended kalman filter로 퓨전해 보다 정확한 위치를 추정한다.</p>
<h2 id="지도-압축풀기"><a href="#지도-압축풀기" class="headerlink" title="지도 압축풀기"></a>지도 압축풀기</h2><p>중앙 클라우드 서버는 contour만 있는 지도를 현재 운행중인 차량에 송신한다. 운행중인 차량은 RTK와 같은 고급 센서는 없다. 아마 front facing camera와 wheel 정보만 있을 것이다.</p>
<p>차량은 contour 지도를 수신한 후, contour 내부의 빈 곳을 채워넣는 작업을 한다. 이로써 다시 dense한 semantic map을 만들었다.</p>
<h2 id="위치-추정"><a href="#위치-추정" class="headerlink" title="위치 추정"></a>위치 추정</h2><img src="/20230218-qin-2021-roadmap/Screenshot%202023-02-28%20at%201.55.15%20PM.png" class="" title="Vis loc">

<p>현재 front facing camera로 얻은 이미지에 semantic segmentation을 수행하고, 이를 IPM을 통해 BEV segmentation 이미지로 만든다.</p>
<p>여기서 얻은 데이터를 복원한 semantic map에 대조해서 현재 pose를 알아낼 수 있다. BEV segmentation에 있는 semantic 정보를 모두 포인트 클라우드처럼 다루고, semantic map에 있는 정보도 포인트 클라우드처럼 다룬다면, iterative closest point (ICP) 알고리즘을 통해 3D-3D 매칭을 수행해서 6dof 포즈를 알아낼 수 있을 것이다 (이 과정이 visual localization이 된다).</p>
<p>하지만 매 순간마다 ICP를 계산히기는 너무 계산량이 클 것이다. ICP는 기회가 될 때마다 계산하고, 이를 자동차의 wheel odometry와 함께 visual localization 결과를 Extended Kalman Filter (EKF)로 퓨전하면 위치 추정의 결과도 더 정확하게 만들 수 있고 trajectory smoothing 도 가능하다.</p>
<p> </p>
<hr>
<h1 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h1><p>Literature review 섹션에서 Traditional VSLAM과 Road-based localization 기법을 소개한다.</p>
<p>Traditional VSLAM은 다른 VSLAM 논문들에서도 충분히 볼 수 있는 내용이라 이번 리뷰에서는 스킵한다.</p>
<p>Road-based localization에서는 다음과 같은 논문을 추천한다.</p>
<ul>
<li><a href="">Schreiber 2013 - LaneLoc: Lane marking based localization using highly accurate maps</a> - Curbs and lane을 검출해 visual localization</li>
<li><a href="">Ranganathan 2013 - Light-weight localization for vehicles using road marking</a> - Road marker와 road marker에서 검출한 corner point 기반 visual localization</li>
<li><a href="">Lu 2017 - Monocular localization in urban environments using road marking</a> - Road marker 기준 non-linear optimization 기법 사용</li>
<li><a href="">Rehder 2015 - Submap-based slam for road markings</a> - Odometry 기반 포즈 추정 + 이미지 기반 lane 검출 -&gt; Local grid lane map 생성</li>
<li><a href="">Jeong 2017 - Road-slam: Road marking based slam with lane-level accuracy</a> - Road marker classification</li>
<li><a href="">Qin 2020 - Avp-slam: Semantic visual mapping and localization for autonomous vehicles in the parking lot</a> - Underground parking lot mapping</li>
<li><a href="">Herb 2019 - Crowd-sourced semantic edge mapping for autonomous vehicles</a> - Crowd-sourced semantic map generation</li>
</ul>
<p> </p>
<hr>
<h1 id="관련-유투브-영상"><a href="#관련-유투브-영상" class="headerlink" title="관련 유투브 영상"></a>관련 유투브 영상</h1><div class="video-container"><iframe src="https://www.youtube.com/embed/KB5Ci0Yxquc" frameborder="0" loading="lazy" allowfullscreen=""></iframe></div>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Semantic SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Fang 2022 - Self-Supervised Camera Self-Calibration from Video 논문 리뷰</title>
    <url>/20230315-Fang-2022-self-supervised-camera-self-calibration-from-video/</url>
    <content><![CDATA[<h1 id="논문-배경"><a href="#논문-배경" class="headerlink" title="논문 배경"></a>논문 배경</h1><blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9zaXRlcy5nb29nbGUuY29tL3R0aWMuZWR1L3NlbGYtc3VwLXNlbGYtY2FsaWI=">프로젝트 페이지<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1RSSS1NTC92aWRhcg==">GitHub 코드 레포지토리<i class="fa fa-external-link-alt"></i></span></p>
</blockquote>
<p>카메라 캘리브레이션은 로보틱스와 컴퓨터 비전 문제를 푸는데에 굉장히 중요한 역할을 한다. 하지만 캘리를 하는 과정 자체가 엄청나게 많은 노력이 들어갈 뿐 더러, 카메라 파라미터가 조금만 바뀌어도 (e.g. 진동이나 열로 인해 바뀌어도) 새로 캘리브레이션을 해야하기 때문에 전반적으로 캘리브레이션 과정 자체가 골칫거리로 여겨지는 경우가 많다.</p>
<p>그에 비해, self-supervised depth / ego-motion 추정 기법들은 view-synthesis를 목적으로 매 프레임마다 projection model을 계산하는 방법을 통해 이러한 캘리 과정을 건너뛰는 경우가 많다.</p>
<p>이 논문에서는 in-the-wild 비디오 시퀀스 (i.e. 캘리브레이션 타겟이 전혀 없는 실제 환경)에서 카메라 파라미터를 학습하는 방법을 통해 캘리브레이션을 수행하는 방법을 제안한다. 이 논문에서 제안하는 방식은 sub-pixel 단위의 에러를 보여주며, perspective, fisheye, catadioptric 카메라와 같은 다양한 카메라 모델에도 in-the-wild 캘리브레이션이 가능하다는 것을 보여준다. 또, 이 과정에 있어서 depth estimation 기법의 정확도가 올라가는 것도 보여준다.</p>
<img src="/20230315-Fang-2022-self-supervised-camera-self-calibration-from-video/Screenshot%202023-04-17%20at%204.41.19%20PM.png" class="" title="Pipeline">

<p> </p>
<hr>
<h1 id="포스터"><a href="#포스터" class="headerlink" title="포스터"></a>포스터</h1><p>이 논문은 ICRA 2022에 나왔다.</p>
<img src="/20230315-Fang-2022-self-supervised-camera-self-calibration-from-video/calib_poster.png" class="" title="poster">

<p> </p>
<hr>
<h1 id="Proxy-network-Self-supervised-monocular-depth-estimation"><a href="#Proxy-network-Self-supervised-monocular-depth-estimation" class="headerlink" title="Proxy network - Self-supervised monocular depth estimation"></a>Proxy network - Self-supervised monocular depth estimation</h1>]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.2 Machine Learning &amp; Deep Learning</category>
        <category>논문 리뷰</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>Visual-SLAM</tag>
        <tag>Deep Learning</tag>
        <tag>Self-supervised learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu에서 cmake gui 사용하는 방법 (ccmake, cmake-curses-gui)</title>
    <url>/20230321-ccmake/</url>
    <content><![CDATA[<h1 id="당신은-CMake-GUI가-쓰고싶어진다"><a href="#당신은-CMake-GUI가-쓰고싶어진다" class="headerlink" title="당신은 CMake-GUI가 쓰고싶어진다"></a>당신은 CMake-GUI가 쓰고싶어진다</h1><p>cmake를 자주 쓰다보면 자주 쓰는 옵션들을 기억하게 된다. <code>-DCMAKE_BUILD_TYPE</code> 이라던지, <code>-DCMAKE_INSTALL_PREFIX</code> 라던지 등등…</p>
<p>하지만 새로 사용해보는 3rdParty를 사용할 때 마다 CMakeLists.txt 를 뜯어보면서 어떤 옵션이 있는지 공부를 해야한다. 옵션들마다 nested condition이 있기도 하기 때문이다.</p>
<p>근데 적당히 잘 만들어놓은 오픈소스 3rdParty 에서는 옵션 관리를 잘 해놔서, nested condition을 걱정하지 않고 적당히 옵션을 쓱 보면 사용하는 방법이 이해가 된다. 예를 들어 <code>WITH_EIGEN</code>, <code>BUILD_JPEG</code> 같은 옵션들이다.</p>
<p>이런 옵션들이 몇십개씩 있을 때, CMakeLists.txt를 보면서 파악하려면 시간과 노력이 많이 들어간다. Windows 환경에서는 cmake-gui를 쓰면 쉽게 볼 수 있다 (사실 Windows OS에서 튜토리얼들을 보면 왠만하면 다 이걸 쓴다). 그에 비해 Ubuntu 환경에서는 cmake-gui 튜토리얼 같은건 없다. 소프트웨어도 없는 것 같고, 왠만하면 다 커맨드라인에서 cmake를 사용하기 때문이다.</p>
<p>Ubuntu 환경에서 이런 옵션들을 한눈에 보는 방법이 없을까? 라는 질문으로 해결방법을 찾게 되었다.</p>
<p> </p>
<h1 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h1><p><code>sudo apt-get install cmake-curses-gui</code> 를 통해 gui 패키지를 받는다.</p>
<p>이후, cmake configure를 할 때, <code>cmake .....</code>를 적지 말고, <code>ccmake ....</code>를 적는다.</p>
<p>그러면 터미널 위에서 GUI 스러운 화면이 뜨면서, <code>EMPTY_CACHES</code>라고 뜰 것이다. cmake configure가 되지 않아서 아무 캐쉬가 없기 때문에 이렇게 뜨는 것이다. 여기서 <code>c</code>를 눌러 configure를 해준다.</p>
<p>아래와 같이 익숙한 configure 화면이 나온다.</p>
<img src="/20230321-ccmake/Screenshot%20from%202023-03-21%2013-42-19.png" class="" title="loading">

<p>configure가 끝나고나면 GUI 스러운 화면이 나타나면서 이 CMakeLists.txt에서 제공하는 모든 빌드 옵션이 보이게 된다. Windows 환경에서 사용하는 cmake-gui와 거의 똑같은 기능을 제공한다. 내용을 업데이트하고 <code>c</code>를 눌러 새로 configure 해준 후, <code>q</code>로 나간 후 <code>make -j</code> 또는 <code>ninja</code> 해주면 평소와 같이 빌드를 할 수 있다.</p>
<img src="/20230321-ccmake/Screenshot%20from%202023-03-21%2013-42-34.png" class="" title="ccmake">]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.8 Ubuntu Linux</category>
      </categories>
      <tags>
        <tag>CMake</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>디스플레이 없이 OpenGL 및 X11 렌더링 하는 방법</title>
    <url>/20230325-xvfb/</url>
    <content><![CDATA[<h1 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h1><p>모니터가 없는 CI/CD 서버나 GPU 서버에 종종 다음과 같은 작업을 요청할 때가 있다.</p>
<ol>
<li>GUI 생성 코드를 빌드 및 런타임 테스트를 실행 </li>
<li>모델 학습 후 인퍼런스 결과를 GUI로 띄워서 리모트 머신으로 X11 포워딩</li>
</ol>
<p>이럴 때 마다 나타나는 문제가 있다. </p>
<p>CI/CD 서버나 GPU 서버에는 모니터가 없는 경우가 많다는 점이다.</p>
<p>GUI 코드는 대부분 내부에서 OpenGL 및 GLFW와 같은 OpenGL window handler 라이브러리를 사용하는데, X11 또는 Wayland가 없으면 동작하지 않는다.</p>
<p>그리고 X11 / Wayland의 경우, 시스템에 모니터 드라이버가 없는 경우 동작하지 않는다.</p>
<p>데스크탑에서는 이러한 문제를 발견하기 쉽지 않다. 실제로 모니터를 컴퓨터와 연결하지 않은 상태에서도 돌아가기 때문이다. 하지만 이는 X11 / Wayland가 설치되어있고, 모니터 드라이버를 발견했기 때문에 자동으로 offscreen rendering 모드로 전환했기 때문이다. OpenGL 및 GLFW 옵션에서도 hidden window를 만들어서 offscreen rendering을 지원한다. 하지만 모니터 드라이버가 없거나, X11 / Wayland가 없이는 hidden window를 생성조차 할 수 없기 때문에 아예 코드가 동작하지 않는다. (관련 링크 <span class="exturl" data-url="aHR0cHM6Ly9kaXNjb3Vyc2UuZ2xmdy5vcmcvdC9vZmYtc2NyZWVuLXJlbmRlcmluZy1hbmQteC13aW5kb3dzLzc4NA==">1<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dsZncvZ2xmdy9pc3N1ZXMvNjQ4">2<i class="fa fa-external-link-alt"></i></span>)</p>
<p>CI/CD 서버나 GPU 서버에는 X11 / Wayland가 안 깔려있는 경우도 많다. 이 경우, GUI 관련 코드는 전부 죽게 된다. 운 나쁘게 사용하려는 코드가 실행 극초반부터 GUI window를 만들고 실행하는 경우에는, 아무것도 돌리지 못하고 곧바로 죽어버릴 것이다.</p>
<p> </p>
<h1 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h1><p><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvWHZmYg==">xvfb<i class="fa fa-external-link-alt"></i></span>는 가상 X11 프레임버퍼를 구현하며 X11와 같은 프로토콜을 가진다.</p>
<p>두개의 차이점이라면, xvfb는 실제로 아무런 디스플레이 아웃풋을 만들지 않기 때문에 ‘모니터가 없어도’, ‘X11이 없어도’ 동작한다는 점이다.</p>
<p>X11으로 연결될 수 있는 OpenGL 기능들을 xvfb로 라우팅을 시켜놓는다면, 실제로 코드는 모두 돌아가나 아무런 디스플레이 결과를 보지 않을 것이다.</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.8 Ubuntu Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>X11</tag>
      </tags>
  </entry>
  <entry>
    <title>2023년 1분기 SLAM 뉴스</title>
    <url>/20230405-q1-2023-slam-news/</url>
    <content><![CDATA[<h2 id="이번-달-내가-관심가지는-논문들"><a href="#이번-달-내가-관심가지는-논문들" class="headerlink" title="이번 달 내가 관심가지는 논문들"></a>이번 달 내가 관심가지는 논문들</h2><details>
  <summary> GlueStick: Robust Image Matching by Sticking Points and Lines Together </summary>

<ul>
<li><a href="arxiv.org/abs/2304.02008">논문 링크</a>, <a href="github.com/cvg/gluestick">코드 링크</a></li>
<li>Marc Pollefeys 교수님, Viktor Larsson 교수님 랩실 연구</li>
<li>Line까지 고려하는 SuperGlue 느낌. Line-connectivity를 message passing 방식으로 처리함</li>
</ul>
</details>

<details>
  <summary> AirLoc: Object-based Indoor Relocalization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDQuMDA5NTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Sebastian Scherer, PyPose의 저자인 Chen Wang 교수님 랩실 연구</li>
<li>Object의 apperance와 relative object geometry를 이용해서 relocalization을 하는 연구<ul>
<li>Semantic SLAM 연구라고 봐도 됨</li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDQuMDA1ODMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>hourglass CNN-&gt;detection; Warper Net-&gt;deformation-aware matches; feature fusion-&gt;description</li>
</ul>
</details>

<details>
  <summary> Consistent View Synthesis with Pose-Guided Diffusion Models </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9wb3NlZ3VpZGVkLWRpZmZ1c2lvbi5naXRodWIuaW8v">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Epipolar constraint를 pose-guided diffusion model에 적용해서 single image novel view synthesis </li>
</ul>
</details>

<details>
  <summary> RI Seminar: Luca Carlone : Next-Generation Robot Perception: Hierarchical Representations, Certifiable Algorithms, and Self-Supervised Learning </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj0tUHg4TUNyQVdaZw==">영상 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>CMU RI 세미나. 발표자는 MIT Luca Carlone 교수님</li>
</ul>
</details>

<details>
  <summary> Formal Algorithms for Transformers </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDcuMDkyMzgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>딥마인드에서 적은 Transformer 알고리즘 논문. 수식적으로 가장 잘 풀어낸 논문임.</li>
</ul>
</details>

<details>
  <summary> NeRF-Supervised Deep Stereo </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTc2MDMucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>neural rendering + user-collected images -&gt; stereo training data; rendered stereo triplets + depth maps -&gt; occlusions and enhance fine details</li>
</ul>
</details>

<details>
  <summary> 3D Line Mapping Revisited </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTc1MDQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Marc Pollefeys / Viktor Larsson 교수님 랩실 연구</li>
<li>Line mapping을 하는데에 필요한 Structural prior 사용</li>
</ul>
</details>

<details>
  <summary> Learning Rotation-Equivariant Features for Visual Correspondence </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTU0NzIucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>국내 연구진 연구. CVPR 2023</li>
<li>rotation-equivariant CNNs+group aligning-&gt;rotation-invariant descriptors</li>
</ul>
</details>

<details>
  <summary> Efficient solutions to the relative pose of three calibrated cameras from four points using virtual correspondences </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTYwNzgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Jiri Matas, Torsten Sattler, Zuzana Kukelova 교수님 랩실 연구</li>
<li>4개의 매칭 포인트 중 virtual correspondence를 이용해서 4p3v 문제를 해결</li>
</ul>
</details>

<details>
  <summary> DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTQ0NzgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Gimhee Lee 교수님 랩실 연구</li>
<li>deep bundle adjusting camera poses with Generalizable NeRFs</li>
</ul>
</details>

<details>
  <summary> NEWTON: Neural View-Centric Mapping for On-the-Fly Large-Scale SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIzMDMuMTM2NTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>키프레임마다 Neural field를 생성해서 수행하는 SLAM</li>
</ul>
</details>

<details>
  <summary> LERF: Language Embedded Radiance Fields </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDMuMDk1NTM=">논문 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly93d3cubGVyZi5pby8=">프로젝트 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Matthew Tancik, Angjoo Kanazawa 교수님 랩실 연구</li>
<li>rounding CLIP vectors volumetrically inside a NeRF allows flexible natural language queries in 3D</li>
</ul>
</details>

<details>
  <summary> Learning a Depth Covariance Function </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDMuMTIxNTc=">논문 링크<i class="fa fa-external-link-alt"></i></span>, <a href="edexheim.github.io/depth_cov/">프로젝트 링크</a></li>
<li>ICL의 Andrew Davison 교수님 랩실 연구</li>
<li>Depth를 학습하지 말고, 이미지로부터 pixel depth correlation (i.e. depth covariance)를 학습한다. 이후 SLAM backend로 최적화한다.</li>
</ul>
</details>

<details>
  <summary> PyPose: A Library for Robot Learning with Physics-based Optimization </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDkuMTU0MjgucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Chen Wang 교수님의 주도로 작성된 라이브러리. 수많은 교수님들의 참여.</li>
<li>파이썬으로 작성된 로보틱스 포즈 최적화 라이브러리</li>
</ul>
</details>

<details>
  <summary> NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDIuMDM1OTQ=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Songyou Peng은 진짜 전설이야… Viktor Larsson, Andreas Geiger, Marc Pollefey 교수님 랩실 연구</li>
<li>NICE-SLAM에서 요구하던 RGB-D 인풋 조건을 RGB로 완화시킴. Monodepth, Mono-normal, optical flow, hierarchical grid 추가</li>
</ul>
</details>

<details>
  <summary> Factor Fields: A Unified Framework for Neural Fields and Beyond </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDIuMDEyMjY=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Siyu Tang, Hao Su, Andreas Geiger 교수님 랩실 연구</li>
<li>100500 variants of NERF construction tested</li>
</ul>
</details>

<details>
  <summary> Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMDYuMTEyMTUucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Luca Carlone 교수님 랩실 연구</li>
<li>C-3PO라는 certifiable + self-supervised pose estimation 모델을 제안.</li>
</ul>
</details>

<details>
  <summary> vMAP: Vectorised Object Mapping for Neural Field SLAM </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDIuMDE4Mzg=">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>ICL의 Andrew Davison 교수님 랩실 연구</li>
<li>NERF-SLAM + instance segmentation -&gt; per-object MLP decomposable representation</li>
</ul>
</details>

<details>
  <summary> SDF Studio: A Unified Framework for Surface Reconstruction </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9hdXRvbm9tb3VzdmlzaW9uLmdpdGh1Yi5pby9zZGZzdHVkaW8v">프로젝트 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2F1dG9ub21vdXN2aXNpb24vc2Rmc3R1ZGlv">코드 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Apratim Bhattacharyya, Michale Niemeyer, Siyu Tang, Torsten Sattler, Andreas Geiger 교수님 랩실 합작</li>
<li>Neural impicit surface reconstruction을 위한 통합/모듈러 프레임워크. 기존의 Nerfstudio 위에 코드를 추가한 형태임</li>
</ul>
</details>

<details>
  <summary> Learned Inertial Odometry for Autonomous Drone Racing
 </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMDU4MTY5">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Scaramuzza 교수님 랩실 연구</li>
<li>엄청 빠르게 돌아가는 뉴럴넷 기반 IMU odometry</li>
</ul>
</details>

<details>
  <summary> PointSLOT: Real-Time Simultaneous Localization and Object Tracking for Dynamic Environment </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMDY4NzMy">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>SLAMMOT은 못참지…</li>
</ul>
</details>

<details>
  <summary> Optimizing Fiducial Marker Placement for Improved Visual Localization  </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzEwMDc5MTAw">논문 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>John Leonard 교수님 랩실 연구</li>
</ul>
</details>

<details>
  <summary> Visual Language Maps for Robot Navigation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly92bG1hcHMuZ2l0aHViLmlvLw==">프로젝트 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIyMTAuMDU3MTQucGRm">논문 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3ZsbWFwcy92bG1hcHM=">코드 링크<i class="fa fa-external-link-alt"></i></span></li>
<li>Wolfram Burgard 교수님 랩실 / 구글 리서치 연구</li>
<li>SLAM으로 딴 맵 위에 visual-language index를 추가 + LLM을 로봇이 이해하고 map 탐색 및 경로계획에 사용할 수 있음</li>
</ul>
</details>

<details>
  <summary> CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation </summary>

<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9jb3cuY3MuY29sdW1iaWEuZWR1Lw==">프로젝트 링크<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvbHVtYmlhLWFpLXJvYm90aWNzL2Nvdw==">코드 링크<i class="fa fa-external-link-alt"></i></span>, <a href="">논문 링크</a></li>
<li>VLMaps 연구와 비슷함. Language-driven zero-shot object navigation 연구.<ul>
<li>CLIP on Wheels (CoW)</li>
</ul>
</li>
</ul>
</details>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>월간 SLAM 뉴스</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>논문 소식</tag>
      </tags>
  </entry>
  <entry>
    <title>Rosinol 2021 - Kimera - from SLAM to Spatial Perception with 3D Dynamic Scene Graphs 논문 리뷰</title>
    <url>/20230329-rosinol-2021-kimera-3d-dgs/</url>
    <content><![CDATA[<h1 id="논문-요약"><a href="#논문-요약" class="headerlink" title="논문 요약"></a>논문 요약</h1><p>이 논문에서는 복잡한 실내 환경에서 지도를 만들고 시간에 따른 변화를 기록하는 프레임워크를 제안한다. 알고리즘 하나하나 다 설명하기보다는, 전체적인 시스템을 설명하는 논문이다.</p>
<p>지도는 scene graph라는 특수한 구조를 가지고 있는데, 공간을 표현하는 방법을 여러 layer로 나누어 단계적으로 표현하는 방법이다. 이 논문에서는 낮은 layer 순서대로 Metric-semantic mesh, Objects and agents, Places and Structures, Rooms, Buildings 순서로 표현한다. 이 중 agents와 같이 움직이는 객체들이 있기 때문에 Dynamic Scene Graph라고 부른다.</p>
<p>가장 낮은 layer에 위치한 metric semantic mesh는 semantic segmentation과 mesh를 생성하는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtVklP">visual-inertial odometry<i class="fa fa-external-link-alt"></i></span>로 생성한다.</p>
<p>Objects와 Agents를 검출할 때, 움직이는 객체는 agent로, 움직이지 않는 객체는 object로 분류한다. Object를 검출할 때에는 물체의 모양을 알고 있는 경우와 알지 못하는 경우를 따로 분리해서 검출할 수 있다. Agents는 로봇이나 사람 등을 검출할 수 있는데, 논문에서는 사람의 자세를 검출하고 트랙킹 하는 방법을 소개한다.</p>
<p>Places and Structures 및 Room 구분은 바닥/벽/천장을 구분하고 같은 공간끼리 클러스터링하는 방법을 제안한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.16.14%20AM.gif" class="" title="overview">

<p> </p>
<hr>
<h1 id="논문-배경"><a href="#논문-배경" class="headerlink" title="논문 배경"></a>논문 배경</h1><h2 id="Spatial-Perception-시스템이-필요한-이유"><a href="#Spatial-Perception-시스템이-필요한-이유" class="headerlink" title="Spatial Perception 시스템이 필요한 이유"></a>Spatial Perception 시스템이 필요한 이유</h2><blockquote>
<p>로봇이 사람의 수준만큼 공간을 이해하려면 어떤 조건이 필요할까?</p>
<ul>
<li>공간의 기하학적, 의미론적, 물리적 특성을 이해해야한다. </li>
<li>물체들 사이의 공간-시간적 현상을 이해해야한다. </li>
<li>모든 관계를 단계적으로 이해할 수 있어야한다.</li>
</ul>
</blockquote>
<p>로봇이 공간 속에서 안전하게 이동하고 작업을 하기 위해서는 어떤 기능이 필요한가? 로봇이 사람과 안전하게 상호작용하기 위해서는 어떤 기능이 필요한가?</p>
<p>예를 들어서, 건물에 불이 난 화재 현장에서 소방 로봇에게 ‘2층에서 생존자를 찾아라!’ 라는 명령을 줬다고 해보자. 로봇은 건물 입구에서부터 2층까지 이동하고 탐색을 해야한다. 이를 위해 로봇이 해야하는 작업은 이동 경로를 미리 <strong>계획</strong>하고, 경로에서 벗어나지 않게 <strong>제어</strong>를 하며 이동을 하며, 이동 중에는 주변 환경을 실시간으로 <strong>인식</strong>하며 생존자를 찾아야한다. 사람은 이것을 굉장히 쉽게 할 수 있다 - 후다닥 뛰면서 주변을 두리번두리번 거리면 된다. 현재 기술로는 로봇보다 사람이 작업하는 것이 전체적으로 더 쉽고 저렴하기 때문에 소방로봇보다 소방관이 더 많은 것이다.</p>
<p>공간을 인지하고, 이해하고, 공간 속에서 안전하고 효율적으로 이동하는 것은 사람에게는 굉장히 쉬운 일이다. 로봇의 공간 인지 능력과 이동 능력을 사람의 수준만큼 끌어올리는 것이 자율주행의 핵심이다. 이러한 목표를 달성하기 위해서는 아무래도 사람의 사고 방식을 모방하는 것이 쉬울 수 있다. 사람은 공간에서 공간으로 이동하는데에 어떤 사고 단계를 거칠까?</p>
<p>사람은 로봇과 다르게 <strong>High-level understanding</strong>을 기준으로 공간을 이해하고 계획하는 경우가 많다. 보스턴에서 로마로 이동을 한다고 할 때, 사람은 집-자동차-운전-공항-체크인-비행기-공항-빠져나오기 와 같은 방식으로의 이동 방법을 계획한다. 이는 <strong>로봇이 공간이동을 할 때 사용하는 미터 단위의 움직임 트랙킹과는 아주 다르다</strong>. 하지만 사람이 항상 이렇게 topological하게만 생각하는 것은 아니다. 사람도 미터 단위의 움직임을 인식하고, 또 state 변화도 인식할 수 있다. 예를 들어, 멀리 보이는 자동차가 주차되어있는 것인지 아니면 이동하고 있는 것인지 구분할 수 있으며, 사람이 벽을 향해 걸어가다가 방향을 돌리지 않으면 몇초 후에 부딪힐지 예상도 할 수 있다.</p>
<p>이처럼 사람은 동적 객체가 존재하는 복잡한 3D 공간을 굉장히 잘 이해할 수 있다. 몇가지 특징을 분류해보면 다음과 같다.</p>
<ul>
<li>공간의 <strong>기하학적 특성</strong>을 이해한다. (Geometry)<ul>
<li>e.g. 나와 저 벽은 5m 떨어져있다.</li>
</ul>
</li>
<li>공간의 <strong>의미론적인 특성</strong>을 이해한다. (Semantics)<ul>
<li>e.g. 이것은 의자이다. 저것은 벽이다. 저것은 천장이다.</li>
</ul>
</li>
<li>공간의 <strong>물리적인 특성</strong>을 이해한다. (Physics)<ul>
<li>e.g. 저기 자동차는 주행중이다. 전진 중이기 때문에 갑작스럽게 후진할 수 없다.</li>
</ul>
</li>
<li>공간을 <strong>여러 단계</strong>로 나누어서 이해할 수 있다. (Multiple levels of abstraction)<ul>
<li>e.g. 내 앞의 벽은 3m 떨어져있다. 이 벽은 내 방의 벽이다. 내 방은 우리 집의 일부이다. 우리 집은 우리 동네의 일부이다. 우리 동네는 용인시에 있다. 용인시는 경기도에 있다.</li>
</ul>
</li>
<li>객체의 <strong>공간-시간적 특성</strong>을 이해할 수 있다 (Spatio-temporal relations)<ul>
<li>e.g. 사람이 의자에 앉아있다. 사람이 일어났다. 사람이 앞으로 걸어가면서, 사람과 의자 사이의 거리가 멀어진다.</li>
</ul>
</li>
</ul>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%201.19.35%20AM.png" class="" title="inspiration">

<p> </p>
<h2 id="Geometry-Semantics-Abstraction-Physics-Spatio-temporal-relations-관련-연구-리스트"><a href="#Geometry-Semantics-Abstraction-Physics-Spatio-temporal-relations-관련-연구-리스트" class="headerlink" title="Geometry / Semantics, Abstraction, Physics / Spatio-temporal relations 관련 연구 리스트"></a>Geometry / Semantics, Abstraction, Physics / Spatio-temporal relations 관련 연구 리스트</h2><details>
  <summary> Geometry / Semantics 연구 리스트 (클릭하면 열립니다) </summary>

<ul>
<li><strong>SLAM</strong><ul>
<li><a href="">Cadena 2016 - Past, Present, and Future of Simultaneous Localization and Mapping: Toward the robust perception age </a></li>
</ul>
</li>
<li><strong>Structure from Motion (SfM)</strong><ul>
<li><a href="">Enqvist 2011 - Non-sequential Structure from Motion</a></li>
</ul>
</li>
<li><strong>Multi-view stereo</strong><ul>
<li><a href="">Schops 2017 - A Multi-view Stereo Benchmark with High-Resolution Images and Multi-Camera Videos</a></li>
</ul>
</li>
<li><strong>Semantic segmentation</strong><ul>
<li><a href="">Garcia-Garcia 2017 - A review on deep learning techniques applied to semantic segmentation</a></li>
<li><a href="">Krizhevsky 2012 - ImageNet classification with deep convolutional neural networks</a></li>
<li><a href="">Redmon and Farhadi 2017 - YOLO9000: Better, faster, stronger</a></li>
<li><a href="">Ren 2015 - Faster R-CNN: Towards realtime object detection with region proposal networks</a></li>
<li><a href="">He 2017 - Mask R-CNN</a></li>
<li><a href="">Hu 2017 - Learning to segment everything</a></li>
<li><a href="">Badrinarayanan 2017 - SegNet: A deep convolutional encoder-decoder architucture for image segmentation</a></li>
</ul>
</li>
<li><strong>Fusion of above</strong><ul>
<li><a href="">Bao and Saverese 2011 - Semantic structure from motion</a></li>
<li><a href="">Bowman 2017 - Probabilistic data associaation for semantic slam</a></li>
<li><a href="">Hackel 2017 - Semantic3d.net: A new large-scale point cloud classification benchmark</a></li>
<li><a href="">Grinvald 2019 - Volumetric instance-aware semantic mapping and 3D object discovery</a></li>
<li><a href="">Zheng 2019 - Active understanding via online semantic reconstruction</a></li>
<li><a href="">Zheng 2019 - From pixels to buildings: end-to-end probabilistic deep networks for large-scale semantic mapping</a></li>
<li><a href="">Davison 2018 - Futuremapping: The computational structure of spatial ai system</a></li>
</ul>
</li>
</ul>
</details>

<details>
  <summary> Multiple levels of abstraction 관련 연구 (클릭하면 열립니다)) </summary>

<ul>
<li><strong>Early researches on map representation in robotics</strong><ul>
<li><a href="">Kuipers 2000 - The Spatial Semantic Hierarchy</a></li>
<li><a href="">Chatila and Laumond 1985 - Position referencing and consistsent world modelling for mobile robotics</a></li>
<li><a href="">Vasudewvan 2006 - Cognitive maps for mobile robots: An object based approach</a></li>
<li><a href="">Galindo 2005 - Multi-hierarchical semantic maps for mobile robotics</a></li>
<li><a href="">Zender 2008 - Conceptual spatial representations for indoor mobile robots</a></li>
</ul>
</li>
<li><strong>Metric semantic mapping</strong><ul>
<li><a href="">Salas-Moreno 2013 - SLAM++: Simultaneous localisation and mapping at the level of objects</a></li>
<li><a href="">Bowman 2017 - Probabilistic data associaation for semantic slam</a></li>
<li><a href="">Behley - A Dataset for Semantic Scene Understanding fof LiDAR Sequences</a></li>
<li><a href="">Tateno 2017 - CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction</a></li>
<li><a href="">Rosinol 2020 - Kimera: an open-source library for real-time metric-semantic localization and mapping</a></li>
<li><a href="">Grinvald 2019 - Volumetric instance-aware semantic mapping and 3D object discovery</a></li>
<li><a href="">McCorman 2017 - SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks</a></li>
</ul>
</li>
</ul>
</details>


<details>
  <summary>  Physics / Spatio-temporal relations 관련 연구 (클릭하면 열립니다)) </summary>

<ul>
<li><strong>2D Scene graphs</strong><ul>
<li><a href="">Choi 2013 - Understanding indoor scene using 3d geometric phrases</a></li>
<li><a href="">Zhao and Zhu 2013 - Scene parsing by integration function, geometry and appearance models</a></li>
<li><a href="">Huang 2018 - Holistic 3d scene parsing and reconstruction from a single rgb image</a></li>
<li><a href="">Jiang 2018 - Configurable 3d scene synthesis and 2d image rendering with per-pixel ground truth using stochastic grammars</a></li>
</ul>
</li>
<li><strong>3D Scene graphs</strong><ul>
<li><a href="">Armeni 2019 - 3D scene graph: A structure for unified semantics, 3D space, and camera</a></li>
<li><a href="">Kim 2019 - 3-d scene graph: A sparse and semantic representation of physical environments for intelligent agents</a></li>
</ul>
</li>
</ul>
</details>

<p> </p>
<hr>
<h1 id="3D-Dynamic-Scene-Graph란"><a href="#3D-Dynamic-Scene-Graph란" class="headerlink" title="3D Dynamic Scene Graph란?"></a>3D Dynamic Scene Graph란?</h1><blockquote>
<p>Kimera = 카메라+IMU로 취득한 데이터로부터 공간정보를 인식해서 3D DSG를 생성하는 시스템<br>5개의 layer로 이뤄짐 - Metric-semantic mesh, Objects and Agents, Places and Structures, Rooms, Buildings</p>
</blockquote>
<p>Kimera 시스템의 개발 목적은 인간-로봇의 상호작용을 위한 <strong>공간 인식 시스템</strong> (spatial perception)을 만들기 위함이다. 이를 위해 효과적인 공간 표현법이 필요한데, Kimera는 <strong>3D Dynamic Scene Graph</strong> (3D DSG)를 사용하였다. DSG는 여러개의 계층를 가진 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvR3JhcGhfKGFic3RyYWN0X2RhdGFfdHlwZSk=">그래프 자료구조<i class="fa fa-external-link-alt"></i></span>이며, 각각의 node는 공간 객체 (e.g. object, rooms, agents)를 의미하고 각각의 edge는 공간-시간적인 관계를 의미한다 (pairwise spatio-temporal relation).</p>
<p>DSG의 node는 공간 객체를 의미하고, edge는 공간-시간적 관계를 의미한다. Kimera의 DSG는 공간 객체를 표현할 때 1. pose (6dof 방향+위치), 2. shape (형태), 3. bounding box (object detection의 결과인 바운딩 박스)를 함께 표현한다. 또, 각각의 node는 unique ID를 가지고 있으며 (i.e. 독립적이다), 아래와 같은 계층 구조로 표현된다.</p>
<ol>
<li>Metric-semantic mesh</li>
<li>Objects and Agents</li>
<li>Places and structures</li>
<li>Rooms</li>
<li>Building</li>
</ol>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%201.14.39%20AM.png" class="" title="3D DGS">

<p> </p>
<h2 id="Layer-1-Metric-semantic-mesh"><a href="#Layer-1-Metric-semantic-mesh" class="headerlink" title="Layer 1: Metric-semantic mesh"></a>Layer 1: Metric-semantic mesh</h2><p>Metric-semantic mesh의 뜻은 다음과 같이 풀어 쓸 수 있다. </p>
<ul>
<li>Metric: Meter-ic - 미터-단위의 스케일을 가지고 있는 -&gt; 즉, 실제 세상과 동일한 비율을 가지고 있는 지도를 만든다.</li>
<li>Semantic: 시맨틱 정보를 가지고 있는 -&gt; 즉, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hdHRlcnBvcnQvTWFza19SQ05O">Semantic segmentation<i class="fa fa-external-link-alt"></i></span>의 결과가 함께 적용될 것이다.</li>
<li>Mesh: 공간을 표현할 때 그래프 형태로 엮여있는 표현 방법 (i.e. <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvUG9seWdvbl9tZXNo">폴리곤 메쉬<i class="fa fa-external-link-alt"></i></span>). Mesh는 node와 edge로 이뤄져있다.</li>
</ul>
<p>즉, 3D DSG는 ‘<strong>실제 세상과 동일한 스케일을 가진 geometry 정보를 mesh로 표현했고, 이 mesh에는 semantic 정보도 함께 담겨있다</strong>‘ 라는 뜻이 된다.</p>
<p>Metric-semantic mesh의 node는 각각 다음과 같은 정보를 담고 있다 - 1. 3D position, 2. Normal, 3. RGB 색, 4. Semantic label. Edge를 연결하는 방법은 polygon mesh가 되도록 (i.e. 생성되는 면이 삼각형이 되도록) 만든다. </p>
<p>Metric-semantic mesh는 <strong>정적인 물체 (i.e. static)한 정보만 담는다</strong>. 예를 들어, 바닥, 벽, 천장, 가구 와 같은 것들이 될것이다. 움직이는 로봇이나 사람과 같이 동적인 물체는 곧 설명할 Agents로써 다루며 metric-semantic mesh에는 포함되지 않는다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%201.46.15%20AM.png" class="" title="metric_semantic_mesh">

<p> </p>
<h2 id="Layer-2-Objects-and-Agents"><a href="#Layer-2-Objects-and-Agents" class="headerlink" title="Layer 2: Objects and Agents"></a>Layer 2: Objects and Agents</h2><h3 id="Objects"><a href="#Objects" class="headerlink" title="Objects"></a>Objects</h3><p>Objects는 <strong>건물의 일부가 아닌 정적인 물체</strong>를 의미한다. ‘건물의 일부’로써의 예시로는 바닥, 벽, 천장, 기둥과 같은 것들이 있다. </p>
<p>Object node는 다음과 같은 정보를 담고 있다 - 1. 3D object pose, 2. Bounding box, 3. Semantic class. Kimera에서는 여기까지만 저장하지만, 사실 원한다면 더 많이 저장할 수도 있다. 예를 들어, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMDI1Mjc=">Armeni 2019 - 3D scene graph: A structure for unified semantics, 3D space, and camera<i class="fa fa-external-link-alt"></i></span> 에서는 재질과 같은 정보도 담기도 한다. Node를 이어주는 edge에는 1. co-visibility, 2. 상대적인 크기 차이, 3. 상대적인 거리, 4. 닿아있는지에 대한 여부 와 같은 정보를 저장한다.</p>
<p>모든 object node는 가장 가까운 place node와 연결되어있다 (Place node는 Layer 3에 있다).</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-03%20at%2011.30.19%20PM.png" class="" title="Objects">

<h3 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h3><p>Agents는 <strong>동적인 물체</strong>를 의미한다. Kimera에서는 주로 ‘<strong>로봇</strong>‘과 ‘<strong>사람</strong>‘을 동적인 물체로 분류했다. (논문/코드에서 로봇을 검출&amp;트랙킹 하는 코드는 없고, 사람 검출&amp;트랙킹 관련 부분만 존재한다. 다만, ‘데이터를 수집하는 자기자신’을 로봇으로 인지하긴 한다.)</p>
<p>Agent node는 다음과 같은 정보를 담고 있다 - 1. 시간에 따른 움직임 정보를 담은 3D pose graph, 2. 시간마다 기록해둔 물체의 형태 (사람의 경우, non-rigid mesh), 3. Semantic class (i.e. 로봇인지, 사람인지).</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-03%20at%2011.31.21%20PM.png" class="" title="Objects">

<p> </p>
<h2 id="Layer-3-Places-and-Structures"><a href="#Layer-3-Places-and-Structures" class="headerlink" title="Layer 3: Places and Structures"></a>Layer 3: Places and Structures</h2><h3 id="Places"><a href="#Places" class="headerlink" title="Places"></a>Places</h3><p>Place는 ‘<strong>비어있는 공간</strong>‘을 의미하며, 하나의 node로써 표현이 가능하다. 그리고 이러한 node를 잇는 edge는 place에서 place로 이동이 가능하다는 것을 의미하는 ‘<strong>횡단 가능성</strong>‘가 (traversability) 되겠다.</p>
<p>Place는 topological 한 성격을 띈다. 이는 place가 다른 하위 node들과 굉장히 다른 성격을 가진다는 것을 의미하는데, 하위 node들은 정확한 위치/형태/크기를 표현하는 geometric한 성격이 강한데에 비해, Place는 단순히 node에 ‘비어있는 공간의 평균 위치’인 3D position 정보만 가지기 때문이다. </p>
<p>이러한 topological 한 성격은 로봇이 움직이는 방법 (i.e. 실제 scale의 occupancy map에서의 경로 계획)이 아닌 좀 더 사람이 움직이는 방법과 유사하게 생각할 수 있게 해준다 (i.e. 1번 place에서 2번 place로 이동한다). 이를 통해 좀 더 효율적인 경로 계획이 가능해지는데, 이 부분에 대해서는 이후 소개할 Hierarchical planning에서 좀 더 다루기로 한다.</p>
<p>Layer 2에 있는 object node들은 해당 object가 검출된 가장 가까운 place에 해당하는 node와 연결이 된다. 또, place node들은 가장 가까운 (Layer 4에서 소개할) room node에도 연결된다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-03%20at%2011.53.48%20PM.png" class="" title="Structures">

<h3 id="Structures"><a href="#Structures" class="headerlink" title="Structures"></a>Structures</h3><p>Structures는 <strong>벽, 바닥, 천장, 기둥</strong>과 같은 것들이며, <strong>place를 나누는 기준</strong>이 된다.</p>
<p>Structure node는 1. 3D pose, 2. bounding box, 3. semantic class 정보를 담고 있다. 구현에 따라, 어떤 방을 감싸고 있는지에 대한 정보도 저장할 수 있다.</p>
<p>Layer 2에 있는 object node가 structure node에 연결이 될 수 있다. 예를 들어, ‘액자가 벽에 걸려있다’ 라던지, ‘전등이 천장에 걸려있다’와 같은 경우에 가능하다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-03%20at%2011.30.52%20PM.png" class="" title="Structures">

<p> </p>
<h2 id="Layer-4-Rooms"><a href="#Layer-4-Rooms" class="headerlink" title="Layer 4: Rooms"></a>Layer 4: Rooms</h2><p>Rooms는 <strong>방, 복도, 거실</strong>과 같은 공간을 분리하는 기준이 된다. </p>
<p>Room node는 1. 3D pose, 2. bounding box, 3. Semantic class를 담고 있다.</p>
<p>Layer 3의 object node들은 object들이 위치해있는 room node로 연결 된다.</p>
<p>모든 room node는 해당되는 Layer 5의 building node로 연결된다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-03%20at%2011.53.56%20PM.png" class="" title="Structures">

<p> </p>
<h2 id="Layer-5-Building"><a href="#Layer-5-Building" class="headerlink" title="Layer 5: Building"></a>Layer 5: Building</h2><p>Room들을 모아 하나의 빌딩을 만들 수 있다.</p>
<p>Building node에 1. 3D pose, 2. Bounding box, 3. Semantic class 정보를 담을 수 있다.</p>
<p>사실 Kimera 논문에서는 거의 개념만 존재하는 느낌이다. Multi-building 시나리오를 생각하고 만든 것 같지만, 실제로 실험을 진행하거나 하진 않았다. </p>
<p> </p>
<h2 id="DSG는-왜-이런-형태를-띄는가"><a href="#DSG는-왜-이런-형태를-띄는가" class="headerlink" title="DSG는 왜 이런 형태를 띄는가?"></a>DSG는 왜 이런 형태를 띄는가?</h2><p>우리는 5개의 layer로 이뤄진 DSG 구조를 보았다. 각각의 layer의 성격과 layer들관의 관계가 명확하게 정의되어있다. 이게 최선일까? 라는 생각이 들 수도 있다. 저자들은 이렇게 디자인한데에는 2가지 이유가 있다고 한다.</p>
<p>첫째는 <strong>Task planning 와 motion planning을 효율적으로 하기 위함</strong>이다. 높은 수준의 인간-로봇 상호작용을 위해서는 분명 사람의 사고흐름을 따르는 task가 주어질 것인데, 이는 주로 추상적인 경우가 많다 (e.g. ‘거실에 가서 커피 한잔 타와’). 이에 비해서 로봇의 경로 계획 방법은 기하학적인 움직임을 따르는 경우가 많기 떄문에 (e.g. 0.5 m/s로 3m 전진, 이후 45도 좌회전 후 동일 속도로 2m 전진.) 3D DSG는 상위 layer에서는 semantic 정보를 기반으로 한 topological 한 성격을 띄기 때문에 추상적인 task planning에 효과적이며, 이는 사람의 사고흐름과 비슷하게 경로 계획을 할 수 있다는 것을 의미한다. 상위 layer에서 경로 계획이 끝나면, geometric한 성격을 띄는 하위 layer의 정보에 접근하여 로봇의 경로 계획법을 따라 안전하게 구동부를 작동시킬 수 있다.</p>
<p>둘째는 <strong>확정성</strong>에 있다. 환경에 따라 places, structures, rooms, buildings와 같은 구조는 적합하지 않을 수 있다. 예를 들어서, 고속도로 같은 환경에는 이와 같은 것들이 전혀 없을 것이다. 하지만 그런 경우에는 몇몇 layer를 제거하고, 적절한 layer로 대체할 수 있다는 장점이 있다. 또, 경우에 따라서, 위/아래로 layer를 추가할 수도 있다. 논문에서 소개하는 3D DSG는 하나 또는 다수의 빌딩을 하나의 그래프 안에 담을 수 있을 것이다. 하지만 이론 상 위에 layer를 계속 추가해서 동네, 구, 시, 나라 까지 엮을 수도 있을 것이다.</p>
<p> </p>
<hr>
<h1 id="Kimera-Spatial-Perception-Engine-개요"><a href="#Kimera-Spatial-Perception-Engine-개요" class="headerlink" title="Kimera: Spatial Perception Engine 개요"></a>Kimera: Spatial Perception Engine 개요</h1><p>Kimera를 한줄로 표현하면 ‘<strong>카메라+IMU로 취득한 데이터로부터 공간정보를 인식해서 DSG를 생성하는 시스템</strong>‘이다.</p>
<p>Kimera 시스템은 <strong>Kimera-core</strong>와 <strong>Kimera-DSG</strong>라는 2개의 모듈로 나눠진다.</p>
<p> </p>
<hr>
<h1 id="Kimera-Core"><a href="#Kimera-Core" class="headerlink" title="Kimera-Core"></a>Kimera-Core</h1><h2 id="Kimera-Core-시스템-오버뷰"><a href="#Kimera-Core-시스템-오버뷰" class="headerlink" title="Kimera-Core 시스템 오버뷰"></a>Kimera-Core 시스템 오버뷰</h2><img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-04%20at%208.36.08%20PM.png" class="" title="Structures">

<p>Kimera-Core는 <strong>실시간으로 metric-semantic mesh를 생성</strong>한다.</p>
<p>시스템 인풋으로는 <strong>Stereo 또는 RGB-D 카메라</strong>, 그리고 <a href="https://ko.wikipedia.org/wiki/%EA%B4%80%EC%84%B1_%EC%B8%A1%EC%A0%95_%EC%9E%A5%EB%B9%84"><strong>IMU</strong></a>를 필요로 한다. 위와 같은 데이터를 통해 Segmentation image, Depth image, RGB image, IMU data stream을 만들 수 있다. </p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtVklP">Kimera-VIO<i class="fa fa-external-link-alt"></i></span>는 Stereo RGB image와 IMU를 받는 VIO 모듈을 이용해서 정확하고 빠른 3D 자세 추정을 수행한다. VIO 모듈은 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JvcmdsYWIvZ3RzYW0=">GTSAM<i class="fa fa-external-link-alt"></i></span> 라이브러리를 통해 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MTIuMDIzNjM=">IMU pre-integration<i class="fa fa-external-link-alt"></i></span>과 fixed-lag smoothing 기법을 사용한 것이 특징이다. (참고 논문 <span class="exturl" data-url="aHR0cHM6Ly93d3cucmVzZWFyY2gtY29sbGVjdGlvbi5ldGh6LmNoL2hhbmRsZS8yMC41MDAuMTE4NTAvMjk3NjQ1">1<i class="fa fa-external-link-alt"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg3OTQ0NTY=">2<i class="fa fa-external-link-alt"></i></span>). <span class="exturl" data-url="aHR0cHM6Ly93d3cubnZpZGlhLmNvbS9rby1rci9hdXRvbm9tb3VzLW1hY2hpbmVzL2VtYmVkZGVkLXN5c3RlbXMvamV0c29uLXR4Mi8=">NVIDIA Jetson TX2<i class="fa fa-external-link-alt"></i></span>에서도 실시간으로 동작 가능할 정도로 가볍다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtVklP">Kimera-Mesher<i class="fa fa-external-link-alt"></i></span>는 VIO의 포즈와 맵 정보를 받아 매 프레임 / 다수의 프레임에서 빠른 local mesh을 생성한다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtU2VtYW50aWNz">Kimera-Semantics<i class="fa fa-external-link-alt"></i></span>는 Kimera-mesher의 결과를 받아 global 3D mesh 생성한다. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2V0aHotYXNsL3ZveGJsb3g=">VoxBlox<i class="fa fa-external-link-alt"></i></span>의 ESDF (Euclidean Signed Distance Function) 기법 사용하며, 3D Bayesian update 방법론을 통해 mesh에 semantic 정보 부여해 metric-semantic mesh를 생성한다.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtVklP">Kimera-PGMO<i class="fa fa-external-link-alt"></i></span>는 Loop closure를 이용해 Kimera-Semantics의 global metric-semantic mesh를 최적화한다. PGMO는 Pose-graph and mesh optimization을 줄인 말이다. Kimera-PGMO는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtUlBHTw==">Kimera-RPGO<i class="fa fa-external-link-alt"></i></span> (Robust pose-graph optimization) 라이브러리의 mesh 버전이다.</li>
</ul>
<p>Kimera-core는 기본적으로 <strong>5개의 쓰레드</strong>를 사용한다.</p>
<ul>
<li>1번 쓰레드<ul>
<li>Kimera-VIO의 frontend가 stereo image와 IMU 데이터를 인풋으로 받는다. 아웃풋으로 feature track과 pre-integrated IMU 값들을 준다.</li>
</ul>
</li>
<li>2번 쓰레드<ul>
<li>Kimera-VIO의 backend가 최적화를 수행하고, 최적화된 맵/포즈 값을 준다.</li>
</ul>
</li>
<li>3번 쓰레드<ul>
<li>Kimera-Mesher가 매 프레임마다 3D mesh를 연산하고 (&lt;20ms), 또 여러 프레임 정보를 통합한 3D mesh도 연산한다.</li>
</ul>
</li>
<li>4번 쓰레드<ul>
<li>Kimera-Semantics가 metric-semantic mesh를 생성한다. 앞의 3개 쓰레드보다는 상당히 많이 느리게 동작한다. Depth map, 2D semantic label, pose 정보를 필요로 한다. Depth map은 RGB-D 센서에서 바로 얻어내거나, Stereo images에서 dense stereo를 통해 얻어낸다. 2D semantic label은 RGB 이미지에 semantic segmentation을 수행해서 얻어낸다. pose 정보는 Kimera-VIO로 추정한 최적 포즈 값이다.</li>
</ul>
</li>
<li>5번 쓰레드<ul>
<li>Kimera-PGMO를 이용해 loop closure를 수행한다. 최적 metric-semantic mesh가 필요하지 않다면 Kimera-RPGO를 사용할 수도 있다.</li>
</ul>
</li>
</ul>
<p>아래 서브 섹션부터는 각각의 기능들에 대해 상세히 리뷰한다.</p>
<p> </p>
<h2 id="Kimera-VIO-Visual-Inertial-Odometry"><a href="#Kimera-VIO-Visual-Inertial-Odometry" class="headerlink" title="Kimera-VIO: Visual-Inertial Odometry"></a>Kimera-VIO: Visual-Inertial Odometry</h2><blockquote>
<p>Stereo + Pre-integrated IMU를 이용한 VIO</p>
</blockquote>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.31.30%20AM1.gif" class="" title="Structures">

<p>Kimera-VIO는 카메라와 IMU의 데이터를 혼합하는 visual-inertial odometry (VIO) 기법을 통해 실시간으로 카메라+IMU 시스템의 6dof pose (i.e. 3D 공간 속에서의 방향 + 3D 공간 속에서의 위치)을 추정한다. 동시에, 3D visual landmark를 통해 sparse하게 scene reconstruction을 할 수 있는데, 이 정보를 기반으로 실제 세상과 동일한 scale을 가지고 있는 (i.e. metric-scale을 가지고 있는) mesh를 만들 수 있다.</p>
<p>Kimera-VIO는 현재 많은 VIO 시스템이 채택하는 방법인 <strong>keyframe-based 기반 fixed-lag smoothing 기법</strong>을 사용한다. 이 방법은 카메라가 취득한 모든 이미지 데이터를 사용하는 것이 아닌, sliding window 기법을 통해 특정 갯수의 keyframe 데이터로부터만 연산을 하는 방법인데, 효과적으로 연산량을 줄일 수 있다는 장점 때문에 2015년 이후 수많은 VIO 기법들이 이와 같은 keyframe-based 기반 fixed-lag smoothing 기법을 사용한다. 물론 더 많은 이미지 데이터를 사용함으로써 정확도를 높일 수 있는 full smoothing 기법도 프로그램 옵션 변경을 통해 사용 가능하다.  </p>
<p>내부적으로 사용하는  알고리즘은 다음과 같다.</p>
<ul>
<li>Frontend<ul>
<li>IMU preintegration (Forster 2017)</li>
<li>Shi-Tomasi corner detection (Shi and Tomasi 1994)<ul>
<li>매 keyframe마다 수행</li>
</ul>
</li>
<li>Lukas-Kanade tracker (Bouget 2000)<ul>
<li>Initial 값은 IMU rotation 값을 사용 (Hwangbo 2009)</li>
<li>매 프레임마다 수행</li>
</ul>
</li>
<li>left-right stereo match + geometric verification<ul>
<li>Monocular verification -&gt; 5-point RANSAC (Nister 2004)</li>
<li>Stereo verification -&gt; 3-point RANSAC (Horn 1987)</li>
<li>Monocular + IMU verification -&gt; 2-point RANSAC (Kneip 2011)</li>
<li>Stereo + IMU verification -&gt; 1-point RANSAC (Kneip 2011)</li>
<li>매 keyframe마다 수행</li>
</ul>
</li>
</ul>
</li>
<li>Backend<ul>
<li>Preintegrated IMU model + Structureless vision model (Forster 2017)</li>
<li>GTSAM 라이브러리 (Dellaert 2012) 속 iSAM2 구현을 사용 (Kaess 2012)</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="Kimera-Mesher-3D-Mesh-Reconstruction"><a href="#Kimera-Mesher-3D-Mesh-Reconstruction" class="headerlink" title="Kimera-Mesher: 3D Mesh Reconstruction"></a>Kimera-Mesher: 3D Mesh Reconstruction</h2><blockquote>
<p>Per-frame mesh: 매 프레임 빠르게 mesh 생성<br>Multi-frame mesh: 여러 프레임의 mesh를 통합해서 생성</p>
</blockquote>
<p>Kimera-Mesher는 2가지 종류의 mesh를 만드는 기능을 가지고 있다.</p>
<p>우선 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9DNWZGREVKOWNGUQ==">Rosinol 2019 - Incremental Visual-Inertial 3D mesh Generationwith Structural Regularities<i class="fa fa-external-link-alt"></i></span> 논문에서 나온 기법대로 매 프레임 메쉬를 만든다 (논문에서는 <strong>Per-frame mesh</strong>라고 칭한다). 성공적으로 tracking된 2D feature들을 모아 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRGVsYXVuYXlfdHJpYW5ndWxhdGlvbg==">2D Delaunay triangulation<i class="fa fa-external-link-alt"></i></span>을 한 후, back-projection(역투영)을 통해 3D map에 있는 점들과 association을 수행하여 3D mesh를 생성한다. Per-frame mesh는 정확하지는 않으나 짧은 시간 내에 연산이 가능하기 때문에 간단한 장애물 회피 목적으로 사용할 수 있다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.31.30%20AM2.gif" class="" title="per frame mesh">

<p>여러개의 Per-frame mesh를 취합해서 좀 더 정확한 mesh를 만들면 <strong>Multi-frame mesh</strong>가 만들어진다. 새로운 Per-frame mesh가 만들어질 때 마다 t-1 시점의 Multi-frame mesh와 비교해서 새로운 vertices를 추가한다. 또, VIO backend에서 최적화 연산이 끝날 때 마다 Multi-frame mesh에서 모든 vertices 포지션을 업데이트한다. 또, 오래된 vertices는 삭제한다. Multi-frame mesh는 sliding-window mesh라고 보면 쉽게 이해할 수 있다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.33.45%20AM.gif" class="" title="multi frame mesh">

<p> </p>
<h2 id="Kimera-Semantics-3D-Metric-Semantic-Reconstruction"><a href="#Kimera-Semantics-3D-Metric-Semantic-Reconstruction" class="headerlink" title="Kimera-Semantics: 3D Metric-Semantic Reconstruction"></a>Kimera-Semantics: 3D Metric-Semantic Reconstruction</h2><blockquote>
<p>Bundled raycasting을 이용해서 global mesh 생성. 3D bayesian update를 이용해서 Metric-semantic mesh 생성</p>
</blockquote>
<p>Kimera-Semantics는 Global mesh를 만들고 semantic annotation을 하는 기능을 가지고있다.</p>
<p><strong>Global Mesh</strong>는 scene 전체에 해당하는 mesh를 의미하며, <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2V0aHotYXNsL3ZveGJsb3g=">Voxblox<i class="fa fa-external-link-alt"></i></span>의 bundled raycasting 기법을 사용해서 <span class="exturl" data-url="aHR0cHM6Ly9yc3MxNi1yZXByZXNlbnRhdGlvbnMubWl0LmVkdS9wYXBlcnMvQmV5b25kR2VvbWV0cnlSU1NXMTZfMl9DYW1lcmFSZWFkeVN1Ym1pc3Npb25fT2xleW5pa292YS5wZGY=">TSDF<i class="fa fa-external-link-alt"></i></span> (Truncated signed distance field) 모델을 만들어 노이즈를 제거한 후, <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTWFyY2hpbmdfY3ViZXM=">Marching cubes<i class="fa fa-external-link-alt"></i></span> 알고리즘을 통해 만들어집니다. 이 기법을 사용하기 위해서는 매 keyframe마다 depth map이 필요한데, RGB-D 센서를 사용할 경우에는 depth 센서로부터 이를 취득하고, Stereo 센서를 사용할 경우에는 Dense stereo (i.e. <span class="exturl" data-url="aHR0cHM6Ly9jb3JlLmFjLnVrL2Rvd25sb2FkL3BkZi8xMTEzNDg2Ni5wZGY=">semi-global matching<i class="fa fa-external-link-alt"></i></span>)를 통해 구할 수 있습니다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.34.11%20AM.gif" class="" title="global mesh">

<p><strong>Semantic annotation</strong>은 global mesh의 vertex마다 semantic class label을 부여해주는 기능을 가지고 있다. Semantic class 추론은 주로 2D semantic segmentation을 통해서 하는데, 다양한 뉴럴넷 기반의 방법이 있겠지만 논문에서는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21hdHRlcnBvcnQvTWFza19SQ05O">Mask-RCNN<i class="fa fa-external-link-alt"></i></span>을 사용했다고 한다. 2D label을 3D로 전파하는 방법은 다음과 같다: 1. 2D 이미지에 Semantic segmentation을 수행한다. 2. Global mesh를 생성할 때 스테레오 2D 이미지를 재료로 Dense stereo 기법을 통해 3D point를 만들었을텐데, semantic label을 3D point에 할당한다. 3. Global mesh를 생성할 때 bundled raycasting 기법을 사용했을 텐데, 이 때 ray들마다 semantic class가 몇번 출현했는지를 세어서 label probability를 기록한다. 4. TSDF 연산을 할 때 이 정보도 함께 파싱하고, 이후 매 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE2MDkuMDUxMzA=">SemanticFusion<i class="fa fa-external-link-alt"></i></span>에서 사용한 방법과 비슷하게 bayesian update 기법을 통해 voxel마다 semantic clas probability를 연산한다. 5. 가장 높은 확률을 가진 label을 할당시킨다. 이후, Global mesh 생성의 마지막 단계인 marching cubes 기법을 사용해서 최종 metric-semantic mesh를 생성한다. 이 연산 과정은 굉장히 오래 걸리나 (~0.1s), 최종 결과는 multi-frame mesh보다도 훨씬 정확하다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.34.39%20AM.gif" class="" title="semantic annotation">

<p> </p>
<h2 id="Kimera-PGMO-Pose-graph-and-Mesh-Optimization-with-Loop-Closures"><a href="#Kimera-PGMO-Pose-graph-and-Mesh-Optimization-with-Loop-Closures" class="headerlink" title="Kimera-PGMO: Pose graph and Mesh Optimization with Loop Closures"></a>Kimera-PGMO: Pose graph and Mesh Optimization with Loop Closures</h2><blockquote>
<p>PCM 알고리즘 기반 Loop closure detection. Pose graph + Mesh 최적화</p>
</blockquote>
<p>Kimera-Semantics에서 꽤나 정확한 mesh가 생성된다고 해도, 이 mesh는 결국 VIO 모듈에서 에러가 누적된 pose를 (i.e. drift) 기반으로 만들어진 것이기 때문에 global scale에서 부정확한 mesh가 만들어질 것이다. 보통 SLAM에서는 <span class="exturl" data-url="aHR0cHM6Ly95b3V0dS5iZS9CYXFTUmY1cEFaMA==">loop closure<i class="fa fa-external-link-alt"></i></span>를 통해 drift를 해결하는데, Kimera에서는 loop closure detection 후 landmark 최적화를 할 때 mesh deformation 모델을 사용한다. 이는 매 최적화 스텝마다 새롭게 mesh를 생성하는 기법이나, point-cloud 로 바꿔주는 de-integration 기법보다는 훨씬 효율적인 방법이다.</p>
<p>Loop closure detection은 Feature-based SLAM에서 많이 사용하는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2RvcmlhbjNkL0RCb1cy">DBoW2<i class="fa fa-external-link-alt"></i></span> 기법을 사용한다. 기본적인 outlier rejetion을 위해 5-point RANSAC (monocular)과 Stereo 3-point RANSAC을 사용한다. 이후, Kimera의 목적에 맞게 개량한 <strong>Pairwise Consistent Measurement Set Maximization (PCM)</strong> 이라는 기법을 사용해 추가적인 outlier rejection을 수행한다. Kimera PCM은 기존의 multi-robot 사이의 loop closure를 위한 PCM 기법을, 단일 로봇 odometry의 loop closure로 개량한 것이다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-12%20at%203.42.28%20PM.png" class="" title="Kimera PCM">

<p>Kimera PCM은 다음과 같이 동작한다. (절대 그럴리는 없겠지만) drift가 전혀 없다고 했을 때, 이론적으로는 loop closure가 나타난다면 pose들을 쌓았을 때 loop closure가 나타나는 부분은 identity로 포즈가 나타나야한다. 하지만 drift는 항상 존재하는데, Kimera PCM에서는 이 drift를 measurement noise로 취급하여 값을 추적하고 있고, loop closure detection이 수행될 때 이 measurement noise를 Chi-squared test를 통해 ‘drift가 없었다면 factor graph 속 loop의 pose 합들이 identity로 수렴할 것인가?’를 묻는다. 이 테스트를 통과하면 1차 테스트를 통과한 것이다 (논문에서는 이를 <strong>odometry check</strong>라고 부른다). 2차 테스트는 <strong>pairwise consistent check</strong> 라고 부르는데, 과거의 loop closure와 현재의 loop closure가 둘 다 같은 loop에 들어있어야만 성공한다는 조건을 건다 (위 그림을 보면 좀 더 이해가 쉽다 - l1와 l2가 둘 다 같은 루프에 들어있어야한다).</p>
<p>성공적으로 Loop closure detection을 수행하면 <strong>pose graph와 mesh 최적화</strong>를 진행한다. Pose graph만 최적화를 하려면 Kimera-RPGO를, Pose graph와 Mesh 최적화를 하려면 Kimera-PGMO를 사용한다. Mesh 최적화는 deformation 기반의 방법을 사용하는데, 기존의 point cloud 최적화와는 다른 Loss function을 가진다. Kimera-semantic으로 생성된 mesh는 굉장히 촘촘한데, 이를 최적화에 바로 넣기에는 너무 많은 연산량을 요구하게 된다. 연산량을 효과적으로 감소시키기 위해 <strong>mesh를 간소화</strong>하는데, <span class="exturl" data-url="aHR0cHM6Ly9rby53aWtpcGVkaWEub3JnL3dpa2kvJUVEJThDJTk0JUVDJUE3JTg0JUVEJThBJUI4JUVCJUE2JUFD">octree<i class="fa fa-external-link-alt"></i></span> 구조에 mesh 정보를 담고, 동일 voxel에 있는 vertex는 하나로 합쳐버림으로써 mesh 간소화를 진행할 수 있다. 물론, 이 방식은 정확도를 희생해 속도를 얻는 방법이기 때문에, 성능이 좋은 하드웨어 플랫폼을 사용한다면 voxel 크기를 조정해서 정확도를 더 높일 수 있다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-12%20at%204.52.45%20PM.png" class="" title="Mesh 최적화">

<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-12%20at%204.55.38%20PM.png" class="" title="Mesh 최적화 수식">

<p>최적화를 위한 Factor graph에는 Mesh vertices와 VIO pose가 node로 있고, Pose-Pose (i.e. odometry), Pose-Mesh_vertex 의 association, Mesh_vertex-Mesh-vertex의 local rigidity association이 edge로 있다. Vertex <code>k</code>의 위치는 Mesh의 coordinate frame에서 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.714ex" xmlns="http://www.w3.org/2000/svg" width="14.545ex" height="2.628ex" role="img" focusable="false" viewBox="0 -846 6429 1161.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="TeXAtom" transform="translate(970, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1666.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2722, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msubsup" transform="translate(3000, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(759, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(759, -307.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4552.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(4996.8, 0)"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="TeXAtom" transform="translate(361, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(361, -307.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6151, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>로 표현된다.</p>
<p>Mesh 최적화는 위의 수식을 따른다. <code>X</code>는 VIO 시스템의 pose, <code>g</code>는 mesh 속 vertex의 초기 위치 (i.e. deformation 이전의 위치), <code>Z</code>는 loop closure detection으로 얻은 상대적인 자세 변화 (i.e. relative transformation), <code>R</code>과 <code>t</code>는 뒤에 <code>^M</code>이 붙을 경우 Mesh의 rotation/translation, <code>^X</code>가 붙을 경우 VIO pose의 rotation/translation을 의미한다. 총 3개의 값들이 더해져서 최종 Loss를 정의하는데, 각각 다음과 같은 점을 의미한다.</p>
<ul>
<li>첫번째 값: Kimera PCM이 정의하는 loop closure가 이뤄질 경우 누적된 odometry의 오차가 최소화되야한다는 점</li>
<li>두번째 값: 연결되어있는 mesh vertices끼리 Local rigidity를 유지해야한다는 점 (i.e. 최적화 과정에서 소수의 vertex가 구조를 깨부수며 튀어나오거나 사라지는 것을 방지한다)</li>
<li>세번째 값: 연결되어있는 VIO_pose - Mesh vertices 끼리 local rigidity를 유지해야한다는 점 (i.e. 최적화 과정에서 mesh 전체가 튀어버리거나 pose가 튀어버리는 것을 방지한다)</li>
</ul>
<p>각각의 값들에 대한 loss는 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.712ex" xmlns="http://www.w3.org/2000/svg" width="5.932ex" height="2.599ex" role="img" focusable="false" viewBox="0 -833.9 2621.9 1148.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(444.7, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="msubsup" transform="translate(1783.3, 0)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" transform="translate(278, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(278, -314.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"></path></g></g></g></g></g></svg></mjx-container>는 <span class="exturl" data-url="aHR0cHM6Ly9tYXRod29ybGQud29sZnJhbS5jb20vRnJvYmVuaXVzTm9ybS5odG1s">weighted Frobenius norm<i class="fa fa-external-link-alt"></i></span>을 사용한다 (논문에서는 제곱식을 빼먹었는데, 이는 명백한 오타이다). Frobenius norm은 matrix에 적용된 L2 norm이라고 생각하면 편하다 (L2 norm은 주로 vector에 사용하는 단어이고, 비슷한 의미를 matrix에 적용할 시 Frobenius norm이라고 표현한다). Weighted Frobenius norm은 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.441ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 4615 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(361, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(812, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1201, 0)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="TeXAtom" transform="translate(903.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g><g data-mml-node="mi" transform="translate(2652, 0)"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"></path></g><g data-mml-node="mi" transform="translate(3374, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(4226, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 로 계산된다. 여기서 Weight (i.e. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.633ex" height="1.593ex" role="img" focusable="false" viewBox="0 -704 722 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"></path></g></g></g></svg></mjx-container>) 값은 보통 초기 값으로 1을 넣은 후, weighted least squares에서 차차 업데이트 되는 방식을 택하는게 일반적인데, 이 부분은 아직 코드를 확인해보지 않아서 실제로 이렇게 구현되어있는지는 잘 모르겠다.</p>
<p>논문에서는 vio_rotation, vio_translation, mesh_rotation, mesh_translation와 같은 변수들을 아래의 치환식을 통해 <span class="exturl" data-url="aHR0cDovL2ppbnlvbmdqZW9uZy5naXRodWIuaW8vMjAxNi8wNi8wNy9zZTNfc28zX3RyYW5zZm9ybWF0aW9uLw==">SE(3) pose matrix<i class="fa fa-external-link-alt"></i></span> 형태로 바꿈으로써 문제를 좀 더 간단하게 만들었다. 논문에서는 간단하게 만든 이 수식을 ‘augmented pose graph optimization problem’이라고 칭한다 (하지만 사실 치환만 한 것일 뿐, 내용은 크게 바뀌는게 없다).</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-14%20at%205.01.41%20PM.png" class="">

<p>치환식을 거치고나면 기존의 loss 식이 아래와 같이 바뀐다. <code>Z</code>는 모든 odometry와 loop closure edge를 담은 set이 된다. <code>G</code>는 모든 mesh vertex끼리의 edge들의 set을 의미한다. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.778ex" height="2.111ex" role="img" focusable="false" viewBox="0 -911 786 933"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(226.3, 221)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g></g></g></svg></mjx-container>는 mesh vertex - vio pose를 잇는 모든 edge들의 set을 의미한다. 여기서 Mesh local rigidity와 pose-mesh local rigidity에 해당하는 부분 (i.e. 2/3번째 값들)에서 rotation에 대한 information matrix (i.e. weight)는 0로 놓음으로써 translation만 최적화를 한다고 한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-14%20at%205.01.48%20PM.png" class="">

<p>논문에서는 위 수식은 결국 더욱 간단한 형태로 표현된다고 하는데, 솔직히 너무 간단하게 표현해서 정작 중요한 내용은 하나도 보여주지 않는 수식이 하나 있다. <code>T</code>는 pose/mesh vertex의 transformation을, <code>E</code>는 edge의 transformation을 의미하는데, 결국은 ‘이동한 vertex와 edge간의 오차가 최소화되는 SE(3)움직임은 무엇인가?’라는 문제를 푸는 것이다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-14%20at%205.24.43%20PM.png" class="">

<p> </p>
<hr>
<h1 id="Kimera-DSG"><a href="#Kimera-DSG" class="headerlink" title="Kimera-DSG"></a>Kimera-DSG</h1><img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-04%20at%208.36.14%20PM.png" class="" title="Structures">

<p>Kimera-DSG는 <strong>Kimera-core 종료된 후, Kimera-core가 생성한 3D metric-semantic mesh로부터 비-실시간으로 DSG를 생성</strong>한다.</p>
<p><strong>Kimera-Humans</strong>는 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL25rb2xvdC9HcmFwaENNUg==">GraphCMR<i class="fa fa-external-link-alt"></i></span>을 이용해서 사람의 dense mesh를 생성한다. Dense mesh는 Skinned Multi-Person Linear Model (SMPL)을 사용한다. 이후, pose graph model을 이용해서 trajectory를 생성하고 최적화한다.</p>
<p><strong>Kimera-Objects</strong>는 1. 사전에 shape을 정확하게 모르는 객체들에 대해서는 bounding box를 추정, 2. 사전에 shape를 정확하게 알고있는 객체 (i.e. CAD 모델이 존재하는 경우) <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9URUFTRVItcGx1c3BsdXM=">TEASER++<i class="fa fa-external-link-alt"></i></span>를 이용해서 point cloud fitting 및 포즈 추정을 수행한다.</p>
<p><strong>Kimera-BuildingParser</strong>는 최상단 3개의 layer를 생성한다. 우선 Layer 3를 생성할 때는 Metric-semantic mesh로부터 structure를 검출하고 places 에 대한 topological graph도 생성한다 (<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDMuMDQzNDU=">참고 링크<i class="fa fa-external-link-alt"></i></span>). 이후 Layer 3 정보를 기반으로 layer 4를 생성, 이후 layer 4 정보를 기반으로 layer 5를 생성한다.</p>
<p> </p>
<h2 id="Kimera-Humans-Humans-Shape-Estimation-and-Robust-Tracking"><a href="#Kimera-Humans-Humans-Shape-Estimation-and-Robust-Tracking" class="headerlink" title="Kimera-Humans: Humans Shape Estimation and Robust Tracking"></a>Kimera-Humans: Humans Shape Estimation and Robust Tracking</h2><blockquote>
<p>로봇 검출: 자기 자신만 트랙킹함<br>사람 검출: GraphCMR을 통해 SMPL 모델 검출. 다양한 방법으로 안정적인 트랙킹을 수행하며, pose graph 형태로 자세 저장.</p>
</blockquote>
<p>Kimera-Humans에는 <strong>Robot node</strong>와 <strong>Human node</strong>가 있다.</p>
<p>Kimera 시스템 컨셉에서는 환경에서 여러 로봇들도 함께 존재할 것을 생각했다. 주변 환경에 위치한 로봇을 인식해 Robot node로 등록해 인식하고, 중앙 서버에서 pose graph를 관리함으로써 동시 최적화 및 multi-map SLAM이 가능할 것이다. 이 논문에서는 멀티-로봇 환경을 지원하지 않아 Robot node에는 ‘자기 자신’밖에 없다. 하지만 이후에 나온 논문인 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9LaW1lcmEtTXVsdGk=">Kimera-Multi<i class="fa fa-external-link-alt"></i></span>에서는 이와 같은 기능을 지원한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%2012.24.42%20AM.png" class="" title="Kimera multi">

<p>Kimera 시스템은 사람에 대한 dense mesh와 시간에 따른 이동치를 pose grpah 형태로 표현해서 Human node에 담는다. 사람을 인지하기 위해서는 왼쪽 카메라에서 수행한 2D image segmentation 결과에서 사람이 나온 bounding box를 crop한 후, cropped image에서 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL25rb2xvdC9HcmFwaENNUg==">GraphCMR<i class="fa fa-external-link-alt"></i></span> 기법을 수행함으로써 <span class="exturl" data-url="aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS9zZWFyY2g/Y2xpZW50PXNhZmFyaSZybHM9ZW4mcT1TTVBMOitBK3NraW5uZWQrbXVsdGktcGVyc29uK2xpbmVhcittb2RlbCZpZT1VVEYtOCZvZT1VVEYtOA==">SMPL<i class="fa fa-external-link-alt"></i></span> (Skinned Multi-Person Linear Model - 사람의 형태를 가진 dense mesh)를 얻을 수 있다. 이 Dense mesh <a href="">PnP</a> 기법을 통해서 정확한 pose를 구할 수 있는데, 여기서 사람의 골반뼈 부분을 coordinate frame origin으로 잡아서 pose tracking을 수행한다. SNPL mesh 정보는 Kimera-semantics로도 보내지는데, Metric-semantic mesh를 생성할 때 사람의 mesh는 제거하기 위함이다.</p>
<p>여기서 눈여겨볼점은 ‘어떻게 안정적으로 사람의 움직임을 트랙킹하는가?’이다. GraphCMR은 아주 효과적인 detection 방식이지만, 1. temporal tracking 정보를 전혀 사용하지 않고, 2. partial occlusion이 있을 경우 잘못된 detection을 할 수 있다는 치명적인 단점이 있다. Detection이 불안정하면 자세가 크게 튀기 때문에 안정적으로 트랙킹하는 것이 굉장히 중요하나, 사람의 움직임을 안정적으로 트랙킹하는 것은 생각보다 쉽지 않다. Kimera가 사용하는 방법 중 첫번째 방법은 <strong>human pose를 pose graph 형태로 관리</strong>하는 것이다. Kimera-RPGO와 PCM outlier rejection 방식을 사용해서 경로가 스무하게 나오게 하고 또 잘못된 트랙킹이 나타나지 않도록 잡아줄 수 있다. 두번째 방법은 굉장히 야매(…)방법이긴 하다 - 새로운 human node가 나타났을 때 주변에 가장 가까운 human node와 이어져있다는 가정을 걸어버리는 것인데, 이는 <strong>사람이 생각보다 빠르게 움직이지 않는다는 전제</strong> 하에 만든 것이다. 저자들이 참고한 논문에서는 평균 사람의 걷는 속도는 1.25 m/s (<span class="exturl" data-url="aHR0cHM6Ly9wdWJtZWQubmNiaS5ubG0ubmloLmdvdi8yMTg1MzEwNy8=">Schimpl 2011<i class="fa fa-external-link-alt"></i></span> 발췌)라고 하는데, Kimera에서는 안정적이게 3 m/s로 잡고, 또 관절과 몸통의 거리 차이가 3 m 이상 나지 않는 것을 제약조건으로 두었다. 세번째 방법은 <strong>SMPL 모델이 사람의 형태를 정의하는 방법인 beta parameter를 사용</strong>하는 것이다. Beta parameter에는 사람의 형태에 대한 8가지 속성이 있는데, 예를 들어 신장이나 어깨 넓이 같은 것들이 있다. 이 정보를 이용해서도 data association이 가능하다. 마지막으로, 애초에 <strong>GraphCMR이 잘 동작하기 어려운 환경은 아예 동작을 막아버리는 방법</strong>도 있다. Bounding box가 30 픽셀 미만으로 작게 검출될 경우에는 아예 검출 자체를 해버리지 않고, 또 검출이 되어서 pose grpah가 생성되었다고 해도 10개 이상의 node를 지니지 않으면 추후에 DSG를 수행할 때 아예 연산에서 빼버리기도 한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.25.51%20AM.gif" class="" title="Human tracking">

<p> </p>
<h2 id="Kimera-Objects-Object-Pose-Estimation"><a href="#Kimera-Objects-Object-Pose-Estimation" class="headerlink" title="Kimera-Objects: Object Pose Estimation"></a>Kimera-Objects: Object Pose Estimation</h2><blockquote>
<p>형태를 아는 경우: CAD 모델에서 추출한 point cloud를 이용해서 TEASER++로 point cloud registration<br>형태를 모르는 경우: Point cloud clustering 후 bouding box 생성</p>
</blockquote>
<p>Kimera-Objects는 Metric-semantic mesh로부터 정적인 물체들을 검출하는 기능이다. <strong>사전에 형태를 알고 있는 경우</strong> (Objects with known shapes)와 <strong>클래스만 알고 형태는 모르는 경우</strong> (Objects with Unknown shape) 둘 다 모두 검출해서 object node로 넣는다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%201.21.09%20AM.png" class="" title="Human tracking">

<p>형태를 모르는 물체들은 의외로 쉽게 진행될 수 있는데, 이미 metric-semantic mesh에서 semantic한 정보를 가지고 있기 때문이다. 하지만 Mesh 데이터를 순회하면서 다수의 object instance를 떼네 최대한 빠르게 수행하기 위해 metric-semantic mesh 자체를 간단하게 만들 필요가 있다. 우선 <span class="exturl" data-url="aHR0cHM6Ly9wb2ludGNsb3Vkcy5vcmcv">PCL<i class="fa fa-external-link-alt"></i></span> 라이브러리에서 지원하는 <span class="exturl" data-url="aHR0cHM6Ly9wY2wucmVhZHRoZWRvY3MuaW8vZW4vbGF0ZXN0L2NsdXN0ZXJfZXh0cmFjdGlvbi5odG1s">Euclidean clustering<i class="fa fa-external-link-alt"></i></span> 기능을 이용해서 0.1 m 단위로 mesh를 여러개의 object instance로 만든다. 이후, object centroid를 연산해서 position을 연산, world frame과 동일한 orientation을 할당한 후, 이 정보들을 기반으로 3D bounding box를 연산해서 object node에 저장한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.22.45%20AM.gif" class="" title="TEASER">

<p>형태를 알고 있는 물체들은 pose를 더 정확하게 구할 수 있다는 장점이 있다. 형태를 알고 있다는 것은 주로 <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcHV0ZXItYWlkZWRfZGVzaWdu">CAD 모델<i class="fa fa-external-link-alt"></i></span>을 가지고 있다는 것을 의미하는데, Kimera-objects는 CAD 모델을 3D point cloud로 변환한 후 <span class="exturl" data-url="aHR0cHM6Ly93d3cuaXZhbi1zaXBpcmFuLmNvbS9wYXBlcnMvU0IxMWIucGRm">3D Harris keypoints<i class="fa fa-external-link-alt"></i></span>를 추출한 후, Metric-semantic mesh에 있는 모든 포인트들과 매칭을 해본다. 이 때 연산량이 아주 높을 수 있는데, 정확하고 빠르게 point cloud registration을 수행하기 위해 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL01JVC1TUEFSSy9URUFTRVItcGx1c3BsdXM=">TEASER++<i class="fa fa-external-link-alt"></i></span>를 이용한다. Registration에 성공하면 object의 3d pose를 정확하게 구할 수 있다.</p>
<p> </p>
<h2 id="Kimera-BuildingParser-Extracting-Places-Rooms-and-Structures"><a href="#Kimera-BuildingParser-Extracting-Places-Rooms-and-Structures" class="headerlink" title="Kimera-BuildingParser: Extracting Places, Rooms and Structures"></a>Kimera-BuildingParser: Extracting Places, Rooms and Structures</h2><blockquote>
<p>Places<br>Structures<br>Rooms</p>
</blockquote>
<p>Kimera-BuildingParser는 places, structures, rooms를 검출하는 각각의 방법을 담아놓은 패키지이다.</p>
<p><strong>Places</strong>를 검출하기 위해서는 우선 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2V0aHotYXNsL3ZveGJsb3g=">Voxblox<i class="fa fa-external-link-alt"></i></span>에서 지원하는 global mesh 재구성법 (i.e. bundled raycasting)과 ESDF 재구성법을 이용한다. 이후 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE4MDMuMDQzNDU=">Voxblox의 저자가 적은 논문<i class="fa fa-external-link-alt"></i></span>에서 사용하는 기법처럼 ESDF에서부터 free space를 sparse sampling을 통해 검출하여 topological map을 만든다. Topological map은 그래프 형태로 되어있는데, node는 free space (즉, place)를 의미하고, edge는 traversability (i.e. 이동 가능성)을 의미한다. 이후, 가장 가까운 objects와 agents를 places에 이어준다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.02.31%20AM.png" class="" title="places">

<p><strong>Structures</strong>는 이미 Kimera-semantics에서 정보를 다 추출해놓았기 때문에 특별한 검출 단계를 필요로 하지 않는다. 벽 (wall)에 대한 normal 방향을 구한 후, 해당 벽에 대해 가장 가까운 place를 찾아서 연결을 시켜놓은다.</p>
<p><strong>Rooms</strong>를 연산할 때는, Kimera-VIO를 통해 얻을 수 있는 중력의 방향을 기준으로 3D ESDF 지도를 cross-section으로 잘라서 2D map을 생성한다. 이 때 천장의 높이보다 0.3 m 정도 낮게 자르는데, 큰 물건들이 벽으로 인식되는 것을 피하기 위해 이러한 휴리스틱을 사용한다. 이 룰을 따라 만든 2D 지도를 우리는 <strong>2D ESDF</strong>라고 부른다. 이 2D ESDF 지도에서 두께가 0.2 m가 넘는 것들만 남김으로써 문 중턱이라던지 작은 파티셔닝들을 제거한다. 큰 파티셔닝으로 나눠진 공간들로부터 Room을 검출할 수 있게 된다. 아래 그림에서 좌측이 2D ESDF, 우측이 Truncated 2D ESDF 이다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.01.07%20AM.png" class="" title="rooms">

<p> </p>
<hr>
<h1 id="오픈소스를-하는-사람들이-Kimera에서-배울만한-점"><a href="#오픈소스를-하는-사람들이-Kimera에서-배울만한-점" class="headerlink" title="오픈소스를 하는 사람들이 Kimera에서 배울만한 점"></a>오픈소스를 하는 사람들이 Kimera에서 배울만한 점</h1><ul>
<li>Jenkins 기반의 CI 서버를 통해 빌드 테스트, 유닛 테스트, 런타임 테스트를 수행한다.</li>
<li>VIO 디버깅을 위한 정보를 Jupyter Notebook을 통해 확인할 수 있다 (e.g. feature tracking의 품질, IMU preintegration 에러)</li>
<li>3D reconstruction의 결과를 Open3D로 볼 수 있다.</li>
</ul>
<p> </p>
<hr>
<h1 id="실험-결과"><a href="#실험-결과" class="headerlink" title="실험 결과"></a>실험 결과</h1><h2 id="VIO-정확도-RMSE"><a href="#VIO-정확도-RMSE" class="headerlink" title="VIO 정확도 (RMSE)"></a>VIO 정확도 (RMSE)</h2><p>많이 사용되는 VIO인 OKVIS, MSCKF, ROVIO, VINS-Mono, SVO 보다 대체적으로 더 좋은 결과를 보여준다.</p>
<p>DVIO는 Kimera가 지원하는 motion estimation 알고리즘 중 5-point나 2-point가 아닌 IMU-aware feature tracking + 2-point stereo RANSAC을 사용한 경우를 DVIO라고 부른다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.06.48%20AM.png" class="" title="rmse1">

<p> </p>
<h2 id="VIO-Dynamic-Masking"><a href="#VIO-Dynamic-Masking" class="headerlink" title="VIO + Dynamic Masking"></a>VIO + Dynamic Masking</h2><p>VIO를 수행할 때 주변 환경에 움직이는 객체가 있을 경우 정확도가 많이 떨어지기도 한다.</p>
<p>Kimera에서는 Kimera-Humans를 이용해서 mesh로부터 dynamic 객체에 대한 부분을 제거하기 때문에, 더욱 높은 정확도를 얻어낼 수 있었다는 것을 보여준다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screen%20Recording%202023-04-16%20at%201.26.28%20AM.gif" class="" title="dynamic masking">

<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.09.15%20AM.png" class="" title="dynamic masking rmse">

<p> </p>
<h2 id="PCM이-왜-좋은가"><a href="#PCM이-왜-좋은가" class="headerlink" title="PCM이 왜 좋은가?"></a>PCM이 왜 좋은가?</h2><p>Loop closure를 할 때 PCM은 파라미터를 어떤 것을 설정하던지 좋은 결과를 보인다. PCM을 사용하지 않는다면, scene마다 사용자가 직접 파라미터 튜닝을 해줘야할 것이다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.13.54%20AM.png" class="" title="pcm ate">

<p> </p>
<h2 id="Global-mesh의-정확도"><a href="#Global-mesh의-정확도" class="headerlink" title="Global mesh의 정확도"></a>Global mesh의 정확도</h2><p>Multi-frame mesh를 기반으로 최적화를 수행해서 global mesh가 되었을 때, 정확도가 대부분의 경우 많이 올라간다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.14.02%20AM.png" class="" title="multi->global">

<p>최종 mesh를 Ground truth 결과와 비교했을 때, 상당히 높은 정확도 + 높은 완성도를 보인다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.12.04%20AM.png" class="" title="mesh rmse">

<p> </p>
<h2 id="사람-검출"><a href="#사람-검출" class="headerlink" title="사람 검출"></a>사람 검출</h2><p>수많은 check를 거쳐서 localization error가 꽤 줄어들음.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.21.43%20AM.png" class="" title="human detection">

<p> </p>
<h2 id="방-검출"><a href="#방-검출" class="headerlink" title="방 검출"></a>방 검출</h2><p>간단한 건축 구조에서는 잘 됨 (좌측)</p>
<p>중앙에 탁자가 있는 방이 복도와 이어져서 다른 방과 연결되는 복잡한 건축 구조에서는 (i.e. 명확하게 문을 통해 분리가 되지 않은 건축 구조) 잘 안됨. (우측)</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.22.55%20AM.png" class="" title="room parsing">

<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.37.36%20AM.png" class="">

<p> </p>
<h2 id="잘-안되는-곳"><a href="#잘-안되는-곳" class="headerlink" title="잘 안되는 곳"></a>잘 안되는 곳</h2><p>논문에서는 종종 정확도가 팍 떨어지는 부분이 있다고 하는데, 주로 stereo 카메라를 사용해서 dense stereo로 depth를 추정할 때 feature가 잘 잡히지 않는 하얀 벽과 같은 곳에서 정확도가 떨어진다고 한다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.19.27%20AM.png" class="" title="fail">

<p> </p>
<h2 id="속도-PC"><a href="#속도-PC" class="headerlink" title="속도 (PC)"></a>속도 (PC)</h2><p>아쉽게도 컴퓨터 스펙이 안나와있다. 아래 알려져있는 속도 벤치마크이다.</p>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.36.14%20AM.png" class="" title="PC timings">
<ul>
<li>Kimera-VIO<ul>
<li>IMU preintegration: 40us</li>
<li>Feature tracking: 7.5ms</li>
<li>Feature detection, Stereo matching, Geometric verification (Keyframe-only): 51ms</li>
</ul>
</li>
<li>Kimera-Mesher<ul>
<li>Per-frame 3D mesh generation: 7ms</li>
<li>Multi-frame 3D mesh generation: 15ms</li>
<li>Factor graph optimization: 60ms</li>
</ul>
</li>
<li>Kimera-Objects<ul>
<li>3 min for ~100m^2</li>
<li>12min for ~3000m^2</li>
</ul>
</li>
<li>Kimera-Humans<ul>
<li>GraphCMR: 33ms (NVIDIA RTX 2080Ti)</li>
<li>Tracking: 10ms</li>
</ul>
</li>
<li>Kimera-BuildingParser<ul>
<li>ESDF generatioN: 10 min</li>
<li>ESDF sparse sampling (i.e. places): 10 min</li>
<li>Room detection: 2 min</li>
</ul>
</li>
</ul>
<p> </p>
<h2 id="속도-NVIDIA-Jetson-TX2"><a href="#속도-NVIDIA-Jetson-TX2" class="headerlink" title="속도 (NVIDIA Jetson TX2)"></a>속도 (NVIDIA Jetson TX2)</h2><img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.35.25%20AM.png" class="" title="tx2">

<p>논문에서는 ‘Kimera는 Embedded device에서도 돌아요!’라고 홍보하지만, 사실상 VIO만 돈다는게 정론.<br>Kimera-Semantics에 대한 부분도 벤치마크 했다고 하는데, 사실상 Mesh화 알고리즘과 Segmentation도 뺐기 때문에 거의 없다고 보면 된다.<br>그 외의 DSG 관련 작업은 아예 안돈다고 보면 된다.</p>
<p>MAXN 모드를 사용 (가장 전력을 많이 소비하되, 가장 빠른 모드)</p>
<p>Kimera-Core만 측정했다고 함 (사실 다른건 TX2에서 돌 수 없음 ㅋㅋ)</p>
<p>‘Faster’ config는 fast의 250-&gt;200 개 feature만 트랙킹하고, backend optimization도 5-&gt;4.5초 window로 줄어들음.</p>
<ul>
<li>Frontend: 10ms (Non-keyframe)</li>
<li>Frontend: 70ms (Keyframe-only)</li>
<li>Backend: 60ms</li>
<li>Kimera-Semantics: 65.8 ms</li>
</ul>
<p> </p>
<hr>
<h1 id="Kimera-DSG의-사용처"><a href="#Kimera-DSG의-사용처" class="headerlink" title="Kimera/DSG의 사용처"></a>Kimera/DSG의 사용처</h1><p>아래는 저자들이 추천/생각하는 Kimera / 3D DSG의 사용처이다.</p>
<ul>
<li>Obstacle avoidance and planning<ul>
<li>Hierarchical path planning</li>
<li>Semantic path palnning</li>
</ul>
</li>
<li>Human-Robot interaction<ul>
<li>Question Answering</li>
</ul>
</li>
<li>Long-term autonomy</li>
<li>Prediction</li>
</ul>
<img src="/20230329-rosinol-2021-kimera-3d-dgs/Screenshot%202023-04-16%20at%202.45.04%20AM.png" class="" title="tx2">]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>Semantic SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Python 디버깅 빠르고 편하게 하기</title>
    <url>/20230411-python3-debugging-tip/</url>
    <content><![CDATA[<h1 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h1><p>파이썬 코드를 디버깅 할 때 두가지 방법이 있다.</p>
<ol>
<li>IDE로 디버깅한다</li>
<li><code>print()</code>를 한다.</li>
</ol>
<p>1번은 IDE에서 지원하는 디버거를 통해 아주 디테일한 내용까지 볼 수 있다는 장점이 있다. 하지만, CPython을 사용하는 경우 초반 로딩이 느릴 수 있으며, 또 원하지 않는 여러가지 정보까지 보여주기 떄문에 때로는 더 복잡할 수도 있다.</p>
<p>2번은 원하는 변수의 값을 바로 볼 수 있다는 점에서 아주 유용하다. 하지만 break point를 찍을 수 없기 때문에 프로그램이 계속 실행된다는 단점이 있고, 종종 데이터를 덮어씌운다던지 하는 치명적인 실수를 일으킬 수도 있다.</p>
<p> </p>
<hr>
<h1 id="솔루션"><a href="#솔루션" class="headerlink" title="솔루션"></a>솔루션</h1><p><code>print()</code> 대신 <code>import pdb; pdb.set_trace()</code> 를 사용하자.</p>
<p>물론 명령어가 길어져서 귀찮아질 수도 있다. 하지만 1번과 2번의 장점을 그대로 가질 수 있는게 이 방법이다.</p>
<p>위 커맨드를 입력하고 코드를 실행하면, IDE의 로딩 없이 바로 코드를 원래 속도대로 실행할 수 있다.</p>
<p>그 후, breakpoint 셋팅 없이 우리가 입력한 코드까지 가서 멈춘다.</p>
<p>거기서 원하는 변수를 입력하면 print 해준다.</p>
<p>완벽해!</p>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.3 Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Norm 세는 방법 (L1, L2, L3, L4... L_infinity)</title>
    <url>/20230413-vector-norms/</url>
    <content><![CDATA[<p>Vector norm을 세는 수식이 있었다… (문화충격! 이걸 몰랐다!)</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.827ex" xmlns="http://www.w3.org/2000/svg" width="20.446ex" height="7.367ex" role="img" focusable="false" viewBox="0 -2006.9 9037.3 3256.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(278, 0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="msub" transform="translate(885, 0)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278, -150) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g><g data-mml-node="mo" transform="translate(1846.5, 0)"><path data-c="2261" d="M56 444Q56 457 70 464H707Q722 456 722 444Q722 430 706 424H72Q56 429 56 444ZM56 237T56 250T70 270H707Q722 262 722 250T707 230H70Q56 237 56 250ZM56 56Q56 71 72 76H706Q722 70 722 56Q722 44 707 36H70Q56 43 56 56Z"></path></g><g data-mml-node="msup" transform="translate(2902.2, 0)"><g data-mml-node="mrow"><g data-mml-node="mo"><path data-c="28" d="M758 -1237T758 -1240T752 -1249H736Q718 -1249 717 -1248Q711 -1245 672 -1199Q237 -706 237 251T672 1700Q697 1730 716 1749Q718 1750 735 1750H752Q758 1744 758 1741Q758 1737 740 1713T689 1644T619 1537T540 1380T463 1176Q348 802 348 251Q348 -242 441 -599T744 -1218Q758 -1237 758 -1240Z"></path></g><g data-mml-node="munder" transform="translate(792, 0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="mi" transform="translate(600, -1084.4) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(2402.7, 0)"><g data-mml-node="mrow"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(278, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1144, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g><g data-mml-node="mi" transform="translate(1422, 476.6) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g><g data-mml-node="mo" transform="translate(4230.3, 0)"><path data-c="29" d="M33 1741Q33 1750 51 1750H60H65Q73 1750 81 1743T119 1700Q554 1207 554 251Q554 -707 119 -1199Q76 -1250 66 -1250Q65 -1250 62 -1250T56 -1249Q55 -1249 53 -1249T49 -1250Q33 -1250 33 -1239Q33 -1236 50 -1214T98 -1150T163 -1052T238 -910T311 -727Q443 -335 443 251Q443 402 436 532T405 831T339 1142T224 1438T50 1716Q33 1737 33 1741Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(5022.3, 1476.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(1000, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></g></g></svg></mjx-container></p>
<p> <br>그렇기 때문에 (1, 2, 3)이라는 vector가 있다면 </p>
<ul>
<li>L1 norm = 6</li>
<li>L2 norm = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.213ex" xmlns="http://www.w3.org/2000/svg" width="4.192ex" height="2.398ex" role="img" focusable="false" viewBox="0 -966 1853 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853, 0)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500, 0)"></path></g></g><g data-mml-node="mo" transform="translate(0, 106)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="1000" height="60" x="853" y="846"></rect></g></g></g></svg></mjx-container></li>
<li>L3 norm = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="3.644ex" height="2.139ex" role="img" focusable="false" viewBox="0 -923.4 1610.7 945.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(500, 393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mn" transform="translate(1000, 0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></g></svg></mjx-container></li>
<li>L4 norm = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.239ex" xmlns="http://www.w3.org/2000/svg" width="6.705ex" height="2.398ex" role="img" focusable="false" viewBox="0 -954.5 2963.7 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(500, 393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500, 0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mn" transform="translate(1000, 0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g></g><g data-mml-node="msqrt" transform="translate(1610.7, 0)"><g transform="translate(853, 0)"><g data-mml-node="mn"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 94.5)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="500" height="60" x="853" y="834.5"></rect></g></g></g></svg></mjx-container></li>
<li>L_infinity norm = 3</li>
</ul>
<p>이 된다.</p>
]]></content>
      <categories>
        <category>4. Maths</category>
      </categories>
      <tags>
        <tag>Linear algebra</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - Pretraining in robotics 워크샵 (Day 1)</title>
    <url>/20230529-icra2023-day-1-1/</url>
    <content><![CDATA[<h2 id="Pretraining-in-Robotics"><a href="#Pretraining-in-Robotics" class="headerlink" title="Pretraining in Robotics"></a>Pretraining in Robotics</h2><p>Brown 대학에서 박사과정 중인 Ifrah Idrees의 <span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9UGpZVWxmcExKRQ==">Building Long-term Spatial Temporal Semantic Map<i class="fa fa-external-link-alt"></i></span> 스포트라이트 토크를 보았다.</p>
<img src="/20230529-icra2023-day-1-1/Screenshot%202023-05-30%20at%201.40.17%20PM.png" class="" title="d3a 2">

<p>기본적인 SLAM 알고리즘을 기반으로 지도를 만들면서 object detection을 수행할 때, 해당 지도에 특정 object에 대해 누적된 view를 저장해준다는 아이디어이다.</p>
<p>어떤 물체에 대해 object detection을 성공했을 때, temporal window 내부에 정보를 저장하고 비슷한 정보끼리 clustering 함으로써 물체를 바라보고 있을 때의 view를 모을 수 있다.</p>
<p>어떤 물체가 어디에 있는지 query를 할 수 있는데, 이는 CLIP 기반 visual language model을 사용해서 language 기반 쿼리를 통해 지도에서 어디에 object view에 대한 temporal window가 존재하는지 찾아낼 수 있다.</p>
<img src="/20230529-icra2023-day-1-1/Screenshot%202023-05-30%20at%201.39.59%20PM.png" class="" title="d3a 1">

]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>ICRA</tag>
        <tag>Workshop</tag>
        <tag>Pretraining in robotics</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - Unconventional spatial representations Opportunities for robotics 워크샵 (Day 1)</title>
    <url>/20230529-icra2023-day-1-2/</url>
    <content><![CDATA[<blockquote>
<p>Teresa Vidal Calleja의 “Physics driven, continuous and probabilistic representations for localisation, mapping and planning” 발표를 중간부터 청취</p>
</blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly91c3IyMDIzLmdpdGh1Yi5pby8=">워크샵 링크<i class="fa fa-external-link-alt"></i></span>]</p>
<ul>
<li><p>Mapping을 통해 지도륾 만들고 해당 지도를 기반으로 Planning을 하는 방법이 아닌, <strong>Joint optimization of mapping and planning</strong>을 제안한다.</p>
<ul>
<li>Mapping과 Planning을 따로 하면 문제가 2개가 된다.</li>
<li>Joint optimization으로 만듦으로써 하나의 문제가 되므로, 속도가 빨라질 수 있는 여지가 있다.<ul>
<li>실시간 모바일 로보틱스에서는 좋을 수 있겠다.</li>
<li>흠… 이 방식이 스탠다드가 된다면, 많은 Perception 회사들에게는 타격이 될 수 있겠다.</li>
</ul>
</li>
</ul>
</li>
<li><p>Physics-based localization and mapping을 제안한다.</p>
<ul>
<li>자기장의 변화를 인지하여 다수의 agent의 trajectory를 추정하는 방법을 제안했다.</li>
<li>다중 agent가 움직이는 환경에서 빠르게 trajectory planning을 하는 CHOMP라는 planning 기법을 제안했다.</li>
</ul>
</li>
<li><p>전체적으로 모든 프로세스에 Gaussian process가 들어갔다.</p>
<ul>
<li>나는 Gaussian process에 대한 지식이 부족하다. 좀 더 공부해야겠다.</li>
</ul>
</li>
</ul>
<p> </p>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>ICRA</tag>
        <tag>Workshop</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - Distributed graph algorithms 워크샵 (Day 1)</title>
    <url>/20230529-icra2023-day-1/</url>
    <content><![CDATA[<h2 id="Distributed-planning-for-Multi-quadrotor"><a href="#Distributed-planning-for-Multi-quadrotor" class="headerlink" title="Distributed planning for Multi-quadrotor"></a>Distributed planning for Multi-quadrotor</h2><blockquote>
<p>Angela Schoellig 교수님의 발표</p>
</blockquote>
<ul>
<li>Distributed algorithm을 사용하는 이유는 무엇인가?<ul>
<li>(Central 알고리즘은 하나의 성능 좋은 컴퓨터에서 모든 것을 계산하는 것, Distributed algorithm은 다수의 소형 컴퓨터들이 서로 소통을 하며 계산을 하는 것이다)</li>
<li>시스템 적으로 scale up 하기가 쉽다. 시스템이 3개의 agent를 쓰나, 10개를 쓰나, 100개를 쓰나 알고리즘 자체는 똑같이 유지할 수 있다. </li>
<li>Robustness를 높일 수 있다. (조금 약한 주장이긴 한데, distributed algorithm은 각가의 소형 컴퓨터에서 빠르게 계산을 함으로써, quadrotor motion에 문제가 생겼을 때 다시 빠르게 re-planning을 할 수 있기 때문에 robustness가 높아진다는 주장을 했다. Planning에서만 가능한 주장으로 보인다.)</li>
</ul>
</li>
</ul>
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%204.59.43%20PM.png" class="" title="multirotor planning">

<p>(사진 색깔이 왜 이러지…)</p>
<ul>
<li>교수님의 랩실에서는 순차적으로 아래와 같은 연구를 진행하셨다.<ul>
<li>Offline central multi-agent path planning</li>
<li>Offline distributed multi-agent path planning</li>
<li>Online distributed multi-agent path planning</li>
</ul>
</li>
<li>드론 하나가 다른 드론 위로 날아가게 되면, Downward force가 생기니까 trajectory가 꼬이지 않을까? Planning이 이거까지 고려하고 하는건지?</li>
</ul>
<img src="/20230529-icra2023-day-1/IMG_0741.gif" class="" title="distributed">

<h2 id="Robotics-perception-in-search-for-effective-representations"><a href="#Robotics-perception-in-search-for-effective-representations" class="headerlink" title="Robotics perception - in search for effective representations"></a>Robotics perception - in search for effective representations</h2><blockquote>
<p>Margarita Chli 교수님의 발표</p>
</blockquote>
<ul>
<li><p>교수님 연구실에서 풀려고 하시는 연구</p>
<ul>
<li>High-fidelity SLAM</li>
<li>Scene representations for interaction and path planning</li>
<li>Multi-agent collaboration</li>
</ul>
</li>
<li><p>Motion을 어떻게 표현하는게 가장 좋을까?: Continuous-time SLAM</p>
<ul>
<li>Continuous-time SLAM을 할 경우, 센서가 데이터를 asynchronous하게 취득해도 괜찮다.</li>
<li>Motion trajectory는 Cubic B-spline으로 표현한다.</li>
<li>Gaussian Belief Propagation을 통해 backend 를 풀어보려는 시도를 하고 있다.</li>
</ul>
</li>
<li><p>Collaborative SLAM (CCM-SLAM, COVINS-G)</p>
<ul>
<li>CCM-SLAM: 여러 agent가 local BA를 통해 맵을 만들다가, 중앙 서버에서 map merging + global optimziation을 하는 것</li>
<li>COVINS-G: Collaborative SLAM을 위한 백엔드 프레임워크?</li>
</ul>
</li>
<li><p>Towards generic, decentralized SLAM…</p>
<ul>
<li>Map sharing<ul>
<li>Ownership of local maps</li>
</ul>
</li>
</ul>
</li>
<li><p>Distributed SLAM 시스템의 bottleneck은 무엇일까? 로봇을 100개-1000개를 쓰려고 한다면 어떤 bottleneck이 생길까?</p>
<ul>
<li>agent들마다 서로 통신을 해야하는데, 통신을 할 때 keyframe 같은 정보를 주고 받는다면 네트워크 bandwidth가 이걸 감당할 수 있어야한다.</li>
<li>결국 가벼운 정보를 가지고 서로 통신을 해야한다는건데… 어떤 정보를 주고받아야 가벼울지 고민이 필요할 것 같다.</li>
<li>Mobile agent들끼리의 통신이 bottleneck이 될 수 있다면, Distributed central server는 어떨까…?</li>
</ul>
</li>
</ul>
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%205.12.07%20PM.png" class="" title="distributed1">
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%205.12.19%20PM.png" class="" title="distributed2">
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%205.12.27%20PM.png" class="" title="distributed3">
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%205.12.43%20PM.png" class="" title="distributed4">

<p> </p>
<h2 id="Open-Problems-in-Machine-Perception-amp-Always-on-AI"><a href="#Open-Problems-in-Machine-Perception-amp-Always-on-AI" class="headerlink" title="Open Problems in Machine Perception & Always-on AI"></a>Open Problems in Machine Perception &amp; Always-on AI</h2><blockquote>
<p>Meta의 Richard Newcombe가 발표했다.</p>
</blockquote>
<img src="/20230529-icra2023-day-1/IMG_0762.gif" class="" title="meta-ai">

<ul>
<li>AI 기술이 많이 발전했지만, Context 정보가 없다. (Contextual AI)<ul>
<li>All-day context = All-day wearable + Always-on machine perception<ul>
<li>All-day wearable: Project ARIA (안경에 카메라. Ego-centric computer vision)</li>
</ul>
</li>
</ul>
</li>
<li>ChatGPT와 같이 인터넷 대화를 통해 얻어낸 reasoning은 굉장히 잘 된다.<ul>
<li>하지만 실제 세상에서 Physical reality와 digital reality를 엮고 context 정보를 넣어주는 시스템이 없다.</li>
<li>이 ‘context gap’을 채우기 위해서, wearable machine perception이 필요하다.<ul>
<li>All-day wearable을 하기 위햇는 저전력 고성능의 디바이스가 필요하다.<ul>
<li>2개의 SLAM용 카메라 (20FPS), 2개의 RGB 카메라 (25FPS), 2개의 gaze 카메라 (60FPS), gaze를 보기 위한 LED, 7개의 마이크2개의 IMU, GNSS</li>
</ul>
</li>
<li>Project ARIA 디바이스를 받기 위해서는 <span class="exturl" data-url="aHR0cHM6Ly9hYm91dC5tZXRhLmNvbS91ay9yZWFsaXR5bGFicy9wcm9qZWN0YXJpYS8=">웹사이트<i class="fa fa-external-link-alt"></i></span>에 들어가서 요청을 하라고 한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230529-icra2023-day-1/Screenshot%202023-05-29%20at%205.52.54%20PM.png" class="" title="meta-ai2">

<ul>
<li>문제 1: What is the sufficient information needed from physical reality for Contextualized AI?<ul>
<li>100 명이 ARIA를 쓰고 다니면 31 페타바이트가 필요하다,,<ul>
<li>그렇기 때문에, 대충 적당한 데이터셋에서 학습해서 real-life에 deploy해서 잘될거라고 희망하는건 말이 되지 않는다.</li>
</ul>
</li>
<li>Can state-estimation capture &amp; compress a day-in-the-life?</li>
<li>Attributes:<ul>
<li>Location - Where am I?</li>
<li>Gaze - Where am I looking at?</li>
<li>Voice - What am I saying?</li>
<li>Kinematic state - Body pose</li>
<li>Interaction - What am I interacting with?</li>
<li>Object Index - What’s in my environment?</li>
<li>…</li>
<li>Other’s states</li>
<li>위의 attribute에 대한 데이터를 저장한다면, raw data에 비해 훨씬 적은 양을 저장할 것이다.</li>
</ul>
</li>
</ul>
</li>
<li>문제 2: ARIA의 무게는 70g, Power는 2.5Wh, Sensor module size 4mm^2이다.<ul>
<li>&lt;30g, 1mm^2, 1.0Wh 정도가 되야한다.</li>
<li>1.0Wh 를 10시간동안 실행하면서, 10%의 전력만 perception에 쓴다면 약 10mW가 필요하다.<ul>
<li>LED 하나를 키는게 5mW 이다…</li>
</ul>
</li>
<li>대충 데스크탑의 1만배 정도 더 적은 전력을 사용해야한다.</li>
</ul>
</li>
<li>문제 3:<ul>
<li>Resolution, Low oice, Continuous lens focusing, Field of view, No motion blur, HDR </li>
</ul>
</li>
</ul>
<p> </p>
<h3 id="Panel-discussion"><a href="#Panel-discussion" class="headerlink" title="Panel discussion"></a>Panel discussion</h3><ul>
<li><p>Q. Distributed algorithm을 할 때 어떤 레벨에서 distribution을 해야할지가 고민이 될 수 있다. SLAM을 모바일 디바이스에서 다 끝내고 odometry와 map 정보만 서로 통신을 하는 방법이 있는 반면에, 완전 초기의 센서 데이터도 서로 주고 받을 수 있을 것이다. 적당한 중간을 찾아야할텐데, 그 ‘적당한 중간’은 어디에 있을까?</p>
<ul>
<li>Richard Newcombe: 데이터의 흐름을 파악해서 최종 문제를 풀었을 때 minimum energy가 사용되는 방향으로 가야한다. 단순히 알고리즘을 푸는데에 필요한 에너지가 아니라, agent간의 통신에 필요한 에너지도 고려를 해야할 것이다.</li>
<li>Magarita Chli: ‘적당한 중간’을 찾기는 꽤 어렵다. 왜냐하면 어떤 문제를 푸느냐에따라 optimal 에너지를 소비하는 distribution 레벨이 다를 것이기 때문이다. 특히나 환경이 바뀐다던지, 문제가 바뀐다던지, 상황이 바뀐다던지 할 경우 정보를 송신하는 쪽과 수신하는 쪽에서 내부 알고리즘에 대한 정보 요구조건이 달라질 경우, 최적 에너지 소비 조건이 달라질 가능성이 굉장히 높다. Distributed algorithm은 문제를 효율적이고 빠르게 풀 수 있게 해주는 방법론이지 솔루션 그 자체가 아니다. </li>
</ul>
</li>
<li><p>Q. Distributed algorithm에는 안전 문제가 있는 것 같다. (Andrew Davision 교수님): 우리 랩실에서 낸 논문에 의하면, 여러개의 agent 중 하나가 고장나면 localisation의 경우 모든 agent의 정확도가 조금 떨어지고 끝날 것인데, planning의 경우 완전 큰일나게 될 수도 있다. 이는, 해커가 맘을 먹고 하나의 agent에서 다른 agent로 보내는 메세지들에 거짓말을 섞거나 또는 아무 메세지도 보내지 않는다면 (i.e. 존재하지 않는 척 한다면), 전체 시스템을 망가트릴 수 있다. 이 안전 문제를 해결하기 위해서는 어떻게 해야할까?</p>
<ul>
<li>Mustafa: Encrypted message를 사용하는건 어떨까?</li>
<li>Richard Newcombe: Observation + consistency로 풀어야한다. 단순히 비밀번호 기반의 시스템이 아닌, 여러 agent가 observation을 통해 적절한 space-time 데이터가 나오는 경우 consistent하다고 판단하는 consensus 알고리즘이 필요하다.</li>
<li>Frank Dellaert: 난 드립을 하나 치려고 하는데, Meta와 Twitter에서 fake news를 없애는 알고리즘을 써보는 것도 좋겠다.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Meta</tag>
        <tag>Workshop</tag>
        <tag>Distributed graph algorithms</tag>
        <tag>Gaussian Belief Propagation</tag>
        <tag>Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - 포스터 (Day 1)</title>
    <url>/20230529-icra2023-day1-posters/</url>
    <content><![CDATA[<h1 id="Multi-S-graphs-A-Collaborative-Semantic-SLAM-Architecture"><a href="#Multi-S-graphs-A-Collaborative-Semantic-SLAM-Architecture" class="headerlink" title="Multi S-graphs: A Collaborative Semantic SLAM Architecture"></a>Multi S-graphs: A Collaborative Semantic SLAM Architecture</h1><ul>
<li>룩셈베르크 대학 주관의 Scene graph 연구</li>
<li>Kimera와 비슷한 scene graph hierarchy를 가지고 있음<ul>
<li>하지만 Vision 기반인 Kimera와는 다르게 LiDAR를 사용함.</li>
</ul>
</li>
<li>Kimera-multi 와 비슷한 multi-robot SLAM 아키텍처를 가지고 있음</li>
<li>Room을 검출하기 위한 ‘room descriptor’를 제안함.<ul>
<li>LiDAR 기반 place recognition 기술인 scan context 기반으로 맵을 생성함.</li>
</ul>
</li>
</ul>
<img src="/20230529-icra2023-day1-posters/IMG_0754.jpeg" class="" title="Multi S-graphs: A Collaborative Semantic SLAM Architecture">

<p> </p>
<hr>
<h1 id="A-Robot-Web-for-Distributed-Many-Device-Localisation"><a href="#A-Robot-Web-for-Distributed-Many-Device-Localisation" class="headerlink" title="A Robot Web for Distributed Many-Device Localisation"></a>A Robot Web for Distributed Many-Device Localisation</h1><ul>
<li>ICL의 Andrew Davison 교수님 아래 5년차 박사과정 중인 Riku Murai의 연구임.</li>
<li>다수의 agent를 포함하는 factor graph를 실시간으로 최적화하는 방법을 제안함.<ul>
<li>기존의 factor graph optimization은 전체 그래프를 동시에 최적화하기 때문에, 1. 중앙 서버가 필요함, 2. 연산이 오래 걸림.</li>
<li>Distributed 알고리즘 (i.e. Gaussian Belief Propagation - GBP)를 통해 그래프를 한번에 업데이트 하는 것이 아닌, local 하게 업데이트가 가능함.<ul>
<li>Asynchronous Socket 통신을 통해 서로에게 factor를 보내줄 수 있음. 구현은 ROS2로 함.</li>
</ul>
</li>
</ul>
</li>
<li>이번 연구의 초점은 ‘distributed algorithm’이 outlier의 존재에도 잘 동작하는지를 확인하기 위함임.<ul>
<li>이를 위해, 다른 복잡한 부분 (e.g. 로봇의 포지셔닝)을 간단하게 함. 예를 들어, 로봇간의 factor는 AR 마커를 이용한 거리 측정 값으로 변경함.</li>
<li>frontend 알고리즘이 무거워질 경우 실시간 동작이 안될 수 있음을 의미.</li>
</ul>
</li>
</ul>
<img src="/20230529-icra2023-day1-posters/IMG_0755.jpeg" class="" title="A Robot Web for Distributed Many-Device Localisation">

<p> </p>
<hr>
<h1 id="ConceptFusion-A-Real-time-Open-set-Multimodal-3D-Mapping"><a href="#ConceptFusion-A-Real-time-Open-set-Multimodal-3D-Mapping" class="headerlink" title="ConceptFusion: A Real-time Open-set Multimodal 3D Mapping"></a>ConceptFusion: A Real-time Open-set Multimodal 3D Mapping</h1><ul>
<li>Open-set 비전 알고리즘을 3D SLAM에 섞음<ul>
<li>CLIP / DINO</li>
</ul>
</li>
<li>Image, Audio, Text, Click 쿼리가 가능함.</li>
<li>저자가 포스터만 걸어놓고 자기는 다른 오랄 들으러가서 설명이 없음…</li>
</ul>
<img src="/20230529-icra2023-day1-posters/IMG_0756.jpeg" class="" title="ConceptFusion: A Real-time Open-set Multimodal 3D Mapping">

<p> </p>
<hr>
<h1 id="CLIPGraphs-Multimodal-Graph-Networks-to-Infer-Object-Room-Affinities"><a href="#CLIPGraphs-Multimodal-Graph-Networks-to-Infer-Object-Room-Affinities" class="headerlink" title="CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities"></a>CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities</h1><img src="/20230529-icra2023-day1-posters/IMG_0757.jpeg" class="" title="CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities">

<p> </p>
<hr>
<h1 id="VL-Fields-Towards-Language-Grounded-Neural-Implicit-Spatial-Representation"><a href="#VL-Fields-Towards-Language-Grounded-Neural-Implicit-Spatial-Representation" class="headerlink" title="VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representation"></a>VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representation</h1><ul>
<li>Depth, RGB, CLIP loss를 모두 최적화한 neural field</li>
<li>CLIP loss가 있기 때문에, semantic segmentation도 잘 될 것이라는 전제를 가짐.</li>
</ul>
<img src="/20230529-icra2023-day1-posters/IMG_0758.jpeg" class="" title="VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representation">

<p> </p>
<hr>
<h1 id="Grounding-Pretrained-eatures-in-3D-Representations"><a href="#Grounding-Pretrained-eatures-in-3D-Representations" class="headerlink" title="Grounding Pretrained eatures in 3D Representations"></a>Grounding Pretrained eatures in 3D Representations</h1><ul>
<li>저자가 포스터 걸어놓고 오랄 보러감…</li>
</ul>
<img src="/20230529-icra2023-day1-posters/IMG_0759.jpeg" class="" title="Grounding Pretrained eatures in 3D Representations">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Posters</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - 포스터 (Day 2)</title>
    <url>/20230530-icra2023-day2-poster/</url>
    <content><![CDATA[<h1 id="Loosely-Coupled-Localization-Fusion-System-Based-on-Track-To-Track-Fusion-with-Bias-Alignment"><a href="#Loosely-Coupled-Localization-Fusion-System-Based-on-Track-To-Track-Fusion-with-Bias-Alignment" class="headerlink" title="Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment"></a>Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment</h1><ul>
<li>Localization 시스템은 다양한 이유로 오차를 만들어냄.<ul>
<li>센서 캘리브레이션이 잘 안되면 calibration error</li>
<li>지도를 기반으로 위치를 추정하는데 지도가 부정확하면 map bias error</li>
<li>INS와 같은 시스템에서는 drift error</li>
</ul>
</li>
<li>시간이 지날수록 누적되는 에러를 reference 시스템과 비교해 bias를 추적함. 추적한 bias를 기반으로 target 시스템의 bias를 수정할 수 있음.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/1.png" class="" title="Loosely-Coupled Localization Fusion System Based on Track-To-Track Fusion with Bias Alignment">

<p> </p>
<hr>
<h1 id="Mapping-with-only-the-Ground"><a href="#Mapping-with-only-the-Ground" class="headerlink" title="Mapping with only the Ground"></a>Mapping with only the Ground</h1><ul>
<li>바닥 아스팔트 노면만 바라보며 수행하는 visual odometry.</li>
<li>이미지 추출 -&gt; 키포인트 검출 -&gt; 바닥평면으로 투영 -&gt; 이후 기본적인 VO 파이프라인 사용</li>
<li>루프클로저가 사용될 경우 정확도가 꽤 좋음</li>
<li>루프클로저를 사용하지 않을 경우 drift가 누적됨<ul>
<li>특히, 회전할 때 누적이 많이 됨</li>
</ul>
</li>
<li>하지만 테스트 해본 공간이 겨우 3m x 2m 라서 큰 공간에서 동작할지 확신할 수 없음.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/2.png" class="" title="Mapping with only the Ground">

<p> </p>
<hr>
<h1 id="Structure-PLP-SLAM-Efficient-Sparse-Mapping-and-Localization-using-Point-Line-and-Plane-for-Monocular-RGB-D-and-Stereo-Cameras"><a href="#Structure-PLP-SLAM-Efficient-Sparse-Mapping-and-Localization-using-Point-Line-and-Plane-for-Monocular-RGB-D-and-Stereo-Cameras" class="headerlink" title="Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras"></a>Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras</h1><ul>
<li>Line Segment Detection (LSD)로 선을 검출해서 2가지 용도로 사용한다.<ul>
<li>Point-Line map 생성 (Geometry)</li>
<li>Instance planar segmentation 이미지 생성 (Semantics)</li>
</ul>
</li>
<li>두개를 퓨전하면 Line-Planar map이 생성된다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/3.png" class="" title="Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras">

<p> </p>
<hr>
<h1 id="Object-based-SLAM-utilizing-unambigous-pose-parameters-considering-general-symmetry-types"><a href="#Object-based-SLAM-utilizing-unambigous-pose-parameters-considering-general-symmetry-types" class="headerlink" title="Object-based SLAM utilizing unambigous pose parameters considering general symmetry types"></a>Object-based SLAM utilizing unambigous pose parameters considering general symmetry types</h1><ul>
<li>서울대에서 나온 연구</li>
<li>Object-based SLAM을 할 때는 각각의 object의 pose를 아는 것이 중요하다.<ul>
<li>하지만, 종종 symmetry를 가진 물체들은 방향을 정의하기 어렵기 때문에 pose를 구하기 어려울 수 있다. 2. Contribution 파트를 보면…</li>
<li>[Asymmetry] 물체는 6dof 포즈를 바로 추정할 수 있다</li>
<li>[Discrete symmetry] 물체는 2개의 평면으로 ambiguous pose가 나타난다</li>
<li>[Continuous symmetry] 물체는 z축 기준으로 빙빙 돌려도 모두 똑같이 생겼기 때문에 무한개의 ambiguous pose가 생긴다.</li>
</ul>
</li>
<li>Multiple hypothesis tracking을 통해 특정 object가 asymmetry/discrete symmetry/continuous symmetry인지 구분을 할 수 있다.</li>
<li>이후, 각각의 타입에 맞춘 factor graph 최적화 방법을 제안한다.</li>
<li>내가 드렸던 질문으로는 object label마다 asymmetry/discrete/continuous 클래스를 나눠놓으면 무거운 multiple hypothesis 연산을 하지 않아도 되는데 왜 이 방법을 사용하셨는지 여쭤봤다.<ul>
<li>제시해주신 답으로는 ‘같은 object class임에도 디자인에 따라 symmetry가 생길수도 있기 때문에, symmetry를 기준으로 여러 object class를 학습하는건 효율적이지 않다. 특히나 새로운 디자인이 나오면 재학습 해야하는 경우도 생긴다’ 라고 하셨다.</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/4.png" class="" title="Object-based SLAM utilizing unambigous pose parameters considering general symmetry types">

<p> </p>
<hr>
<h1 id="Towards-View-invariant-and-Accurate-Loop-Detection-Based-on-Scene-Graph"><a href="#Towards-View-invariant-and-Accurate-Loop-Detection-Based-on-Scene-Graph" class="headerlink" title="Towards View-invariant and Accurate Loop Detection Based on Scene Graph"></a>Towards View-invariant and Accurate Loop Detection Based on Scene Graph</h1><ul>
<li>Semantic graph를 통해서 loop closure를 할 수 있는 descriptor를 생성하는 연구.</li>
<li>Illumination이나 appearance가 바뀌어도 잘 동작한다는 장점이 있다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/5.png" class="" title="Towards View-invariant and Accurate Loop Detection Based on Scene Graph">

<p> </p>
<hr>
<h1 id="Zero-shot-Active-Visual-Search-ZAVIS-Intelligent-Object-Search-for-Robotic-Assistants"><a href="#Zero-shot-Active-Visual-Search-ZAVIS-Intelligent-Object-Search-for-Robotic-Assistants" class="headerlink" title="Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants"></a>Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants</h1><ul>
<li>고려대 최성준 교수님 랩실 연구</li>
<li>Active visual search (AVS) 방법론에 대한 연구</li>
<li>방법론:<ul>
<li>Scan landmark: 라이다와 PTZ 카메라를 이용해서 semantic view 생성<ul>
<li>Open-set detector를 사용해서 사전에 학습되지 않은 class의 물체도 검출한다.</li>
<li>‘Unknown’ class를 검출했을 때, 유저 프롬프트 정보에 대해 CLIP 기반 text-image matching을 통해 실제 class를 찾아낸다.</li>
</ul>
</li>
<li>Waypoint generation: Semantic uncertainty (vision-language model의 text-image matching의 entropy)와 pre-trained language model의 common sense knowledge를 바탕으로 한 cost function을 통해 waypoint 생성<ul>
<li>COMET ATOMIC 2020이라는 common sense knowledge 데이터셋이 있다고 함.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/6.png" class="" title="Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants">

<p> </p>
<hr>
<h1 id="COVINS-G-A-Generic-Back-end-for-Collaborative-Visual-Inertial-SLAM"><a href="#COVINS-G-A-Generic-Back-end-for-Collaborative-Visual-Inertial-SLAM" class="headerlink" title="COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM"></a>COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</h1><ul>
<li>기존의 COVINS 프레임워크를 조금 더 확장한 버전<ul>
<li>중앙 서버에서 multi-agent를 사용한 collaborative SLAM을 위한 백엔드 연산을 하는 프레임워크</li>
<li>COVINS는 ORB-SLAM3 만 사용할 수 있었고, Multi-agent bundle adjustment를 하느라 연산량이 너무 많았음.<ul>
<li>BA를 한다는건 모든 keypoint, pose, landmark를 통신으로 보내야하는건데, 통신 부하량도 너무 많았음.</li>
</ul>
</li>
</ul>
</li>
<li>COVINS-G는 keyframe의 keypoint 정보와 pose 정보만 송수신함.<ul>
<li>keypoint 정보를 기반으로 데이터베이스 관리를 하고 loop closure가 가능함.</li>
<li>Loop closure를 할 때, 2개의 agent의 trajectory간 absolute scale을 복원할 수 있어야함. 여기서 하나의 트릭을 사용<ul>
<li>각각의 agent들의 consecutive frame을 마치 stereo camera처럼 이용해서 동일 landmark를 보고있다고 하면 두개의 trajectory 간의 absolute scale을 알 수 있음. (포스터에서는 17 point algorithm이라고 하지만, 사실 그냥 2개의 stereo camera간의 absolute trajectory를 비교하는 것과 동일함)</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/7.png" class="" title="COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM">

<p> </p>
<hr>
<h1 id="A-Probabilistic-Framework-for-Visual-Localization-in-Ambiguous-Scenes"><a href="#A-Probabilistic-Framework-for-Visual-Localization-in-Ambiguous-Scenes" class="headerlink" title="A Probabilistic Framework for Visual Localization in Ambiguous Scenes"></a>A Probabilistic Framework for Visual Localization in Ambiguous Scenes</h1><ul>
<li>(솔직히 잘 모르겠는 연구…)</li>
<li>이미지 상 비슷하게 보이는 오브젝트/뷰가 많은 경우, visual localization을 하기 어렵다 (Problem 섹션 참고)</li>
<li>MLP를 이용해서 pose regression을 한다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/8.png" class="" title="A Probabilistic Framework for Visual Localization in Ambiguous Scenes">

<p> </p>
<hr>
<h1 id="DytanVO-Joint-Refinement-of-Visual-Odometry-and-Motion-Segmentation-in-Dynamic-Environments"><a href="#DytanVO-Joint-Refinement-of-Visual-Odometry-and-Motion-Segmentation-in-Dynamic-Environments" class="headerlink" title="DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments"></a>DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments</h1><img src="/20230530-icra2023-day2-poster/9.png" class="" title="DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments">

<p> </p>
<hr>
<h1 id="NoCal-Calibration-Free-Semi-Supervised-Learning-of-Odometry-and-Camera-Intrinsics"><a href="#NoCal-Calibration-Free-Semi-Supervised-Learning-of-Odometry-and-Camera-Intrinsics" class="headerlink" title="NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics"></a>NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics</h1><ul>
<li>비디오 시퀀스가 주어졌을 때, 1. NeRF 기반 scene rendering, 2. relative pose estimation (i.e. odometry), 3. camera calibration을 동시에 수행하는 시스템을 제안.</li>
<li>저자들에 의하면 사실 View syenthesis가 잘 되지 않는다고 한다. 하지만 view synthesis를 같이 함으로써 인해 odometry 추정 및 calibration이 꽤 잘 된다고 한다.</li>
<li>Camera model은 OpenCV perspective를 사용했다고 한다. 이 이야기를 듣고 나는 differentiable한 camera model을 사용하면 모델 자체가 backprop으로 인해 업데이트가 되며 자동 캘리브레이션이 될 것이라고 조언해줬다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/10.png" class="" title="NoCal: Calibration-Free Semi-Supervised Learning of Odometry and Camera Intrinsics">

<p> </p>
<hr>
<h1 id="MVTrans-Multi-view-Perception-of-Transparent-Objects"><a href="#MVTrans-Multi-view-Perception-of-Transparent-Objects" class="headerlink" title="MVTrans: Multi-view Perception of Transparent Objects"></a>MVTrans: Multi-view Perception of Transparent Objects</h1><img src="/20230530-icra2023-day2-poster/11.png" class="" title="MVTrans: Multi-view Perception of Transparent Objects">

<p> </p>
<hr>
<h1 id="Swarm-LIO-Decentralized-Swam-LiDAR-inertial-Odometry"><a href="#Swarm-LIO-Decentralized-Swam-LiDAR-inertial-Odometry" class="headerlink" title="Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry"></a>Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry</h1><img src="/20230530-icra2023-day2-poster/12.png" class="" title="Swarm-LIO: Decentralized Swam LiDAR-inertial Odometry">

<p> </p>
<hr>
<h1 id="Robust-Incremental-Smoothing-and-Mapping-riSAM"><a href="#Robust-Incremental-Smoothing-and-Mapping-riSAM" class="headerlink" title="Robust Incremental Smoothing and Mapping (riSAM)"></a>Robust Incremental Smoothing and Mapping (riSAM)</h1><img src="/20230530-icra2023-day2-poster/13.png" class="" title="Robust Incremental Smoothing and Mapping (riSAM)">

<p> </p>
<hr>
<h1 id="Loc-NeRF-Monte-Carlo-Localization-using-Neural-Radiance-Fields"><a href="#Loc-NeRF-Monte-Carlo-Localization-using-Neural-Radiance-Fields" class="headerlink" title="Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields"></a>Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields</h1><ul>
<li>Monte Carlo Localization (i.e. Particle filter 기반 위치 추정 기법)에 NeRF를 섞어본 실험적인 방법<ul>
<li>Monte Carlo Localization은 보통 이렇게 동작한다: 1. Motion model 기반으로 이동치 추정, 2. 이동 후 위치할 가능성이 있는 곳에 particle을 뿌림, 3. 각각의 particle 마다 observation (보통 LiDAR 스캔)을 지도와 매칭해서 가장 확률이 높은 particle을 정답으로 삼음.</li>
</ul>
</li>
<li>Loc-NeRF도 Monte Carlo Localization의 파이프라인을 따라가지만, observation이 NeRF의 novel view rendering을 이용함.<ul>
<li>사전에 Scene에 대해서 Neural radiance field를 학습해놓음.</li>
<li>위치 추정을 하기 위해 초기 motion model 값을 기반으로 particle을 뿌림.</li>
<li>각각의 particle마다 (i.e. pose candidate)마다 novel view rendering을 수행 함. 이 때, 모든 픽셀을 다 렌더링하면 너무 오래 걸리기 때문에, 64개의 픽셀만 렌더링을 수행함.</li>
<li>렌더링 된 결과와 실제 카메라에서 나온 결과가 가장 비슷한 particle이 다음 위치가 됨.</li>
</ul>
</li>
<li>기존의 Monte Carlo Localization 기법에 Particle annealing이라는 기법을 함께 사용함.<ul>
<li>첫 프레임에서 위치를 찾을 때에는 넓은 공간 속 위치를 찾기 때문에 정확한 위치를 찾을 수 없으나, particle filter가 진행되면서 점점 정답 위치로 수렴해가기 때문에, particle의 수가 많을 필요가 없다.</li>
<li>그렇기 때문에, 초기 iteration에서는 수많은 particle을 뿌려서 위치를 찾되, iteration이 높아질수록 particle의 수를 줄여 실시간 프로세싱이 가능하게 하는 기법 역시 제안한다.</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/14.png" class="" title="Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields">

<p> </p>
<hr>
<h1 id="Ditto-in-the-House-Building-Articulated-Models-of-Indoor-Scenes-through-INteractive-Perception"><a href="#Ditto-in-the-House-Building-Articulated-Models-of-Indoor-Scenes-through-INteractive-Perception" class="headerlink" title="Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception"></a>Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception</h1><ul>
<li>Affordance network를 이용해 ‘움직일 수 있는 static object’를 검출한다. 이를 통해 어떤 형태의 state change가 나타날 수 있는지 예상하는 네트워크를 만든다. (e.g. 냉장고 문이 열리고 닫힘)</li>
<li>이런 프로그램을 짤 때 어떤 라이브러리를 썼냐고 물어보니, PCL과 Open3D를 썼다고 한다. </li>
</ul>
<img src="/20230530-icra2023-day2-poster/15.png" class="" title="Ditto in the House: Building Articulated Models of Indoor Scenes through INteractive Perception">

<p> </p>
<hr>
<h1 id="Category-level-Shape-Estimation-for-Densely-Cluttered-Objects"><a href="#Category-level-Shape-Estimation-for-Densely-Cluttered-Objects" class="headerlink" title="Category-level Shape Estimation for Densely Cluttered Objects"></a>Category-level Shape Estimation for Densely Cluttered Objects</h1><ul>
<li>다수의 물체들이 서로 엉켜있어 아주 강한 occlusion이 존재할 때, 물체들의 Shape을 계산한다는 연구이다.<ul>
<li>이 때, 우리는 ‘어떤 class의 물체들이 있는지’는 알고 있지만, ‘해당 class의 물체들은 모두 길이, 넓이, 높이가 조금씩 다르다’ 라는 전제를 가지고 물체의 형태를 추정한다.</li>
</ul>
</li>
<li>우선 Instance segmentation을 통해 point cloud segmentation을 수행한다.<ul>
<li>Occlusion이 심하기 때문에, point cloud는 부분적으로만 복원이 될 것이다.</li>
<li>Class는 정확하게 추론했다는 전제를 가진다.</li>
</ul>
</li>
<li>Segmentation을 통해 추론한 class를 우리가 가지고 있는 물체의 shape template과 비교한다.</li>
<li>3D GCN을 통해 geometric transformation parameter를 추정한다.<ul>
<li>이 파라미터들은 길이, 넓이, 높이의 차이를 의미한다.</li>
<li>이 파라미터들을 구하면, 길이/넓이/높이에 맞춰 변형한 shape template을 구할 수 있다.<ul>
<li>즉, 실제로 point cloud에서 빈 곳을 채우는게 아니라, 적당히 맞는 길이/넓이/높이를 추정해 shape deformation을 하는 것이라고 볼 수 있다. </li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/16.png" class="" title="Category-level Shape Estimation for Densely Cluttered Objects">

<p> </p>
<hr>
<h1 id="Category-level-Global-Camera-Pose-Estimation-with-Multi-Hypothesis-Point-Cloud-Correspondence"><a href="#Category-level-Global-Camera-Pose-Estimation-with-Multi-Hypothesis-Point-Cloud-Correspondence" class="headerlink" title="Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence"></a>Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence</h1><ul>
<li>딥러닝을 사용하지 않고 object를 바라보는 camera pose estimation을 하는 방법을 소개한다.<ul>
<li>이 때, 우리가 바라보고 있는 object는 class는 알고 있지만, ground truth 3D model과 다른 형태를 가지고 있는 상황이다.</li>
<li>예를 들어, 우리가 가진 모델은 BMW 520d 자동차의 3D 모델이지만, 실제로 바라보고 있는 자동차는 BMW 320d 일 수 있다는 것이다. 대충 전체 쉐입은 비슷하지만, 디테일 적인 부분에서 다를 수 있다.</li>
</ul>
</li>
<li>3D 모델에서 3D keypoint를 추출해서 매칭을 한 후, affinity matrix라는 것을 통해서 제대로 매칭을 했는지 확인을 한다. 이 후, Least squares를 이용해서 초기 포즈를 추정한 후, Weighted pairwise distance loss를 기반으로 포즈 최적화를 진행한다.</li>
</ul>
<img src="/20230530-icra2023-day2-poster/17.png" class="" title="Category-level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondence">

<p> </p>
<hr>
<h1 id="Simple-BEV-What-Really-Matters-for-Multi-Sensor-BEV-Perception"><a href="#Simple-BEV-What-Really-Matters-for-Multi-Sensor-BEV-Perception" class="headerlink" title="Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?"></a>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</h1><ul>
<li><ol>
<li>최신 BEV 생성 방식을 비교하면서 ‘BEV를 생성하는데에 어떤 점들이 중요한가?’를 정리하고, 2. 본인들의 BEV 알고리즘을 소개한다.</li>
</ol>
</li>
<li>BEV를 생성하는데에 중요한 점들<ul>
<li>‘Predicting semantic map representations from images using pyramid occupancy networks’, ‘Lift, splat, shoot: Encoding images from arbitrary camera rigs by Implicitly unprojecting to 3D’, ‘Translating images into maps’, ‘Learning bird’s eye-view representation from multi-camera images via spatiotemporal transformers’와 저자의 BEV 알고리즘을 비교한다.<ul>
<li>위 논문들 사이의 성능 차이는, 사실 논문에 있는 성능 벤치마크에서 이야기하는 것 보다 훨씬 작다 (다들 고만고만하다…)</li>
<li>Unweighted splatiing, depth-based splatting, deformable attention과 같은 멋있어보이지만 무거운 알고리즘들보다, 사실 우리가 쓴 bilinear sampling과 같은 가벼운 알고리즘도 성능이 잘 나온다.</li>
</ul>
</li>
<li>BEV 모델을 학습할 때, batch size를 키우면 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, 고해상도 이미지를 생성할 경우 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, 더 깊은 backbone을 사용하면 정확도가 높아지는 것을 볼 수 있다.</li>
<li>BEV 모델을 학습할 때, augmentation으로 cropping/resizing/camera shuffling을 사용하면 정확도가 높아진다.<ul>
<li>camera dropout는 도움이 되지 않는다.</li>
</ul>
</li>
<li>BEV 모델을 학습할 때, LiDAR와 RADAR 정보를 섞으면 정확도가 많이 올라간다.<ul>
<li>LiDAR + RADAR 정보를 섞기 위해서는 그냥 단순히 feature에 concatenate하면 된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/18.png" class="" title="Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?">

<p> </p>
<hr>
<h1 id="BEVFusion-Multi-Task-Multi-Sensor-Fusion-with-Unified-Bird’s-Eye-View-Representation"><a href="#BEVFusion-Multi-Task-Multi-Sensor-Fusion-with-Unified-Bird’s-Eye-View-Representation" class="headerlink" title="BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation"></a>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation</h1><img src="/20230530-icra2023-day2-poster/19.png" class="" title="BEVFusion: Multi-Task Multi-SEnsor Fusion with Unified Bird" alt="s-Eye View Representation">

<p> </p>
<hr>
<h1 id="Cerberus-Low-Drift-Visual-Inertial-Leg-Odometry-for-Agile-Locomotion"><a href="#Cerberus-Low-Drift-Visual-Inertial-Leg-Odometry-for-Agile-Locomotion" class="headerlink" title="Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion"></a>Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion</h1><img src="/20230530-icra2023-day2-poster/20.png" class="" title="Cerberus: Low-Drift Visual-Inertial-Leg Odometry for Agile Locomotion">

<p> </p>
<hr>
<h1 id="Cross-Modal-Monocular-Localization-in-Prior-LiDAR-Maps-Utilizing-Semantic-Consistency"><a href="#Cross-Modal-Monocular-Localization-in-Prior-LiDAR-Maps-Utilizing-Semantic-Consistency" class="headerlink" title="Cross-Modal Monocular Localization in Prior LiDAR Maps Utilizing Semantic Consistency"></a>Cross-Modal Monocular Localization in Prior LiDAR Maps Utilizing Semantic Consistency</h1><ul>
<li>Semantic LiDAR 지도가 있을 때, 카메라만 가지고 위치를 추정하는 방법이다.</li>
<li>우선 카메라 이미지로부터 semantic segmentation과 ORB-SLAM3를 돌려서 semantic visual map을 생성한다.<ul>
<li>여기에 semantic LiDAR map을 가져와서 ICP를 통해서 포즈를 구한다.</li>
</ul>
</li>
<li>내 질문: 실제 3D 공간에서의 LiDAR의 포인트 클라우드 위치와 Visual 포인트 클라우드의 위치가 다르지 않나? 그러면 정합이 잘 안되서 정확도가 잘 안나올텐데.<ul>
<li>답변: 그러고보니 그렇네… 근데 실험 결과 보면 잘됨.</li>
<li>(ㅋㅋㅋ… 왜 되는지는 모르겠지만 일단 된다 라는 답변…)</li>
</ul>
</li>
</ul>
<img src="/20230530-icra2023-day2-poster/21.png" class="" title="Cross-Modal Monouclar Localization in Prior LiDAR Maps Utilizing Semantic Consistency">

<p> </p>
<hr>
<h1 id="L-C-Visual-inertial-Loose-Coupling-for-Resilient-and-Lightweight-Direct-Visual-Localization"><a href="#L-C-Visual-inertial-Loose-Coupling-for-Resilient-and-Lightweight-Direct-Visual-Localization" class="headerlink" title="L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization"></a>L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization</h1><img src="/20230530-icra2023-day2-poster/22.png" class="" title="L-C*: Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Posters</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - 포스터 (Day 3)</title>
    <url>/20230531-icra2023-day3-poster/</url>
    <content><![CDATA[<h1 id="Convolutional-Bayesian-Kernel-Inference-for-3D-Semantic-Mapping"><a href="#Convolutional-Bayesian-Kernel-Inference-for-3D-Semantic-Mapping" class="headerlink" title="Convolutional Bayesian Kernel Inference for 3D Semantic Mapping"></a>Convolutional Bayesian Kernel Inference for 3D Semantic Mapping</h1><ul>
<li>3D mapping의 트렌드에는 2가지 방식이 있었다: Probabilistic과 Deep Learning 방식이다.<ul>
<li>Probabilistic 방식은 uncertainty 기반으로 백트랙킹이 가능하지만, 느리고 핸드 튜닝이 필요했다.</li>
<li>Deep Learning 방식은 빠르게 동작하고 데이터셋에 맞춰서 잘 동작하지만, 동작 방식이 블랙박스였다.</li>
<li>이 둘을 합할 수 없을까?</li>
</ul>
</li>
<li>Probabilistic 방식의 Gaussian process (GP)에서 영감을 얻어, 각각의 semantic class마다 geometric probability distribution을 Deep Learning 방식으로 학습하는 Convolutional bayesian Kernel Inference (ConvBKI) layer를 만든다.</li>
<li>Semantic point cloud가 있을 때 ConvBKI layer를 통해 geometric uncertainty를 추론하고, 이 값을 기반으로 mapping을 하면 DL 방식의 추론과 Probabilistic 방식의 reconstruction 방식을 둘 다 취할 수 있다는게 이 논문의 contribution이다.</li>
</ul>
<img src="/20230531-icra2023-day3-poster/1.png" class="" title="Convolutional Bayesian Kernel Inference for 3D Semantic Mapping">

<p> </p>
<hr>
<h1 id="SHINE-Mapping-Large-Scale-3D-Mapping-using-Sparse-Hierarchical-Implicit-Neural-Represetations"><a href="#SHINE-Mapping-Large-Scale-3D-Mapping-using-Sparse-Hierarchical-Implicit-Neural-Represetations" class="headerlink" title="SHINE-Mapping: Large-Scale 3D Mapping using Sparse Hierarchical Implicit Neural Represetations"></a>SHINE-Mapping: Large-Scale 3D Mapping using Sparse Hierarchical Implicit Neural Represetations</h1><ul>
<li>기존의 large-scale 3D mapping 방식은 mapping의 정확도와 memory 사용의 trade-off가 있었다.<ul>
<li>Memory를 적게 쓰면 정확도가 낮아지고, 반대로 정확도를 높이면 memory도 많이 써야했다.</li>
</ul>
</li>
<li>Octree 구조와 뉴럴네트워크를 함께 사용해서 정확도는 유지하나 메모리 사용량을 획기적으로 낮추는 방법을 제안한다.<ul>
<li>뉴럴네트워크가 implicit하게 geometric 정보를 담게 될 것이다.</li>
</ul>
</li>
<li>Octree의 각각의 level마다 node에 대해 1D feature vector를 생성한다. 그리고 이 feature vector를 모두 더해서, 크기가 작은 뉴럴네트워크에 넣었을 때 Signed distance function (SDF)가 나오도록 학습을 한다. 전체 프로세스는 differentiable하기 때문에 backprop을 통해 뉴럴네트워크와 feature octree를 둘 다 최적화 할 수 있다.</li>
<li>특정 scene에서 어떤 octree의 형태가 어떤 sdf를 만드는지 추정하는 작은 네트워크만 사용는거다보니, 메모리 사용량은 뉴럴넷의 크기로 한정된다. 즉, scene의 크기는 훨씬 커도 된다는 뜻.</li>
<li>내 질문: Scene이 어느정도까지 커도 되나요? Scene이 커지면 뉴럴넷 크기도 커져야하는게 아닐까요?<ul>
<li>답변: 꽤 작은 뉴럴넷으로도 왠만한 large dataset은 다 될거다. 뉴럴넷의 크기를 키울 필요를 느끼지 못했다. </li>
<li>질문 2: 예를 들어서, 중국 전체를 다 스캐닝한 지도가 있다고하면 이게 잘 될까요?</li>
<li>답변: (당황) 그러면 아마 뉴럴넷 크기를 키워야할거다.</li>
</ul>
</li>
</ul>
<img src="/20230531-icra2023-day3-poster/2.png" class="" title="SHINE-Mapping: Large-Scale 3D Mapping using Sparse Hierarchical Implicit Neural Represetations">

<p> </p>
<hr>
<h1 id="Inverse-Perspective-Mapping-Based-Neural-Occupancy-Grid-Map-for-Visual-Parking"><a href="#Inverse-Perspective-Mapping-Based-Neural-Occupancy-Grid-Map-for-Visual-Parking" class="headerlink" title="Inverse Perspective Mapping-Based Neural Occupancy Grid Map for Visual Parking"></a>Inverse Perspective Mapping-Based Neural Occupancy Grid Map for Visual Parking</h1><ul>
<li>주차 시나리오에서 버드아이뷰를 이용해 occupancy grid map을 만드는 연구 (라이다를 카메라로 대체한다고 보면 됨)<ul>
<li>Tong Qin이 저자이다… 화웨이의 주차 솔루션을 만들고 있는 사람이며, <a href="www.cv-learn.com/20230218-qin-2020-avpslam">AVP-SLAM</a>, <a href="www.cv-learn.com/20230218-qin-2021-roadmap">RoadMap</a>등 연구의 후속 연구이다. VINS-Mono의 저자이기도 하다.</li>
</ul>
</li>
<li>Inverse projection mapping을 수행해 BEV를 만든 후, 16x16 m 공간에서 나타나는 LiDAR 정보를 같이 모아 학습을 진행한다. 네트워크는 BiSeNet을 백본으로 사용한다. 이 백본은 lane, marker, stop line과 같은 정보를 segmentation하는 multi-task framework로도 사용될 수 있다.</li>
<li>Local occupancy grid map을 추론한다면, 이는 probabilistic 방식으로 쌓아가면서 global occupancy grid map을 생성할 수 있다.<ul>
<li>즉, 카메라만을 이용해서 라이다 SLAM의 occupancy grid map을 생성할 수 있다는 것이다.</li>
</ul>
</li>
</ul>
<img src="/20230531-icra2023-day3-poster/3.png" class="" title="Inverse Perspective Mapping-Based Neural Occupancy Grid Map for Visual Parking">

<p> </p>
<hr>
<h1 id="Cross-Modality-Time-Variant-Relation-Learning-for-Generative-Dynamic-Scene-Graph"><a href="#Cross-Modality-Time-Variant-Relation-Learning-for-Generative-Dynamic-Scene-Graph" class="headerlink" title="Cross Modality Time-Variant Relation Learning for Generative Dynamic Scene Graph"></a>Cross Modality Time-Variant Relation Learning for Generative Dynamic Scene Graph</h1><ul>
<li>비디오로부터 dynamic scene graph를 표현하는 기술인 Time-variant Relation-aware TRansformer (TR2)를 소개함.<ul>
<li>Dynamic scene graph는 물체들 사이의 다양한 interaction/action을 소개하는 표현 방법임.</li>
<li><a href="www.cv-learn.com/20230329-rosinol-2021-kimera-3d-dgs">Kimera</a>와 같은 방법에서 소개하는 dynamic scene graph와는 조금 다름.</li>
</ul>
</li>
<li>영상으로부터 text embedding을 추출해서, 해당 text embedding이 시간에 따라 변화하는 것을 감지해 dynamic scene graph를 업데이트 함.</li>
<li>Action genome이라는 데이터셋에서 실험을 수행함. </li>
</ul>
<img src="/20230531-icra2023-day3-poster/4.png" class="" title="Cross Modality Time-Variant Relation Learning for Generative Dynamic Scene Graph">

<p> </p>
<hr>
<h1 id="3D-VSG-Long-Term-Semantic-Scene-Change-Prediction-through-3D-Variable-Scene-Graphs"><a href="#3D-VSG-Long-Term-Semantic-Scene-Change-Prediction-through-3D-Variable-Scene-Graphs" class="headerlink" title="3D-VSG: Long-Term Semantic Scene Change Prediction through 3D Variable Scene Graphs"></a>3D-VSG: Long-Term Semantic Scene Change Prediction through 3D Variable Scene Graphs</h1><ul>
<li>3D dynamic scene graph가 존재할 때, 이동할 수 있는 물체가 어떤 것인지 graph neural network로 학습함으로써 ‘이동할 확률’을 prediction하는 내용.<ul>
<li>이 Dynamic scene graph에서 node는 object class instance이고 edge는 두 물체간의 euclidean distance임.</li>
<li>물체를 여러번 옮기면서 매번 scene capture를 통해 데이터를 생성해서 뉴럴넷을 학습함.</li>
</ul>
</li>
<li>Scene graph는 graph이기 때문에, 분석 용도로 graph neural network를 사용하기 좋다는 점을 보여줌.</li>
</ul>
<img src="/20230531-icra2023-day3-poster/5.png" class="" title="3D-VSG: Long-Term Semantic Scene Change Prediction through 3D Variable Scene Graphs">

<p> </p>
<hr>
<h1 id="The-Reflectance-Field-Map-Mapping-Glass-and-Specular-Surfaces-in-Dynamic-Environments"><a href="#The-Reflectance-Field-Map-Mapping-Glass-and-Specular-Surfaces-in-Dynamic-Environments" class="headerlink" title="The Reflectance Field Map: Mapping Glass and Specular Surfaces in Dynamic Environments"></a>The Reflectance Field Map: Mapping Glass and Specular Surfaces in Dynamic Environments</h1><img src="/20230531-icra2023-day3-poster/6.png" class="" title="The Reflectance Field Map: Mapping Glass and Specular Surfaces in Dynamic Environments">

<p> </p>
<hr>
<h1 id="Efficient-Bundle-Adjustment-for-Coplanar-Points-and-Lines"><a href="#Efficient-Bundle-Adjustment-for-Coplanar-Points-and-Lines" class="headerlink" title="Efficient Bundle Adjustment for Coplanar Points and Lines"></a>Efficient Bundle Adjustment for Coplanar Points and Lines</h1><ul>
<li>무려 Guoquan Huang 교수님께서 직접 포스터 발표를 하신 ㄷㄷ,,</li>
</ul>
<img src="/20230531-icra2023-day3-poster/7.png" class="" title="Efficient Bundle Adjustment for Coplanar Points and Lines">

<p> </p>
<hr>
<h1 id="Detecting-Spatio-Temporal-Relations-by-Combining-a-Semantic-Map-with-a-Stream-Processing-Engine"><a href="#Detecting-Spatio-Temporal-Relations-by-Combining-a-Semantic-Map-with-a-Stream-Processing-Engine" class="headerlink" title="Detecting Spatio-Temporal Relations by Combining a Semantic Map with a Stream Processing Engine"></a>Detecting Spatio-Temporal Relations by Combining a Semantic Map with a Stream Processing Engine</h1><img src="/20230531-icra2023-day3-poster/8.png" class="" title="Detecting Spatiao-Temporal Relations by Combining a Semantic Map with a Stream Processing Engine">

<p> </p>
<hr>
<h1 id="Feature-Realistic-Neural-Fusion-for-Real-Time-Open-Set-Scene-Understanding"><a href="#Feature-Realistic-Neural-Fusion-for-Real-Time-Open-Set-Scene-Understanding" class="headerlink" title="Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding"></a>Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding</h1><ul>
<li>엄청나게 큰 데이터셋에서 pre-trained된 feature detector (e.g. EfficientNet, DINO)와 같은 feature map들을 NeRF를 통해서 3D fusion을 하면 어떨까?<ul>
<li>color 값이 아닌, feature 값을 가진 neural implicit feature-field가 생성될 것이다.</li>
<li>이 feature field는 segmentation과 같은 downstream task를 하기에 아주 적합한 구조를 가졌다.</li>
</ul>
</li>
<li>Human interaction (e.g. 클릭 한번)을 통해 새로운 sub-class를 생성하고 곧바로 실시간으로 scene에서 모든 feature에 해당 class를 부여하여 open-set interactive labelling을 할 수 있다.</li>
</ul>
<img src="/20230531-icra2023-day3-poster/9.png" class="" title="Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding">

<p> </p>
<hr>
<h1 id="SceneCalib-Automatic-Targetless-Calibration-of-Cameras-and-LiDARs-in-Autonomous-Driving"><a href="#SceneCalib-Automatic-Targetless-Calibration-of-Cameras-and-LiDARs-in-Autonomous-Driving" class="headerlink" title="SceneCalib: Automatic Targetless Calibration of Cameras and LiDARs in Autonomous Driving"></a>SceneCalib: Automatic Targetless Calibration of Cameras and LiDARs in Autonomous Driving</h1><img src="/20230531-icra2023-day3-poster/10.png" class="" title="SceneCalib: Automatic Targetless Calibration of Cameras and LiDARs in Autonomous Driving">

<p> </p>
<hr>
<h1 id="Mask3D-Mask-Transformer-for-3D-Instance-Segmentation"><a href="#Mask3D-Mask-Transformer-for-3D-Instance-Segmentation" class="headerlink" title="Mask3D: Mask Transformer for 3D Instance Segmentation"></a>Mask3D: Mask Transformer for 3D Instance Segmentation</h1><img src="/20230531-icra2023-day3-poster/11.png" class="" title="Mask3D: Mask Transformer for 3D Instance Segmentation">

<p> </p>
<hr>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Posters</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - 포스터 (Day 4)</title>
    <url>/20230531-icra2023-day4-poster/</url>
    <content><![CDATA[<h1 id="Energy-Based-Models-for-Cross-Modal-Localization-using-Covolutional-Transformers"><a href="#Energy-Based-Models-for-Cross-Modal-Localization-using-Covolutional-Transformers" class="headerlink" title="Energy-Based Models for Cross-Modal Localization using Covolutional Transformers"></a>Energy-Based Models for Cross-Modal Localization using Covolutional Transformers</h1><img src="/20230531-icra2023-day4-poster/1.png" class="" title="Energy-Based Models for Cross-Modal Localization using Covolutional Transformers">

<p> </p>
<hr>
<h1 id="SAMLoc-Structure-Aware-Constraints-With-Multi-Trask-Distillation-for-Long-Term-Visual-Localization"><a href="#SAMLoc-Structure-Aware-Constraints-With-Multi-Trask-Distillation-for-Long-Term-Visual-Localization" class="headerlink" title="SAMLoc: Structure-Aware Constraints With Multi-Trask Distillation for Long-Term Visual Localization"></a>SAMLoc: Structure-Aware Constraints With Multi-Trask Distillation for Long-Term Visual Localization</h1><ul>
<li>Paul Eduard Sarlin의 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2N2Zy9IaWVyYXJjaGljYWwtTG9jYWxpemF0aW9u">HLoc (Hierarchical Localization)<i class="fa fa-external-link-alt"></i></span> 방식을 teacher-student 방식으로 distillation해서 훨씬 더 가벼운 visual localization 방식을 만들었다.<ul>
<li>이 때, global structure-aware constraint와 local structure-aware constraint를 추출해서 학습에 사용함으로써 안정성을 추구했다.</li>
</ul>
</li>
</ul>
<img src="/20230531-icra2023-day4-poster/2.png" class="" title="SAMLoc: Structure-Aware Constraints With Multi-Trask Distillation for Long-Term Visual Localization">

<p> </p>
<hr>
<h1 id="Implicit-Map-Representation-and-Localization-with-Invertible-Neural-Network"><a href="#Implicit-Map-Representation-and-Localization-with-Invertible-Neural-Network" class="headerlink" title="Implicit Map Representation and Localization with Invertible Neural Network"></a>Implicit Map Representation and Localization with Invertible Neural Network</h1><ul>
<li>LiDAR Mapping과 Localization을 둘 다 할 수 있는 뉴럴네트워크를 만듬.<ul>
<li>특이한 점이라면, Invertible neural network를 사용했다는 점.<ul>
<li>Invertible neural network는 양방향으로 사용할 수 있는 네트워크를 의미함. 즉, 네트워크를 정방향으로 쓸 때의 input과 output은, 역방향으로 사용할 때는 output과 input이 된다는 점.</li>
<li>이 논문에서는 정방향 사용 시 Pose를 input으로 사용하면 LiDAR scan이 output으로 나오고 (i.e. Mapping), 역방향 사용 시 LiDAR scan이 input이 되고 Pose가 output이 된다 (i.e. Localization).</li>
</ul>
</li>
</ul>
</li>
<li>어떤 scene에 대해서 이 invertible neural network 를 학습하면 다음과 같은 방법으로 사용할 수 있다.<ul>
<li>Pose가 주어졌을 때, 주변은 어떻게 생길 것인가? (이 때, pose는 실제로 취득한 데이터가 아닌 novel pose도 가능하다 -&gt; LiDAR novel view synthesis)</li>
<li>Scan이 주어졌을 때, 로봇은 어디에 위치해있을까? (이 때, pose에 대한 값은 확률적으로 나타난다. 그렇기 때문에 LiDAR로 헷갈리기 쉬운 scene에서도 여러 확률의 값이 나타나게 된다. 확률값이 주어진다면 추후 보정을 하기에 아주 유용하다는 얘기다.)</li>
</ul>
</li>
<li>이를 통해 꽤 큰 공간에서도 큰 용량의 point cloud map 파일을 저장하고 있을 필요가 없다. 뉴럴넷 weight만 저장하고 있으면 된다.</li>
<li>뉴럴넷 자체도 상당히 가벼운 편이라 Jetson NX에서도 270Hz로 돌아간다.</li>
<li>ROS2 코드도 준비되어있다.</li>
</ul>
<img src="/20230531-icra2023-day4-poster/3.png" class="" title="Implicit Map Representation and Localization with Invertible Neural Network">

<p> </p>
<hr>
<h1 id="Combining-Scene-Coordinate-Regression-and-Absolute-Pose-Regression-for-Visual-Localization"><a href="#Combining-Scene-Coordinate-Regression-and-Absolute-Pose-Regression-for-Visual-Localization" class="headerlink" title="Combining Scene Coordinate Regression and Absolute Pose Regression for Visual Localization"></a>Combining Scene Coordinate Regression and Absolute Pose Regression for Visual Localization</h1><img src="/20230531-icra2023-day4-poster/4.png" class="" title="Combining Scene Coordinate Regression and Absolute Pose Regression for Visual Localization">

<p> </p>
<hr>
<h1 id="A-Graph-Based-Optimization-Framework-for-Hand-Eye-Calibration-for-Multi-Camera-Setups"><a href="#A-Graph-Based-Optimization-Framework-for-Hand-Eye-Calibration-for-Multi-Camera-Setups" class="headerlink" title="A Graph-Based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups"></a>A Graph-Based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups</h1><ul>
<li>다수의 카메라가 벽에 설치되어있고, 로봇의 위치를 알아내기 위한 hand-eye calibration을 할 때, 문제 자체를 factor graph로 표현한 후 캘리브레이션을 하는 방법을 소개한다.</li>
<li>단순히 다중 카메라간의 extrinsic calib을 수행한 후, 각각의 카메라에서 mono_view -&gt; robot hand-eye calib을 하는 것보다 훨씬 정확하다.</li>
</ul>
<img src="/20230531-icra2023-day4-poster/5.png" class="" title="A Graph-Based Optimization Framework for Hand-Eye Calibration for Multi-Camera Setups">

<p> </p>
<hr>
<h1 id="Online-Hand-Eye-Calibration-with-Decoupling-by-3D-Textureless-Object-Tracking"><a href="#Online-Hand-Eye-Calibration-with-Decoupling-by-3D-Textureless-Object-Tracking" class="headerlink" title="Online Hand-Eye Calibration with Decoupling by 3D Textureless Object Tracking"></a>Online Hand-Eye Calibration with Decoupling by 3D Textureless Object Tracking</h1><img src="/20230531-icra2023-day4-poster/6.png" class="" title="Online Hand-Eye Calibration with Decoupling by 3D Textureless Object Tracking">

<p> </p>
<hr>
<h1 id="Open-vocabulary-Queryable-Scene-Representations-or-Real-World-Planning"><a href="#Open-vocabulary-Queryable-Scene-Representations-or-Real-World-Planning" class="headerlink" title="Open-vocabulary Queryable Scene Representations or Real World Planning"></a>Open-vocabulary Queryable Scene Representations or Real World Planning</h1><ul>
<li>Everyday robots와 Robotics at Google에서 진행한 연구.</li>
<li>Open vocabulary와 CLIP을 함께 사용함으로써 object에 대한 novel prompt를 생성한 후, 각각 물체에 대한 task를 만들어내는 연구이다.</li>
</ul>
<img src="/20230531-icra2023-day4-poster/7.png" class="" title="Open-vocabulary Queryable Scene Representations or Real World Planning">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Posters</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - Scalable Autonomous Driving 워크샵 (Day 5)</title>
    <url>/20230602-icra-day-5/</url>
    <content><![CDATA[<p>(적는 중)</p>
<h1 id="Toyota"><a href="#Toyota" class="headerlink" title="Toyota"></a>Toyota</h1><p>Foundation models == Zero-shot generalization</p>
<p>Pre-trained models are great out of the box. We should leverage geometry more. Mathematical formulation in geometry (e.g. epipolar geometry) are essentially the best pre-trained models!</p>
<p>Geometric foundation models? (== Data + Principles)</p>
<p> </p>
<h1 id="Fisher-Yu-calable-4D-Scene-Understanding-with-Less-Hassle"><a href="#Fisher-Yu-calable-4D-Scene-Understanding-with-Less-Hassle" class="headerlink" title="Fisher Yu - calable 4D Scene Understanding with Less Hassle"></a>Fisher Yu - calable 4D Scene Understanding with Less Hassle</h1><ul>
<li><p>Scaling with Less hassle</p>
<ul>
<li>Learn and benefit from large-scale data</li>
<li>Avoid manual designs / manual labeling</li>
</ul>
</li>
<li><p>Representation Learning in 4D world</p>
<ul>
<li>Learning robust 2D tracking</li>
<li>Learning high-fidelity instalce-level representation</li>
<li>Leanring 3D motion</li>
</ul>
</li>
<li><p>Current tracking algorithms</p>
<ul>
<li>Object detection -&gt; Motion estimation -&gt; Initial Assocation (Location, Appearance) -&gt; Association Optimization</li>
<li>Pipeline is very complicated and also is fragile.</li>
<li>Do humans go through all these steps?<ul>
<li>Humans only need to do ‘Object detection -&gt; Appearance association’</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>ICRA</tag>
        <tag>Workshop</tag>
        <tag>Scalable Autonomous Driving</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - 포스터 (Day 5)</title>
    <url>/20230602-icra2023-day5-poster/</url>
    <content><![CDATA[<h1 id="F2BEV-Bird’s-Eye-View-Generation-from-Surround-View-Fisheye-Camera-Images-for-Automated-Driving"><a href="#F2BEV-Bird’s-Eye-View-Generation-from-Surround-View-Fisheye-Camera-Images-for-Automated-Driving" class="headerlink" title="F2BEV: Bird’s Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving"></a>F2BEV: Bird’s Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving</h1><img src="/20230602-icra2023-day5-poster/1.png" class="" title="F2BEV: Bird" alt="s Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving">

<p> </p>
<hr>
<h1 id="Graph-based-Global-Robot-Localization-and-Maping-using-Architectural-Plans"><a href="#Graph-based-Global-Robot-Localization-and-Maping-using-Architectural-Plans" class="headerlink" title="Graph-based Global Robot Localization and Maping using Architectural Plans"></a>Graph-based Global Robot Localization and Maping using Architectural Plans</h1><img src="/20230602-icra2023-day5-poster/2.png" class="" title="Graph-based Global Robot Localization and Maping using Architectural Plans">

<p> </p>
<hr>
<h1 id="Real-time-Localization-and-Mapping-leveraging-Hierarchical-Representations"><a href="#Real-time-Localization-and-Mapping-leveraging-Hierarchical-Representations" class="headerlink" title="Real-time Localization and Mapping leveraging Hierarchical Representations"></a>Real-time Localization and Mapping leveraging Hierarchical Representations</h1><img src="/20230602-icra2023-day5-poster/3.png" class="" title="Real-time Localization and Mapping leveraging Hierarchical Representations">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>Robotics</tag>
        <tag>SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>ICRA</tag>
        <tag>Posters</tag>
      </tags>
  </entry>
  <entry>
    <title>ICRA 2023 - Hilti SLAM Challenge (Day 5)</title>
    <url>/20230602-icra2023-hilti/</url>
    <content><![CDATA[<p>(적는 중)</p>
<h2 id="Maplab-2-0"><a href="#Maplab-2-0" class="headerlink" title="Maplab 2.0"></a>Maplab 2.0</h2><ul>
<li>Process <ul>
<li>Odometry</li>
<li>Single session optimization</li>
<li>Rigid alignment with vision</li>
<li>GLobal map optimization</li>
<li>Global LiDAR fine tuning</li>
</ul>
</li>
<li>Raw Maps:<ul>
<li>OKVIS or FAST-LIO2<ul>
<li>SuperPoint + SuperGlue</li>
</ul>
</li>
</ul>
</li>
<li>Rigid Alignment<ul>
<li>PCA for compression</li>
<li>Search with HNSW (Malkov 2018 - Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs)</li>
</ul>
</li>
</ul>
<h2 id="KAIST"><a href="#KAIST" class="headerlink" title="KAIST"></a>KAIST</h2><ul>
<li>AdaLIO<ul>
<li>Detects degenerate cases and chanages parameters adaptively.</li>
</ul>
</li>
<li>Quatro</li>
</ul>
<p> </p>
<hr>
<h2 id="Tencent-TXR-SLAM"><a href="#Tencent-TXR-SLAM" class="headerlink" title="Tencent TXR SLAM"></a>Tencent TXR SLAM</h2><ul>
<li>어두운 곳에서는 Histogram equalization을 사용함</li>
</ul>
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>학회 발표 리뷰</category>
      </categories>
      <tags>
        <tag>ICRA</tag>
        <tag>Workshop</tag>
        <tag>Hilti SLAM Challenge</tag>
      </tags>
  </entry>
  <entry>
    <title>Rust처럼 안전하게 Python 코드 작성하기</title>
    <url>/20230609-python-like-rust/</url>
    <content><![CDATA[<blockquote>
<p>Kobzol’s blog의 <span class="exturl" data-url="aHR0cHM6Ly9rb2J6b2wuZ2l0aHViLmlvL3J1c3QvcHl0aG9uLzIwMjMvMDUvMjAvd3JpdGluZy1weXRob24tbGlrZS1pdHMtcnVzdC5odG1s">Writing Python like it’s Rust<i class="fa fa-external-link-alt"></i></span>를 번역한 글입니다.</p>
</blockquote>
<h1 id="왜-Python을-Rust처럼-적게되었나"><a href="#왜-Python을-Rust처럼-적게되었나" class="headerlink" title="왜 Python을 Rust처럼 적게되었나?"></a>왜 Python을 Rust처럼 적게되었나?</h1><p>저는 몇 년 전부터 Rust로 프로그래밍을 시작했고, 다른 프로그래밍 언어로 (특히 Python으로) 프로그램을 설계할 때 Rust 프로그램의 설계 방식을 사용하고 있습니다. Rust를 사용하기 전 까지는, 저도 여느 사람들과 같이 Python으로 프로그래밍을 할 때는 타입 힌트 없이 함수를 작성하거나 느슨한 타입을 사용하는 코드를 작성하고, 사방에 dict를 전달하고 반환하며, 때때로 ‘str 타입’ 인터페이스를 사용하는 경우도 많았습니다. 그러나 Rust로부터 <strong>타입 시스템의 엄격함</strong>을 경험하고 수많은 문제들을 설계상 방지할 수 있다는 것을 발견한 후, 저는 <strong>Python 프로그래밍에서 Rust처럼 동일한 안전 수준이 보장되지 않는 것이 상당히 불안</strong>해졌습니다.</p>
<p>여기서 ‘보장’이란 메모리 안전성을 의미하는 것이 아니라 (Python은 현재도 상당히 메모리 안전합니다), 오용이 매우 어렵거나 완전히 불가능한 API를 설계하여 정의되지 않은 동작 및 다양한 버그를 방지하는 개념인 ‘<strong>타입 건전성</strong>‘을 의미합니다. Rust에서는 잘못 사용된 API의 경우 일반적으로 컴파일 단계에서 에러를 유발함으로써 잘못된 코드가 실행되는 것을 방지합니다. Python에서는 이러한 컴파일 단계가 없어 잘못된 코드를 실행할 수도 있지만, pyright 같은 타입 체커나 PyCharm과 같은 타입 분석기가 있는 IDE를 사용하면 타입 건정성을 사전에 체크할 수 있습니다.</p>
<p>결국 저는 Python 프로그램에 Rust의 몇 가지 개념을 도입하기 시작했습니다. 기본적으로 <strong>타입 힌트를 최대한 많이 사용</strong>하고, <strong>illegal state가 없게 만듬</strong>으로써 두 가지 원칙으로 요약할 수 있습니다. 장기간 유지보수 해야 할 프로그램뿐만 아니라, 일회성 유틸리티 스크립트에도 이 원칙을 적용하려고 노력합니다. 제 경험상 후자는 종종 전자로 바뀌기 때문입니다 :) 제 경험상 이 접근 방식은 이해하고 변경하기 더 쉬운 프로그램으로 이어집니다.</p>
<p>이 글에서는 Python 프로그램에 적용된 이러한 패턴의 몇 가지 예를 보여드리겠습니다. 엄청나게 대단한 기술은 아니지만 그래도 문서화하는 것이 유용할 것 같았습니다.</p>
<blockquote>
<p>참고: 이 글에는 Python 코드 작성에 대한 많은 개인적인 의견이 포함되어 있습니다. 모든 문장에 “IMHO”를 붙이고 싶지는 않으므로 이 글의 모든 내용은 보편적인 진리를 주장하려는게 아닌, 단순히 해당 문제에 대한 제 의견으로 받아들여주세요 :) 또한 제시된 아이디어가 모두 Rust에서 발명되었다고 주장하는 것은 아니며 다른 언어에서도 물론 이미 사용되고 있는 개념일 수 있습니다.</p>
</blockquote>
<p> </p>
<hr>
<h1 id="타입-힌트"><a href="#타입-힌트" class="headerlink" title="타입 힌트"></a>타입 힌트</h1><p>가장 중요한 것은 가능한 경우, 특히 <em>함수 서명</em>과 <em>클래스 속성</em>에서 <strong>타입 힌트</strong>를 사용하는 것입니다. 다음과 같은 함수 시그니처를 읽으면 다음과 같습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_item</span>(<span class="params">records, check</span>):</span></span><br></pre></td></tr></table></figure>

<p>함수 시그니처만 읽었을 때 무슨 일이 벌어지고 있는지 전혀 알 수 없습니다. records는 list일까요? dict일까요? 아니면 데이터베이스 연결일까요? check는 bool인가요, 아니면 함수인가요? 이 함수는 무엇을 반환하나요? 실패하면 어떻게 되나요, 예외를 발생시키나요, 아니면 None을 반환하나요? </p>
<p>이러한 질문에 대한 답을 찾으려면 함수 본문(그리고 종종 함수가 호출하는 다른 함수의 본문까지 재귀적으로 읽어야 하는데, 이는 매우 귀찮은 일입니다.)을 읽거나 문서(있는 경우)를 읽어야 합니다. 문서에는 함수가 수행하는 작업에 대한 유용한 정보가 포함되어 있을 수 있지만, 이전 질문에 대한 답변을 문서화하는 데까지 사용할 필요는 없습니다. </p>
<p>대부분의 질문은 내장된 메커니즘인 타입 힌트를 통해 답을 찾을 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_item</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">  records: List[Item],</span></span></span><br><span class="line"><span class="function"><span class="params">  check: Callable[[Item], <span class="built_in">bool</span>]</span></span></span><br><span class="line"><span class="function"><span class="params"></span>) -&gt; Optional[Item]:</span></span><br></pre></td></tr></table></figure>

<p>함수 시그니처를 작성하는 데 시간이 더 걸렸나요? 네. 그게 문제가 될까요? 아니요, 분당 작성하는 문자 수로 인해 코딩이 병목 현상을 일으키지 않는 한, 실제로 그런 일은 일어나지 않습니다. 타입을 명시적으로 작성하면 함수가 제공하는 실제 인터페이스가 무엇인지, 호출자가 함수를 잘못된 방식으로 사용하지 못하도록 최대한 엄격하게 만들 수 있는 방법이 무엇인지 생각하게 됩니다. 위의 함수 시그니처를 사용하면 <strong>함수를 어떻게 사용할 수 있는지</strong>, <strong>인자로 무엇을 전달해야 하는지</strong>, <strong>함수를 통해 무엇을 반환할 수 있는지</strong> 명확하게 알 수 있습니다. 또한 코드가 변경되면 쉽게 구식이 될 수 있는 문서 주석과 달리, 타입을 변경하고 함수 호출자를 업데이트하지 않으면 <strong>정적 타입 검사기</strong>가 저에게 수많은 에러를 쏟아낼겁니다. Item이 무엇인지 궁금하다면 해당 클래스의 정의로 이동하여 해당 타입이 어떻게 생겼는지 즉시 확인할 수 있습니다.</p>
<p>하나의 매개변수를 설명하기 위해 엄청나게 복잡한 타입을 사용해야한다면 (e.g. 5개의 중첩된 타입 힌트가 필요하다면), 때에 따라 엄격함을 포기하고 부정확하지만 더 간단한 타입을 제공할 수 있습니다 (저는 이 점에서 절대 꽉 막히고 빡빡하게 사는 사람이 아닙니다). 하지만 제 경험상 이러한 상황은 자주 발생하지 않습니다. 함수 매개변수가 숫자, str tuple 또는 {str:int} dict일 수 있다면 코드에 문제가 있다는 신호일 수 있으므로, 이는 함수를 리팩토링해서 단순하게 바꿔야하는 또 다른 주제의 문제일 수 있습니다.</p>
<p> </p>
<hr>
<h1 id="Tuple이나-Dict-대신-Dataclass-사용"><a href="#Tuple이나-Dict-대신-Dataclass-사용" class="headerlink" title="Tuple이나 Dict 대신 Dataclass 사용"></a>Tuple이나 Dict 대신 Dataclass 사용</h1><p>타입 힌트를 사용하는 것도 한 가지 방법이지만, 이는 함수의 인터페이스가 무엇인지 설명할 뿐입니다. 그 다음 단계는 실제로 이러한 인터페이스를 가능한 한 명확한 데이터 타입으로 ‘고정’하는 것입니다. </p>
<p>일반적인 예로 함수에서 여러 값(또는 하나의 복합 값)을 반환하는 것을 들 수 있습니다. 가장 느슨하게 짜는 방식 (많은 파이썬 유저들이 채택하는 방식)은 Tuple을 반환하는 것입니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_person</span>(<span class="params">...</span>) -&gt; Tuple[str, str, int]:</span></span><br></pre></td></tr></table></figure>

<p>좋습니다, 세 개의 값을 반환한다는 것을 알았습니다. 하지만 이 값들은 무엇을 의미할까요? 첫 번째 문자열은 사람의 이름인가요? 두 번째 문자열은 성인가요? 숫자는 무엇인가요? 나이인가요? 목록에서 어떤 위치인가요? 사회보장번호? </p>
<p>이런 종류의 입력은 <strong>불투명</strong>하며 함수 본문을 들여다보지 않는 한 여기서 무슨 일이 일어나는지 알 수 없습니다.</p>
<p>이를 개선하기 위한 다음 단계는 Dict를 반환하는 것입니다 (하지만 이 방법 역시 좋은 방법은 아닙니다):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_person</span>(<span class="params">...</span>) -&gt; Dict[str, Any]:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> {</span><br><span class="line">        <span class="string">"name"</span>: ...,</span><br><span class="line">        <span class="string">"city"</span>: ...,</span><br><span class="line">        <span class="string">"age"</span>: ...</span><br><span class="line">    }</span><br></pre></td></tr></table></figure>

<p>이제 실제로 반환된 개별 속성이 무엇인지 알 수 있지만, 이를 알아내기 위해 다시 함수 본문을 검사해야 합니다. 이제 우리는 개별 속성의 개수와 타입조차 모르기 때문에 어떤 의미에서는 타입이 더 나빠졌습니다. 게다가 함수가 변경되어 반환된 dict의 키가 이름이 바뀌거나 제거되면 타입 검사기로 쉽게 알 수 있는 방법이 없기 때문에, 보통 매우 수동적이고 번거로운 디버깅을 통해 함수가 호출되는 모든 코드를 변경해야 합니다.</p>
<p>적절한 해결책은 타입이 첨부된 명명된 매개변수와 함께 <strong>강타입</strong>화된 (Strong-typed) 객체를 반환하는 것입니다. Python에서는 클래스를 만들어야 한다는 뜻입니다. 이러한 상황에서 Tuple과 Dict가 자주 사용되는 이유는 클래스를 정의하고(그리고 이름을 생각해서), 매개변수가 있는 생성자를 만들고, 매개변수를 필드 등에 저장하는 것보다 훨씬 쉽기 때문이라고 생각합니다. Python 3.7(그리고 package polyfill을 사용하면 더 빨리)부터는 훨씬 더 빠른 솔루션인 Dataclass가 있습니다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclasses.dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">City</span>:</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    address: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataclasses.dataclass</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> :</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    city: City</span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_person</span>(<span class="params">...</span>) -&gt; Person:</span></span><br></pre></td></tr></table></figure>

<p>여전히 생성된 클래스의 이름을 생각해야 하지만, 그 외에는 최대한 간결하고 모든 속성에 대한 타입을 부여할 수 있습니다.</p>
<p>Dataclass를 사용하면 함수가 반환하는 객체와 그 세부 타입들에 대한 명시적인 설명이 있습니다. 이 함수를 호출하고 반환된 값으로 작업하면 IDE 자동 완성 기능이 해당 타입의 이름과 타입을 표시해 줍니다. 사소하게 들릴지 모르지만 제게는 생산성 향상에 큰 도움이 됩니다. 또한 코드가 리팩터링되어 속성이 변경되면 제가 프로그램을 실행할 필요 없이 IDE와 타입 검사기가 변경해야 하는 모든 위치를 표시해 줍니다. 일부 간단한 리팩터링(예: 속성 이름 바꾸기)의 경우 IDE가 이러한 변경을 대신 수행하기도 합니다. 또한 명시적으로 이름이 지정된 타입을 사용하면 다른 함수 및 클래스와 공유할 수 있는 용어 어휘집(Person, City)을 만들 수 있습니다.</p>
<blockquote>
<p>필드가 있는 객체를 입력하는 다른 방법도 있습니다. 예를 들어 TypedDict 또는 NamedTuple이 있습니다.</p>
</blockquote>
<p> </p>
<hr>
<h1 id="Algebraic-data-type"><a href="#Algebraic-data-type" class="headerlink" title="Algebraic data type"></a>Algebraic data type</h1><p>대부분의 주류 언어에서 가장 부족한 점이 있다면 아마도 Algebraic data type (ADT)일 것입니다. ADT는 제 코드에서 작업하는 데이터의 형태를 명시적으로 설명할 수 있는 매우 강력한 도구입니다. </p>
<p>예를 들어 Rust에서 <code>Packet</code> 객체를 만들 때 (i.e. 소켓통신 프로그래밍을 할 때) 수신할 수 있는 다양한 종류의 <code>Packet</code>을 모두 명시적으로 열거하고 각 <code>Packet</code>에 서로 다른 데이터(필드)를 할당할 수 있습니다. Pattern matching을 사용하면 개별 타입에 반응할 수 있고 컴파일러는 실수를 하지 않았는지 확인합니다:</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Packet</span></span> {</span><br><span class="line">  Header {</span><br><span class="line">    protocol: Protocol,</span><br><span class="line">    size: <span class="built_in">usize</span></span><br><span class="line">  },</span><br><span class="line">  Payload {</span><br><span class="line">    data: <span class="built_in">Vec</span>&lt;<span class="built_in">u8</span>&gt;</span><br><span class="line">  },</span><br><span class="line">  Trailer {</span><br><span class="line">    data: <span class="built_in">Vec</span>&lt;<span class="built_in">u8</span>&gt;,</span><br><span class="line">    checksum: <span class="built_in">usize</span></span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">handle_packet</span></span>(packet: Packet) {</span><br><span class="line">  <span class="keyword">match</span> packet {</span><br><span class="line">    Packet::Header { protocol, size } =&gt; ...,</span><br><span class="line">    Packet::Payload { data } |</span><br><span class="line">    Packet::Trailer { data, ...} =&gt; <span class="built_in">println!</span>(<span class="string">"{data:?}"</span>)</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure>

<p>이는 illegal state를 표현할 수 없도록 하여 많은 런타임 오류를 방지하는 데 매우 유용합니다. 정적으로 타입이 지정된 언어에서 ADT는 특히 유용하며, 통일된 방식으로 일련의 타입으로 작업하려면 이를 참조할 공유 ‘이름’이 필요합니다. ADT가 없으면 일반적으로 OOP 인터페이스 및/또는 상속을 사용하여 이 작업을 수행합니다. 인터페이스와 가상 메서드는 사용되는 타입 집합이 개방형일 때 적합하지만, 타입 집합이 폐쇄형이고 가능한 모든 변형을 처리해야 하는 경우에는 ADT와 패턴 매칭이 훨씬 더 적합합니다.</p>
<p>Python과 같이 동적으로 타입이 지정된 언어에서는 애초에 프로그램에서 사용되는 타입에 이름을 지정할 필요가 없기 때문에 타입 집합에 공유 ‘이름’을 지정할 필요는 없습니다. 하지만 <strong>유니온 타입</strong>을 생성하여 ADT와 유사한 것을 사용하는 것이 여전히 유용할 수 있습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Header</span>:</span></span><br><span class="line">  protocol: Protocol</span><br><span class="line">  size: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Payload</span>:</span></span><br><span class="line">  data: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trailer</span>:</span></span><br><span class="line">  data: <span class="built_in">str</span></span><br><span class="line">  checksum: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line">Packet = typing.Union[Header, Payload, Trailer]</span><br><span class="line"><span class="comment"># or `Packet = Header | Payload | Trailer` since Python 3.10</span></span><br></pre></td></tr></table></figure>

<p>여기서 <code>Packet</code>은 <code>Header</code>, <code>Payload</code>, <code>Trailer</code>와 같이 <code>Packet</code>이 될 수 있는 새로운 타입을 정의합니다. 이제 이 세 가지 클래스만 유효하도록 하고 싶을 때 나머지 프로그램에서 이 타입(이름)을 사용할 수 있습니다. 클래스에 명시적인 “태그”가 첨부되어 있지 않으므로 클래스를 구분하려면 <code>instanceof</code> 또는 Pattern matching 등을 사용해야 합니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_is_instance</span>(<span class="params">packet: Packet</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(packet, Header):</span><br><span class="line">        print(<span class="string">"header {packet.protocol} {packet.size}"</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(packet, Payload):</span><br><span class="line">        print(<span class="string">"payload {packet.data}"</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(packet, Trailer):</span><br><span class="line">        print(<span class="string">"trailer {packet.checksum} {packet.data}"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_pattern_matching</span>(<span class="params">packet: Packet</span>):</span></span><br><span class="line">    match packet:</span><br><span class="line">        case Header(protocol, size): print(<span class="string">f"header <span class="subst">{protocol}</span> <span class="subst">{size}</span>"</span>)</span><br><span class="line">        case Payload(data): print(<span class="string">"payload {data}"</span>)</span><br><span class="line">        case Trailer(data, checksum): print(<span class="string">f"trailer <span class="subst">{checksum}</span> <span class="subst">{data}</span>"</span>)</span><br><span class="line">        case _: <span class="keyword">assert</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>안타깝게도 여기서는 예기치 않은 데이터를 수신할 때 함수가 터지도록 성가신 <code>assert False</code> 브랜치를 포함해야 합니다. Rust에서는 이렇게 만들 필요 없이 컴파일 타임 에러로 대체할 수 있습니다.</p>
<blockquote>
<p>Reddit의 여러 사람들이 최적화 빌드(python -O …)에서는 실제로 <code>assert False</code>가 완전히 최적화되어 있다는 점을 알려주었습니다. 따라서 예외를 직접 발생시키는 것이 더 안전할 것입니다. Python 3.11의 <code>typing.assert_never</code>도 있는데, 이는 타입 검사기에 이 브랜치로 떨어지는 것이 “컴파일 타임” 에러가 되어야 한다고 명시적으로 알려줍니다.</p>
</blockquote>
<p>Union 타입의 좋은 특성은 Union의 일부인 클래스 외부에 정의된다는 것입니다. 따라서 클래스는 자신이 Union에 포함된다는 사실을 알지 못하므로 코드의 강결합이 줄어듭니다. 또한 동일한 타입을 사용하여 여러 개의 다른 Union을 만들 수도 있습니다:</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line">Packet = Header | Payload | Trailer</span><br><span class="line">PacketWithData = Payload | Trailer</span><br></pre></td></tr></table></figure>

<p>Union 타입은 자동 (역)직렬화에도 매우 유용합니다. 최근에 저는 유서 깊은 Rust의 serde 직렬화 프레임워크를 기반으로 하는 <strong>pyserde</strong>라는 멋진 직렬화 라이브러리를 발견했습니다. 다른 많은 멋진 기능 중에서도 코드 주석을 활용하여 추가 코드 없이 Union 타입을 직렬화 및 역직렬화할 수 있습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> serde</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Packet = Header | Payload | Trailer</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>:</span></span><br><span class="line">    packet: Packet</span><br><span class="line"></span><br><span class="line">serialized = serde.to_dict(Data(packet=Trailer(data=<span class="string">"foo"</span>, checksum=<span class="number">42</span>)))</span><br><span class="line"><span class="comment"># {'packet': {'Trailer': {'data': 'foo', 'checksum': 42}}}</span></span><br><span class="line"></span><br><span class="line">deserialized = serde.from_dict(Data, serialized)</span><br><span class="line"><span class="comment"># Data(packet=Trailer(data='foo', checksum=42))</span></span><br></pre></td></tr></table></figure>

<p>serde와 마찬가지로 Union 태그를 어떻게 직렬화할지 선택할 수도 있습니다. Union 타입을 (역)직렬화하는 데 매우 유용하기 때문에 비슷한 기능을 오랫동안 찾고 있었습니다. 그러나 제가 시도한 대부분의 다른 직렬화 라이브러리(예: dataclasses_json 또는 dacite)에서 구현하는 것이 상당히 번거로웠습니다.</p>
<p>예를 들어, 머신 러닝 모델로 작업할 때 Union을 사용하여 다양한 유형의 신경망(예: classification 또는 segmentation CNN 모델)을 단일 config 파일 형식 안에 저장하고 있습니다. 또한 이와 같이 다양한 형식의 데이터(제 경우에는 config 파일)를 버전별로 저장하는 것이 유용하다는 것을 알게 되었습니다:</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line">Config = ConfigV1 | ConfigV2 | ConfigV3</span><br></pre></td></tr></table></figure>

<p>Config를 역직렬화하면 이전 버전의 모든 config 형식을 읽을 수 있으므로 이전 버전과의 호환성을 유지할 수 있습니다.</p>
<p> </p>
<hr>
<h1 id="newtype-사용하기"><a href="#newtype-사용하기" class="headerlink" title="newtype 사용하기"></a>newtype 사용하기</h1><p>Rust에서는 일반적인 데이터 타입 (e.g. 정수)에 새로운 동작을 추가하지 않고, 도메인에 걸맞는 용도로 새롭게 이름을 지정하는 데이터 타입을 정의하는게 일반적입니다. 이 패턴을 “newtype”이라고 하며 Python에서도 사용할 수 있습니다.</p>
<p>아래의 상황을 한번 봅시다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Database</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_car_id</span>(<span class="params">self, brand: <span class="built_in">str</span></span>) -&gt; int:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_driver_id</span>(<span class="params">self, name: <span class="built_in">str</span></span>) -&gt; int:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_ride_info</span>(<span class="params">self, car_id: <span class="built_in">int</span>, driver_id: <span class="built_in">int</span></span>) -&gt; RideInfo:</span></span><br><span class="line"></span><br><span class="line">db = Database()</span><br><span class="line">car_id = db.get_car_id(<span class="string">"Mazda"</span>)</span><br><span class="line">driver_id = db.get_driver_id(<span class="string">"Stig"</span>)</span><br><span class="line">info = db.get_ride_info(driver_id, car_id)</span><br></pre></td></tr></table></figure>

<p>오류를 발견하셨나요?</p>
<p>…</p>
<p>…</p>
<p><code>get_ride_info</code>의 인수가 잘못들어갔습니다 (db.get_rider_info(car_id, driver_id)가 되어야합니다). 하지만, 차량 ID와 운전자 ID는 모두 정수에 불과하므로 의미상 함수 호출이 잘못되었더라도 타입 자체는 올바르므로 타입 오류는 없습니다.</p>
<p>이 문제는 “NewType”을 사용하여 서로 다른 종류의 ID에 대해 별도의 타입을 정의함으로써 해결할 수 있습니다:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> NewType</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a new type called "CarId", which is internally an `int`</span></span><br><span class="line">CarId = NewType(<span class="string">"CarId"</span>, <span class="built_in">int</span>)</span><br><span class="line"><span class="comment"># Ditto for "DriverId"</span></span><br><span class="line">DriverId = NewType(<span class="string">"DriverId"</span>, <span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Database</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_car_id</span>(<span class="params">self, brand: <span class="built_in">str</span></span>) -&gt; CarId:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_driver_id</span>(<span class="params">self, name: <span class="built_in">str</span></span>) -&gt; DriverId:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_ride_info</span>(<span class="params">self, car_id: CarId, driver_id: DriverId</span>) -&gt; RideInfo:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">db = Database()</span><br><span class="line">car_id = db.get_car_id(<span class="string">"Mazda"</span>)</span><br><span class="line">driver_id = db.get_driver_id(<span class="string">"Stig"</span>)</span><br><span class="line"><span class="comment"># Type error here -&gt; DriverId used instead of CarId and vice-versa</span></span><br><span class="line">info = db.get_ride_info(&lt;error&gt;driver_id&lt;/error&gt;, &lt;error&gt;car_id&lt;/error&gt;)</span><br></pre></td></tr></table></figure>

<p>이렇게 매우 간단한 방식으로 다른 방법으로는 발견하기 어려운 오류를 포착하는 데 도움이 될 수 있습니다. 예를 들어, 여러 종류의 ID(<code>CarId</code> 대 <code>DriverId</code>)를 다루거나, 함께 섞어서는 안 되는 일부 메트릭(<code>Speed</code>, <code>Length</code>, <code>Temperature</code> 등)을 처리하는 경우 특히 유용합니다.</p>
<p> </p>
<hr>
<h1 id="생성자-함수-사용하기"><a href="#생성자-함수-사용하기" class="headerlink" title="생성자 함수 사용하기"></a>생성자 함수 사용하기</h1><p>제가 Rust에서 아주 좋아하는 점 중 하나는 생성자 자체가 없다는 점입니다. 대신 일반 함수를 사용해 구조체의 인스턴스(이상적으로는 적절하게 초기화된 인스턴스)를 생성하는 경향이 있습니다. Python에서는 생성자 오버로딩이 없기 때문에 객체를 여러 가지 방법으로 생성해야 하는 경우, 때로는 초기화를 위해 여러 가지 방법으로 사용되는 매개변수가 많은 <code>__init__</code> 메서드를 사용해야 하는데 실제로는 함께 사용할 수 없습니다.</p>
<p>대신, 저는 객체를 구성하는 방법과 어떤 데이터에서 객체를 구성하는지 명확히 알 수 있는 명시적인 이름을 가진 “생성” 함수를 만드는 것을 좋아합니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_x1x2y1y2</span>(<span class="params">x1: <span class="built_in">float</span>, ...</span>) -&gt; "Rectangle":</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_tl_and_size</span>(<span class="params">top: <span class="built_in">float</span>, left: <span class="built_in">float</span>, width: <span class="built_in">float</span>, height: <span class="built_in">float</span></span>) -&gt; "Rectangle":</span></span><br></pre></td></tr></table></figure>

<p>위와 같은 방법을 통해 객체를 훨씬 명확하고 깔끔하게 생성할 수 있으며, 클래스 사용자가 객체를 구성할 때 잘못된 데이터를 전달할 수 없습니다(예: y1과 width를 결합하는 경우).</p>
<p> </p>
<hr>
<h1 id="타입을-사용해-불변성-인코딩하기"><a href="#타입을-사용해-불변성-인코딩하기" class="headerlink" title="타입을 사용해 불변성 인코딩하기"></a>타입을 사용해 불변성 인코딩하기</h1><p>타입 시스템 자체를 사용해 런타임에만 추적할 수 있는 불변성을 인코딩하는 것은 매우 일반적이고 강력한 개념입니다. Python(뿐만 아니라 다른 주류 언어에서도)에서는 mutable한 데이터가 얽히고 섥혀서 사용되는 클래스들을 자주 볼 수 있습니다. 이러한 현상의 원인 중 하나는  객체의 불변성을 런타임에 추적하려고 하기 때문입니다. 타입 시스템에서 금지시킨 것이 아니기 때문에 (i.e. 컴파일 단계에서 설계상 막아둔 것이 아니기 때문에), 발생할 수 있는 많은 상황을 가정하고 각각의 상황에 대한 대처 방식을 고려해야 합니다(“클라이언트가 연결을 끊으라는 요청을 받았는데 누군가 메시지를 보내려고 하는데 소켓이 여전히 연결되어 있으면 어떻게 할까요?” 등).</p>
<h2 id="클라이언트"><a href="#클라이언트" class="headerlink" title="클라이언트"></a>클라이언트</h2><p>다음은 전형적인 예입니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Rules:</span></span><br><span class="line"><span class="string">  - Do not call `send_message` before calling `connect` and then `authenticate`.</span></span><br><span class="line"><span class="string">  - Do not call `connect` or `authenticate` multiple times.</span></span><br><span class="line"><span class="string">  - Do not call `close` without calling `connect`.</span></span><br><span class="line"><span class="string">  - Do not call any method after calling `close`.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, address: <span class="built_in">str</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">self</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span>(<span class="params">self, password: <span class="built_in">str</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send_message</span>(<span class="params">self, msg: <span class="built_in">str</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>

<p>…쉽죠? 문서를 주의 깊게 읽고, 언급된 규칙을 위반하지 않도록(정의되지 않은 동작이나 충돌을 유발하지 않도록) 주의하기만 하면 됩니다 (껄껄). 다른 대안은 런타임에 언급된 모든 규칙을 검사하는 다양한 assertion으로 클래스를 채우는 것인데, 이는 코드가 지저분해지고 edge-case를 놓치며 문제가 발생했을 때 피드백이 느려집니다(컴파일 타임과 런타임 비교). 문제의 핵심은 클라이언트가 다양한(상호 배타적인) 상태로 존재할 수 있지만 이러한 상태를 개별적으로 모델링하는 대신 모두 단일 타입으로 병합한다는 것입니다.</p>
<p>다양한 상태를 별도의 타입으로 분할하여 이 문제를 개선할 수 있는지 살펴봅시다.</p>
<ul>
<li>우선, 아무 것도 연결되지 않은 ‘Client’를 갖는 것이 합리적일까요? 그렇지 않은 것 같습니다. 연결되지 않은 client는 어차피 <code>connect</code>를 호출할 때까지 아무것도 할 수 없습니다. 그렇다면 왜 이런 상태가 존재하도록 허용할까요? <code>ConnectedClient</code>를 생성하고 반환하는 <code>connect</code>라는 함수를 만들 수 있습니다:</li>
</ul>
<p>함수가 성공하면 “연결됨” 불변성을 유지하는 클라이언트를 반환합니다. 이러한 구조는 우리가 실수로 다시 <code>connect</code>를 호출했을 때 다시 connect를 하려는 이상한 상태 변화를 피할 수 있게 됩니다. connect가 실패하면 함수는 예외를 발생시키거나 <code>None</code> 또는 명시적인 오류를 반환할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">connect</span>(<span class="params">address: <span class="built_in">str</span></span>) -&gt; Optional[ConnectedClient]:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConnectedClient</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span>(<span class="params">...</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send_message</span>(<span class="params">...</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">...</span>):</span></span><br></pre></td></tr></table></figure>

<ul>
<li>인증된 상태에도 비슷한 접근 방식을 사용할 수 있습니다. 클라이언트가 연결되고 인증되었다는 (<code>authenticate</code>) 불변성을 보유하는 다른 타입을 도입할 수 있습니다:</li>
</ul>
<p>실제로 <code>AuthenticatedClient</code>의 인스턴스가 있어야만 실제로 메시지 전송을 시작할 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ConnectedClient 클래스:</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span>(<span class="params">...</span>) -&gt; Optional["AuthenticatedClient"]:</span></span><br><span class="line"></span><br><span class="line">클래스 AuthenticatedClient:</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send_message</span>(<span class="params">...</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">...</span>):</span></span><br></pre></td></tr></table></figure>

<ul>
<li>마지막 문제는 <code>close</code> 메서드에 있습니다. Rust에서는 (destructive move semantics 때문에) <code>close</code> 메서드가 호출되면, Client를 더 이상 사용할 수 없다는 사실을 표현할 수 있습니다. Python에서는 이러한 표현 방식이 불가능하므로 몇 가지 우회 방법을 사용해야 합니다. 한 가지 해결책은 런타임 트래킹으로 돌아가서 Client에 bool 속성을 도입하고 <code>close</code> 및 <code>send_message</code>에서 아직 닫히지 않았다고 assert하는 것입니다. 또 다른 접근 방식은 <code>close</code> 메서드를 완전히 제거하고 Client를 context manager로만 사용하는 것입니다:</li>
</ul>
<p><code>close</code> 메서드를 사용할 수 없게 만든다면,실수로 Client를 두 번 닫을 수 없을겁니다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">connect(...)를 client로 사용합니다:</span><br><span class="line">    client.send_message(<span class="string">"foo"</span>)</span><br><span class="line"><span class="comment"># 여기서 클라이언트가 닫힙니다.</span></span><br></pre></td></tr></table></figure>

<h2 id="강타입의-바운딩-박스"><a href="#강타입의-바운딩-박스" class="headerlink" title="강타입의 바운딩 박스"></a>강타입의 바운딩 박스</h2><p>Object detection은 제가 가끔 작업하는 컴퓨터 비전 작업으로, 프로그램이 이미지에서 일련의 bounding box를 추론해야 하는 작업입니다. Bbox는 기본적으로 기본 직사각형에 left,top과 같은 추가 정보를 가지는 객체로 object detection 알고리즘을 구현할 때 항상 사용됩니다.한 가지 성가신 점은 때로는 정규화(사각형의 좌표와 크기가 [0.0, 1.0] 간격에 있음)되지만 때로는 비정규화(좌표와 크기가 첨부된 이미지의 크기에 의해 경계됨)된다는 것입니다. 예를 들어 데이터 전처리 또는 후처리를 처리하는 많은 함수에 Bbox를 보내면 이를 엉망으로 만들기 쉽고, 예를 들어 Bbox를 두 번 정규화하여 디버깅하기 매우 성가신 오류가 발생합니다.</p>
<p>이런 일이 몇 번 있었기 때문에 한 번은 이 두 가지 타입의 bbox를 두 개의 개별 타입으로 분할하여 이 문제를 완전히 해결하기로 결정했습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>:</span></span><br><span class="line">  left: <span class="built_in">float</span></span><br><span class="line">  top: <span class="built_in">float</span></span><br><span class="line">  width: <span class="built_in">float</span></span><br><span class="line">  height: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line">DenormalizedBBox 클래스:</span><br><span class="line">  left: <span class="built_in">float</span></span><br><span class="line">  top: <span class="built_in">float</span></span><br><span class="line">  width: <span class="built_in">float</span></span><br><span class="line">  height: <span class="built_in">float</span></span><br></pre></td></tr></table></figure>

<p>이렇게 분리하면 정규화된 Bbox와 비정규화된 BBox가 더 이상 쉽게 섞일 수 없으므로 문제가 대부분 해결됩니다. 하지만 코드를 보다 읽기 쉽게 만들기 위해 개선할 수 있는 몇 가지 사항이 있습니다:</p>
<ul>
<li>통합(Composition) 또는 상속(Inheritance)를 통한 코드 중복 감소:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BBoxBase</span>:</span></span><br><span class="line">  left: <span class="built_in">float</span></span><br><span class="line">  top: <span class="built_in">float</span></span><br><span class="line">  width: <span class="built_in">float</span></span><br><span class="line">  height: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Composition</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>:</span></span><br><span class="line">  bbox: BBoxBase</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenormalizedBBox</span>:</span></span><br><span class="line">  bbox: BBoxBase</span><br><span class="line"></span><br><span class="line">Bbox = Union[NormalizedBBox, DenormalizedBBox]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inheritance</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>(<span class="params">BBoxBase</span>):</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenormalizedBBox</span>(<span class="params">BBoxBase</span>):</span></span><br></pre></td></tr></table></figure>

<ul>
<li>런타임 검사를 추가하여 정규화된 Bbox가 실제로 정규화되었는지 확인:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>(<span class="params">BboxBase</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__post_init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0.0</span> &lt;= self.left &lt;= <span class="number">1.0</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<ul>
<li>두 타입 간의 변환 방법을 추가. 어떤 곳에서는 명시적인 표현을 알고 싶지만, 다른 곳에서는 일반 인터페이스(“모든 타입의 BBox”)로 작업하고 싶을 수도 있습니다. 이 경우 ‘모든 BBox’를 두 가지 표현 중 하나로 변환할 수 있어야 합니다:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BBoxBase</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_normalized</span>(<span class="params">self, size: Size</span>) -&gt; "NormalizeBBox":</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_denormalized</span>(<span class="params">self, size: Size</span>) -&gt; "DenormalizedBBox":</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>(<span class="params">BBoxBase</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_normalized</span>(<span class="params">self, size: Size</span>) -&gt; "NormalizedBBox":</span></span><br><span class="line">    <span class="keyword">return</span> self</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_denormalized</span>(<span class="params">self, size: Size</span>) -&gt; "DenormalizedBBox":</span></span><br><span class="line">    <span class="keyword">return</span> self.denormalize(size)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenormalizedBBox</span>(<span class="params">BBoxBase</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_normalized</span>(<span class="params">self, size: Size</span>) -&gt; "NormalizedBBox":</span></span><br><span class="line">    <span class="keyword">return</span> self.normalize(size)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">as_denormalized</span>(<span class="params">self, size: Size</span>) -&gt; "DenormalizedBBox":</span></span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>

<p>이 인터페이스를 사용하면 정확성을 위해 분리된 타입과 편의성을 동시에 취하는 통합된 인터페이스라는 두 가지 장점을 모두 누릴 수 있습니다.</p>
<p>참고: 부모/베이스 클래스에 해당 클래스의 인스턴스를 반환하는 일부 공유 메서드를 추가하려면 Python 3.11부터 typing.Self를 사용할 수 있습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BBoxBase</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">move</span>(<span class="params">self, x: <span class="built_in">float</span>, y: <span class="built_in">float</span></span>) -&gt; typing.Self:</span> ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NormalizedBBox</span>(<span class="params">BBoxBase</span>):</span></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">bbox = NormalizedBBox(...)</span><br><span class="line"><span class="comment"># The type of `bbox2` is `NormalizedBBox`, not just `BBoxBase`</span></span><br><span class="line">bbox2 = bbox.move(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="더-안전한-뮤텍스"><a href="#더-안전한-뮤텍스" class="headerlink" title="더 안전한 뮤텍스"></a>더 안전한 뮤텍스</h2><p>Rust의 뮤텍스와 잠금은 일반적으로 두 가지 이점이 있는 매우 멋진 인터페이스 뒤에 제공됩니다:</p>
<ul>
<li>뮤텍스를 잠그면 guard 객체가 반환되며, 이 객체가 파괴되면 유서 깊은 RAII 메커니즘을 활용하여 뮤텍스의 잠금을 자동으로 해제합니다: </li>
</ul>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="keyword">let</span> guard = mutex.lock(); <span class="comment">// locked here</span></span><br><span class="line">  ...</span><br><span class="line">} <span class="comment">// automatically unlocked here</span></span><br></pre></td></tr></table></figure>

<p>즉, 실수로 뮤텍스 잠금을 해제하는 것을 잊어버릴 수 없습니다. C++에서도 매우 유사한 메커니즘이 일반적으로 사용되지만, guard 객체가 없는 명시적 <code>lock</code>/<code>unlock</code> 인터페이스는 std::mutex에도 사용할 수 있으므로 여전히 잘못 사용될 수 있습니다.</p>
<ul>
<li>뮤텍스에 의해 보호되는 데이터는 뮤텍스(구조체)에 직접 저장됩니다. 이 설계에서는 뮤텍스를 실제로 잠그지 않고는 보호된 데이터에 액세스할 수 없습니다. guard를 얻으려면 먼저 뮤텍스를 잠근 다음 guard 자체를 사용하여 데이터에 액세스해야 합니다:</li>
</ul>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> lock = Mutex::new(<span class="number">41</span>); <span class="comment">// Create a mutex that stores the data inside</span></span><br><span class="line"><span class="keyword">let</span> guard = lock.lock().unwrap(); <span class="comment">// Acquire guard</span></span><br><span class="line">*guard += <span class="number">1</span>; <span class="comment">// Modify the data using the guard</span></span><br></pre></td></tr></table></figure>

<p>이는 뮤텍스와 뮤텍스가 보호하는 데이터가 분리되어 있어 데이터에 액세스하기 전에 뮤텍스를 실제로 잠그는 것을 쉽게 잊어버릴 수 있는 Python을 비롯한 주류 언어의 일반적인 뮤텍스 API와는 완전히 대조적인 방식입니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mutex = Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">thread_fn</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="comment"># Acquire mutex. There is no link to the protected variable.</span></span><br><span class="line">    mutex.acquire()</span><br><span class="line">    data.append(<span class="number">1</span>)</span><br><span class="line">    mutex.release()</span><br><span class="line"></span><br><span class="line">data = []</span><br><span class="line">t = Thread(target=thread_fn, args=(data,))</span><br><span class="line">t.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here we can access the data without locking the mutex.</span></span><br><span class="line">data.append(<span class="number">2</span>)  <span class="comment"># Oops</span></span><br></pre></td></tr></table></figure>

<p>Python에서 Rust에서와 똑같은 이점을 얻을 수는 없지만 모든 이점을 잃은 것은 아닙니다. Python 잠금은 컨텍스트 관리자 인터페이스를 구현하므로 <code>with</code> 블록에서 잠금을 사용하여 범위가 끝날 때 자동으로 잠금이 해제되도록 할 수 있습니다. 그리고 약간의 노력만 더하면 더 대단한걸 할 수 있습니다:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Lock</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> ContextManager, Generic, TypeVar</span><br><span class="line"></span><br><span class="line">T = TypeVar(<span class="string">"T"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make the Mutex generic over the value it stores.</span></span><br><span class="line"><span class="comment"># In this way we can get proper typing from the `lock` method.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mutex</span>(<span class="params">Generic[T]</span>):</span></span><br><span class="line">  <span class="comment"># Store the protected value inside the mutex </span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, value: T</span>):</span></span><br><span class="line">    <span class="comment"># Name it with two underscores to make it a bit harder to accidentally</span></span><br><span class="line">    <span class="comment"># access the value from the outside.</span></span><br><span class="line">    self.__value = value</span><br><span class="line">    self.__lock = Lock()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Provide a context manager `lock` method, which locks the mutex,</span></span><br><span class="line">  <span class="comment"># provides the protected value, and then unlocks the mutex when the</span></span><br><span class="line">  <span class="comment"># context manager ends.</span></span><br><span class="line"><span class="meta">  @contextlib.contextmanager</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lock</span>(<span class="params">self</span>) -&gt; ContextManager[T]:</span></span><br><span class="line">    self.__lock.acquire()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">yield</span> self.__value</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        self.__lock.release()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a mutex wrapping the data</span></span><br><span class="line">mutex = Mutex([])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lock the mutex for the scope of the `with` block</span></span><br><span class="line"><span class="keyword">with</span> mutex.lock() <span class="keyword">as</span> value:</span><br><span class="line">  <span class="comment"># value is typed as `list` here</span></span><br><span class="line">  value.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>이 설계를 사용하면 뮤텍스를 실제로 잠근 후에만 보호된 데이터에 액세스할 수 있습니다. 물론 이것은 여전히 Python이므로 뮤텍스 외부에 보호된 데이터에 대한 다른 포인터를 저장하는 등의 방법으로 불변성을 깨뜨릴 수 있습니다. 그러나 적대적으로 행동하지 않는 한, Python의 뮤텍스 인터페이스는 사용하기에 훨씬 더 안전합니다.</p>
<p>어쨌든, 제가 Python 코드에서 사용하는 “건전성 패턴”이 더 있을 거라고 확신하지만, 현재로서는 이것이 제가 생각할 수 있는 전부입니다. 비슷한 아이디어의 예가 있거나 다른 의견이 있으시면 댓글을 달아주세요.</p>
<ol>
<li>공평하게 말하자면, 문서 주석의 매개변수 타입에 대한 설명에서 구조화된 형식(예: reStructuredText)을 사용하는 경우에도 마찬가지일 수 있습니다. 이 경우 타입 검사기가 이를 사용하여 타입이 일치하지 않는 경우 경고를 표시할 수 있습니다. 하지만 어쨌든 타입 검사기를 사용한다면 타입을 지정하는 “네이티브” 메커니즘인 타입 힌트를 활용하는 것이 더 좋을 것 같습니다.</li>
<li>일명 discriminated/tagged unions, sum types, sealed classes 등입니다.</li>
<li>newtype에는 여기에 설명된 것 이외의 다른 사용 사례도 있습니다.</li>
<li>이를 타입 상태 패턴이라고 합니다 (typestate pattern).</li>
<li>예를 들어 마법의 <strong>exit</strong> 메서드를 수동으로 호출하는 등 열심히 노력하지 않는 한 말입니다.</li>
</ol>
]]></content>
      <categories>
        <category>2. Programming</category>
        <category>2.3 Python</category>
        <category>2.4 Rust</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title>Buy-write 전략이란?</title>
    <url>/20230614-buy-write-strategy/</url>
    <content><![CDATA[<h1 id="Buy-write-전략"><a href="#Buy-write-전략" class="headerlink" title="Buy-write 전략"></a>Buy-write 전략</h1><p>“Buy-Write” 전략은 단일/다수의 주식을 구입하고 동시에 해당 주식의 콜 옵션을 판매하는 전략을 말합니다.</p>
<ol>
<li>우선, 특정 주식을 구입합니다.</li>
<li>그 다음, 그 주식에 대한 콜 옵션을 판매합니다. 소유한 주식의 수는 판매하는 콜 옵션의 기초가 되는 주식의 수와 같아야 합니다 (이러한 이유 떄문에 ‘Covered call’ 전략이라고 불리기도 합니다 - 판매하는 콜 옵션은 소유한 주식에 의해 ‘커버’됩니다). 콜 옵션을 판매한다는 것은, 옵션의 만기일에 주식을 특정 가격에 판매할 권리를 다른 사람에게 팔아 주는 것입니다.</li>
<li>만약 주식의 가격이 콜 옵션의 행사 가격보다 높아진다면, 옵션은 행사되고, 주식은 행사 가격에 팔리게 됩니다. 이 경우, 수익은 행사 가격과 원래 주식을 구입한 가격, 그리고 팔린 콜 옵션의 프리미엄으로 이루어집니다.</li>
<li>만약 주식의 가격이 콜 옵션의 행사 가격보다 낮다면, 옵션은 행사되지 않고 만료되며, 주식은 여전히 소유하게 됩니다. 이 경우, 수익은 팔린 콜 옵션의 프리미엄으로 이루어집니다.</li>
</ol>
<p>이 전략의 목표는 주식의 수익과 팔린 옵션의 프리미엄을 모두 얻는 것입니다. 프리미엄은 옵션의 판매자가 받는 지불액입니다.</p>
<p>주식의 가격이 동일하거나 하락하면 투자자는 프리미엄을 유지하고, 콜 옵션의 행사 가격까지 주식이 오르면 이익을 볼 수도 있습니다.</p>
<p>그러나 이익의 잠재력은 콜 옵션의 행사 가격과 받은 프리미엄까지로 제한됩니다. 주식의 가격이 행사 가격 이상으로 오르면 콜 옵션은 아마도 행사될 것이고, 투자자는 행사 가격에서 주식을 팔아야 하며, 추가적인 이익은 놓치게 됩니다.</p>
<p>이 전략은 투자자가 시장이나 특정 주식에 대해 중립적에서 약간 상승적인 전망을 가질 때 종종 사용됩니다. 이것은 수익을 창출하고 중간 정도의 손실을 완화하는 방법이지만, 큰 이익의 잠재력은 제한합니다.</p>
<p> </p>
<hr>
<h1 id="Call-option"><a href="#Call-option" class="headerlink" title="Call option"></a>Call option</h1><p>콜 옵션은 특정 가격(행사 가격이라고 함)에서 특정 기간 내에 기초 자산을 사는 권리(하지만 의무는 아님)를 보유자(구매자)에게 부여하는 옵션 시장의 금융 계약 유형입니다.</p>
<p>콜 옵션의 주요 구성 요소를 분석해 보겠습니다:</p>
<ul>
<li>구매자: 콜 옵션을 구매하는 사람입니다. 그들은 기초 자산을 사는 권리를 얻기 위해 판매자에게 프리미엄을 지불합니다.</li>
<li>판매자(또는 작성자): 콜 옵션을 판매하는 사람입니다. 그들은 프리미엄을 받고, 구매자가 옵션을 행사하기로 결정하면 기초 자산을 팔도록 의무가 있습니다.</li>
<li>프리미엄: 이것은 콜 옵션을 얻기 위해 구매자가 판매자에게 지불하는 가격입니다. 주식 가격과 행사 가격 간의 차이, 만기까지의 시간, 기초 자산의 변동성 등 여러 요인에 의해 결정됩니다.</li>
<li>행사 가격: 이것은 구매자가 콜 옵션을 행사하기로 결정하면 기초 자산을 구매할 수 있는 가격입니다.</li>
<li>만기일: 이것은 옵션이 만료되는 날짜입니다. 이 날짜까지 옵션을 행사하지 않으면 옵션은 무가치가 되고 구매자는 옵션에 대해 지불한 금액(프리미엄)을 잃게 됩니다.</li>
</ul>
<p>콜 옵션의 구매자는 옵션이 만료되기 전에 기초 자산의 가격이 상승할 것이라는 베팅을 합니다. 그래서 그들은 더 낮은 행사 가격으로 자산을 사고 현재 시장 가격에 팔아 이익을 낼 수 있습니다. 반면에 판매자는 기초 자산의 가격이 행사 가격을 초과하지 않을 것이라는 베팅을 하여 자산을 팔지 않고도 프리미엄을 유지할 수 있습니다.</p>
]]></content>
      <categories>
        <category>3. Life</category>
        <category>3.1 일상</category>
      </categories>
      <tags>
        <tag>주식</tag>
      </tags>
  </entry>
  <entry>
    <title>Mazur 2022 - Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding 논문 리뷰</title>
    <url>/20230614-feature-realistic-neural-fusion-for-real-time-open-set-scene-understanding/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>논문 리뷰</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Visual-SLAM</tag>
        <tag>Spatial AI</tag>
        <tag>Semantic SLAM</tag>
        <tag>NeRF</tag>
        <tag>Open-set detection</tag>
      </tags>
  </entry>
  <entry>
    <title>&#39;NVIDIA Jetson Nano와 함께하는 SLAM의 이해와 구현&#39; - 책 PDF 무료 공개</title>
    <url>/20230616-slam-book-kr/</url>
    <content><![CDATA[<blockquote>
<p><span class="exturl" data-url="aHR0cHM6Ly9kcml2ZS5nb29nbGUuY29tL2ZpbGUvZC8xTzA2cHplT21zMVA2MUJSZVQ1a1NWTFFpekpkalk0aUcvdmlldz91c3A9c2hhcmluZw==">책 pdf 다운로드 링크<i class="fa fa-external-link-alt"></i></span><br>PDF 비밀번호: slamkr</p>
</blockquote>
<h1 id="책-소개"><a href="#책-소개" class="headerlink" title="책 소개"></a>책 소개</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tL2dyb3Vwcy9zbGFta3I=">SLAM KR<i class="fa fa-external-link-alt"></i></span> 커뮤니티에서 몇년간 활동을 하며 SLAM에 대해 많은 것들을 배웠다. </p>
<p>그동안 배운 것들을 기반으로 <span class="exturl" data-url="aHR0cHM6Ly93d3cuY3YtbGVhcm4uY29tLzIwMjMwMTAyLTIwMjItc2VtaW5hci1zaG93ZG93bi8=">다수의 강의<i class="fa fa-external-link-alt"></i></span>도 진행하고, <span class="exturl" data-url="aHR0cHM6Ly9vcGVuLmtha2FvLmNvbS9vL2c4VDVreExi">카카오톡 커뮤니티 채팅<i class="fa fa-external-link-alt"></i></span>도 열며 지식을 공유했지만, 항상 답답했던 점이 있었다. </p>
<p>처음 SLAM을 배울 때에는 한국어로 된 SLAM 자료가 없어서 공부하는데에 많이 불편했었다. 이해가 되지않는 부분은 몇년동안 끈기있게 잡고있다가 겨우 해결하거나, 또는 그냥 이해하기를 포기해야하는 경우도 많았다. 5년이 지난 지금은 그 당시보다는 조금 나아지긴 했지만, 아직도 한국어 자료가 많이 부족하다는 건 매 한가지이다. </p>
<p>마침 <span class="exturl" data-url="aHR0cHM6Ly9kb25nd29uc2hpbi5vb3B5LmlvLw==">SLAM KR의 신동원 님<i class="fa fa-external-link-alt"></i></span>께서 함께 SLAM에 대한 책을 써보자고 제안을 주셨다. 이번 기회에 내가 지금까지 답답하게 느꼈던 점을 해소할, VSLAM을 공부하는데에 있어서 기본이 되는 자료들을 한 곳에 모을 기회가 생겼다고 생각해 열심히 책을 적었다. </p>
<p>이 책을 통해 새롭게 SLAM을 공부하시는 분들께서는 내가 겪었던 불편함을 겪지 않기를 바라며, 국내에 더 많은 SLAM 개발자가 많아졌으면 좋겠다. (좋은 기회를 주신 신동원 님 감사합니다 :))</p>
<p> </p>
<hr>
<h1 id="목차"><a href="#목차" class="headerlink" title="목차"></a>목차</h1><img src="/20230616-slam-book-kr/2.png" class="" title="contents1">
<img src="/20230616-slam-book-kr/3.png" class="" title="contents2">
<img src="/20230616-slam-book-kr/4.png" class="" title="contents3">
<img src="/20230616-slam-book-kr/5.png" class="" title="contents4">
<img src="/20230616-slam-book-kr/6.png" class="" title="contents5">
]]></content>
      <categories>
        <category>1. Spatial AI</category>
        <category>1.1 SLAM</category>
        <category>1.4 Robotics</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>NVIDIA</tag>
        <tag>Tutorial</tag>
        <tag>Jetson</tag>
      </tags>
  </entry>
</search>
